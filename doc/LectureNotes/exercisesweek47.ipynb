{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc2501d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek47.do.txt  -->\n",
    "<!-- dom:TITLE: Exercise week 47-48 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae5111",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercise week 47-48\n",
    "**November 17-28, 2025**\n",
    "\n",
    "Date: **Deadline is Friday November 28 at midnight**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef837a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Overarching aims of the exercises this week\n",
    "\n",
    "The exercise set this week is meant as a summary of many of the\n",
    "central elements in various machine learning algorithms we have discussed throught the semester. You don't need to answer all questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ef66b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear and logistic regression methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c9231",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 1:\n",
    "\n",
    "Which of the following is not an assumption of ordinary least squares linear regression?\n",
    "\n",
    "* There is a linearity between predictors/features and target/outout\n",
    "\n",
    " * The inputs/features distributed according to a normal/gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acef906",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 2:\n",
    "\n",
    "The mean squared error cost function for linear regression is convex in the parameters, guaranteeing a unique global minimum. True or False? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bf02e",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 3:\n",
    "\n",
    "Which statement about logistic regression is false?\n",
    "\n",
    "* Logistic regression is used for binary classification.\n",
    "\n",
    " * It uses the sigmoid function to map linear scores to probabilities.\n",
    "\n",
    " * It has an analytical closed-form solution.\n",
    "\n",
    " * Its log-loss (cross-entropy) is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab306a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 4:\n",
    "\n",
    "Logistic regression produces a linear decision boundary in the input space. True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695e6bb",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 5:\n",
    "\n",
    "Give two reasons why logistic regression is preferred over linear regression for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c398642",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fac35",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 6:\n",
    "\n",
    "Which statement is not true for fully-connected neural networks?\n",
    "\n",
    "* Without nonlinear activation functions they reduce to a single linear model.\n",
    "\n",
    " * Training relies on backpropagation using the chain rule.\n",
    "\n",
    " * A single hidden layer can approximate any continuous function on a compact set.\n",
    "\n",
    " * The loss surface of a deep neural network is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed2727",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 7:\n",
    "\n",
    "Using sigmoid activations in many layers of a deep neural network can cause vanishing gradients. True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1865d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 8:\n",
    "\n",
    "Describe the vanishing gradient problem: Why does it occur? Mention one technique to mitigate it and explain briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ad1a8",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 9:\n",
    "\n",
    "Consider a fully-connected network with layer sizes $n_0$ (the input\n",
    "layer) ,$n_1$ (first hidden layer), $\\dots, n_L$, where $n_L$ is the\n",
    "output layer. Derive a general formula for the total number of\n",
    "trainable parameters (weights + biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2ed47",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d54a83",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 10:\n",
    "\n",
    "Which of the following is not a typical property or advantage of CNNs?\n",
    "\n",
    "* Local receptive fields\n",
    "\n",
    " * Weight sharing\n",
    "\n",
    " * More parameters than fully-connected layers\n",
    "\n",
    " * Pooling layers offering some translation invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefcc46",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 11:\n",
    "\n",
    "Using zero-padding in convolutional layers can preserve the input\n",
    "spatial dimensions when using a $3 \\times 3$ kernel/filter, stride 1,\n",
    "and padding $P = 1$. True or False?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b6806",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 12:\n",
    "\n",
    "Given input width $W$, kernel size $K$, stride S, and padding P,\n",
    "derive the formula for the output width $W_{\\text{out}} = \\frac{W - K+ 2P}{S} + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629397f",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 13:\n",
    "\n",
    "A convolutional layer has: $C_{\\text{in}}$ input channels,\n",
    "$C_{\\text{out}}$ output channels (filters) and kernel size $K_h \\times\n",
    "K_w$. Compute the number of trainable parameters including biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087780b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd5f95",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 14:\n",
    "\n",
    "Which statement about simple  RNNs is false?\n",
    "\n",
    "* They maintain a hidden state updated each time step.\n",
    "\n",
    " * They use the same weight matrices at every time step.\n",
    "\n",
    " * They handle sequences of arbitrary length.\n",
    "\n",
    " * They eliminate the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70bb6d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 15:\n",
    "\n",
    "LSTMs mitigate the vanishing gradient problem by using gating mechanisms (input, forget, output gates). True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ec77a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 16:\n",
    "\n",
    "What is Backpropagation Through Time (BPTT) and why is it required for training RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e01d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 17:\n",
    "\n",
    "What does a sliding window do? And why would we use it?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
