
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3. Linear Regression &#8212; Applied Data Analysis and Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter1';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4. Ridge and Lasso Regression" href="chapter2.html" />
    <link rel="prev" title="2. Linear Algebra, Handling of Arrays and more Python Features" href="linalg.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Applied Data Analysis and Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Applied Data Analysis and Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Applied Data Analysis and Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About the course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Course setting</a></li>
<li class="toctree-l1"><a class="reference internal" href="teachers.html">Teachers and Grading</a></li>
<li class="toctree-l1"><a class="reference internal" href="textbooks.html">Textbooks</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Review of Statistics with Resampling Techniques and Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="statistics.html">1. Elements of Probability Theory and Statistical Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">2. Linear Algebra, Handling of Arrays and more Python Features</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Regression to Support Vector Machines</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter2.html">4. Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter3.html">5. Resampling Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter4.html">6. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapteroptimization.html">7. Optimization, the central part of any Machine Learning algortithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter5.html">8. Support Vector Machines, overarching aims</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Decision Trees, Ensemble Methods and Boosting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter6.html">9. Decision trees, overarching aims</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter7.html">10. Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Dimensionality Reduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter8.html">11. Basic ideas of the Principal Component Analysis (PCA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">12. Clustering and Unsupervised Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter9.html">13. Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter10.html">14. Building a Feed Forward Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter11.html">15. Solving Differential Equations  with Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter12.html">16. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter13.html">17. Recurrent neural networks: Overarching view</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Weekly material, notes and exercises</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="exercisesweek34.html">Exercises week 34</a></li>
<li class="toctree-l1"><a class="reference internal" href="week34.html">Week 34: Introduction to the course, Logistics and Practicalities</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek35.html">Exercises week 35</a></li>
<li class="toctree-l1"><a class="reference internal" href="week35.html">Week 35: From Ordinary Linear Regression to Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek36.html">Exercises week 36</a></li>
<li class="toctree-l1"><a class="reference internal" href="week36.html">Week 36: Linear Regression and Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek37.html">Exercises week 37</a></li>
<li class="toctree-l1"><a class="reference internal" href="week37.html">Week 37: Gradient descent methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek38.html">Exercises week 38</a></li>
<li class="toctree-l1"><a class="reference internal" href="week38.html">Week 38: Statistical analysis, bias-variance tradeoff and resampling methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek39.html">Exercises week 39</a></li>
<li class="toctree-l1"><a class="reference internal" href="week39.html">Week 39: Resampling methods and logistic regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="week40.html">Week 40: Gradient descent methods (continued) and start Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="week41.html">Week 41 Neural networks and constructing a neural network code</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek41.html">Exercises week 41</a></li>








<li class="toctree-l1"><a class="reference internal" href="week42.html">Week 42 Constructing a Neural Network code with examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek42.html">Exercises week 42</a></li>









<li class="toctree-l1"><a class="reference internal" href="week43.html">Week 43: Deep Learning: Constructing a Neural Network code and solving differential equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek43.html">Exercises week 43</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="project1.html">Project 1 on Machine Learning, deadline October 6 (midnight), 2025</a></li>
<li class="toctree-l1"><a class="reference internal" href="project2.html">Project 2 on Machine Learning, deadline November 10 (Midnight)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chapter1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">3.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-machine-learning">3.2. What is Machine Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-frequentist-approach-to-data-analysis">3.2.1. A Frequentist approach to data analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-good-model">3.2.2. What is a good model?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression-model-using-scikit-learn">3.3. Simple linear regression model using <strong>scikit-learn</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#organizing-our-data">3.3.1. Organizing our data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-basic-elements">3.4. Linear Regression, basic elements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chi-2-function">3.4.1. The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-an-equation-of-state-for-dense-nuclear-matter">3.4.2. Fitting an Equation of State for Dense Nuclear Matter</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-our-data-in-training-and-test-data">3.5. Splitting our Data in Training and Test data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reducing-the-number-of-degrees-of-freedom-overarching-view">3.6. Reducing the number of degrees of freedom, overarching view</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-the-means-squared-error-as-function-of-complexity">3.7. Testing the Means Squared Error as function of Complexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">3.8. Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-setting-up-various-python-environments">3.9. Exercise 1: Setting up various Python environments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-making-your-own-data-and-exploring-scikit-learn">3.10. Exercise 2: making your own data and exploring scikit-learn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-normalizing-our-data">3.11. Exercise 3: Normalizing our data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-adding-ridge-regression">3.12. Exercise 4: Adding Ridge Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-5-analytical-exercises">3.13. Exercise 5: Analytical exercises</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)
doconce format html chapter1.do.txt  --><section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<h1><span class="section-number">3. </span>Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2><span class="section-number">3.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Our emphasis throughout this series of lectures is on understanding
the mathematical aspects of different algorithms used in the fields of
data analysis and machine learning.</p>
<p>However, where possible we will emphasize the importance of using
available software. We start thus with a hands-on and top-down
approach to machine learning. The aim is thus to start with relevant
data or data we have produced and use these to introduce statistical
data analysis concepts and machine learning algorithms before we delve
into the algorithms themselves. The examples we will use in the
beginning, start with simple polynomials with random noise added. We
will use the Python software package
<a class="reference external" href="http://scikit-learn.org/stable/">Scikit-Learn</a> and introduce various
machine learning algorithms to make fits of the data and
predictions. We move thereafter to more interesting cases such as data
from say experiments (below we will look at experimental nuclear
binding energies as an example).  These are examples where we can
easily set up the data and then use machine learning algorithms
included in for example <strong>Scikit-Learn</strong>.</p>
<p>These examples will serve us the purpose of getting
started. Furthermore, they allow us to catch more than two birds with
a stone. They will allow us to bring in some programming specific
topics and tools as well as showing the power of various Python
libraries for machine learning and statistical data analysis.</p>
<p>Here, we will mainly focus on two specific Python packages for Machine
Learning, Scikit-Learn and Tensorflow (see below for links etc).
Moreover, the examples we introduce will serve as inputs to many of
our discussions later, as well as allowing you to set up models and
produce your own data and get started with programming.</p>
</section>
<section id="what-is-machine-learning">
<h2><span class="section-number">3.2. </span>What is Machine Learning?<a class="headerlink" href="#what-is-machine-learning" title="Link to this heading">#</a></h2>
<p>Statistics, data science and machine learning form important fields of
research in modern science.  They describe how to learn and make
predictions from data, as well as allowing us to extract important
correlations about physical process and the underlying laws of motion
in large data sets. The latter, big data sets, appear frequently in
essentially all disciplines, from the traditional Science, Technology,
Mathematics and Engineering fields to Life Science, Law, education
research, the Humanities and the Social Sciences.</p>
<p>It has become more
and more common to see research projects on big data in for example
the Social Sciences where extracting patterns from complicated survey
data is one of many research directions.  Having a solid grasp of data
analysis and machine learning is thus becoming central to scientific
computing in many fields, and competences and skills within the fields
of machine learning and scientific computing are nowadays strongly
requested by many potential employers. The latter cannot be
overstated, familiarity with machine learning has almost become a
prerequisite for many of the most exciting employment opportunities,
whether they are in bioinformatics, life science, physics or finance,
in the private or the public sector. This author has had several
students or met students who have been hired recently based on their
skills and competences in scientific computing and data science, often
with marginal knowledge of machine learning.</p>
<p>Machine learning is a subfield of computer science, and is closely
related to computational statistics.  It evolved from the study of
pattern recognition in artificial intelligence (AI) research, and has
made contributions to AI tasks like computer vision, natural language
processing and speech recognition. Many of the methods we will study are also
strongly rooted in basic mathematics and physics research.</p>
<p>Ideally, machine learning represents the science of giving computers
the ability to learn without being explicitly programmed.  The idea is
that there exist generic algorithms which can be used to find patterns
in a broad class of data sets without having to write code
specifically for each problem. The algorithm will build its own logic
based on the data.  You should however always keep in mind that
machines and algorithms are to a large extent developed by humans. The
insights and knowledge we have about a specific system, play a central
role when we develop a specific machine learning algorithm.</p>
<p>Machine learning is an extremely rich field, in spite of its young
age. The increases we have seen during the last three decades in
computational capabilities have been followed by developments of
methods and techniques for analyzing and handling large data sets,
relying heavily on statistics, computer science and mathematics.  The
field is rather new and developing rapidly. Popular software packages
written in Python for machine learning like
<a class="reference external" href="http://scikit-learn.org/stable/">Scikit-learn</a>,
<a class="reference external" href="https://www.tensorflow.org/">Tensorflow</a>,
<a class="reference external" href="http://pytorch.org/">PyTorch</a> and <a class="reference external" href="https://keras.io/">Keras</a>, all
freely available at their respective GitHub sites, encompass
communities of developers in the thousands or more. And the number of
code developers and contributors keeps increasing. Not all the
algorithms and methods can be given a rigorous mathematical
justification, opening up thereby large rooms for experimenting and
trial and error and thereby exciting new developments.  However, a
solid command of linear algebra, multivariate theory, probability
theory, statistical data analysis, understanding errors and Monte
Carlo methods are central elements in a proper understanding of many
of algorithms and methods we will discuss.</p>
<p>The approaches to machine learning are many, but are often split into
two main categories.  In <em>supervised learning</em> we know the answer to a
problem, and let the computer deduce the logic behind it. On the other
hand, <em>unsupervised learning</em> is a method for finding patterns and
relationship in data sets without any prior knowledge of the system.
Some authors also operate with a third category, namely
<em>reinforcement learning</em>. This is a paradigm of learning inspired by
behavioral psychology, where learning is achieved by trial-and-error,
solely from rewards and punishment.</p>
<p>Another way to categorize machine learning tasks is to consider the
desired output of a system.  Some of the most common tasks are:</p>
<ul class="simple">
<li><p>Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.</p></li>
<li><p>Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.</p></li>
<li><p>Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.</p></li>
</ul>
<p>The methods we cover have three main topics in common, irrespective of
whether we deal with supervised or unsupervised learning.</p>
<ul class="simple">
<li><p>The first ingredient is normally our data set (which can be subdivided into training, validation  and test data). Many find the most difficult part of using Machine Learning to be the set up of your data in a meaningful way.</p></li>
<li><p>The second item is a model which is normally a function of some parameters.  The model reflects our knowledge of the system (or lack thereof). As an example, if we know that our data show a behavior similar to what would be predicted by a polynomial, fitting our data to a polynomial of some degree would then determin our model.</p></li>
<li><p>The last ingredient is a so-called <strong>cost/loss</strong> function (or error or risk function) which allows us to present an estimate on how good our model is in reproducing the data it is supposed to train.</p></li>
</ul>
<p>At the heart of basically all Machine Learning algorithms we will encounter so-called minimization or optimization algorithms. A large family of such methods are so-called <strong>gradient methods</strong>.</p>
<section id="a-frequentist-approach-to-data-analysis">
<h3><span class="section-number">3.2.1. </span>A Frequentist approach to data analysis<a class="headerlink" href="#a-frequentist-approach-to-data-analysis" title="Link to this heading">#</a></h3>
<p>When you hear phrases like <strong>predictions and estimations</strong> and
<strong>correlations and causations</strong>, what do you think of?  May be you think
of the difference between classifying new data points and generating
new data points.
Or perhaps you consider that correlations represent some kind of symmetric statements like
if <span class="math notranslate nohighlight">\(A\)</span> is correlated with <span class="math notranslate nohighlight">\(B\)</span>, then <span class="math notranslate nohighlight">\(B\)</span> is correlated with
<span class="math notranslate nohighlight">\(A\)</span>. Causation on the other hand is directional, that is if <span class="math notranslate nohighlight">\(A\)</span> causes <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(B\)</span> does not
necessarily cause <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>These concepts are in some sense the difference between machine
learning and statistics. In machine learning and prediction based
tasks, we are often interested in developing algorithms that are
capable of learning patterns from given data in an automated fashion,
and then using these learned patterns to make predictions or
assessments of newly given data. In many cases, our primary concern
is the quality of the predictions or assessments, and we are less
concerned about the underlying patterns that were learned in order
to make these predictions.</p>
<p>In machine learning we normally use <a class="reference external" href="https://en.wikipedia.org/wiki/Frequentist_inference">a so-called frequentist approach</a>,
where the aim is to make predictions and find correlations. We focus
less on for example extracting a probability distribution function (PDF). The PDF can be
used in turn to make estimations and find causations such as given <span class="math notranslate nohighlight">\(A\)</span>
what is the likelihood of finding <span class="math notranslate nohighlight">\(B\)</span>.</p>
</section>
<section id="what-is-a-good-model">
<h3><span class="section-number">3.2.2. </span>What is a good model?<a class="headerlink" href="#what-is-a-good-model" title="Link to this heading">#</a></h3>
<p>In science and engineering we often end up in situations where we want to infer (or learn) a
quantitative model <span class="math notranslate nohighlight">\(M\)</span> for a given set of sample points <span class="math notranslate nohighlight">\(\boldsymbol{X} \in [x_1, x_2,\dots x_N]\)</span>.</p>
<p>As we will see repeatedly in these lectures, we could try to fit these data points to a model given by a
straight line, or if we wish to be more sophisticated to a more complex
function.</p>
<p>The reason for inferring such a model is that it
serves many useful purposes. On the one hand, the model can reveal information
encoded in the data or underlying mechanisms from which the data were generated. For instance, we could discover important
correlations that relate interesting physics interpretations.</p>
<p>In addition, it can simplify the representation of the given data set and help
us in making predictions about  future data samples.</p>
<p>A first important consideration to keep in mind is that inferring the <em>correct</em> model
for a given data set is an elusive, if not impossible, task. The fundamental difficulty
is that if we are not specific about what we mean by a <em>correct</em> model, there
could easily be many different models that fit the given data set <em>equally well</em>.</p>
<p>The central question is this: what leads us to say that a model is correct or
optimal for a given data set? To make the model inference problem well posed, i.e.,
to guarantee that there is a unique optimal model for the given data, we need to
impose additional assumptions or restrictions on the class of models considered. To
this end, we should not be looking for just any model that can describe the data.
Instead, we should look for a <strong>model</strong> <span class="math notranslate nohighlight">\(M\)</span> that is the best among a restricted class
of models. In addition, to make the model inference problem computationally
tractable, we need to specify how restricted the class of models needs to be. A
common strategy is to start
with the simplest possible class of models that is just necessary to describe the data
or solve the problem at hand. More precisely, the model class should be rich enough
to contain at least one model that can fit the data to a desired accuracy and yet be
restricted enough that it is relatively simple to find the best model for the given data.</p>
<p>Thus, the most popular strategy is to start from the
simplest class of models and increase the complexity of the models only when the
simpler models become inadequate. For instance, if we work with a regression problem to fit a set of sample points, one
may first try the simplest class of models, namely linear models, followed obviously by more complex models.</p>
<p>How to evaluate which model fits best the data is something we will come back to over and over again in these sets of lectures.</p>
</section>
</section>
<section id="simple-linear-regression-model-using-scikit-learn">
<h2><span class="section-number">3.3. </span>Simple linear regression model using <strong>scikit-learn</strong><a class="headerlink" href="#simple-linear-regression-model-using-scikit-learn" title="Link to this heading">#</a></h2>
<p>We start with perhaps our simplest possible example, using
<strong>Scikit-Learn</strong> to perform linear regression analysis on a data set
produced by us.</p>
<p>What follows is a simple Python code where we have defined a function
<span class="math notranslate nohighlight">\(y\)</span> in terms of the variable <span class="math notranslate nohighlight">\(x\)</span>. Both are defined as vectors with  <span class="math notranslate nohighlight">\(100\)</span> entries.
The numbers in the vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> are given
by random numbers generated with a uniform distribution with entries
<span class="math notranslate nohighlight">\(x_i \in [0,1]\)</span> (more about probability distribution functions
later). These values are then used to define a function <span class="math notranslate nohighlight">\(y(x)\)</span>
(tabulated again as a vector) with a linear dependence on <span class="math notranslate nohighlight">\(x\)</span> plus a
random noise added via the normal distribution.</p>
<p>The Numpy functions are imported used the <strong>import numpy as np</strong>
statement and the random number generator for the uniform distribution
is called using the function <strong>np.random.rand()</strong>, where we specificy
that we want <span class="math notranslate nohighlight">\(100\)</span> random variables.  Using Numpy we define
automatically an array with the specified number of elements, <span class="math notranslate nohighlight">\(100\)</span> in
our case.  With the Numpy function <strong>randn()</strong> we can compute random
numbers with the normal distribution (mean value <span class="math notranslate nohighlight">\(\mu\)</span> equal to zero and
variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> set to one) and produce the values of <span class="math notranslate nohighlight">\(y\)</span> assuming a linear
dependence as function of <span class="math notranslate nohighlight">\(x\)</span></p>
<div class="math notranslate nohighlight">
\[
y = 2x+N(0,1),
\]</div>
<p>where <span class="math notranslate nohighlight">\(N(0,1)\)</span> represents random numbers generated by the normal
distribution.  From <strong>Scikit-Learn</strong> we import then the
<strong>LinearRegression</strong> functionality and make a prediction <span class="math notranslate nohighlight">\(\tilde{y} =
\alpha + \theta x\)</span> using the function <strong>fit(x,y)</strong>. We call the set of
data <span class="math notranslate nohighlight">\((\boldsymbol{x},\boldsymbol{y})\)</span> for our training data. The Python package
<strong>scikit-learn</strong> has also a functionality which extracts the above
fitting parameters <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span> (see below). Later we will
distinguish between training data and test data.</p>
<p>For plotting we use the Python package
<a class="reference external" href="https://matplotlib.org/">matplotlib</a> which produces publication
quality figures. Feel free to explore the extensive
<a class="reference external" href="https://matplotlib.org/gallery/index.html">gallery</a> of examples. In
this example we plot our original values of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> as well as the
prediction <strong>ypredict</strong> (<span class="math notranslate nohighlight">\(\tilde{y}\)</span>), which attempts at fitting our
data with a straight line.  Note also that <strong>Scikit-Learn</strong> requires a
matrix as input for the input values <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. In the above code we
have solved this by declaring <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> as arrays of dimension
<span class="math notranslate nohighlight">\(n\times 1\)</span>.</p>
<p>In the code here we have also made a new array for <span class="math notranslate nohighlight">\(x\in [0,1]\)</span>. Our
prediction is computed for these values, meaning that they were not
included in the data set used to <em>train</em> (or fit) the model.
This is a recurrring theme in machine learning and data analysis. We would like to train a model on a specific given data set.
Thereafter we wish to apply it to data which were not included in the training. Below we will encounter this again in the so-called <em>train-validate-test</em> spliting. We will typically split our data into different sets, oen for training, one for validation and finally, our data from the untouched test vault!</p>
<p>The Python code follows here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>%matplotlib inline

# Importing various packages
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

x = np.random.rand(100,1)
y = 2*x+np.random.randn(100,1)
linreg = LinearRegression()
linreg.fit(x,y)
# This is our new x-array to which we test our model
xnew = np.array([[0],[1]])
ypredict = linreg.predict(xnew)

plt.plot(xnew, ypredict, &quot;r-&quot;)
plt.plot(x, y ,&#39;ro&#39;)
plt.axis([0,1.0,0, 5.0])
plt.xlabel(r&#39;$x$&#39;)
plt.ylabel(r&#39;$y$&#39;)
plt.title(r&#39;Simple Linear Regression&#39;)
plt.show()
</pre></div>
</div>
</div>
</div>
<p>This example serves several aims. It allows us to demonstrate several
aspects of data analysis and later machine learning algorithms. The
immediate visualization shows that our linear fit is not
impressive. It goes through the data points, but there are many
outliers which are not reproduced by our linear regression.  We could
now play around with this small program and change for example the
factor in front of <span class="math notranslate nohighlight">\(x\)</span> and the normal distribution.  Try to change the
function <span class="math notranslate nohighlight">\(y\)</span> to</p>
<div class="math notranslate nohighlight">
\[
y = 10x+0.01 \times N(0,1),
\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is defined as before.  Does the fit look better? Indeed, by
reducing the role of the noise given by the normal distribution we see immediately that
our linear prediction seemingly reproduces better the training
set. However, this testing ‘by the eye’ is obviously not satisfactory in the
long run. Here we have only defined the training data and our model, and
have not discussed a more rigorous approach to the <strong>cost</strong> function.</p>
<p>We need more rigorous criteria in defining whether we have succeeded or
not in modeling our training data.  You will be surprised to see that
many scientists seldomly venture beyond this ‘by the eye’ approach. A
standard approach for the <em>cost</em> function is the so-called <span class="math notranslate nohighlight">\(\chi^2\)</span>
function (a variant of the mean-squared error (MSE))</p>
<div class="math notranslate nohighlight">
\[
\chi^2 = \frac{1}{n}
\sum_{i=0}^{n-1}\frac{(y_i-\tilde{y}_i)^2}{\sigma_i^2},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_i^2\)</span> is the variance (to be defined later) of the entry
<span class="math notranslate nohighlight">\(y_i\)</span>.  We may not know the explicit value of <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>, it serves
however the aim of scaling the equations and make the cost function
dimensionless.</p>
<p>Minimizing the cost function is a central aspect of
our discussions to come. Finding its minima as function of the model
parameters (<span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span> in our case) will be a recurring
theme in these series of lectures. Essentially all machine learning
algorithms we will discuss center around the minimization of the
chosen cost function. This depends in turn on our specific
model for describing the data, a typical situation in supervised
learning. Automatizing the search for the minima of the cost function is a
central ingredient in all algorithms. Typical methods which are
employed are various variants of <strong>gradient</strong> methods. These will be
discussed in more detail later. Again, you’ll be surprised to hear that
many practitioners minimize the above function ‘’by the eye’, popularly dubbed as
‘chi by the eye’. That is, change a parameter and see (visually and numerically) that
the  <span class="math notranslate nohighlight">\(\chi^2\)</span> function becomes smaller.</p>
<p>There are many ways to define the cost function. A simpler approach is to look at the relative difference between the training data and the predicted data, that is we define
the relative error (why would we prefer the MSE instead of the relative error?) as</p>
<div class="math notranslate nohighlight">
\[
\epsilon_{\mathrm{relative}}= \frac{\vert \boldsymbol{y} -\boldsymbol{\tilde{y}}\vert}{\vert \boldsymbol{y}\vert}.
\]</div>
<p>The squared cost function results in an arithmetic mean-unbiased
estimator, and the absolute-value cost function results in a
median-unbiased estimator (in the one-dimensional case, and a
geometric median-unbiased estimator for the multi-dimensional
case). The squared cost function has the disadvantage that it has the tendency
to be dominated by outliers.</p>
<p>We can modify easily the above Python code and plot the relative error instead</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
# Number of data points
n = 100
x = np.random.rand(100,1)
y = 5*x+0.01*np.random.randn(100,1)
linreg = LinearRegression()
linreg.fit(x,y)
ypredict = linreg.predict(x)

plt.plot(x, np.abs(ypredict-y)/abs(y), &quot;ro&quot;)
plt.axis([0,1.0,0.0, 0.5])
plt.xlabel(r&#39;$x$&#39;)
plt.ylabel(r&#39;$\epsilon_{\mathrm{relative}}$&#39;)
plt.title(r&#39;Relative error&#39;)
plt.show()
</pre></div>
</div>
</div>
</div>
<p>Depending on the parameter in front of the normal distribution, we may
have a small or larger relative error. Try to play around with
different training data sets and study (graphically) the value of the
relative error.</p>
<p>As mentioned above, <strong>Scikit-Learn</strong> has an impressive functionality.
We can for example extract the values of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span> and
their error estimates, or the variance and standard deviation and many
other properties from the statistical data analysis.</p>
<p>Here we show an
example of the functionality of <strong>Scikit-Learn</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.linear_model import LinearRegression 
from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error, mean_absolute_error

x = np.random.rand(100,1)
y = 2.0+ 5*x+0.5*np.random.randn(100,1)
linreg = LinearRegression()
linreg.fit(x,y)
ypredict = linreg.predict(x)
print(&#39;The intercept alpha: \n&#39;, linreg.intercept_)
print(&#39;Coefficient theta : \n&#39;, linreg.coef_)
# The mean squared error                               
print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(y, ypredict))
# Explained variance score: 1 is perfect prediction                                 
print(&#39;Variance score: %.2f&#39; % r2_score(y, ypredict))
# Mean squared log error                                                        
print(&#39;Mean squared log error: %.2f&#39; % mean_squared_log_error(y, ypredict) )
# Mean absolute error                                                           
print(&#39;Mean absolute error: %.2f&#39; % mean_absolute_error(y, ypredict))
plt.plot(x, ypredict, &quot;r-&quot;)
plt.plot(x, y ,&#39;ro&#39;)
plt.axis([0.0,1.0,1.5, 7.0])
plt.xlabel(r&#39;$x$&#39;)
plt.ylabel(r&#39;$y$&#39;)
plt.title(r&#39;Linear Regression fit &#39;)
plt.show()
</pre></div>
</div>
</div>
</div>
<p>The function <strong>coef</strong> gives us the parameter <span class="math notranslate nohighlight">\(\theta\)</span> of our fit while <strong>intercept</strong> yields
<span class="math notranslate nohighlight">\(\alpha\)</span>. Depending on the constant in front of the normal distribution, we get values near or far from <span class="math notranslate nohighlight">\(alpha =2\)</span> and <span class="math notranslate nohighlight">\(\theta =5\)</span>. Try to play around with different parameters in front of the normal distribution. The function <strong>meansquarederror</strong> gives us the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss defined as</p>
<div class="math notranslate nohighlight">
\[
MSE(\boldsymbol{y},\boldsymbol{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2,
\]</div>
<p>The smaller the value, the better the fit. Ideally we would like to
have an MSE equal zero.  The attentive reader has probably recognized
this function as being similar to the <span class="math notranslate nohighlight">\(\chi^2\)</span> function defined above.</p>
<p>The <strong>r2score</strong> function computes <span class="math notranslate nohighlight">\(R^2\)</span>, the coefficient of
determination. It provides a measure of how well future samples are
likely to be predicted by the model. Best possible score is 1.0 and it
can be negative (because the model can be arbitrarily worse). A
constant model that always predicts the expected value of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>,
disregarding the input features, would get a <span class="math notranslate nohighlight">\(R^2\)</span> score of <span class="math notranslate nohighlight">\(0.0\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{y}}_i\)</span> is the predicted value of the <span class="math notranslate nohighlight">\(i-th\)</span> sample and <span class="math notranslate nohighlight">\(y_i\)</span> is the corresponding true value, then the score <span class="math notranslate nohighlight">\(R^2\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
R^2(\boldsymbol{y}, \tilde{\boldsymbol{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
\]</div>
<p>where we have defined the mean value  of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
\]</div>
<p>Another quantity taht we will meet again in our discussions of regression analysis is
the mean absolute error (MAE), a risk metric corresponding to the expected value of the absolute error loss or what we call the <span class="math notranslate nohighlight">\(l1\)</span>-norm loss. In our discussion above we presented the relative error.
The MAE is defined as follows</p>
<div class="math notranslate nohighlight">
\[
\text{MAE}(\boldsymbol{y}, \boldsymbol{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n-1} \left| y_i - \tilde{y}_i \right|.
\]</div>
<p>We present the
squared logarithmic (quadratic) error</p>
<div class="math notranslate nohighlight">
\[
\text{MSLE}(\boldsymbol{y}, \boldsymbol{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n - 1} (\log_e (1 + y_i) - \log_e (1 + \tilde{y}_i) )^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\log_e (x)\)</span> stands for the natural logarithm of <span class="math notranslate nohighlight">\(x\)</span>. This error
estimate is best to use when targets having exponential growth, such
as population counts, average sales of a commodity over a span of
years etc.</p>
<p>Finally, another cost function is the Huber cost function used in robust regression.</p>
<p>The rationale behind this possible cost function is its reduced
sensitivity to outliers in the data set. In our discussions on
dimensionality reduction and normalization of data we will meet other
ways of dealing with outliers.</p>
<p>The Huber cost function is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_{\delta}(\boldsymbol{a})=\left\{\begin{array}{cc}\frac{1}{2} \boldsymbol{a}^{2}&amp; \text{for }|\boldsymbol{a}|\leq \delta\\ \delta (|\boldsymbol{a}|-\frac{1}{2}\delta ),&amp;\text{otherwise}.\end{array}\right.
\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\boldsymbol{a}=\boldsymbol{y} - \boldsymbol{\tilde{y}}\)</span>.</p>
<p>We will discuss in more
detail these and other functions in the various lectures.  We conclude this part with another example. Instead of
a linear <span class="math notranslate nohighlight">\(x\)</span>-dependence we study now a cubic polynomial and use the polynomial regression analysis tools of scikit-learn.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
import numpy as np
import random
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression

x=np.linspace(0.02,0.98,200)
noise = np.asarray(random.sample((range(200)),200))
y=x**3*noise
yn=x**3*100
poly3 = PolynomialFeatures(degree=3)
X = poly3.fit_transform(x[:,np.newaxis])
clf3 = LinearRegression()
clf3.fit(X,y)

Xplot=poly3.fit_transform(x[:,np.newaxis])
poly3_plot=plt.plot(x, clf3.predict(Xplot), label=&#39;Cubic Fit&#39;)
plt.plot(x,yn, color=&#39;red&#39;, label=&quot;True Cubic&quot;)
plt.scatter(x, y, label=&#39;Data&#39;, color=&#39;orange&#39;, s=15)
plt.legend()
plt.show()

def error(a):
    for i in y:
        err=(y-yn)/yn
    return abs(np.sum(err))/len(err)

print (error(y))
</pre></div>
</div>
</div>
</div>
<p>Let us now dive into  nuclear physics and remind ourselves briefly about some basic features about binding
energies.  A basic quantity which can be measured for the ground
states of nuclei is the atomic mass <span class="math notranslate nohighlight">\(M(N, Z)\)</span> of the neutral atom with
atomic mass number <span class="math notranslate nohighlight">\(A\)</span> and charge <span class="math notranslate nohighlight">\(Z\)</span>. The number of neutrons is <span class="math notranslate nohighlight">\(N\)</span>. There are indeed several sophisticated experiments worldwide which allow us to measure this quantity to high precision (parts per million even).</p>
<p>Atomic masses are usually tabulated in terms of the mass excess defined by</p>
<div class="math notranslate nohighlight">
\[
\Delta M(N, Z) =  M(N, Z) - uA,
\]</div>
<p>where <span class="math notranslate nohighlight">\(u\)</span> is the Atomic Mass Unit</p>
<div class="math notranslate nohighlight">
\[
u = M(^{12}\mathrm{C})/12 = 931.4940954(57) \hspace{0.1cm} \mathrm{MeV}/c^2.
\]</div>
<p>The nucleon masses are</p>
<div class="math notranslate nohighlight">
\[
m_p =  1.00727646693(9)u,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
m_n = 939.56536(8)\hspace{0.1cm} \mathrm{MeV}/c^2 = 1.0086649156(6)u.
\]</div>
<p>In the <a class="reference external" href="http://nuclearmasses.org/resources_folder/Wang_2017_Chinese_Phys_C_41_030003.pdf">2016 mass evaluation of by W.J.Huang, G.Audi, M.Wang, F.G.Kondev, S.Naimi and X.Xu</a>
there are data on masses and decays of 3437 nuclei.</p>
<p>The nuclear binding energy is defined as the energy required to break
up a given nucleus into its constituent parts of <span class="math notranslate nohighlight">\(N\)</span> neutrons and <span class="math notranslate nohighlight">\(Z\)</span>
protons. In terms of the atomic masses <span class="math notranslate nohighlight">\(M(N, Z)\)</span> the binding energy is
defined by</p>
<div class="math notranslate nohighlight">
\[
BE(N, Z) = ZM_H c^2 + Nm_n c^2 - M(N, Z)c^2 ,
\]</div>
<p>where <span class="math notranslate nohighlight">\(M_H\)</span> is the mass of the hydrogen atom and <span class="math notranslate nohighlight">\(m_n\)</span> is the mass of the neutron.
In terms of the mass excess the binding energy is given by</p>
<div class="math notranslate nohighlight">
\[
BE(N, Z) = Z\Delta_H c^2 + N\Delta_n c^2 -\Delta(N, Z)c^2 ,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta_H c^2 = 7.2890\)</span> MeV and <span class="math notranslate nohighlight">\(\Delta_n c^2 = 8.0713\)</span> MeV.</p>
<p>A popular and physically intuitive model which can be used to parametrize
the experimental binding energies as function of <span class="math notranslate nohighlight">\(A\)</span>, is the so-called
<strong>liquid drop model</strong>. The ansatz is based on the following expression</p>
<div class="math notranslate nohighlight">
\[
BE(N,Z) = a_1A-a_2A^{2/3}-a_3\frac{Z^2}{A^{1/3}}-a_4\frac{(N-Z)^2}{A},
\]</div>
<p>where <span class="math notranslate nohighlight">\(A\)</span> stands for the number of nucleons and the <span class="math notranslate nohighlight">\(a_i\)</span>s are parameters which are determined by a fit
to the experimental data.</p>
<p>To arrive at the above expression we have assumed that we can make the following assumptions:</p>
<ul class="simple">
<li><p>There is a volume term <span class="math notranslate nohighlight">\(a_1A\)</span> proportional with the number of nucleons (the energy is also an extensive quantity). When an assembly of nucleons of the same size is packed together into the smallest volume, each interior nucleon has a certain number of other nucleons in contact with it. This contribution is proportional to the volume.</p></li>
<li><p>There is a surface energy term <span class="math notranslate nohighlight">\(a_2A^{2/3}\)</span>. The assumption here is that a nucleon at the surface of a nucleus interacts with fewer other nucleons than one in the interior of the nucleus and hence its binding energy is less. This surface energy term takes that into account and is therefore negative and is proportional to the surface area.</p></li>
<li><p>There is a Coulomb energy term <span class="math notranslate nohighlight">\(a_3\frac{Z^2}{A^{1/3}}\)</span>. The electric repulsion between each pair of protons in a nucleus yields less binding.</p></li>
<li><p>There is an asymmetry term <span class="math notranslate nohighlight">\(a_4\frac{(N-Z)^2}{A}\)</span>. This term is associated with the Pauli exclusion principle and reflects the fact that the proton-neutron interaction is more attractive on the average than the neutron-neutron and proton-proton interactions.</p></li>
</ul>
<p>We could also add a so-called pairing term, which is a correction term that
arises from the tendency of proton pairs and neutron pairs to
occur. An even number of particles is more stable than an odd number.</p>
<section id="organizing-our-data">
<h3><span class="section-number">3.3.1. </span>Organizing our data<a class="headerlink" href="#organizing-our-data" title="Link to this heading">#</a></h3>
<p>Let us start with reading and organizing our data.
We start with the compilation of masses and binding energies from 2016.
After having downloaded this file to our own computer, we are now ready to read the file and start structuring our data.</p>
<p>We start with preparing folders for storing our calculations and the data file over masses and binding energies. We import also various modules that we will find useful in order to present various Machine Learning methods. Here we focus mainly on the functionality of <strong>scikit-learn</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Common imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.linear_model as skl
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import os

# Where to save the figures and data files
PROJECT_ROOT_DIR = &quot;Results&quot;
FIGURE_ID = &quot;Results/FigureFiles&quot;
DATA_ID = &quot;DataFiles/&quot;

if not os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

if not os.path.exists(FIGURE_ID):
    os.makedirs(FIGURE_ID)

if not os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

def image_path(fig_id):
    return os.path.join(FIGURE_ID, fig_id)

def data_path(dat_id):
    return os.path.join(DATA_ID, dat_id)

def save_fig(fig_id):
    plt.savefig(image_path(fig_id) + &quot;.png&quot;, format=&#39;png&#39;)

infile = open(data_path(&quot;MassEval2016.dat&quot;),&#39;r&#39;)
</pre></div>
</div>
</div>
</div>
<p>Our next step is to read the data on experimental binding energies and
reorganize them as functions of the mass number <span class="math notranslate nohighlight">\(A\)</span>, the number of
protons <span class="math notranslate nohighlight">\(Z\)</span> and neutrons <span class="math notranslate nohighlight">\(N\)</span> using <strong>pandas</strong>.  Before we do this it is
always useful (unless you have a binary file or other types of compressed
data) to actually open the file and simply take a look at it!</p>
<p>In particular, the program that outputs the final nuclear masses is written in Fortran with a specific format. It means that we need to figure out the format and which columns contain the data we are interested in. Pandas comes with a function that reads formatted output. After having admired the file, we are now ready to start massaging it with <strong>pandas</strong>. The file begins with some basic format information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&quot;&quot;&quot;                                                                                                                         
This is taken from the data file of the mass 2016 evaluation.                                                               
All files are 3436 lines long with 124 character per line.                                                                  
       Headers are 39 lines long.                                                                                           
   col 1     :  Fortran character control: 1 = page feed  0 = line feed                                                     
   format    :  a1,i3,i5,i5,i5,1x,a3,a4,1x,f13.5,f11.5,f11.3,f9.3,1x,a2,f11.3,f9.3,1x,i3,1x,f12.5,f11.5                     
   These formats are reflected in the pandas widths variable below, see the statement                                       
   widths=(1,3,5,5,5,1,3,4,1,13,11,11,9,1,2,11,9,1,3,1,12,11,1),                                                            
   Pandas has also a variable header, with length 39 in this case.                                                          
&quot;&quot;&quot;
</pre></div>
</div>
</div>
</div>
<p>The data we are interested in are in columns 2, 3, 4 and 11, giving us
the number of neutrons, protons, mass numbers and binding energies,
respectively. We add also for the sake of completeness the element name. The data are in fixed-width formatted lines and we will
covert them into the <strong>pandas</strong> DataFrame structure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Read the experimental data with Pandas
Masses = pd.read_fwf(infile, usecols=(2,3,4,6,11),
              names=(&#39;N&#39;, &#39;Z&#39;, &#39;A&#39;, &#39;Element&#39;, &#39;Ebinding&#39;),
              widths=(1,3,5,5,5,1,3,4,1,13,11,11,9,1,2,11,9,1,3,1,12,11,1),
              header=39,
              index_col=False)

# Extrapolated values are indicated by &#39;#&#39; in place of the decimal place, so
# the Ebinding column won&#39;t be numeric. Coerce to float and drop these entries.
Masses[&#39;Ebinding&#39;] = pd.to_numeric(Masses[&#39;Ebinding&#39;], errors=&#39;coerce&#39;)
Masses = Masses.dropna()
# Convert from keV to MeV.
Masses[&#39;Ebinding&#39;] /= 1000

# Group the DataFrame by nucleon number, A.
Masses = Masses.groupby(&#39;A&#39;)
# Find the rows of the grouped DataFrame with the maximum binding energy.
Masses = Masses.apply(lambda t: t[t.Ebinding==t.Ebinding.max()])
</pre></div>
</div>
</div>
</div>
<p>We have now read in the data, grouped them according to the variables we are interested in.
We see how easy it is to reorganize the data using <strong>pandas</strong>. If we
were to do these operations in C/C++ or Fortran, we would have had to
write various functions/subroutines which perform the above
reorganizations for us.  Having reorganized the data, we can now start
to make some simple fits using both the functionalities in <strong>numpy</strong> and
<strong>Scikit-Learn</strong> afterwards.</p>
<p>Now we define five variables which contain
the number of nucleons <span class="math notranslate nohighlight">\(A\)</span>, the number of protons <span class="math notranslate nohighlight">\(Z\)</span> and the number of neutrons <span class="math notranslate nohighlight">\(N\)</span>, the element name and finally the energies themselves.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A = Masses[&#39;A&#39;]
Z = Masses[&#39;Z&#39;]
N = Masses[&#39;N&#39;]
Element = Masses[&#39;Element&#39;]
Energies = Masses[&#39;Ebinding&#39;]
print(Masses)
</pre></div>
</div>
</div>
</div>
<p>The next step, and we will define this mathematically later, is to set up the so-called <strong>design matrix</strong>. We will throughout call this matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.
It has dimensionality <span class="math notranslate nohighlight">\(n\times p\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of data points and <span class="math notranslate nohighlight">\(p\)</span> are the so-called predictors. In our case here they are given by the number of polynomials in <span class="math notranslate nohighlight">\(A\)</span> we wish to include in the fit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Now we set up the design matrix X
X = np.zeros((len(A),5))
X[:,0] = 1
X[:,1] = A
X[:,2] = A**(2.0/3.0)
X[:,3] = A**(-1.0/3.0)
X[:,4] = A**(-1.0)
</pre></div>
</div>
</div>
</div>
<p>Note well that we have made life simple here. We perform a fit in
terms of the number of nucleons only.  A more sophisticated fit can be
done by including an explicit dependence on the number of protons and
neutrons in the asymmetry and Coulomb terms. We leave this as an exercise to you the reader.</p>
<p>With <strong>Scikit-Learn</strong> we are now ready to use linear regression and fit our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>clf = skl.LinearRegression().fit(X, Energies)
fity = clf.predict(X)
</pre></div>
</div>
</div>
</div>
<p>Pretty simple!<br />
Now we can print measures of how our fit is doing, the coefficients from the fits and plot the final fit together with our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># The mean squared error                               
print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(Energies, fity))
# Explained variance score: 1 is perfect prediction                                 
print(&#39;Variance score: %.2f&#39; % r2_score(Energies, fity))
# Mean absolute error                                                           
print(&#39;Mean absolute error: %.2f&#39; % mean_absolute_error(Energies, fity))

Masses[&#39;Eapprox&#39;]  = fity
# Generate a plot comparing the experimental with the fitted values values.
fig, ax = plt.subplots()
ax.set_xlabel(r&#39;$A = N + Z$&#39;)
ax.set_ylabel(r&#39;$E_\mathrm{bind}\,/\mathrm{MeV}$&#39;)
ax.plot(Masses[&#39;A&#39;], Masses[&#39;Ebinding&#39;], alpha=0.7, lw=2,
            label=&#39;Ame2016&#39;)
ax.plot(Masses[&#39;A&#39;], Masses[&#39;Eapprox&#39;], alpha=0.7, lw=2, c=&#39;m&#39;,
            label=&#39;Fit&#39;)
ax.legend()
save_fig(&quot;Masses2016&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
<p>As a teaser, let us now see how we can do this with decision trees using <strong>Scikit-Learn</strong>. Later we will switch to so-called <strong>random forests</strong>!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>
#Decision Tree Regression
from sklearn.tree import DecisionTreeRegressor
regr_1=DecisionTreeRegressor(max_depth=5)
regr_2=DecisionTreeRegressor(max_depth=7)
regr_3=DecisionTreeRegressor(max_depth=9)
regr_1.fit(X, Energies)
regr_2.fit(X, Energies)
regr_3.fit(X, Energies)


y_1 = regr_1.predict(X)
y_2 = regr_2.predict(X)
y_3=regr_3.predict(X)
Masses[&#39;Eapprox&#39;] = y_3
# Plot the results
plt.figure()
plt.plot(A, Energies, color=&quot;blue&quot;, label=&quot;Data&quot;, linewidth=2)
plt.plot(A, y_1, color=&quot;red&quot;, label=&quot;max_depth=5&quot;, linewidth=2)
plt.plot(A, y_2, color=&quot;green&quot;, label=&quot;max_depth=7&quot;, linewidth=2)
plt.plot(A, y_3, color=&quot;m&quot;, label=&quot;max_depth=9&quot;, linewidth=2)

plt.xlabel(&quot;$A$&quot;)
plt.ylabel(&quot;$E$[MeV]&quot;)
plt.title(&quot;Decision Tree Regression&quot;)
plt.legend()
save_fig(&quot;Masses2016Trees&quot;)
plt.show()
print(Masses)
print(np.mean( (Energies-y_1)**2))
</pre></div>
</div>
</div>
</div>
<p>With a deeper and deeper tree level, we can almost reproduce every
single data point by increasing the max depth of the tree.
We can actually decide to make a decision tree which fits every single point.
As we will
see later, this has the benefit that we can really train a model which
traverses every single data point. However, the price we pay is that
we will easily overfit. That is, if we apply our model to unseen data,
we will most likely fail miserably in our attempt at making
predictions. As an exercise, try to make the tree level larger by adjusting the maximum depth variable. When printing out the predicition, you will note that the binding energy of every nucleus is accurately reproduced.</p>
<p>The <strong>seaborn</strong> package allows us to visualize data in an efficient way. Note that we use <strong>scikit-learn</strong>’s multi-layer perceptron (or feed forward neural network)
functionality.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.neural_network import MLPRegressor
from sklearn.metrics import accuracy_score
import seaborn as sns

X_train = X
Y_train = Energies
n_hidden_neurons = 100
epochs = 100
# store models for later use
eta_vals = np.logspace(-5, 1, 7)
lmbd_vals = np.logspace(-5, 1, 7)
# store the models for later use
DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)
train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))
sns.set()
for i, eta in enumerate(eta_vals):
    for j, lmbd in enumerate(lmbd_vals):
        dnn = MLPRegressor(hidden_layer_sizes=(n_hidden_neurons), activation=&#39;logistic&#39;,
                            alpha=lmbd, learning_rate_init=eta, max_iter=epochs)
        dnn.fit(X_train, Y_train)
        DNN_scikit[i][j] = dnn
        train_accuracy[i][j] = dnn.score(X_train, Y_train)

fig, ax = plt.subplots(figsize = (10, 10))
sns.heatmap(train_accuracy, annot=True, ax=ax, cmap=&quot;viridis&quot;)
ax.set_title(&quot;Training Accuracy&quot;)
ax.set_ylabel(&quot;$\eta$&quot;)
ax.set_xlabel(&quot;$\lambda$&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="linear-regression-basic-elements">
<h2><span class="section-number">3.4. </span>Linear Regression, basic elements<a class="headerlink" href="#linear-regression-basic-elements" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/fys/FYS-STK4155/h20/forelesningsvideoer/LectureAug27.mp4?vrtx=view-as-webpage">Video of Lecture</a>.</p>
<p>Fitting a continuous function with linear parameterization in terms of the parameters  <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<ul class="simple">
<li><p>Method of choice for fitting a continuous function!</p></li>
<li><p>Gives an excellent introduction to central Machine Learning features with <strong>understandable pedagogical</strong> links to other methods like <strong>Neural Networks</strong>, <strong>Support Vector Machines</strong> etc</p></li>
<li><p>Analytical expression for the fitting parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></p></li>
<li><p>Analytical expressions for statistical propertiers like mean values, variances, confidence intervals and more</p></li>
<li><p>Analytical relation with probabilistic interpretations</p></li>
<li><p>Easy to introduce basic concepts like bias-variance tradeoff, cross-validation, resampling and regularization techniques and many other ML topics</p></li>
<li><p>Easy to code! And links well with classification problems and logistic regression and neural networks</p></li>
<li><p>Allows for <strong>easy</strong> hands-on understanding of gradient descent methods</p></li>
<li><p>and many more features</p></li>
</ul>
<p>For more discussions of Ridge and Lasso regression, <a class="reference external" href="https://arxiv.org/abs/1509.09169">Wessel van Wieringen’s</a> article is highly recommended.
Similarly, <a class="reference external" href="https://arxiv.org/abs/1803.08823">Mehta et al’s article</a> is also recommended.</p>
<p>Regression modeling deals with the description of  the sampling distribution of a given random variable <span class="math notranslate nohighlight">\(y\)</span> and how it varies as function of another variable or a set of such variables <span class="math notranslate nohighlight">\(\boldsymbol{x} =[x_0, x_1,\dots, x_{n-1}]^T\)</span>.
The first variable is called the <strong>dependent</strong>, the <strong>outcome</strong> or the <strong>response</strong> or just the output variable while the set of variables <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is called the independent variable, or the predictor variable or the explanatory variable, or just the input variable. We will hereafter call <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{x}x\)</span> for the output and input variables, respectively.</p>
<p>A regression model aims at finding a likelihood function <span class="math notranslate nohighlight">\(p(\boldsymbol{y}\vert \boldsymbol{x})\)</span> (or just a function <span class="math notranslate nohighlight">\(f(\boldsymbol{x}\)</span>) , that is the conditional distribution for <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> with a given <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>. The estimation of  <span class="math notranslate nohighlight">\(p(\boldsymbol{y}\vert \boldsymbol{x})\)</span> is made using a data set with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> cases <span class="math notranslate nohighlight">\(i = 0, 1, 2, \dots, n-1\)</span></p></li>
<li><p>Response/output (target, dependent or outcome) variable <span class="math notranslate nohighlight">\(y_i\)</span> with <span class="math notranslate nohighlight">\(i = 0, 1, 2, \dots, n-1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span> so-called explanatory/input (independent or predictor) variables <span class="math notranslate nohighlight">\(\boldsymbol{x}_i=[x_{i0}, x_{i1}, \dots, x_{ip-1}]\)</span> with <span class="math notranslate nohighlight">\(i = 0, 1, 2, \dots, n-1\)</span> and explanatory variables running from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(p-1\)</span>. See below for more explicit examples.</p></li>
</ul>
<p>The goal of the regression analysis is to extract/exploit relationship between <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> in or to infer causal dependencies, approximations to the likelihood functions, functional relationships and to make predictions, making fits and many other things.</p>
<p>Consider an experiment in which <span class="math notranslate nohighlight">\(p\)</span> characteristics of <span class="math notranslate nohighlight">\(n\)</span> samples are
measured. The data from this experiment, for various explanatory variables <span class="math notranslate nohighlight">\(p\)</span> are normally represented by a matrix<br />
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p>The matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is called the <em>design
matrix</em>. Additional information of the samples is available in the
form of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> (also as above). The variable <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> is
generally referred to as the <em>response variable</em>. The aim of
regression analysis is to explain <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> in terms of
<span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> through a functional relationship like <span class="math notranslate nohighlight">\(y_i =
f(\mathbf{X}_{i,\ast})\)</span>. When no prior knowledge on the form of
<span class="math notranslate nohighlight">\(f(\cdot)\)</span> is available, it is common to assume a linear relationship
between <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>. This assumption gives rise to
the <em>linear regression model</em> where <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = [\theta_0, \ldots,
\theta_{p-1}]^{T}\)</span> are the <em>regression parameters</em>.</p>
<p>Linear regression gives us a set of analytical equations for the parameters <span class="math notranslate nohighlight">\(\theta_j\)</span>.</p>
<p>In order to understand the relation among the predictors <span class="math notranslate nohighlight">\(p\)</span>, the set of data <span class="math notranslate nohighlight">\(n\)</span> and the target (outcome, output etc) <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>,
consider the model we discussed for describing nuclear binding energies.</p>
<p>There we assumed that we could parametrize the data using a polynomial approximation based on the liquid drop model.
Assuming</p>
<div class="math notranslate nohighlight">
\[
BE(A) = a_0+a_1A+a_2A^{2/3}+a_3A^{-1/3}+a_4A^{-1},
\]</div>
<p>we have five predictors, that is the intercept, the <span class="math notranslate nohighlight">\(A\)</span> dependent term, the <span class="math notranslate nohighlight">\(A^{2/3}\)</span> term and the <span class="math notranslate nohighlight">\(A^{-1/3}\)</span> and <span class="math notranslate nohighlight">\(A^{-1}\)</span> terms.
This gives <span class="math notranslate nohighlight">\(p=0,1,2,3,4\)</span>. Furthermore we have <span class="math notranslate nohighlight">\(n\)</span> entries for each predictor. It means that our design matrix is a
<span class="math notranslate nohighlight">\(p\times n\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p>
<p>Here the predictors are based on a model we have made. A popular data set which is widely encountered in ML applications is the
so-called <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0957417407006719?via%3Dihub">credit card default data from Taiwan</a>. The data set contains data on <span class="math notranslate nohighlight">\(n=30000\)</span> credit card holders with predictors like gender, marital status, age, profession, education, etc. In total there are <span class="math notranslate nohighlight">\(24\)</span> such predictors or attributes leading to a design matrix of dimensionality <span class="math notranslate nohighlight">\(24 \times 30000\)</span>. This is however a classification problem and we will come back to it when we discuss Logistic Regression.</p>
<p>Before we proceed let us study a case from linear algebra where we aim at fitting a set of data <span class="math notranslate nohighlight">\(\boldsymbol{y}=[y_0,y_1,\dots,y_{n-1}]\)</span>. We could think of these data as a result of an experiment or a complicated numerical experiment. These data are functions of a series of variables <span class="math notranslate nohighlight">\(\boldsymbol{x}=[x_0,x_1,\dots,x_{n-1}]\)</span>, that is <span class="math notranslate nohighlight">\(y_i = y(x_i)\)</span> with <span class="math notranslate nohighlight">\(i=0,1,2,\dots,n-1\)</span>. The variables <span class="math notranslate nohighlight">\(x_i\)</span> could represent physical quantities like time, temperature, position etc. We assume that <span class="math notranslate nohighlight">\(y(x)\)</span> is a smooth function.</p>
<p>Since obtaining these data points may not be trivial, we want to use these data to fit a function which can allow us to make predictions for values of <span class="math notranslate nohighlight">\(y\)</span> which are not in the present set. The perhaps simplest approach is to assume we can parametrize our function in terms of a polynomial of degree <span class="math notranslate nohighlight">\(n-1\)</span> with <span class="math notranslate nohighlight">\(n\)</span> points, that is</p>
<div class="math notranslate nohighlight">
\[
y=y(x) \rightarrow y(x_i)=\tilde{y}_i+\epsilon_i=\sum_{j=0}^{n-1} \theta_j x_i^j+\epsilon_i,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is the error in our approximation.</p>
<p>For every set of values <span class="math notranslate nohighlight">\(y_i,x_i\)</span> we have thus the corresponding set of equations</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
y_0&amp;=\theta_0+\theta_1x_0^1+\theta_2x_0^2+\dots+\theta_{n-1}x_0^{n-1}+\epsilon_0\\
y_1&amp;=\theta_0+\theta_1x_1^1+\theta_2x_1^2+\dots+\theta_{n-1}x_1^{n-1}+\epsilon_1\\
y_2&amp;=\theta_0+\theta_1x_2^1+\theta_2x_2^2+\dots+\theta_{n-1}x_2^{n-1}+\epsilon_2\\
\dots &amp; \dots \\
y_{n-1}&amp;=\theta_0+\theta_1x_{n-1}^1+\theta_2x_{n-1}^2+\dots+\theta_{n-1}x_{n-1}^{n-1}+\epsilon_{n-1}.\\
\end{align*}
\end{split}\]</div>
<p>Defining the vectors</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = [y_0,y_1, y_2,\dots, y_{n-1}]^T,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta} = [\theta_0,\theta_1, \theta_2,\dots, \theta_{n-1}]^T,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\epsilon} = [\epsilon_0,\epsilon_1, \epsilon_2,\dots, \epsilon_{n-1}]^T,
\]</div>
<p>and the design matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X}=
\begin{bmatrix} 
1&amp; x_{0}^1 &amp;x_{0}^2&amp; \dots &amp; \dots &amp;x_{0}^{n-1}\\
1&amp; x_{1}^1 &amp;x_{1}^2&amp; \dots &amp; \dots &amp;x_{1}^{n-1}\\
1&amp; x_{2}^1 &amp;x_{2}^2&amp; \dots &amp; \dots &amp;x_{2}^{n-1}\\                      
\dots&amp; \dots &amp;\dots&amp; \dots &amp; \dots &amp;\dots\\
1&amp; x_{n-1}^1 &amp;x_{n-1}^2&amp; \dots &amp; \dots &amp;x_{n-1}^{n-1}\\
\end{bmatrix}
\end{split}\]</div>
<p>we can rewrite our equations as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\epsilon}.
\]</div>
<p>The above design matrix is called a <a class="reference external" href="https://en.wikipedia.org/wiki/Vandermonde_matrix">Vandermonde matrix</a>.</p>
<p>We are obviously not limited to the above polynomial expansions.  We
could replace the various powers of <span class="math notranslate nohighlight">\(x\)</span> with elements of Fourier
series or instead of <span class="math notranslate nohighlight">\(x_i^j\)</span> we could have <span class="math notranslate nohighlight">\(\cos{(j x_i)}\)</span> or <span class="math notranslate nohighlight">\(\sin{(j
x_i)}\)</span>, or time series or other orthogonal functions.  For every set
of values <span class="math notranslate nohighlight">\(y_i,x_i\)</span> we can then generalize the equations to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
y_0&amp;=\theta_0x_{00}+\theta_1x_{01}+\theta_2x_{02}+\dots+\theta_{n-1}x_{0n-1}+\epsilon_0\\
y_1&amp;=\theta_0x_{10}+\theta_1x_{11}+\theta_2x_{12}+\dots+\theta_{n-1}x_{1n-1}+\epsilon_1\\
y_2&amp;=\theta_0x_{20}+\theta_1x_{21}+\theta_2x_{22}+\dots+\theta_{n-1}x_{2n-1}+\epsilon_2\\
\dots &amp; \dots \\
y_{i}&amp;=\theta_0x_{i0}+\theta_1x_{i1}+\theta_2x_{i2}+\dots+\theta_{n-1}x_{in-1}+\epsilon_i\\
\dots &amp; \dots \\
y_{n-1}&amp;=\theta_0x_{n-1,0}+\theta_1x_{n-1,2}+\theta_2x_{n-1,2}+\dots+\theta_{n-1}x_{n-1,n-1}+\epsilon_{n-1}.\\
\end{align*}
\end{split}\]</div>
<p><strong>Note that we have <span class="math notranslate nohighlight">\(p=n\)</span> here. The matrix is symmetric. This is generally not the case!</strong></p>
<p>We redefine in turn the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X}=
\begin{bmatrix} 
x_{00}&amp; x_{01} &amp;x_{02}&amp; \dots &amp; \dots &amp;x_{0,n-1}\\
x_{10}&amp; x_{11} &amp;x_{12}&amp; \dots &amp; \dots &amp;x_{1,n-1}\\
x_{20}&amp; x_{21} &amp;x_{22}&amp; \dots &amp; \dots &amp;x_{2,n-1}\\                      
\dots&amp; \dots &amp;\dots&amp; \dots &amp; \dots &amp;\dots\\
x_{n-1,0}&amp; x_{n-1,1} &amp;x_{n-1,2}&amp; \dots &amp; \dots &amp;x_{n-1,n-1}\\
\end{bmatrix}
\end{split}\]</div>
<p>and without loss of generality we rewrite again  our equations as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\epsilon}.
\]</div>
<p>The left-hand side of this equation is kwown. Our error vector <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> and the parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> are our unknow quantities. How can we obtain the optimal set of <span class="math notranslate nohighlight">\(\theta_i\)</span> values?</p>
<p>We have defined the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> via the equations</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
y_0&amp;=\theta_0x_{00}+\theta_1x_{01}+\theta_2x_{02}+\dots+\theta_{n-1}x_{0n-1}+\epsilon_0\\
y_1&amp;=\theta_0x_{10}+\theta_1x_{11}+\theta_2x_{12}+\dots+\theta_{n-1}x_{1n-1}+\epsilon_1\\
y_2&amp;=\theta_0x_{20}+\theta_1x_{21}+\theta_2x_{22}+\dots+\theta_{n-1}x_{2n-1}+\epsilon_1\\
\dots &amp; \dots \\
y_{i}&amp;=\theta_0x_{i0}+\theta_1x_{i1}+\theta_2x_{i2}+\dots+\theta_{n-1}x_{in-1}+\epsilon_1\\
\dots &amp; \dots \\
y_{n-1}&amp;=\theta_0x_{n-1,0}+\theta_1x_{n-1,2}+\theta_2x_{n-1,2}+\dots+\theta_{n-1}x_{n-1,n-1}+\epsilon_{n-1}.\\
\end{align*}
\end{split}\]</div>
<p>As we noted above, we stayed with a system with the design matrix
<span class="math notranslate nohighlight">\(\boldsymbol{X}\in {\mathbb{R}}^{n\times n}\)</span>, that is we have <span class="math notranslate nohighlight">\(p=n\)</span>. For reasons to come later (algorithmic arguments) we will hereafter define
our matrix as <span class="math notranslate nohighlight">\(\boldsymbol{X}\in {\mathbb{R}}^{n\times p}\)</span>, with the predictors refering to the column numbers and the entries <span class="math notranslate nohighlight">\(n\)</span> being the row elements.</p>
<p>In our <a class="reference external" href="https://compphysics.github.io/MachineLearning/doc/pub/How2ReadData/html/How2ReadData.html">introductory notes</a> we looked at the so-called <a class="reference external" href="https://en.wikipedia.org/wiki/Semi-empirical_mass_formula">liquid drop model</a>. Let us remind ourselves about what we did by looking at the code.</p>
<p>We restate the parts of the code we are most interested in.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Common imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display
import os

# Where to save the figures and data files
PROJECT_ROOT_DIR = &quot;Results&quot;
FIGURE_ID = &quot;Results/FigureFiles&quot;
DATA_ID = &quot;DataFiles/&quot;

if not os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

if not os.path.exists(FIGURE_ID):
    os.makedirs(FIGURE_ID)

if not os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

def image_path(fig_id):
    return os.path.join(FIGURE_ID, fig_id)

def data_path(dat_id):
    return os.path.join(DATA_ID, dat_id)

def save_fig(fig_id):
    plt.savefig(image_path(fig_id) + &quot;.png&quot;, format=&#39;png&#39;)

infile = open(data_path(&quot;MassEval2016.dat&quot;),&#39;r&#39;)


# Read the experimental data with Pandas
Masses = pd.read_fwf(infile, usecols=(2,3,4,6,11),
              names=(&#39;N&#39;, &#39;Z&#39;, &#39;A&#39;, &#39;Element&#39;, &#39;Ebinding&#39;),
              widths=(1,3,5,5,5,1,3,4,1,13,11,11,9,1,2,11,9,1,3,1,12,11,1),
              header=39,
              index_col=False)

# Extrapolated values are indicated by &#39;#&#39; in place of the decimal place, so
# the Ebinding column won&#39;t be numeric. Coerce to float and drop these entries.
Masses[&#39;Ebinding&#39;] = pd.to_numeric(Masses[&#39;Ebinding&#39;], errors=&#39;coerce&#39;)
Masses = Masses.dropna()
# Convert from keV to MeV.
Masses[&#39;Ebinding&#39;] /= 1000

# Group the DataFrame by nucleon number, A.
Masses = Masses.groupby(&#39;A&#39;)
# Find the rows of the grouped DataFrame with the maximum binding energy.
Masses = Masses.apply(lambda t: t[t.Ebinding==t.Ebinding.max()])
A = Masses[&#39;A&#39;]
Z = Masses[&#39;Z&#39;]
N = Masses[&#39;N&#39;]
Element = Masses[&#39;Element&#39;]
Energies = Masses[&#39;Ebinding&#39;]

# Now we set up the design matrix X
X = np.zeros((len(A),5))
X[:,0] = 1
X[:,1] = A
X[:,2] = A**(2.0/3.0)
X[:,3] = A**(-1.0/3.0)
X[:,4] = A**(-1.0)
# Then nice printout using pandas
DesignMatrix = pd.DataFrame(X)
DesignMatrix.index = A
DesignMatrix.columns = [&#39;1&#39;, &#39;A&#39;, &#39;A^(2/3)&#39;, &#39;A^(-1/3)&#39;, &#39;1/A&#39;]
display(DesignMatrix)
</pre></div>
</div>
</div>
</div>
<p>With <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\in {\mathbb{R}}^{p\times 1}\)</span>, it means that we will hereafter write our equations for the approximation as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\tilde{y}}= \boldsymbol{X}\boldsymbol{\theta},
\]</div>
<p>throughout these lectures.</p>
<p>With the above we use the design matrix to define the approximation <span class="math notranslate nohighlight">\(\boldsymbol{\tilde{y}}\)</span> via the unknown quantity <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\tilde{y}}= \boldsymbol{X}\boldsymbol{\theta},
\]</div>
<p>and in order to find the optimal parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> instead of solving the above linear algebra problem, we define a function which gives a measure of the spread between the values <span class="math notranslate nohighlight">\(y_i\)</span> (which represent hopefully the exact values) and the parameterized values <span class="math notranslate nohighlight">\(\tilde{y}_i\)</span>, namely</p>
<div class="math notranslate nohighlight">
\[
C(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right\},
\]</div>
<p>or using the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and in a more compact matrix-vector notation as</p>
<div class="math notranslate nohighlight">
\[
C(\boldsymbol{\theta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\}.
\]</div>
<p>This function is one possible way to define the so-called cost function.</p>
<p>It is also common to define
the function <span class="math notranslate nohighlight">\(C\)</span> as</p>
<div class="math notranslate nohighlight">
\[
C(\boldsymbol{\theta})=\frac{1}{2n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2,
\]</div>
<p>since when taking the first derivative with respect to the unknown parameters <span class="math notranslate nohighlight">\(\theta\)</span>, the factor of <span class="math notranslate nohighlight">\(2\)</span> cancels out.</p>
<p>The function</p>
<div class="math notranslate nohighlight">
\[
C(\boldsymbol{\theta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\},
\]</div>
<p>can be linked to the variance of the quantity <span class="math notranslate nohighlight">\(y_i\)</span> if we interpret the latter as the mean value.
When linking (see the discussion below) with the maximum likelihood approach below, we will indeed interpret <span class="math notranslate nohighlight">\(y_i\)</span> as a mean value</p>
<div class="math notranslate nohighlight">
\[
y_{i}=\langle y_i \rangle = \theta_0x_{i,0}+\theta_1x_{i,1}+\theta_2x_{i,2}+\dots+\theta_{n-1}x_{i,n-1}+\epsilon_i,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\langle y_i \rangle\)</span> is the mean value. Keep in mind also that
till now we have treated <span class="math notranslate nohighlight">\(y_i\)</span> as the exact value. Normally, the
response (dependent or outcome) variable <span class="math notranslate nohighlight">\(y_i\)</span> the outcome of a
numerical experiment or another type of experiment and is thus only an
approximation to the true value. It is then always accompanied by an
error estimate, often limited to a statistical error estimate given by
the standard deviation discussed earlier. In the discussion here we
will treat <span class="math notranslate nohighlight">\(y_i\)</span> as our exact value for the response variable.</p>
<p>In order to find the parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> we will then minimize the spread of <span class="math notranslate nohighlight">\(C(\boldsymbol{\theta})\)</span>, that is we are going to solve the problem</p>
<div class="math notranslate nohighlight">
\[
{\displaystyle \min_{\boldsymbol{\theta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\}.
\]</div>
<p>In practical terms it means we will require</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial C(\boldsymbol{\theta})}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}\right)^2\right]=0,
\]</div>
<p>which results in</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial C(\boldsymbol{\theta})}{\partial \theta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}x_{ij}\left(y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}\right)\right]=0,
\]</div>
<p>or in a matrix-vector form as</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial C(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right).
\]</div>
<p>We can rewrite</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial C(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right),
\]</div>
<p>as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X}^T\boldsymbol{y} = \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\theta},
\]</div>
<p>and if the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}^T\boldsymbol{X}\)</span> is invertible we have the solution</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta} =\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.
\]</div>
<p>We note also that since our design matrix is defined as <span class="math notranslate nohighlight">\(\boldsymbol{X}\in
{\mathbb{R}}^{n\times p}\)</span>, the product <span class="math notranslate nohighlight">\(\boldsymbol{X}^T\boldsymbol{X} \in
{\mathbb{R}}^{p\times p}\)</span>.  In the above case we have that <span class="math notranslate nohighlight">\(p \ll n\)</span>,
in our case <span class="math notranslate nohighlight">\(p=5\)</span> meaning that we end up with inverting a small
<span class="math notranslate nohighlight">\(5\times 5\)</span> matrix. This is a rather common situation, in many cases we end up with low-dimensional
matrices to invert. The methods discussed here and for many other
supervised learning algorithms like classification with logistic
regression or support vector machines, exhibit dimensionalities which
allow for the usage of direct linear algebra methods such as <strong>LU</strong> decomposition or <strong>Singular Value Decomposition</strong> (SVD) for finding the inverse of the matrix
<span class="math notranslate nohighlight">\(\boldsymbol{X}^T\boldsymbol{X}\)</span>.</p>
<p><strong>Small question</strong>: Do you think the example we have at hand here (the nuclear binding energies) can lead to problems in inverting the matrix  <span class="math notranslate nohighlight">\(\boldsymbol{X}^T\boldsymbol{X}\)</span>? What kind of problems can we expect?</p>
<p>The following matrix and vector relation will be useful here and for the rest of the course. Vectors are always written as boldfaced lower case letters and
matrices as upper case boldfaced letters.</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial (\boldsymbol{b}^T\boldsymbol{a})}{\partial \boldsymbol{a}} = \boldsymbol{b},
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial (\boldsymbol{a}^T\boldsymbol{A}\boldsymbol{a})}{\partial \boldsymbol{a}} = \boldsymbol{a}^T(\boldsymbol{A}+\boldsymbol{A}^T),
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \left(\boldsymbol{x}-\boldsymbol{A}\boldsymbol{s}\right)^T\left(\boldsymbol{x}-\boldsymbol{A}\boldsymbol{s}\right)}{\partial \boldsymbol{s}} = -2\left(\boldsymbol{x}-\boldsymbol{A}\boldsymbol{s}\right)^T\boldsymbol{A},
\]</div>
<p>These and other relations are discussed in the exercises following this chapter (see the end of the chapter).
The latter equation is similar to the equation for the mean-squared error function we have been discussing.
We can then compute the second derivative of the cost function, which in our case is the second derivative
of the means squared error. This leads to</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 C(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}^T\partial \boldsymbol{\theta}} =\frac{2}{n}\boldsymbol{X}^T\boldsymbol{X}.
\]</div>
<p>This quantity defines the so- called the Hessian matrix.</p>
<p>The Hessian matrix plays an important role and is defined for the mean squared error  as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{H}=\boldsymbol{X}^T\boldsymbol{X}.
\]</div>
<p>The Hessian matrix for ordinary least squares is also proportional to
the covariance matrix. As we will see in the chapter on Ridge and Lasso regression, This means that we can use the Singular Value Decomposition of a matrix  to find
the eigenvalues of the covariance matrix and the Hessian matrix in
terms of the singular values.</p>
<p>The residuals <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> are in turn given by</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\epsilon} = \boldsymbol{y}-\boldsymbol{\tilde{y}} = \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta},
\]</div>
<p>and with</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)= 0,
\]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X}^T\boldsymbol{\epsilon}=\boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)= 0,
\]</div>
<p>meaning that the solution for <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is the one which minimizes the residuals.  Later we will link this with the maximum likelihood approach.</p>
<p>Let us now return to our nuclear binding energies and simply code the above equations.</p>
<p>It is rather straightforward to implement the matrix inversion and obtain the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. After having defined the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> we simply need to
write</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># matrix inversion to find theta
theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Energies)
# and then make the prediction
ytilde = X @ theta
</pre></div>
</div>
</div>
</div>
<p>Alternatively, you can use the least squares functionality in <strong>Numpy</strong> as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>fit = np.linalg.lstsq(X, Energies, rcond =None)[0]
ytildenp = np.dot(fit,X.T)
</pre></div>
</div>
</div>
</div>
<p>And finally we plot our fit with and compare with data</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Masses[&#39;Eapprox&#39;]  = ytilde
# Generate a plot comparing the experimental with the fitted values values.
fig, ax = plt.subplots()
ax.set_xlabel(r&#39;$A = N + Z$&#39;)
ax.set_ylabel(r&#39;$E_\mathrm{bind}\,/\mathrm{MeV}$&#39;)
ax.plot(Masses[&#39;A&#39;], Masses[&#39;Ebinding&#39;], alpha=0.7, lw=2,
            label=&#39;Ame2016&#39;)
ax.plot(Masses[&#39;A&#39;], Masses[&#39;Eapprox&#39;], alpha=0.7, lw=2, c=&#39;m&#39;,
            label=&#39;Fit&#39;)
ax.legend()
save_fig(&quot;Masses2016OLS&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
<p>We can easily test our fit by computing the <span class="math notranslate nohighlight">\(R2\)</span> score that we discussed in connection with the functionality of <strong>Scikit-Learn</strong> in the introductory slides.
Since we are not using <strong>Scikit-Learn</strong> here we can define our own <span class="math notranslate nohighlight">\(R2\)</span> function as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def R2(y_data, y_model):
    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)
</pre></div>
</div>
</div>
</div>
<p>and we would be using it as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>print(R2(Energies,ytilde))
</pre></div>
</div>
</div>
</div>
<p>We can easily add our <strong>MSE</strong> score as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def MSE(y_data,y_model):
    n = np.size(y_model)
    return np.sum((y_data-y_model)**2)/n

print(MSE(Energies,ytilde))
</pre></div>
</div>
</div>
</div>
<p>and finally the relative error as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def RelativeError(y_data,y_model):
    return abs((y_data-y_model)/y_data)
print(RelativeError(Energies, ytilde))
</pre></div>
</div>
</div>
</div>
<section id="the-chi-2-function">
<h3><span class="section-number">3.4.1. </span>The <span class="math notranslate nohighlight">\(\chi^2\)</span> function<a class="headerlink" href="#the-chi-2-function" title="Link to this heading">#</a></h3>
<p>Normally, the response (dependent or outcome) variable <span class="math notranslate nohighlight">\(y_i\)</span> is the
outcome of a numerical experiment or another type of experiment and is
thus only an approximation to the true value. It is then always
accompanied by an error estimate, often limited to a statistical error
estimate given by the standard deviation discussed earlier. In the
discussion here we will treat <span class="math notranslate nohighlight">\(y_i\)</span> as our exact value for the
response variable.</p>
<p>Introducing the standard deviation <span class="math notranslate nohighlight">\(\sigma_i\)</span> for each measurement
<span class="math notranslate nohighlight">\(y_i\)</span>, we define now the <span class="math notranslate nohighlight">\(\chi^2\)</span> function (omitting the <span class="math notranslate nohighlight">\(1/n\)</span> term)
as</p>
<div class="math notranslate nohighlight">
\[
\chi^2(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=0}^{n-1}\frac{\left(y_i-\tilde{y}_i\right)^2}{\sigma_i^2}=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T\frac{1}{\boldsymbol{\Sigma^2}}\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right\},
\]</div>
<p>where the matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is a diagonal matrix with <span class="math notranslate nohighlight">\(\sigma_i\)</span> as matrix elements.</p>
<p>In order to find the parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> we will then minimize the spread of <span class="math notranslate nohighlight">\(\chi^2(\boldsymbol{\theta})\)</span> by requiring</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(\frac{y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}}{\sigma_i}\right)^2\right]=0,
\]</div>
<p>which results in</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}\frac{x_{ij}}{\sigma_i}\left(\frac{y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}}{\sigma_i}\right)\right]=0,
\]</div>
<p>or in a matrix-vector form as</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{A}^T\left( \boldsymbol{b}-\boldsymbol{A}\boldsymbol{\theta}\right).
\]</div>
<p>where we have defined the matrix <span class="math notranslate nohighlight">\(\boldsymbol{A} =\boldsymbol{X}/\boldsymbol{\Sigma}\)</span> with matrix elements <span class="math notranslate nohighlight">\(a_{ij} = x_{ij}/\sigma_i\)</span> and the vector <span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span> with elements <span class="math notranslate nohighlight">\(b_i = y_i/\sigma_i\)</span>.</p>
<p>We can rewrite</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{A}^T\left( \boldsymbol{b}-\boldsymbol{A}\boldsymbol{\theta}\right),
\]</div>
<p>as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{A}^T\boldsymbol{b} = \boldsymbol{A}^T\boldsymbol{A}\boldsymbol{\theta},
\]</div>
<p>and if the matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}^T\boldsymbol{A}\)</span> is invertible we have the solution</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta} =\left(\boldsymbol{A}^T\boldsymbol{A}\right)^{-1}\boldsymbol{A}^T\boldsymbol{b}.
\]</div>
<p>If we then introduce the matrix</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{H} =  \left(\boldsymbol{A}^T\boldsymbol{A}\right)^{-1},
\]</div>
<p>we have then the following expression for the parameters <span class="math notranslate nohighlight">\(\theta_j\)</span> (the matrix elements of <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span> are <span class="math notranslate nohighlight">\(h_{ij}\)</span>)</p>
<div class="math notranslate nohighlight">
\[
\theta_j = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}\frac{y_i}{\sigma_i}\frac{x_{ik}}{\sigma_i} = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}b_ia_{ik}
\]</div>
<p>We state without proof the expression for the uncertainty  in the parameters <span class="math notranslate nohighlight">\(\theta_j\)</span> as (we leave this as an exercise)</p>
<div class="math notranslate nohighlight">
\[
\sigma^2(\theta_j) = \sum_{i=0}^{n-1}\sigma_i^2\left( \frac{\partial \theta_j}{\partial y_i}\right)^2,
\]</div>
<p>resulting in</p>
<div class="math notranslate nohighlight">
\[
\sigma^2(\theta_j) = \left(\sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}a_{ik}\right)\left(\sum_{l=0}^{p-1}h_{jl}\sum_{m=0}^{n-1}a_{ml}\right) = h_{jj}!
\]</div>
<p>The first step here is to approximate the function <span class="math notranslate nohighlight">\(y\)</span> with a first-order polynomial, that is we write</p>
<div class="math notranslate nohighlight">
\[
y=y(x) \rightarrow y(x_i) \approx \theta_0+\theta_1 x_i.
\]</div>
<p>By computing the derivatives of <span class="math notranslate nohighlight">\(\chi^2\)</span> with respect to <span class="math notranslate nohighlight">\(\theta_0\)</span> and <span class="math notranslate nohighlight">\(\theta_1\)</span> show that these are given by</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_0} = -2\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(\frac{y_i-\theta_0-\theta_1x_{i}}{\sigma_i^2}\right)\right]=0,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_1} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}x_i\left(\frac{y_i-\theta_0-\theta_1x_{i}}{\sigma_i^2}\right)\right]=0.
\]</div>
<p>For a linear fit (a first-order polynomial) we don’t need to invert a matrix!!<br />
Defining</p>
<div class="math notranslate nohighlight">
\[
\gamma =  \sum_{i=0}^{n-1}\frac{1}{\sigma_i^2},
\]</div>
<div class="math notranslate nohighlight">
\[
\gamma_x =  \sum_{i=0}^{n-1}\frac{x_{i}}{\sigma_i^2},
\]</div>
<div class="math notranslate nohighlight">
\[
\gamma_y = \sum_{i=0}^{n-1}\left(\frac{y_i}{\sigma_i^2}\right),
\]</div>
<div class="math notranslate nohighlight">
\[
\gamma_{xx} =  \sum_{i=0}^{n-1}\frac{x_ix_{i}}{\sigma_i^2},
\]</div>
<div class="math notranslate nohighlight">
\[
\gamma_{xy} = \sum_{i=0}^{n-1}\frac{y_ix_{i}}{\sigma_i^2},
\]</div>
<p>we obtain</p>
<div class="math notranslate nohighlight">
\[
\theta_0 = \frac{\gamma_{xx}\gamma_y-\gamma_x\gamma_y}{\gamma\gamma_{xx}-\gamma_x^2},
\]</div>
<div class="math notranslate nohighlight">
\[
\theta_1 = \frac{\gamma_{xy}\gamma-\gamma_x\gamma_y}{\gamma\gamma_{xx}-\gamma_x^2}.
\]</div>
<p>This approach (different linear and non-linear regression) suffers
often from both being underdetermined and overdetermined in the
unknown coefficients <span class="math notranslate nohighlight">\(\theta_i\)</span>.  A better approach is to use the
Singular Value Decomposition (SVD) method discussed below. Or using
Lasso and Ridge regression. See below.</p>
</section>
<section id="fitting-an-equation-of-state-for-dense-nuclear-matter">
<h3><span class="section-number">3.4.2. </span>Fitting an Equation of State for Dense Nuclear Matter<a class="headerlink" href="#fitting-an-equation-of-state-for-dense-nuclear-matter" title="Link to this heading">#</a></h3>
<p>Before we continue, let us introduce yet another example. We are going to fit the
nuclear equation of state using results from many-body calculations.
The equation of state we have made available here, as function of
density, has been derived using modern nucleon-nucleon potentials with
<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0370157399001106">the addition of three-body
forces</a>. This
time the file is presented as a standard <strong>csv</strong> file.</p>
<p>The beginning of the Python code here is similar to what you have seen
before, with the same initializations and declarations. We use also
<strong>pandas</strong> again, rather extensively in order to organize our data.</p>
<p>The difference now is that we use <strong>Scikit-Learn’s</strong> regression tools
instead of our own matrix inversion implementation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Common imports
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import sklearn.linear_model as skl
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Where to save the figures and data files
PROJECT_ROOT_DIR = &quot;Results&quot;
FIGURE_ID = &quot;Results/FigureFiles&quot;
DATA_ID = &quot;DataFiles/&quot;

if not os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

if not os.path.exists(FIGURE_ID):
    os.makedirs(FIGURE_ID)

if not os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

def image_path(fig_id):
    return os.path.join(FIGURE_ID, fig_id)

def data_path(dat_id):
    return os.path.join(DATA_ID, dat_id)

def save_fig(fig_id):
    plt.savefig(image_path(fig_id) + &quot;.png&quot;, format=&#39;png&#39;)

infile = open(data_path(&quot;EoS.csv&quot;),&#39;r&#39;)

# Read the EoS data as  csv file and organize the data into two arrays with density and energies
EoS = pd.read_csv(infile, names=(&#39;Density&#39;, &#39;Energy&#39;))
EoS[&#39;Energy&#39;] = pd.to_numeric(EoS[&#39;Energy&#39;], errors=&#39;coerce&#39;)
EoS = EoS.dropna()
Energies = EoS[&#39;Energy&#39;]
Density = EoS[&#39;Density&#39;]
#  The design matrix now as function of various polytrops
X = np.zeros((len(Density),4))
X[:,3] = Density**(4.0/3.0)
X[:,2] = Density
X[:,1] = Density**(2.0/3.0)
X[:,0] = 1

# We use now Scikit-Learn&#39;s linear regressor and ridge regressor
# OLS part
clf = skl.LinearRegression().fit(X, Energies)
ytilde = clf.predict(X)
EoS[&#39;Eols&#39;]  = ytilde
# The mean squared error                               
print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(Energies, ytilde))
# Explained variance score: 1 is perfect prediction                                 
print(&#39;Variance score: %.2f&#39; % r2_score(Energies, ytilde))
# Mean absolute error                                                           
print(&#39;Mean absolute error: %.2f&#39; % mean_absolute_error(Energies, ytilde))
print(clf.coef_, clf.intercept_)


fig, ax = plt.subplots()
ax.set_xlabel(r&#39;$\rho[\mathrm{fm}^{-3}]$&#39;)
ax.set_ylabel(r&#39;Energy per particle&#39;)
ax.plot(EoS[&#39;Density&#39;], EoS[&#39;Energy&#39;], alpha=0.7, lw=2,
            label=&#39;Theoretical data&#39;)
ax.plot(EoS[&#39;Density&#39;], EoS[&#39;Eols&#39;], alpha=0.7, lw=2, c=&#39;m&#39;,
            label=&#39;OLS&#39;)
ax.legend()
save_fig(&quot;EoSfitting&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
<p>The above simple polynomial in density <span class="math notranslate nohighlight">\(\rho\)</span> gives an excellent fit
to the data.</p>
</section>
</section>
<section id="splitting-our-data-in-training-and-test-data">
<h2><span class="section-number">3.5. </span>Splitting our Data in Training and Test data<a class="headerlink" href="#splitting-our-data-in-training-and-test-data" title="Link to this heading">#</a></h2>
<p>It is normal in essentially all Machine Learning studies to split the
data in a training set and a test set (sometimes also an additional
validation set).  <strong>Scikit-Learn</strong> has an own function for this. There
is no explicit recipe for how much data should be included as training
data and say test data.  An accepted rule of thumb is to use
approximately <span class="math notranslate nohighlight">\(2/3\)</span> to <span class="math notranslate nohighlight">\(4/5\)</span> of the data as training data. We will
postpone a discussion of this splitting to the end of these notes and
our discussion of the so-called <strong>bias-variance</strong> tradeoff. Here we
limit ourselves to repeat the above equation of state fitting example
but now splitting the data into a training set and a test set.</p>
<p>Let us study some examples. The first code here takes a simple
one-dimensional second-order polynomial and we fit it to a
second-order polynomial. Depending on the strength of the added noise,
the various measures like the <span class="math notranslate nohighlight">\(R2\)</span> score or the mean-squared error,
the fit becomes better or worse.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split


def R2(y_data, y_model):
    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)
def MSE(y_data,y_model):
    n = np.size(y_model)
    return np.sum((y_data-y_model)**2)/n

x = np.random.rand(100)
y = 2.0+5*x*x+0.1*np.random.randn(100)


#  The design matrix now as function of a given polynomial
X = np.zeros((len(x),3))
X[:,0] = 1.0
X[:,1] = x
X[:,2] = x**2
# We split the data in test and training data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
# matrix inversion to find theta
theta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train
print(theta)
# and then make the prediction
ytilde = X_train @ theta
print(&quot;Training R2&quot;)
print(R2(y_train,ytilde))
print(&quot;Training MSE&quot;)
print(MSE(y_train,ytilde))
ypredict = X_test @ theta
print(&quot;Test R2&quot;)
print(R2(y_test,ypredict))
print(&quot;Test MSE&quot;)
print(MSE(y_test,ypredict))
</pre></div>
</div>
</div>
</div>
<p>Alternatively, you could write your own test-train splitting function as shown here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># equivalently in numpy
def train_test_split_numpy(inputs, labels, train_size, test_size):
    n_inputs = len(inputs)
    inputs_shuffled = inputs.copy()
    labels_shuffled = labels.copy()

    np.random.shuffle(inputs_shuffled)
    np.random.shuffle(labels_shuffled)

    train_end = int(n_inputs*train_size)
    X_train, X_test = inputs_shuffled[:train_end], inputs_shuffled[train_end:]
    Y_train, Y_test = labels_shuffled[:train_end], labels_shuffled[train_end:]

    return X_train, X_test, Y_train, Y_test
</pre></div>
</div>
</div>
</div>
<p>But since <strong>scikit-learn</strong> has its own function for doing this and since
it interfaces easily with <strong>tensorflow</strong> and other libraries, we
normally recommend using the latter functionality.</p>
<p>As another example, we apply the training and testing split to
to the above equation of state fitting example
but now splitting the data into a training set and a test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
# Where to save the figures and data files
PROJECT_ROOT_DIR = &quot;Results&quot;
FIGURE_ID = &quot;Results/FigureFiles&quot;
DATA_ID = &quot;DataFiles/&quot;

if not os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

if not os.path.exists(FIGURE_ID):
    os.makedirs(FIGURE_ID)

if not os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

def image_path(fig_id):
    return os.path.join(FIGURE_ID, fig_id)

def data_path(dat_id):
    return os.path.join(DATA_ID, dat_id)

def save_fig(fig_id):
    plt.savefig(image_path(fig_id) + &quot;.png&quot;, format=&#39;png&#39;)

def R2(y_data, y_model):
    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)
def MSE(y_data,y_model):
    n = np.size(y_model)
    return np.sum((y_data-y_model)**2)/n

infile = open(data_path(&quot;EoS.csv&quot;),&#39;r&#39;)

# Read the EoS data as  csv file and organized into two arrays with density and energies
EoS = pd.read_csv(infile, names=(&#39;Density&#39;, &#39;Energy&#39;))
EoS[&#39;Energy&#39;] = pd.to_numeric(EoS[&#39;Energy&#39;], errors=&#39;coerce&#39;)
EoS = EoS.dropna()
Energies = EoS[&#39;Energy&#39;]
Density = EoS[&#39;Density&#39;]
#  The design matrix now as function of various polytrops
X = np.zeros((len(Density),5))
X[:,0] = 1
X[:,1] = Density**(2.0/3.0)
X[:,2] = Density
X[:,3] = Density**(4.0/3.0)
X[:,4] = Density**(5.0/3.0)
# We split the data in test and training data
X_train, X_test, y_train, y_test = train_test_split(X, Energies, test_size=0.2)
# matrix inversion to find theta
theta = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)
# and then make the prediction
ytilde = X_train @ theta
print(&quot;Training R2&quot;)
print(R2(y_train,ytilde))
print(&quot;Training MSE&quot;)
print(MSE(y_train,ytilde))
ypredict = X_test @ theta
print(&quot;Test R2&quot;)
print(R2(y_test,ypredict))
print(&quot;Test MSE&quot;)
print(MSE(y_test,ypredict))
</pre></div>
</div>
</div>
</div>
</section>
<section id="reducing-the-number-of-degrees-of-freedom-overarching-view">
<h2><span class="section-number">3.6. </span>Reducing the number of degrees of freedom, overarching view<a class="headerlink" href="#reducing-the-number-of-degrees-of-freedom-overarching-view" title="Link to this heading">#</a></h2>
<p>Many Machine Learning problems involve thousands or even millions of
features for each training instance. Not only does this make training
extremely slow, it can also make it much harder to find a good
solution, as we will see. This problem is often referred to as the
curse of dimensionality.  Fortunately, in real-world problems, it is
often possible to reduce the number of features considerably, turning
an intractable problem into a tractable one.</p>
<p>Later  we will discuss some of the most popular dimensionality reduction
techniques: the principal component analysis (PCA), Kernel PCA, and
Locally Linear Embedding (LLE).</p>
<p>Principal component analysis and its various variants deal with the
problem of fitting a low-dimensional <a class="reference external" href="https://en.wikipedia.org/wiki/Affine_space">affine
subspace</a> to a set of of
data points in a high-dimensional space. With its family of methods it
is one of the most used tools in data modeling, compression and
visualization.</p>
<p>Before we proceed however, we will discuss how to preprocess our
data. Till now and in connection with our previous examples we have
not met so many cases where we are too sensitive to the scaling of our
data. Normally the data may need a rescaling and/or may be sensitive
to extreme values. Scaling the data renders our inputs much more
suitable for the algorithms we want to employ.</p>
<p>For data sets gathered for real world applications, it is rather normal that
different features have very different units and
numerical scales. For example, a data set detailing health habits may include
features such as <strong>age</strong> in the range <span class="math notranslate nohighlight">\(0-80\)</span>, and <strong>caloric intake</strong> of order <span class="math notranslate nohighlight">\(2000\)</span>.
Many machine learning methods sensitive to the scales of the features and may perform poorly if they
are very different scales. Therefore, it is typical to scale
the features in a way to avoid such outlier values.</p>
<p><strong>Scikit-Learn</strong> has several functions which allow us to rescale the
data, normally resulting in much better results in terms of various
accuracy scores.  The <strong>StandardScaler</strong> function in <strong>Scikit-Learn</strong>
ensures that for each feature/predictor we study the mean value is
zero and the variance is one (every column in the design/feature
matrix).  This scaling has the drawback that it does not ensure that
we have a particular maximum or minimum in our data set. Another
function included in <strong>Scikit-Learn</strong> is the <strong>MinMaxScaler</strong> which
ensures that all features are exactly between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. The</p>
<p>The <strong>Normalizer</strong> scales each data
point such that the feature vector has a euclidean length of one. In other words, it
projects a data point on the circle (or sphere in the case of higher dimensions) with a
radius of 1. This means every data point is scaled by a different number (by the
inverse of it’s length).
This normalization is often used when only the direction (or angle) of the data matters,
not the length of the feature vector.</p>
<p>The <strong>RobustScaler</strong> works similarly to the StandardScaler in that it
ensures statistical properties for each feature that guarantee that
they are on the same scale. However, the RobustScaler uses the median
and quartiles, instead of mean and variance. This makes the
RobustScaler ignore data points that are very different from the rest
(like measurement errors). These odd data points are also called
outliers, and might often lead to trouble for other scaling
techniques.</p>
<p>Many features are often scaled using standardization to improve
performance. In <strong>Scikit-Learn</strong> this is given by the <strong>StandardScaler</strong>
function as discussed above. It is easy however to write your own.
Mathematically, this involves subtracting the mean and divide by the
standard deviation over the data set, for each feature:</p>
<div class="math notranslate nohighlight">
\[
x_j^{(i)} \rightarrow \frac{x_j^{(i)} - \overline{x}_j}{\sigma(x_j)},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{x}_j\)</span> and <span class="math notranslate nohighlight">\(\sigma(x_j)\)</span> are the mean and standard
deviation, respectively, of the feature <span class="math notranslate nohighlight">\(x_j\)</span>.  This ensures that each
feature has zero mean and unit standard deviation.  For data sets
where we do not have the standard deviation or don’t wish to calculate
it, it is then common to simply set it to one.</p>
<p>Let us consider the following vanilla example where we use both
<strong>Scikit-Learn</strong> and write our own function as well.  We produce a
simple test design matrix with random numbers. Each column could then
represent a specific feature whose mean value is subracted.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import sklearn.linear_model as skl
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import  train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer
import numpy as np
import pandas as pd
from IPython.display import display
np.random.seed(100)
# setting up a 10 x 5 matrix
rows = 10
cols = 5
X = np.random.randn(rows,cols)
XPandas = pd.DataFrame(X)
display(XPandas)
print(XPandas.mean())
print(XPandas.std())
XPandas = (XPandas -XPandas.mean())
display(XPandas)
#  This option does not include the standard deviation
scaler = StandardScaler(with_std=False)
scaler.fit(X)
Xscaled = scaler.transform(X)
display(XPandas-Xscaled)
</pre></div>
</div>
</div>
</div>
<p>Small exercise: perform the standard scaling by including the standard deviation and compare with what Scikit-Learn gives.</p>
<p>Another commonly used scaling method is min-max scaling. This is very
useful for when we want the features to lie in a certain interval. To
scale the feature <span class="math notranslate nohighlight">\(x_j\)</span> to the interval <span class="math notranslate nohighlight">\([a, b]\)</span>, we can apply the
transformation</p>
<div class="math notranslate nohighlight">
\[
x_j^{(i)} \rightarrow (b-a)\frac{x_j^{(i)} - \min(x_j)}{\max(x_j) - \min(x_j)} - a
\]</div>
<p>where <span class="math notranslate nohighlight">\(\min(x_j)\)</span> and <span class="math notranslate nohighlight">\(\max(x_j)\)</span> return the minimum and maximum value of <span class="math notranslate nohighlight">\(x_j\)</span> over the data set, respectively.</p>
</section>
<section id="testing-the-means-squared-error-as-function-of-complexity">
<h2><span class="section-number">3.7. </span>Testing the Means Squared Error as function of Complexity<a class="headerlink" href="#testing-the-means-squared-error-as-function-of-complexity" title="Link to this heading">#</a></h2>
<p>Before we proceed with a more detailed analysis of the so-called
Bias-Variance tradeoff, we present here an example of the relation
between model complexity and the mean squared error for the triaining
data and the test data.</p>
<p>The results here tell us clearly that for the data not included in the
training, there is an optimal model as function of the complexity of
ourmodel (here in terms of the polynomial degree of the model).</p>
<p>The results here will vary as function of model complexity and the amount od data used for training.</p>
<p>Our data is defined by <span class="math notranslate nohighlight">\(x\in [-3,3]\)</span> with a total of for example <span class="math notranslate nohighlight">\(100\)</span> data points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline


np.random.seed(2018)
n = 100
maxdegree = 14
# Make data set.
x = np.linspace(-3, 3, n).reshape(-1, 1)
y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)
TestError = np.zeros(maxdegree)
TrainError = np.zeros(maxdegree)
polydegree = np.zeros(maxdegree)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)


for degree in range(maxdegree):
    model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False))
    clf = model.fit(x_train,y_train)
    y_fit = clf.predict(x_train)
    y_pred = clf.predict(x_test) 
    polydegree[degree] = degree
    TestError[degree] = np.mean( np.mean((y_test - y_pred)**2) )
    TrainError[degree] = np.mean( np.mean((y_train - y_fit)**2) )

plt.plot(polydegree, TestError, label=&#39;Test Error&#39;)
plt.plot(polydegree, TrainError, label=&#39;Train Error&#39;)
plt.legend()
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercises">
<h2><span class="section-number">3.8. </span>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
</section>
<section id="exercise-1-setting-up-various-python-environments">
<h2><span class="section-number">3.9. </span>Exercise 1: Setting up various Python environments<a class="headerlink" href="#exercise-1-setting-up-various-python-environments" title="Link to this heading">#</a></h2>
<p>The first exercise here is of a mere technical art. We want you to have</p>
<ul class="simple">
<li><p>git as a version control software and to establish a user account on a provider like GitHub. Other providers like GitLab etc are equally fine. You can also use the University of Oslo <a class="reference external" href="https://www.uio.no/tjenester/it/maskin/filer/versjonskontroll/github.html">GitHub facilities</a>.</p></li>
<li><p>Install various Python packages</p></li>
</ul>
<p>We will make extensive use of Python as programming language and its
myriad of available libraries.  You will find
IPython/Jupyter notebooks invaluable in your work.  You can run <strong>R</strong>
codes in the Jupyter/IPython notebooks, with the immediate benefit of
visualizing your data. You can also use compiled languages like C++,
Rust, Fortran etc if you prefer. The focus in these lectures will be
on Python.</p>
<p>If you have Python installed (we recommend Python3) and you feel
pretty familiar with installing different packages, we recommend that
you install the following Python packages via <strong>pip</strong> as</p>
<ol class="arabic simple">
<li><p>pip install numpy scipy matplotlib ipython scikit-learn sympy pandas pillow</p></li>
</ol>
<p>For <strong>Tensorflow</strong>, we recommend following the instructions in the text of
<a class="reference external" href="http://shop.oreilly.com/product/0636920052289.do">Aurelien Geron, Hands‑On Machine Learning with Scikit‑Learn and TensorFlow, O’Reilly</a></p>
<p>We will come back to <strong>tensorflow</strong> later.</p>
<p>For Python3, replace <strong>pip</strong> with <strong>pip3</strong>.</p>
<p>For OSX users we recommend, after having installed Xcode, to
install <strong>brew</strong>. Brew allows for a seamless installation of additional
software via for example</p>
<ol class="arabic simple">
<li><p>brew install python3</p></li>
</ol>
<p>For Linux users, with its variety of distributions like for example the widely popular Ubuntu distribution,
you can use <strong>pip</strong> as well and simply install Python as</p>
<ol class="arabic simple">
<li><p>sudo apt-get install python3  (or python for Python2.7)</p></li>
</ol>
<p>If you don’t want to perform these operations separately and venture
into the hassle of exploring how to set up dependencies and paths, we
recommend two widely used distrubutions which set up all relevant
dependencies for Python, namely</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.anaconda.com/">Anaconda</a>,</p></li>
</ul>
<p>which is an open source
distribution of the Python and R programming languages for large-scale
data processing, predictive analytics, and scientific computing, that
aims to simplify package management and deployment. Package versions
are managed by the package management system <strong>conda</strong>.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.enthought.com/product/canopy/">Enthought canopy</a></p></li>
</ul>
<p>is a Python
distribution for scientific and analytic computing distribution and
analysis environment, available for free and under a commercial
license.</p>
<p>We recommend using <strong>Anaconda</strong> if you are not too familiar with setting paths in a terminal environment.</p>
</section>
<section id="exercise-2-making-your-own-data-and-exploring-scikit-learn">
<h2><span class="section-number">3.10. </span>Exercise 2: making your own data and exploring scikit-learn<a class="headerlink" href="#exercise-2-making-your-own-data-and-exploring-scikit-learn" title="Link to this heading">#</a></h2>
<p>We will generate our own dataset for a function <span class="math notranslate nohighlight">\(y(x)\)</span> where <span class="math notranslate nohighlight">\(x \in [0,1]\)</span> and defined by random numbers computed with the uniform distribution. The function <span class="math notranslate nohighlight">\(y\)</span> is a quadratic polynomial in <span class="math notranslate nohighlight">\(x\)</span> with added stochastic noise according to the normal distribution <span class="math notranslate nohighlight">\(\cal {N}(0,1)\)</span>.
The following simple Python instructions define our <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> values (with 100 data points).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>x = np.random.rand(100,1)
y = 2.0+5*x*x+0.1*np.random.randn(100,1)
</pre></div>
</div>
</div>
</div>
<ol class="arabic simple">
<li><p>Write your own code (following the examples under the <a class="reference external" href="https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapter1.html">regression notes</a>) for computing the parametrization of the data set fitting a second-order polynomial.</p></li>
<li><p>Use thereafter <strong>scikit-learn</strong> (see again the examples in the regression slides) and compare with your own code.   When compairing with <em>scikit_learn</em>, make sure you set the option for the intercept to <strong>FALSE</strong>, see <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</a>. This feature will be explained in more detail during the lectures of week 35 and week 36. You can find more in <a class="reference external" href="https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapter3.html#more-on-rescaling-data">https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapter3.html#more-on-rescaling-data</a>.</p></li>
<li><p>Using scikit-learn, compute also the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error defined as</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
MSE(\boldsymbol{y},\boldsymbol{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2,
\]</div>
<p>and the <span class="math notranslate nohighlight">\(R^2\)</span> score function.
If <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{y}}_i\)</span> is the predicted value of the <span class="math notranslate nohighlight">\(i-th\)</span> sample and <span class="math notranslate nohighlight">\(y_i\)</span> is the corresponding true value, then the score <span class="math notranslate nohighlight">\(R^2\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
R^2(\boldsymbol{y}, \tilde{\boldsymbol{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
\]</div>
<p>where we have defined the mean value  of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
\]</div>
<p>You can use the functionality included in scikit-learn. If you feel for it, you can use your own program and define functions which compute the above two functions.
Discuss the meaning of these results. Try also to vary the coefficient in front of the added stochastic noise term and discuss the quality of the fits.</p>
<!-- --- begin solution of exercise --- -->
<p><strong>Solution.</strong>
The code here is an example of where we define our own design matrix and fit parameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

def save_fig(fig_id):
    plt.savefig(image_path(fig_id) + &quot;.png&quot;, format=&#39;png&#39;)

def R2(y_data, y_model):
    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)
def MSE(y_data,y_model):
    n = np.size(y_model)
    return np.sum((y_data-y_model)**2)/n

x = np.random.rand(100)
y = 2.0+5*x*x+0.1*np.random.randn(100)


#  The design matrix now as function of a given polynomial
X = np.zeros((len(x),3))
X[:,0] = 1.0
X[:,1] = x
X[:,2] = x**2
# We split the data in test and training data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
# matrix inversion to find theta
theta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train
print(theta)
# and then make the prediction
ytilde = X_train @ theta
print(&quot;Training R2&quot;)
print(R2(y_train,ytilde))
print(&quot;Training MSE&quot;)
print(MSE(y_train,ytilde))
ypredict = X_test @ theta
print(&quot;Test R2&quot;)
print(R2(y_test,ypredict))
print(&quot;Test MSE&quot;)
print(MSE(y_test,ypredict))
</pre></div>
</div>
</div>
</div>
<!-- --- end solution of exercise --- --></section>
<section id="exercise-3-normalizing-our-data">
<h2><span class="section-number">3.11. </span>Exercise 3: Normalizing our data<a class="headerlink" href="#exercise-3-normalizing-our-data" title="Link to this heading">#</a></h2>
<p>A much used approach before starting to train the data is  to preprocess our
data. Normally the data may need a rescaling and/or may be sensitive
to extreme values. Scaling the data renders our inputs much more
suitable for the algorithms we want to employ.</p>
<p><strong>Scikit-Learn</strong> has several functions which allow us to rescale the
data, normally resulting in much better results in terms of various
accuracy scores.  The <strong>StandardScaler</strong> function in <strong>Scikit-Learn</strong>
ensures that for each feature/predictor we study the mean value is
zero and the variance is one (every column in the design/feature
matrix).  This scaling has the drawback that it does not ensure that
we have a particular maximum or minimum in our data set. Another
function included in <strong>Scikit-Learn</strong> is the <strong>MinMaxScaler</strong> which
ensures that all features are exactly between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. The</p>
<p>The <strong>Normalizer</strong> scales each data
point such that the feature vector has a euclidean length of one. In other words, it
projects a data point on the circle (or sphere in the case of higher dimensions) with a
radius of 1. This means every data point is scaled by a different number (by the
inverse of it’s length).
This normalization is often used when only the direction (or angle) of the data matters,
not the length of the feature vector.</p>
<p>The <strong>RobustScaler</strong> works similarly to the StandardScaler in that it
ensures statistical properties for each feature that guarantee that
they are on the same scale. However, the RobustScaler uses the median
and quartiles, instead of mean and variance. This makes the
RobustScaler ignore data points that are very different from the rest
(like measurement errors). These odd data points are also called
outliers, and might often lead to trouble for other scaling
techniques.</p>
<p>It also common to split the data in a <strong>training</strong> set and a <strong>testing</strong> set. A typical split is to use <span class="math notranslate nohighlight">\(80\%\)</span> of the data for training and the rest
for testing. This can be done as follows with our design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and data <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> (remember to import <strong>scikit-learn</strong>)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># split in training and test data
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)
</pre></div>
</div>
</div>
</div>
<p>Then we can use the standard scaler to scale our data as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
</pre></div>
</div>
</div>
</div>
<p>In this exercise we want you to to compute the MSE for the training
data and the test data as function of the complexity of a polynomial,
that is the degree of a given polynomial. We want you also to compute the <span class="math notranslate nohighlight">\(R2\)</span> score as function of the complexity of the model for both training data and test data.  You should also run the calculation with and without scaling.</p>
<p>One of
the aims is to reproduce Figure 2.11 of <a class="reference external" href="https://github.com/CompPhysics/MLErasmus/blob/master/doc/Textbooks/elementsstat.pdf">Hastie et al</a>.</p>
<p>Our data is defined by <span class="math notranslate nohighlight">\(x\in [-3,3]\)</span> with a total of for example <span class="math notranslate nohighlight">\(100\)</span> data points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>np.random.seed()
n = 100
maxdegree = 14
# Make data set.
x = np.linspace(-3, 3, n).reshape(-1, 1)
y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)
</pre></div>
</div>
</div>
</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> is the function we want to fit with a given polynomial.</p>
<!-- --- begin solution of exercise --- -->
<p><strong>Solution.</strong>
We present here the solution for the last exercise. All elements here can be used to solve exercises a) and b) as well.
Note that in this example we have used the polynomial fitting functions of <strong>scikit-learn</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline


np.random.seed(2018)
n = 30
maxdegree = 14
# Make data set.
x = np.linspace(-3, 3, n).reshape(-1, 1)
y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)
TestError = np.zeros(maxdegree)
TrainError = np.zeros(maxdegree)
polydegree = np.zeros(maxdegree)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)


for degree in range(maxdegree):
    model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False))
    clf = model.fit(x_train,y_train)
    y_fit = clf.predict(x_train)
    y_pred = clf.predict(x_test) 
    polydegree[degree] = degree
    TestError[degree] = np.mean( np.mean((y_test - y_pred)**2) )
    TrainError[degree] = np.mean( np.mean((y_train - y_fit)**2) )

plt.plot(polydegree, TestError, label=&#39;Test Error&#39;)
plt.plot(polydegree, TrainError, label=&#39;Train Error&#39;)
plt.legend()
plt.show()
</pre></div>
</div>
</div>
</div>
<!-- --- end solution of exercise --- --><p><strong>a)</strong>
Write a first code which sets up a design matrix <span class="math notranslate nohighlight">\(X\)</span> defined by a fifth-order polynomial.  Scale your data and split it in training and test data.</p>
<p><strong>b)</strong>
Perform an ordinary least squares and compute the means squared error and the <span class="math notranslate nohighlight">\(R2\)</span> factor for the training data and the test data, with and without scaling.</p>
<p><strong>c)</strong>
Add now a model which allows you to make polynomials up to degree <span class="math notranslate nohighlight">\(15\)</span>.  Perform a standard OLS fitting of the training data and compute the MSE and <span class="math notranslate nohighlight">\(R2\)</span> for the training and test data and plot both test and training data MSE and <span class="math notranslate nohighlight">\(R2\)</span> as functions of the polynomial degree. Compare what you see with Figure 2.11 of Hastie et al. Comment your results. For which polynomial degree do you find an optimal MSE (smallest value)?</p>
</section>
<section id="exercise-4-adding-ridge-regression">
<h2><span class="section-number">3.12. </span>Exercise 4: Adding Ridge Regression<a class="headerlink" href="#exercise-4-adding-ridge-regression" title="Link to this heading">#</a></h2>
<p>This exercise is a continuation of exercise 2. We will use the same function to
generate our data set, still staying with a simple function <span class="math notranslate nohighlight">\(y(x)\)</span>
which we want to fit using linear regression, but now extending the
analysis to include the Ridge regression method.</p>
<p>We will thus again generate our own dataset for a function <span class="math notranslate nohighlight">\(y(x)\)</span> where
<span class="math notranslate nohighlight">\(x \in [0,1]\)</span> and defined by random numbers computed with the uniform
distribution. The function <span class="math notranslate nohighlight">\(y\)</span> is a quadratic polynomial in <span class="math notranslate nohighlight">\(x\)</span> with
added stochastic noise according to the normal distribution <span class="math notranslate nohighlight">\(\cal{N}(0,1)\)</span>.</p>
<p>The following simple Python instructions define our <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> values (with 100 data points).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>x = np.random.rand(100)
y = 2.0+5*x*x+0.1*np.random.randn(100)
</pre></div>
</div>
</div>
</div>
<p>Write your own code for the Ridge method (see chapter 3.4 of Hastie <em>et al.</em>, equations (3.43) and (3.44)) and compute the parametrization for different values of <span class="math notranslate nohighlight">\(\lambda\)</span>. Compare and analyze your results with those from exercise 3. Study the dependence on <span class="math notranslate nohighlight">\(\lambda\)</span> while also varying the strength of the noise in your expression for <span class="math notranslate nohighlight">\(y(x)\)</span>.</p>
<p>Repeat the above but using the functionality of
<strong>Scikit-Learn</strong>. Compare your code with the results from
<strong>Scikit-Learn</strong>. Remember to run with the same random numbers for
generating <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.  Observe also that when you compare with <strong>Scikit-Learn</strong>, you need to pay attention to how the intercept is dealt with.</p>
<p>Finally, using <strong>Scikit-Learn</strong> or your own code, compute also the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error defined as</p>
<div class="math notranslate nohighlight">
\[
MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2,
\]</div>
<p>and the <span class="math notranslate nohighlight">\(R^2\)</span> score function.
If <span class="math notranslate nohighlight">\(\tilde{\hat{y}}_i\)</span> is the predicted value of the <span class="math notranslate nohighlight">\(i-th\)</span> sample and <span class="math notranslate nohighlight">\(y_i\)</span> is the corresponding true value, then the score <span class="math notranslate nohighlight">\(R^2\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
\]</div>
<p>where we have defined the mean value  of <span class="math notranslate nohighlight">\(\hat{y}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
\]</div>
<p>Discuss these quantities as functions of the variable <span class="math notranslate nohighlight">\(\lambda\)</span> in Ridge regression.</p>
<!-- --- begin solution of exercise --- -->
<p><strong>Solution.</strong>
The code here allows you to perform your own Ridge calculation and
perform calculations for various values of the regularization
parameter <span class="math notranslate nohighlight">\(\lambda\)</span>. This program can easily be extended upon.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import linear_model

def R2(y_data, y_model):
    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)
def MSE(y_data,y_model):
    n = np.size(y_model)
    return np.sum((y_data-y_model)**2)/n


# A seed just to ensure that the random numbers are the same for every run.
# Useful for eventual debugging.
np.random.seed(3155)

x = np.random.rand(100)
y = 2.0+5*x*x+0.1*np.random.randn(100)

# number of features p (here degree of polynomial
p = 3
#  The design matrix now as function of a given polynomial
X = np.zeros((len(x),p))
X[:,0] = 1.0
X[:,1] = x
X[:,2] = x*x
# We split the data in test and training data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# matrix inversion to find theta
OLStheta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train
print(OLStheta)
# and then make the prediction
ytildeOLS = X_train @ OLStheta
print(&quot;Training R2 for OLS&quot;)
print(R2(y_train,ytildeOLS))
print(&quot;Training MSE for OLS&quot;)
print(MSE(y_train,ytildeOLS))
ypredictOLS = X_test @ OLStheta
print(&quot;Test R2 for OLS&quot;)
print(R2(y_test,ypredictOLS))
print(&quot;Test MSE OLS&quot;)
print(MSE(y_test,ypredictOLS))


# Repeat now for Ridge regression and various values of the regularization parameter
I = np.eye(p,p)
# Decide which values of lambda to use
nlambdas = 20
OwnMSEPredict = np.zeros(nlambdas)
OwnMSETrain = np.zeros(nlambdas)
MSERidgePredict =  np.zeros(nlambdas)
lambdas = np.logspace(-4, 1, nlambdas)
for i in range(nlambdas):
    lmb = lambdas[i]
    OwnRidgetheta = np.linalg.inv(X_train.T @ X_train+lmb*I) @ X_train.T @ y_train
    # and then make the prediction
    OwnytildeRidge = X_train @ OwnRidgetheta
    OwnypredictRidge = X_test @ OwnRidgetheta
    OwnMSEPredict[i] = MSE(y_test,OwnypredictRidge)
    OwnMSETrain[i] = MSE(y_train,OwnytildeRidge)
    # Make the fit using Ridge from Sklearn
    RegRidge = linear_model.Ridge(lmb,fit_intercept=False)
    RegRidge.fit(X_train,y_train)
    # and then make the prediction
    ypredictRidge = RegRidge.predict(X_test)
    # Compute the MSE and print it
    MSERidgePredict[i] = MSE(y_test,ypredictRidge)

# Now plot the results
plt.figure()
plt.plot(np.log10(lambdas), OwnMSETrain, label = &#39;MSE Ridge train, Own code&#39;)
plt.plot(np.log10(lambdas), OwnMSEPredict, &#39;r--&#39;, label = &#39;MSE Ridge Test, Own code&#39;)
plt.plot(np.log10(lambdas), MSERidgePredict, &#39;g--&#39;, label = &#39;MSE Ridge Test, Sklearn code&#39;)
plt.xlabel(&#39;log10(lambda)&#39;)
plt.ylabel(&#39;MSE&#39;)
plt.legend()
plt.show()
</pre></div>
</div>
</div>
</div>
<!-- --- end solution of exercise --- --></section>
<section id="exercise-5-analytical-exercises">
<h2><span class="section-number">3.13. </span>Exercise 5: Analytical exercises<a class="headerlink" href="#exercise-5-analytical-exercises" title="Link to this heading">#</a></h2>
<p>In this exercise we derive the expressions for various derivatives of
products of vectors and matrices. Such derivatives are central to the
optimization of various cost functions. Although we will often use
automatic differentiation in actual calculations, to be able to have
analytical expressions is extremely helpful in case we have simpler
derivatives as well as when we analyze various properties (like second
derivatives) of the chosen cost functions.  Vectors are always written
as boldfaced lower case letters and matrices as upper case boldfaced
letters.</p>
<p>Show that</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial (\boldsymbol{b}^T\boldsymbol{a})}{\partial \boldsymbol{a}} = \boldsymbol{b},
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial (\boldsymbol{a}^T\boldsymbol{A}\boldsymbol{a})}{\partial \boldsymbol{a}} = \boldsymbol{a}^T(\boldsymbol{A}+\boldsymbol{A}^T),
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \left(\boldsymbol{x}-\boldsymbol{A}\boldsymbol{s}\right)^T\left(\boldsymbol{x}-\boldsymbol{A}\boldsymbol{s}\right)}{\partial \boldsymbol{s}} = -2\left(\boldsymbol{x}-\boldsymbol{A}\boldsymbol{s}\right)^T\boldsymbol{A},
\]</div>
<p>and finally find the second derivative of this function with respect to the vector <span class="math notranslate nohighlight">\(\boldsymbol{s}\)</span>.</p>
<!-- --- begin solution of exercise --- -->
<p><strong>Solution.</strong>
In these exercises it is always useful to write out with summation indices the various quantities.
As an example, consider the function</p>
<div class="math notranslate nohighlight">
\[
f(\boldsymbol{x}) =\boldsymbol{A}\boldsymbol{x},
\]</div>
<p>which reads for a specific component <span class="math notranslate nohighlight">\(f_i\)</span> (we define the matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> to have dimension <span class="math notranslate nohighlight">\(n\times n\)</span> and the vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> to have length <span class="math notranslate nohighlight">\(n\)</span>)</p>
<div class="math notranslate nohighlight">
\[
f_i =\sum_{j=0}^{n-1}a_{ij}x_j,
\]</div>
<p>which leads to</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f_i}{\partial x_j}= a_{ij},
\]</div>
<p>and written out in terms of the vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{x}}= \boldsymbol{A}.
\]</div>
<p>For the first derivative</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial (\boldsymbol{b}^T\boldsymbol{a})}{\partial \boldsymbol{a}} = \boldsymbol{b},
\]</div>
<p>we can write out the inner product as (assuming all elements are real)</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{b}^T\boldsymbol{a}=\sum_i b_ia_i,
\]</div>
<p>taking the derivative</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \left( \sum_i b_ia_i\right)}{\partial a_k}= b_k,
\]</div>
<p>leading to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \boldsymbol{b}^T\boldsymbol{a}}{\partial \boldsymbol{a}}= \begin{bmatrix} b_0 \\ b_1 \\ b_2 \\ \dots \\ \dots \\ b_{n-1}\end{bmatrix} = \boldsymbol{b}.
\end{split}\]</div>
<p>For the second exercise we have</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial (\boldsymbol{a}^T\boldsymbol{A}\boldsymbol{a})}{\partial \boldsymbol{a}}.
\]</div>
<p>Defining a vector <span class="math notranslate nohighlight">\(\boldsymbol{f}=\boldsymbol{A}\boldsymbol{a}\)</span> with components <span class="math notranslate nohighlight">\(f_i=\sum_ja_{ij}a_i\)</span>  we have</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial (\boldsymbol{a}^T\boldsymbol{f})}{\partial \boldsymbol{a}}=\boldsymbol{a}^T\boldsymbol{A}+\boldsymbol{f}^T=\boldsymbol{a}^T\left(\boldsymbol{A}+\boldsymbol{A}^T\right),
\]</div>
<p>since <span class="math notranslate nohighlight">\(f\)</span> depends on <span class="math notranslate nohighlight">\(a\)</span> and we have used the chain rule for derivatives on the derivative of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(a\)</span>.</p>
<!-- --- end solution of exercise --- --></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="linalg.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2. </span>Linear Algebra, Handling of Arrays and more Python Features</p>
      </div>
    </a>
    <a class="right-next"
       href="chapter2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Ridge and Lasso Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">3.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-machine-learning">3.2. What is Machine Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-frequentist-approach-to-data-analysis">3.2.1. A Frequentist approach to data analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-good-model">3.2.2. What is a good model?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression-model-using-scikit-learn">3.3. Simple linear regression model using <strong>scikit-learn</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#organizing-our-data">3.3.1. Organizing our data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-basic-elements">3.4. Linear Regression, basic elements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chi-2-function">3.4.1. The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-an-equation-of-state-for-dense-nuclear-matter">3.4.2. Fitting an Equation of State for Dense Nuclear Matter</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-our-data-in-training-and-test-data">3.5. Splitting our Data in Training and Test data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reducing-the-number-of-degrees-of-freedom-overarching-view">3.6. Reducing the number of degrees of freedom, overarching view</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-the-means-squared-error-as-function-of-complexity">3.7. Testing the Means Squared Error as function of Complexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">3.8. Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-setting-up-various-python-environments">3.9. Exercise 1: Setting up various Python environments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-making-your-own-data-and-exploring-scikit-learn">3.10. Exercise 2: making your own data and exploring scikit-learn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-normalizing-our-data">3.11. Exercise 3: Normalizing our data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-adding-ridge-regression">3.12. Exercise 4: Adding Ridge Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-5-analytical-exercises">3.13. Exercise 5: Analytical exercises</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Morten Hjorth-Jensen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>