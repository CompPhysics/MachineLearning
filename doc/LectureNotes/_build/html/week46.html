
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 46: Decision Trees, Ensemble methods and Random Forests &#8212; Applied Data Analysis and Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week46';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Week 47: Recurrent neural networks and Autoencoders" href="week47.html" />
    <link rel="prev" title="Week 45, Convolutional Neural Networks (CCNs)" href="week45.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Applied Data Analysis and Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Applied Data Analysis and Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Applied Data Analysis and Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About the course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Course setting</a></li>
<li class="toctree-l1"><a class="reference internal" href="teachers.html">Teachers and Grading</a></li>
<li class="toctree-l1"><a class="reference internal" href="textbooks.html">Textbooks</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Review of Statistics with Resampling Techniques and Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="statistics.html">1. Elements of Probability Theory and Statistical Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">2. Linear Algebra, Handling of Arrays and more Python Features</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Regression to Support Vector Machines</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter1.html">3. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter2.html">4. Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter3.html">5. Resampling Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter4.html">6. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapteroptimization.html">7. Optimization, the central part of any Machine Learning algortithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter5.html">8. Support Vector Machines, overarching aims</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Decision Trees, Ensemble Methods and Boosting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter6.html">9. Decision trees, overarching aims</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter7.html">10. Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Dimensionality Reduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter8.html">11. Basic ideas of the Principal Component Analysis (PCA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">12. Clustering and Unsupervised Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter9.html">13. Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter10.html">14. Building a Feed Forward Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter11.html">15. Solving Differential Equations  with Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter12.html">16. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter13.html">17. Recurrent neural networks: Overarching view</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Weekly material, notes and exercises</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="exercisesweek34.html">Exercises week 34</a></li>
<li class="toctree-l1"><a class="reference internal" href="week34.html">Week 34: Introduction to the course, Logistics and Practicalities</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek35.html">Exercises week 35</a></li>
<li class="toctree-l1"><a class="reference internal" href="week35.html">Week 35: From Ordinary Linear Regression to Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek36.html">Exercises week 36</a></li>
<li class="toctree-l1"><a class="reference internal" href="week36.html">Week 36: Linear Regression and Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek37.html">Exercises week 37</a></li>
<li class="toctree-l1"><a class="reference internal" href="week37.html">Week 37: Gradient descent methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek38.html">Exercises week 38</a></li>
<li class="toctree-l1"><a class="reference internal" href="week38.html">Week 38: Statistical analysis, bias-variance tradeoff and resampling methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek39.html">Exercises week 39</a></li>
<li class="toctree-l1"><a class="reference internal" href="week39.html">Week 39: Resampling methods and logistic regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="week40.html">Week 40: Gradient descent methods (continued) and start Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="week41.html">Week 41 Neural networks and constructing a neural network code</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek41.html">Exercises week 41</a></li>








<li class="toctree-l1"><a class="reference internal" href="week42.html">Week 42 Constructing a Neural Network code with examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek42.html">Exercises week 42</a></li>









<li class="toctree-l1"><a class="reference internal" href="week43.html">Week 43: Deep Learning: Constructing a Neural Network code and solving differential equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek43.html">Exercises week 43</a></li>

<li class="toctree-l1"><a class="reference internal" href="week44.html">Week 44,  Solving differential equations with neural networks and start Convolutional Neural Networks (CNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek44.html">Exercises week 44</a></li>

<li class="toctree-l1"><a class="reference internal" href="week45.html">Week 45,  Convolutional Neural Networks (CCNs)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 46: Decision Trees, Ensemble methods  and Random Forests</a></li>
<li class="toctree-l1"><a class="reference internal" href="week47.html">Week 47: Recurrent neural networks and Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek47.html">Exercise week 47-48</a></li>

<li class="toctree-l1"><a class="reference internal" href="week48.html">Week 48: Autoencoders and summary of course</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="project1.html">Project 1 on Machine Learning, deadline October 6 (midnight), 2025</a></li>
<li class="toctree-l1"><a class="reference internal" href="project2.html">Project 2 on Machine Learning, deadline November 10 (Midnight)</a></li>
<li class="toctree-l1"><a class="reference internal" href="project3.html">Project 3 on Machine Learning, deadline December 15 (midnight), 2025</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/week46.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 46: Decision Trees, Ensemble methods  and Random Forests</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plan-for-week-46">Plan for week 46</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-recommendations">Reading recommendations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorflow-examples">TensorFlow examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-nns-and-cnns-to-recurrent-neural-networks-rnns">From NNs and CNNs to recurrent neural networks (RNNs)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-recurrent-nn">What is a recurrent NN?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-rnns">Why RNNs?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feedback-connections">Feedback connections</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vanishing-gradients">Vanishing gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-neural-networks-rnns-overarching-view">Recurrent neural networks (RNNs): Overarching view</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-data-only">Sequential data only?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differential-equations">Differential equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-regression-example-using-tensorflow-with-keras">A simple regression example using TensorFlow with Keras</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#corresponding-example-using-pytorch">Corresponding example using PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns">RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-kinds-of-behaviour-can-rnns-exhibit">What kinds of behaviour can RNNs exhibit?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-layout-figures-from-sebastian-rashcka-et-al-machine-learning-with-sickit-learn-and-pytorch">Basic layout,  Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-differential-equations-with-rnns">Solving differential equations with RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-first-order-differential-equations">Two first-order differential equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#velocity-only">Velocity only</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linking-with-rnns">Linking with RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minor-rewrite">Minor rewrite</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail">RNNs in more detail</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-2">RNNs in more detail, part 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-3">RNNs in more detail, part 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-4">RNNs in more detail, part 4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-5">RNNs in more detail, part 5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-6">RNNs in more detail, part 6</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-7">RNNs in more detail, part 7</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-through-time">Backpropagation through time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-pass-is-linear">The backward pass is linear</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-of-exploding-or-vanishing-gradients">The problem of exploding or vanishing gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-setup">Mathematical setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-in-time-through-figures-part-1">Back propagation in time through figures, part 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-in-time-part-2">Back propagation in time, part 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-in-time-part-3">Back propagation in time, part 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-in-time-part-4">Back propagation in time, part 4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-in-time-in-equations">Back propagation in time in equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule-again">Chain rule again</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-of-loss-functions">Gradients of loss functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-rnns">Summary of RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-a-typical-rnn">Summary of a  typical RNN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#four-effective-ways-to-learn-an-rnn">Four effective ways to learn an RNN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mathematics-of-rnns-the-basic-architecture">The mathematics of RNNs, the basic architecture</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)
doconce format html week46.do.txt --no_mako -->
<!-- dom:TITLE: Week 46: Decision Trees, Ensemble methods  and Random Forests --><section class="tex2jax_ignore mathjax_ignore" id="week-46-decision-trees-ensemble-methods-and-random-forests">
<h1>Week 46: Decision Trees, Ensemble methods  and Random Forests<a class="headerlink" href="#week-46-decision-trees-ensemble-methods-and-random-forests" title="Link to this heading">#</a></h1>
<p><strong>Morten Hjorth-Jensen</strong>, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</p>
<p>Date: <strong>Week 46, November 10-14</strong></p>
<section id="plan-for-week-46">
<h2>Plan for week 46<a class="headerlink" href="#plan-for-week-46" title="Link to this heading">#</a></h2>
<p><strong>Material for the lecture on Monday November 10, 2025.</strong></p>
<ol class="arabic simple">
<li><p>Intro to and mathematics of  Recurrent Neural Networks (RNNs)</p></li>
<li><p>Lecture notes at <a class="github reference external" href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week46/ipynb/week46.ipynb">CompPhysics/MachineLearning</a></p></li>
<li><p>Video of lecture at <a class="reference external" href="https://youtu.be/WNWKaBbqOpg">https://youtu.be/WNWKaBbqOpg</a></p></li>
<li><p>Whiteboard notes at <a class="github reference external" href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2025/FYSSTKweek46.pdf">CompPhysics/MachineLearning</a></p></li>
</ol>
<p><strong>Lab sessions on Tuesday and Wednesday.</strong></p>
<ol class="arabic simple">
<li><p>Work on and discussions of project 3, deadline December 15 at midnight. Project 3 can be found at <a class="github reference external" href="https://github.com/CompPhysics/MachineLearning/tree/master/doc/Projects/2025/Project3">CompPhysics/MachineLearning</a>. <strong>Note that possible minor revisions may be done before the lab sessions start on November 11</strong>.</p></li>
</ol>
</section>
<section id="reading-recommendations">
<h2>Reading recommendations<a class="headerlink" href="#reading-recommendations" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>For RNNs, see Goodfellow et al chapter 10, see <a class="reference external" href="https://www.deeplearningbook.org/contents/rnn.html">https://www.deeplearningbook.org/contents/rnn.html</a>.</p></li>
<li><p>Reading suggestions for implementation of RNNs in PyTorch: see Rashcka et al.’s chapter 15 and GitHub site at <a class="github reference external" href="https://github.com/rasbt/machine-learning-book/tree/main/ch15">rasbt/machine-learning-book</a>.</p></li>
</ol>
</section>
<section id="tensorflow-examples">
<h2>TensorFlow examples<a class="headerlink" href="#tensorflow-examples" title="Link to this heading">#</a></h2>
<p>For TensorFlow (using Keras) implementations, we recommend</p>
<ol class="arabic simple">
<li><p>David Foster, Generative Deep Learning with TensorFlow, see chapter 5 at <a class="reference external" href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html">https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html</a></p></li>
<li><p>Joseph Babcock and Raghav Bali Generative AI with Python and their GitHub link, chapters 2 and  3 at <a class="github reference external" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2">PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2</a></p></li>
</ol>
</section>
<section id="from-nns-and-cnns-to-recurrent-neural-networks-rnns">
<h2>From NNs and CNNs to recurrent neural networks (RNNs)<a class="headerlink" href="#from-nns-and-cnns-to-recurrent-neural-networks-rnns" title="Link to this heading">#</a></h2>
<p>There are limitation of NNs, one of which being that FFNNs are not
designed to handle sequential data (data for which the order matters)
effectively because they lack the capabilities of storing information
about previous inputs; each input is being treated indepen-
dently. This is a limitation when dealing with sequential data where
past information can be vital to correctly process current and future
inputs.</p>
</section>
<section id="what-is-a-recurrent-nn">
<h2>What is a recurrent NN?<a class="headerlink" href="#what-is-a-recurrent-nn" title="Link to this heading">#</a></h2>
<p>A recurrent neural network (RNN), as opposed to a regular fully
connected neural network (FCNN) or just neural network (NN), has
layers that are connected to themselves.</p>
<p>In an FCNN there are no connections between nodes in a single
layer. For instance, <span class="math notranslate nohighlight">\((h_1^1\)</span> is not connected to <span class="math notranslate nohighlight">\((h_2^1\)</span>. In
addition, the input and output are always of a fixed length.</p>
<p>In an RNN, however, this is no longer the case. Nodes in the hidden
layers are connected to themselves.</p>
</section>
<section id="why-rnns">
<h2>Why RNNs?<a class="headerlink" href="#why-rnns" title="Link to this heading">#</a></h2>
<p>Recurrent neural networks work very well when working with
sequential data, that is data where the order matters. In a regular
fully connected network, the order of input doesn’t really matter.</p>
<p>Another property of  RNNs is that they can handle variable input
and output. Consider again the simplified breast cancer dataset. If you
have trained a regular FCNN on the dataset with the two features, it
makes no sense to suddenly add a third feature. The network would not
know what to do with it, and would reject such inputs with three
features (or any other number of features that isn’t two, for that
matter).</p>
</section>
<section id="feedback-connections">
<h2>Feedback connections<a class="headerlink" href="#feedback-connections" title="Link to this heading">#</a></h2>
<p>In contrast to NNs, recurrent networks introduce feedback
connections, meaning the information is allowed to be carried to
subsequent nodes across different time steps. These cyclic or feedback
connections have the objective of providing the network with some kind
of memory, making RNNs particularly suited for time- series data,
natural language processing, speech recognition, and several other
problems for which the order of the data is crucial.  The RNN
architectures vary greatly in how they manage information flow and
memory in the network.</p>
</section>
<section id="vanishing-gradients">
<h2>Vanishing gradients<a class="headerlink" href="#vanishing-gradients" title="Link to this heading">#</a></h2>
<p>Different architectures often aim at improving
some sub-optimal characteristics of the network. The simplest form of
recurrent network, commonly called simple or vanilla RNN, for example,
is known to suffer from the problem of vanishing gradients. This
problem arises due to the nature of backpropagation in time. Gradients
of the cost/loss function may get exponentially small (or large) if
there are many layers in the network, which is the case of RNN when
the sequence gets long.</p>
</section>
<section id="recurrent-neural-networks-rnns-overarching-view">
<h2>Recurrent neural networks (RNNs): Overarching view<a class="headerlink" href="#recurrent-neural-networks-rnns-overarching-view" title="Link to this heading">#</a></h2>
<p>Till now our focus has been, including convolutional neural networks
as well, on feedforward neural networks. The output or the activations
flow only in one direction, from the input layer to the output layer.</p>
<p>A recurrent neural network (RNN) looks very much like a feedforward
neural network, except that it also has connections pointing
backward.</p>
<p>RNNs are used to analyze time series data such as stock prices, and
tell you when to buy or sell. In autonomous driving systems, they can
anticipate car trajectories and help avoid accidents. More generally,
they can work on sequences of arbitrary lengths, rather than on
fixed-sized inputs like all the nets we have discussed so far. For
example, they can take sentences, documents, or audio samples as
input, making them extremely useful for natural language processing
systems such as automatic translation and speech-to-text.</p>
</section>
<section id="sequential-data-only">
<h2>Sequential data only?<a class="headerlink" href="#sequential-data-only" title="Link to this heading">#</a></h2>
<p>An important issue is that in many deep learning methods we assume
that the input and output data can be treated as independent and
identically distributed, normally abbreviated to <strong>iid</strong>.
This means that the data we use can be seen as mutually independent.</p>
<p>This is however not the case for most data sets used in RNNs since we
are dealing with sequences of data with strong inter-dependencies.
This applies in particular to time series, which are sequential by
contruction.</p>
</section>
<section id="differential-equations">
<h2>Differential equations<a class="headerlink" href="#differential-equations" title="Link to this heading">#</a></h2>
<p>As an example, the solutions of ordinary differential equations can be
represented as a time series, similarly, how stock prices evolve as
function of time is another example of a typical time series, or voice
records and many other examples.</p>
<p>Not all sequential data may however have a time stamp, texts being a
typical example thereof, or DNA sequences.</p>
<p>The main focus here is on data that can be structured either as time
series or as ordered series of data.  We will not focus on for example
natural language processing or similar data sets.</p>
</section>
<section id="a-simple-regression-example-using-tensorflow-with-keras">
<h2>A simple regression example using TensorFlow with Keras<a class="headerlink" href="#a-simple-regression-example-using-tensorflow-with-keras" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>%matplotlib inline

# Start importing packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model, Sequential 
from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU
from tensorflow.keras import optimizers     
from tensorflow.keras import regularizers           
from tensorflow.keras.utils import to_categorical 

# convert into dataset matrix
def convertToMatrix(data, step):
 X, Y =[], []
 for i in range(len(data)-step):
  d=i+step  
  X.append(data[i:d,])
  Y.append(data[d,])
 return np.array(X), np.array(Y)

step = 4
N = 1000    
Tp = 800    

t=np.arange(0,N)
x=np.sin(0.02*t)+2*np.random.rand(N)
df = pd.DataFrame(x)
df.head()
# Setting up training data
values=df.values
train,test = values[0:Tp,:], values[Tp:N,:]
# add step elements into train and test
test = np.append(test,np.repeat(test[-1,],step))
train = np.append(train,np.repeat(train[-1,],step))
trainX,trainY =convertToMatrix(train,step)
testX,testY =convertToMatrix(test,step)
trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
# Defining the model with a simple RNN
model = Sequential()
model.add(SimpleRNN(units=32, input_shape=(1,step), activation=&quot;relu&quot;))
model.add(Dense(8, activation=&quot;relu&quot;)) 
model.add(Dense(1))
model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;rmsprop&#39;)
model.summary()
# Training
model.fit(trainX,trainY, epochs=100, batch_size=16, verbose=2)
trainPredict = model.predict(trainX)
testPredict= model.predict(testX)
predicted=np.concatenate((trainPredict,testPredict),axis=0)
trainScore = model.evaluate(trainX, trainY, verbose=0)
print(trainScore)
plt.plot(df)
plt.plot(predicted)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="corresponding-example-using-pytorch">
<h2>Corresponding example using PyTorch<a class="headerlink" href="#corresponding-example-using-pytorch" title="Link to this heading">#</a></h2>
<p>The structure of the code here is as follows</p>
<ol class="arabic simple">
<li><p>Generate a sine function  and splits it into training and validation sets</p></li>
<li><p>Create a custom data set for sequence generation</p></li>
<li><p>Define an RNN model with one RNN layer and a final plain linear layer</p></li>
<li><p>Train the model using the mean-squared error as cost function and the Adam optimizer</p></li>
<li><p>Generate predictions using recursive forecasting</p></li>
<li><p>Plot the results and training/validation loss curves</p></li>
</ol>
<p>The model takes sequences of 20 previous values to predict the next
value of the sine function. The recursive prediction uses the model’s own
predictions to generate future values, showing how well it maintains
the sine wave pattern over time.</p>
<p>The final plots show the the predicted values vs. the actual sine wave for the validation period
and the training and validation cost function curves to monitor for overfitting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic sine wave data
t = torch.linspace(0, 4*np.pi, 1000)
data = torch.sin(t)

# Split data into training and validation
train_data = data[:800]
val_data = data[800:]

# Hyperparameters
seq_len = 20
batch_size = 32
hidden_size = 64
num_epochs = 100
learning_rate = 0.001

# Create dataset and dataloaders
class SineDataset(torch.utils.data.Dataset):
   def __init__(self, data, seq_len):
       self.data = data
       self.seq_len = seq_len

   def __len__(self):
       return len(self.data) - self.seq_len

   def __getitem__(self, idx):
       x = self.data[idx:idx+self.seq_len]
       y = self.data[idx+self.seq_len]
       return x.unsqueeze(-1), y  # Add feature dimension

train_dataset = SineDataset(train_data, seq_len)
val_dataset = SineDataset(val_data, seq_len)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Define RNN model
class RNNModel(nn.Module):
   def __init__(self, input_size, hidden_size, output_size):
       super(RNNModel, self).__init__()
       self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
       self.fc = nn.Linear(hidden_size, output_size)

   def forward(self, x):
       out, _ = self.rnn(x)  # out: (batch_size, seq_len, hidden_size)
       out = out[:, -1, :]   # Take last time step output
       out = self.fc(out)
       return out

model = RNNModel(input_size=1, hidden_size=hidden_size, output_size=1)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
train_losses = []
val_losses = []

for epoch in range(num_epochs):
   model.train()
   epoch_train_loss = 0
   for x_batch, y_batch in train_loader:
       optimizer.zero_grad()
       y_pred = model(x_batch)
       loss = criterion(y_pred, y_batch.unsqueeze(-1))
       loss.backward()
       optimizer.step()
       epoch_train_loss += loss.item()

   # Validation
   model.eval()
   epoch_val_loss = 0
   with torch.no_grad():
       for x_val, y_val in val_loader:
           y_pred_val = model(x_val)
           val_loss = criterion(y_pred_val, y_val.unsqueeze(-1))
           epoch_val_loss += val_loss.item()

   # Calculate average losses
   train_loss = epoch_train_loss / len(train_loader)
   val_loss = epoch_val_loss / len(val_loader)
   train_losses.append(train_loss)
   val_losses.append(val_loss)

   print(f&#39;Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}&#39;)

# Generate predictions
model.eval()
initial_sequence = train_data[-seq_len:].reshape(1, seq_len, 1)
predictions = []
current_sequence = initial_sequence.clone()

with torch.no_grad():
   for _ in range(len(val_data)):
       pred = model(current_sequence)
       predictions.append(pred.item())
       # Update sequence by removing first element and adding new prediction
       current_sequence = torch.cat([current_sequence[:, 1:, :], pred.unsqueeze(1)], dim=1)


# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label=&#39;Training Loss&#39;)
plt.plot(val_losses, label=&#39;Validation Loss&#39;)
plt.title(&#39;Training and Validation Loss&#39;)
plt.xlabel(&#39;Epoch&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="rnns">
<h2>RNNs<a class="headerlink" href="#rnns" title="Link to this heading">#</a></h2>
<p>RNNs are very powerful, because they
combine two properties:</p>
<ol class="arabic simple">
<li><p>Distributed hidden state that allows them to store a lot of information about the past efficiently.</p></li>
<li><p>Non-linear dynamics that allows them to update their hidden state in complicated ways.</p></li>
</ol>
<p>With enough neurons and time, RNNs
can compute anything that can be
computed by your computer.</p>
</section>
<section id="what-kinds-of-behaviour-can-rnns-exhibit">
<h2>What kinds of behaviour can RNNs exhibit?<a class="headerlink" href="#what-kinds-of-behaviour-can-rnns-exhibit" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>They can oscillate.</p></li>
<li><p>They can settle to point attractors.</p></li>
<li><p>They can behave chaotically.</p></li>
<li><p>RNNs could potentially learn to implement lots of small programs that each capture a nugget of knowledge and run in parallel, interacting to produce very complicated effects.</p></li>
</ol>
<p>But the extensive computational needs  of RNNs makes them very hard to train.</p>
</section>
<section id="basic-layout-figures-from-sebastian-rashcka-et-al-machine-learning-with-sickit-learn-and-pytorch">
<h2>Basic layout,  <a class="reference external" href="https://sebastianraschka.com/blog/2022/ml-pytorch-book.html">Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch</a><a class="headerlink" href="#basic-layout-figures-from-sebastian-rashcka-et-al-machine-learning-with-sickit-learn-and-pytorch" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN1.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN1.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="solving-differential-equations-with-rnns">
<h2>Solving differential equations with RNNs<a class="headerlink" href="#solving-differential-equations-with-rnns" title="Link to this heading">#</a></h2>
<p>To gain some intuition on how we can use RNNs for time series, let us
tailor the representation of the solution of a differential equation
as a time series.</p>
<p>Consider the famous differential equation (Newton’s equation of motion for damped harmonic oscillations, scaled in terms of dimensionless time)</p>
<div class="math notranslate nohighlight">
\[
\frac{d^2x}{dt^2}+\eta\frac{dx}{dt}+x(t)=F(t),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta\)</span> is a constant used in scaling time into a dimensionless variable and <span class="math notranslate nohighlight">\(F(t)\)</span> is an external force acting on the system.
The constant <span class="math notranslate nohighlight">\(\eta\)</span> is a so-called damping.</p>
</section>
<section id="two-first-order-differential-equations">
<h2>Two first-order differential equations<a class="headerlink" href="#two-first-order-differential-equations" title="Link to this heading">#</a></h2>
<p>In solving the above second-order equation, it is common to rewrite it in terms of two coupled first-order equations
with the velocity</p>
<div class="math notranslate nohighlight">
\[
v(t)=\frac{dx}{dt},
\]</div>
<p>and the acceleration</p>
<div class="math notranslate nohighlight">
\[
\frac{dv}{dt}=F(t)-\eta v(t)-x(t).
\]</div>
<p>With the initial conditions <span class="math notranslate nohighlight">\(v_0=v(t_0)\)</span> and <span class="math notranslate nohighlight">\(x_0=x(t_0)\)</span> defined, we can integrate these equations and find their respective solutions.</p>
</section>
<section id="velocity-only">
<h2>Velocity only<a class="headerlink" href="#velocity-only" title="Link to this heading">#</a></h2>
<p>Let us focus on the velocity only. Discretizing and using the simplest
possible approximation for the derivative, we have Euler’s forward
method for the updated velocity at a time step <span class="math notranslate nohighlight">\(i+1\)</span> given by</p>
<div class="math notranslate nohighlight">
\[
v_{i+1}=v_i+\Delta t \frac{dv}{dt}_{\vert_{v=v_i}}=v_i+\Delta t\left(F_i-\eta v_i-x_i\right).
\]</div>
<p>Defining a function</p>
<div class="math notranslate nohighlight">
\[
h_i(x_i,v_i,F_i)=v_i+\Delta t\left(F_i-\eta v_i-x_i\right),
\]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
v_{i+1}=h_i(x_i,v_i,F_i).
\]</div>
</section>
<section id="linking-with-rnns">
<h2>Linking with RNNs<a class="headerlink" href="#linking-with-rnns" title="Link to this heading">#</a></h2>
<p>The equation</p>
<div class="math notranslate nohighlight">
\[
v_{i+1}=h_i(x_i,v_i,F_i).
\]</div>
<p>can be used to train a feed-forward neural network with inputs <span class="math notranslate nohighlight">\(v_i\)</span> and outputs <span class="math notranslate nohighlight">\(v_{i+1}\)</span> at a time <span class="math notranslate nohighlight">\(t_i\)</span>. But we can think of this also as a recurrent neural network
with inputs <span class="math notranslate nohighlight">\(v_i\)</span>, <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(F_i\)</span> at each time step <span class="math notranslate nohighlight">\(t_i\)</span>, and producing an output <span class="math notranslate nohighlight">\(v_{i+1}\)</span>.</p>
<p>Noting that</p>
<div class="math notranslate nohighlight">
\[
v_{i}=v_{i-1}+\Delta t\left(F_{i-1}-\eta v_{i-1}-x_{i-1}\right)=h_{i-1}.
\]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
v_{i}=h_{i-1}(x_{i-1},v_{i-1},F_{i-1}),
\]</div>
<p>and we can rewrite</p>
<div class="math notranslate nohighlight">
\[
v_{i+1}=h_i(x_i,h_{i-1},F_i).
\]</div>
</section>
<section id="minor-rewrite">
<h2>Minor rewrite<a class="headerlink" href="#minor-rewrite" title="Link to this heading">#</a></h2>
<p>We can thus set up a recurring series which depends on the inputs <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(F_i\)</span> and the previous values <span class="math notranslate nohighlight">\(h_{i-1}\)</span>.
We assume now that the inputs at each step (or time <span class="math notranslate nohighlight">\(t_i\)</span>) is given by <span class="math notranslate nohighlight">\(x_i\)</span> only and we denote the outputs for <span class="math notranslate nohighlight">\(\tilde{y}_i\)</span> instead of <span class="math notranslate nohighlight">\(v_{i_1}\)</span>, we have then the compact equation for our outputs at each step <span class="math notranslate nohighlight">\(t_i\)</span></p>
<div class="math notranslate nohighlight">
\[
y_{i}=h_i(x_i,h_{i-1}).
\]</div>
<p>We can think of this as an element in a recurrent network where our
network (our model) produces an output <span class="math notranslate nohighlight">\(y_i\)</span> which is then compared
with a target value through a given cost/loss function that we
optimize. The target values at a given step <span class="math notranslate nohighlight">\(t_i\)</span> could be the results
of a measurement or simply the analytical results of a differential
equation.</p>
</section>
<section id="rnns-in-more-detail">
<h2>RNNs in more detail<a class="headerlink" href="#rnns-in-more-detail" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN2.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN2.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="rnns-in-more-detail-part-2">
<h2>RNNs in more detail, part 2<a class="headerlink" href="#rnns-in-more-detail-part-2" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN3.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN3.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="rnns-in-more-detail-part-3">
<h2>RNNs in more detail, part 3<a class="headerlink" href="#rnns-in-more-detail-part-3" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN4.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN4.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="rnns-in-more-detail-part-4">
<h2>RNNs in more detail, part 4<a class="headerlink" href="#rnns-in-more-detail-part-4" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN5.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN5.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="rnns-in-more-detail-part-5">
<h2>RNNs in more detail, part 5<a class="headerlink" href="#rnns-in-more-detail-part-5" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN6.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN6.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="rnns-in-more-detail-part-6">
<h2>RNNs in more detail, part 6<a class="headerlink" href="#rnns-in-more-detail-part-6" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN7.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN7.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="rnns-in-more-detail-part-7">
<h2>RNNs in more detail, part 7<a class="headerlink" href="#rnns-in-more-detail-part-7" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN8.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN8.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="backpropagation-through-time">
<h2>Backpropagation through time<a class="headerlink" href="#backpropagation-through-time" title="Link to this heading">#</a></h2>
<p>We can think of the recurrent net as a layered, feed-forward
net with shared weights and then train the feed-forward net
with weight constraints.</p>
<p>We can also think of this training algorithm in the time domain:</p>
<ol class="arabic simple">
<li><p>The forward pass builds up a stack of the activities of all the units at each time step.</p></li>
<li><p>The backward pass peels activities off the stack to compute the error derivatives at each time step.</p></li>
<li><p>After the backward pass we add together the derivatives at all the different times for each weight.</p></li>
</ol>
</section>
<section id="the-backward-pass-is-linear">
<h2>The backward pass is linear<a class="headerlink" href="#the-backward-pass-is-linear" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>There is a big difference between the forward and backward passes.</p></li>
<li><p>In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding.</p></li>
<li><p>The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double.</p></li>
</ol>
<p>The forward pass determines the slope of the linear function used for
backpropagating through each neuron</p>
</section>
<section id="the-problem-of-exploding-or-vanishing-gradients">
<h2>The problem of exploding or vanishing gradients<a class="headerlink" href="#the-problem-of-exploding-or-vanishing-gradients" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>What happens to the magnitude of the gradients as we backpropagate through many layers?</p></li>
</ul>
<p>a. If the weights are small, the gradients shrink exponentially.</p>
<p>b. If the weights are big the gradients grow exponentially.</p>
<ul class="simple">
<li><p>Typical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.</p></li>
<li><p>In an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.</p></li>
</ul>
<p>a. We can avoid this by initializing the weights very carefully.</p>
<ul class="simple">
<li><p>Even with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.</p></li>
</ul>
<p>RNNs have difficulty dealing with long-range dependencies.</p>
</section>
<section id="mathematical-setup">
<h2>Mathematical setup<a class="headerlink" href="#mathematical-setup" title="Link to this heading">#</a></h2>
<p>The expression for the simplest Recurrent network resembles that of a
regular feed-forward neural network, but now with
the concept of temporal dependencies</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \mathbf{a}^{(t)} &amp; = U * \mathbf{x}^{(t)} + W * \mathbf{h}^{(t-1)} + \mathbf{b}, \notag \\
    \mathbf{h}^{(t)} &amp;= \sigma_h(\mathbf{a}^{(t)}), \notag\\
    \mathbf{y}^{(t)} &amp;= V * \mathbf{h}^{(t)} + \mathbf{c}, \notag\\
    \mathbf{\hat{y}}^{(t)} &amp;= \sigma_y(\mathbf{y}^{(t)}).
\end{align*}
\end{split}\]</div>
</section>
<section id="back-propagation-in-time-through-figures-part-1">
<h2>Back propagation in time through figures, part 1<a class="headerlink" href="#back-propagation-in-time-through-figures-part-1" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN9.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN9.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="back-propagation-in-time-part-2">
<h2>Back propagation in time, part 2<a class="headerlink" href="#back-propagation-in-time-part-2" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN10.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN10.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="back-propagation-in-time-part-3">
<h2>Back propagation in time, part 3<a class="headerlink" href="#back-propagation-in-time-part-3" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN11.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN11.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="back-propagation-in-time-part-4">
<h2>Back propagation in time, part 4<a class="headerlink" href="#back-propagation-in-time-part-4" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/RNN12.png, width=700 frac=0.9] -->
<!-- begin figure -->
<p><img src="figslides/RNN12.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="back-propagation-in-time-in-equations">
<h2>Back propagation in time in equations<a class="headerlink" href="#back-propagation-in-time-in-equations" title="Link to this heading">#</a></h2>
<p>To derive the expression of the gradients of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> for
the RNN, we need to start recursively from the nodes closer to the
output layer in the temporal unrolling scheme - such as <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> at final time <span class="math notranslate nohighlight">\(t = \tau\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    (\nabla_{ \mathbf{y}^{(t)}} \mathcal{L})_{i} &amp;= \frac{\partial \mathcal{L}}{\partial L^{(t)}}\frac{\partial L^{(t)}}{\partial y_{i}^{(t)}}, \notag\\
    \nabla_{\mathbf{h}^{(\tau)}} \mathcal{L} &amp;= \mathbf{V}^\mathsf{T}\nabla_{ \mathbf{y}^{(\tau)}} \mathcal{L}.
\end{align*}
\end{split}\]</div>
</section>
<section id="chain-rule-again">
<h2>Chain rule again<a class="headerlink" href="#chain-rule-again" title="Link to this heading">#</a></h2>
<p>For the following hidden nodes, we have to iterate through time, so by the chain rule,</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    \nabla_{\mathbf{h}^{(t)}} \mathcal{L} &amp;= \left(\frac{\partial\mathbf{h}^{(t+1)}}{\partial\mathbf{h}^{(t)}}\right)^\mathsf{T}\nabla_{\mathbf{h}^{(t+1)}}\mathcal{L} + \left(\frac{\partial\mathbf{y}^{(t)}}{\partial\mathbf{h}^{(t)}}\right)^\mathsf{T}\nabla_{ \mathbf{y}^{(t)}} \mathcal{L}.
\end{align*}
\]</div>
</section>
<section id="gradients-of-loss-functions">
<h2>Gradients of loss functions<a class="headerlink" href="#gradients-of-loss-functions" title="Link to this heading">#</a></h2>
<p>Similarly, the gradients of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to the weights and biases follow,</p>
<!-- Equation labels as ordinary links -->
<div id="eq:rnn_gradients3"></div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \nabla_{\mathbf{c}} \mathcal{L} &amp;=\sum_{t}\left(\frac{\partial \mathbf{y}^{(t)}}{\partial \mathbf{c}}\right)^\mathsf{T} \nabla_{\mathbf{y}^{(t)}} \mathcal{L} \notag\\
    \nabla_{\mathbf{b}} \mathcal{L} &amp;=\sum_{t}\left(\frac{\partial \mathbf{h}^{(t)}}{\partial \mathbf{b}}\right)^\mathsf{T}        \nabla_{\mathbf{h}^{(t)}} \mathcal{L} \notag\\
    \nabla_{\mathbf{V}} \mathcal{L} &amp;=\sum_{t}\sum_{i}\left(\frac{\partial \mathcal{L}}{\partial y_i^{(t)} }\right)\nabla_{\mathbf{V}^{(t)}}y_i^{(t)} \notag\\
    \nabla_{\mathbf{W}} \mathcal{L} &amp;=\sum_{t}\sum_{i}\left(\frac{\partial \mathcal{L}}{\partial h_i^{(t)}}\right)\nabla_{\mathbf{w}^{(t)}} h_i^{(t)} \notag\\
    \nabla_{\mathbf{U}} \mathcal{L} &amp;=\sum_{t}\sum_{i}\left(\frac{\partial \mathcal{L}}{\partial h_i^{(t)}}\right)\nabla_{\mathbf{U}^{(t)}}h_i^{(t)}.
    \label{eq:rnn_gradients3} \tag{1}
\end{align*}
\end{split}\]</div>
</section>
<section id="summary-of-rnns">
<h2>Summary of RNNs<a class="headerlink" href="#summary-of-rnns" title="Link to this heading">#</a></h2>
<p>Recurrent neural networks (RNNs) have in general no probabilistic component
in a model. With a given fixed input and target from data, the RNNs learn the intermediate
association between various layers.
The inputs, outputs, and internal representation (hidden states) are all
real-valued vectors.</p>
<p>In a  traditional NN, it is assumed that every input is
independent of each other.  But with sequential data, the input at a given stage <span class="math notranslate nohighlight">\(t\)</span> depends on the input from the previous stage <span class="math notranslate nohighlight">\(t-1\)</span></p>
</section>
<section id="summary-of-a-typical-rnn">
<h2>Summary of a  typical RNN<a class="headerlink" href="#summary-of-a-typical-rnn" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Weight matrices <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(V\)</span> that connect the input layer at a stage <span class="math notranslate nohighlight">\(t\)</span> with the hidden layer <span class="math notranslate nohighlight">\(h_t\)</span>, the previous hidden layer <span class="math notranslate nohighlight">\(h_{t-1}\)</span> with <span class="math notranslate nohighlight">\(h_t\)</span> and the hidden layer <span class="math notranslate nohighlight">\(h_t\)</span> connecting with the output layer at the same stage and producing an output <span class="math notranslate nohighlight">\(\tilde{y}_t\)</span>, respectively.</p></li>
<li><p>The output from the hidden layer <span class="math notranslate nohighlight">\(h_t\)</span> is oftem modulated by a <span class="math notranslate nohighlight">\(\tanh{}\)</span> function <span class="math notranslate nohighlight">\(h_t=\sigma_h(x_t,h_{t-1})=\tanh{(Ux_t+Wh_{t-1}+b)}\)</span> with <span class="math notranslate nohighlight">\(b\)</span> a bias value</p></li>
<li><p>The output from the hidden layer produces <span class="math notranslate nohighlight">\(\tilde{y}_t=\sigma_y(Vh_t+c)\)</span> where <span class="math notranslate nohighlight">\(c\)</span> is a new bias parameter.</p></li>
<li><p>The output from the training at a given stage is in turn compared with the observation <span class="math notranslate nohighlight">\(y_t\)</span> thorugh a chosen cost function.</p></li>
</ol>
<p>The function <span class="math notranslate nohighlight">\(g\)</span> can any of the standard activation functions, that is a Sigmoid, a Softmax, a ReLU and other.
The parameters are trained through the so-called back-propagation through time (BPTT) algorithm.</p>
</section>
<section id="four-effective-ways-to-learn-an-rnn">
<h2>Four effective ways to learn an RNN<a class="headerlink" href="#four-effective-ways-to-learn-an-rnn" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Long Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.</p></li>
<li><p>Hessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.</p></li>
<li><p>Echo State Networks (ESN): Initialize the input a hidden and hidden-hidden and output-hidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input. ESNs only need to learn the hidden-output connections.</p></li>
<li><p>Good initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum</p></li>
</ol>
</section>
<section id="the-mathematics-of-rnns-the-basic-architecture">
<h2>The mathematics of RNNs, the basic architecture<a class="headerlink" href="#the-mathematics-of-rnns-the-basic-architecture" title="Link to this heading">#</a></h2>
<p>See notebook at <a class="github reference external" href="https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week7/ipynb/rnnmath.ipynb">CompPhysics/AdvancedMachineLearning</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="week45.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Week 45,  Convolutional Neural Networks (CCNs)</p>
      </div>
    </a>
    <a class="right-next"
       href="week47.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Week 47: Recurrent neural networks and Autoencoders</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plan-for-week-46">Plan for week 46</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-recommendations">Reading recommendations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorflow-examples">TensorFlow examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-nns-and-cnns-to-recurrent-neural-networks-rnns">From NNs and CNNs to recurrent neural networks (RNNs)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-recurrent-nn">What is a recurrent NN?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-rnns">Why RNNs?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feedback-connections">Feedback connections</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vanishing-gradients">Vanishing gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-neural-networks-rnns-overarching-view">Recurrent neural networks (RNNs): Overarching view</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-data-only">Sequential data only?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differential-equations">Differential equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-regression-example-using-tensorflow-with-keras">A simple regression example using TensorFlow with Keras</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#corresponding-example-using-pytorch">Corresponding example using PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns">RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-kinds-of-behaviour-can-rnns-exhibit">What kinds of behaviour can RNNs exhibit?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-layout-figures-from-sebastian-rashcka-et-al-machine-learning-with-sickit-learn-and-pytorch">Basic layout,  Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-differential-equations-with-rnns">Solving differential equations with RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-first-order-differential-equations">Two first-order differential equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#velocity-only">Velocity only</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linking-with-rnns">Linking with RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minor-rewrite">Minor rewrite</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail">RNNs in more detail</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-2">RNNs in more detail, part 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-3">RNNs in more detail, part 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-4">RNNs in more detail, part 4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-5">RNNs in more detail, part 5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-6">RNNs in more detail, part 6</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnns-in-more-detail-part-7">RNNs in more detail, part 7</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-through-time">Backpropagation through time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-backward-pass-is-linear">The backward pass is linear</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-of-exploding-or-vanishing-gradients">The problem of exploding or vanishing gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-setup">Mathematical setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-in-time-through-figures-part-1">Back propagation in time through figures, part 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-in-time-part-2">Back propagation in time, part 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-in-time-part-3">Back propagation in time, part 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-in-time-part-4">Back propagation in time, part 4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-in-time-in-equations">Back propagation in time in equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule-again">Chain rule again</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-of-loss-functions">Gradients of loss functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-rnns">Summary of RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-a-typical-rnn">Summary of a  typical RNN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#four-effective-ways-to-learn-an-rnn">Four effective ways to learn an RNN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mathematics-of-rnns-the-basic-architecture">The mathematics of RNNs, the basic architecture</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Morten Hjorth-Jensen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>