
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 45, Convolutional Neural Networks (CCNs) &#8212; Applied Data Analysis and Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week45';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Project 1 on Machine Learning, deadline October 6 (midnight), 2025" href="project1.html" />
    <link rel="prev" title="Exercises week 44" href="exercisesweek44.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Applied Data Analysis and Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Applied Data Analysis and Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Applied Data Analysis and Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About the course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Course setting</a></li>
<li class="toctree-l1"><a class="reference internal" href="teachers.html">Teachers and Grading</a></li>
<li class="toctree-l1"><a class="reference internal" href="textbooks.html">Textbooks</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Review of Statistics with Resampling Techniques and Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="statistics.html">1. Elements of Probability Theory and Statistical Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">2. Linear Algebra, Handling of Arrays and more Python Features</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Regression to Support Vector Machines</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter1.html">3. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter2.html">4. Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter3.html">5. Resampling Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter4.html">6. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapteroptimization.html">7. Optimization, the central part of any Machine Learning algortithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter5.html">8. Support Vector Machines, overarching aims</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Decision Trees, Ensemble Methods and Boosting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter6.html">9. Decision trees, overarching aims</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter7.html">10. Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Dimensionality Reduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter8.html">11. Basic ideas of the Principal Component Analysis (PCA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">12. Clustering and Unsupervised Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter9.html">13. Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter10.html">14. Building a Feed Forward Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter11.html">15. Solving Differential Equations  with Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter12.html">16. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter13.html">17. Recurrent neural networks: Overarching view</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Weekly material, notes and exercises</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="exercisesweek34.html">Exercises week 34</a></li>
<li class="toctree-l1"><a class="reference internal" href="week34.html">Week 34: Introduction to the course, Logistics and Practicalities</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek35.html">Exercises week 35</a></li>
<li class="toctree-l1"><a class="reference internal" href="week35.html">Week 35: From Ordinary Linear Regression to Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek36.html">Exercises week 36</a></li>
<li class="toctree-l1"><a class="reference internal" href="week36.html">Week 36: Linear Regression and Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek37.html">Exercises week 37</a></li>
<li class="toctree-l1"><a class="reference internal" href="week37.html">Week 37: Gradient descent methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek38.html">Exercises week 38</a></li>
<li class="toctree-l1"><a class="reference internal" href="week38.html">Week 38: Statistical analysis, bias-variance tradeoff and resampling methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek39.html">Exercises week 39</a></li>
<li class="toctree-l1"><a class="reference internal" href="week39.html">Week 39: Resampling methods and logistic regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="week40.html">Week 40: Gradient descent methods (continued) and start Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="week41.html">Week 41 Neural networks and constructing a neural network code</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek41.html">Exercises week 41</a></li>








<li class="toctree-l1"><a class="reference internal" href="week42.html">Week 42 Constructing a Neural Network code with examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek42.html">Exercises week 42</a></li>









<li class="toctree-l1"><a class="reference internal" href="week43.html">Week 43: Deep Learning: Constructing a Neural Network code and solving differential equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek43.html">Exercises week 43</a></li>

<li class="toctree-l1"><a class="reference internal" href="week44.html">Week 44,  Solving differential equations with neural networks and start Convolutional Neural Networks (CNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek44.html">Exercises week 44</a></li>

<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 45,  Convolutional Neural Networks (CCNs)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="project1.html">Project 1 on Machine Learning, deadline October 6 (midnight), 2025</a></li>
<li class="toctree-l1"><a class="reference internal" href="project2.html">Project 2 on Machine Learning, deadline November 10 (Midnight)</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/week45.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 45,  Convolutional Neural Networks (CCNs)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plans-for-week-45">Plans for week 45</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#material-for-the-lab-sessions">Material for the lab sessions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#material-for-lecture-monday-november-3">Material for Lecture Monday November 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-recognizing-images-reminder-from-last-week">Convolutional Neural Networks (recognizing images), reminder from last week</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-difference">What is the Difference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-vs-cnns">Neural Networks vs CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-cnns-for-images-sound-files-medical-images-from-ct-scans-etc">Why CNNS for images, sound files, medical images from CT scans etc?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regular-nns-dont-scale-well-to-full-images">Regular NNs don’t scale well to full images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-volumes-of-neurons">3D volumes of neurons</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-on-dimensionalities">More on Dimensionalities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-remarks">Further remarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layers-used-to-build-cnns">Layers used to build CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transforming-images">Transforming images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnns-in-brief">CNNs in brief</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-deep-cnn-model-from-raschka-et-al">A deep CNN model (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-idea">Key Idea</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-cnns">Mathematics of CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution-examples-polynomial-multiplication">Convolution Examples: Polynomial multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#efficient-polynomial-multiplication">Efficient Polynomial Multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-simplification">Further simplification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-more-efficient-way-of-coding-the-above-convolution">A more efficient way of coding the above Convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commutative-process">Commutative process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#toeplitz-matrices">Toeplitz matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fourier-series-and-toeplitz-matrices">Fourier series and Toeplitz matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-the-above-one-dimensional-case">Generalizing the above one-dimensional case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-considerations">Memory considerations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#padding">Padding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-vector">New vector</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rewriting-as-dot-products">Rewriting as dot products</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-correlation">Cross correlation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-dimensional-objects">Two-dimensional objects</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnns-in-more-detail-simple-example">CNNs in more detail, simple example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-convolution-stage">The convolution stage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-number-of-parameters">Finding the number of parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-image-or-volume">New image (or volume)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-to-train-common-settings">Parameters to train, common settings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-cnn-setups">Examples of CNN setups</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summarizing-performing-a-general-discrete-convolution-from-raschka-et-al">Summarizing: Performing a general discrete convolution (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling">Pooling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-arithmetic">Pooling arithmetic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-types-from-raschka-et-al">Pooling types (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-convolutional-neural-networks-using-tensorflow-and-keras">Building convolutional neural networks using Tensorflow and Keras</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-it-up">Setting it up</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mnist-dataset-again">The MNIST dataset again</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strong-correlations">Strong correlations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layers-of-a-cnn">Layers of a CNN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#systematic-reduction">Systematic reduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites-collect-and-pre-process-data">Prerequisites: Collect and pre-process data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importing-keras-and-tensorflow">Importing Keras and Tensorflow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-with-keras">Running with Keras</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-part">Final part</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-visualization">Final visualization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cifar01-data-set">The CIFAR01 data set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verifying-the-data-set">Verifying the data set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-the-model">Set up  the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#add-dense-layers-on-top">Add Dense layers on top</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compile-and-train-the-model">Compile and train the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finally-evaluate-the-model">Finally, evaluate the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-code-using-pytorch">Building code using Pytorch</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)
doconce format html week45.do.txt --no_mako -->
<!-- dom:TITLE: Week 45,  Convolutional Neural Networks (CCNs) --><section class="tex2jax_ignore mathjax_ignore" id="week-45-convolutional-neural-networks-ccns">
<h1>Week 45,  Convolutional Neural Networks (CCNs)<a class="headerlink" href="#week-45-convolutional-neural-networks-ccns" title="Link to this heading">#</a></h1>
<p><strong>Morten Hjorth-Jensen</strong>, Department of Physics, University of Oslo</p>
<p>Date: <strong>November 3-7, 2025</strong></p>
<section id="plans-for-week-45">
<h2>Plans for week 45<a class="headerlink" href="#plans-for-week-45" title="Link to this heading">#</a></h2>
<p><strong>Material for the lecture on Monday November 3, 2025.</strong></p>
<ol class="arabic simple">
<li><p>Convolutional Neural Networks, codes and examples (TensorFlow and Pytorch implementations)</p></li>
<li><p>Readings and Videos:</p></li>
<li><p>These lecture notes at <a class="github reference external" href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week45/ipynb/week45.ipynb">CompPhysics/MachineLearning</a></p></li>
<li><p>Video of lecture at <a class="reference external" href="https://youtu.be/dZt6Vm1wjhs">https://youtu.be/dZt6Vm1wjhs</a></p></li>
<li><p>Whiteboard notes at <a class="github reference external" href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2025/FYSSTKweek45.pdf">CompPhysics/MachineLearning</a></p></li>
<li><p>For a more in depth discussion on  CNNs we recommend Goodfellow et al chapters 9. See also chapter 11 and 12 on practicalities and applications</p></li>
<li><p>Reading suggestions for implementation of CNNs, see Raschka et al chapters 14-15 at <a class="github reference external" href="https://github.com/rasbt/machine-learning-book">rasbt/machine-learning-book</a>.</p></li>
</ol>
<!-- o Video  on Recurrent Neural Networks from MIT at <https://www.youtube.com/watch?v=SEnXr6v2ifU&ab_channel=AlexanderAmini> -->
<p>a. Video on Deep Learning at <a class="reference external" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a></p>
</section>
<section id="material-for-the-lab-sessions">
<h2>Material for the lab sessions<a class="headerlink" href="#material-for-the-lab-sessions" title="Link to this heading">#</a></h2>
<p>Discussion of and work on project 2, no exercises this week, only project work</p>
</section>
<section id="material-for-lecture-monday-november-3">
<h2>Material for Lecture Monday November 3<a class="headerlink" href="#material-for-lecture-monday-november-3" title="Link to this heading">#</a></h2>
</section>
<section id="convolutional-neural-networks-recognizing-images-reminder-from-last-week">
<h2>Convolutional Neural Networks (recognizing images), reminder from last week<a class="headerlink" href="#convolutional-neural-networks-recognizing-images-reminder-from-last-week" title="Link to this heading">#</a></h2>
<p>Convolutional neural networks (CNNs) were developed during the last
decade of the previous century, with a focus on character recognition
tasks. Nowadays, CNNs are a central element in the spectacular success
of deep learning methods. The success in for example image
classifications have made them a central tool for most machine
learning practitioners.</p>
<p>CNNs are very similar to ordinary Neural Networks.
They are made up of neurons that have learnable weights and
biases. Each neuron receives some inputs, performs a dot product and
optionally follows it with a non-linearity. The whole network still
expresses a single differentiable score function: from the raw image
pixels on one end to class scores at the other. And they still have a
loss function (for example Softmax) on the last (fully-connected) layer
and all the tips/tricks we developed for learning regular Neural
Networks still apply (back propagation, gradient descent etc etc).</p>
</section>
<section id="what-is-the-difference">
<h2>What is the Difference<a class="headerlink" href="#what-is-the-difference" title="Link to this heading">#</a></h2>
<p><strong>CNN architectures make the explicit assumption that
the inputs are images, which allows us to encode certain properties
into the architecture. These then make the forward function more
efficient to implement and vastly reduce the amount of parameters in
the network.</strong></p>
</section>
<section id="neural-networks-vs-cnns">
<h2>Neural Networks vs CNNs<a class="headerlink" href="#neural-networks-vs-cnns" title="Link to this heading">#</a></h2>
<p>Neural networks are defined as <strong>affine transformations</strong>, that is
a vector is received as input and is multiplied with a matrix of so-called weights (our unknown paramters) to produce an
output (to which a bias vector is usually added before passing the result
through a nonlinear activation function). This is applicable to any type of input, be it an
image, a sound clip or an unordered collection of features: whatever their
dimensionality, their representation can always be flattened into a vector
before the transformation.</p>
</section>
<section id="why-cnns-for-images-sound-files-medical-images-from-ct-scans-etc">
<h2>Why CNNS for images, sound files, medical images from CT scans etc?<a class="headerlink" href="#why-cnns-for-images-sound-files-medical-images-from-ct-scans-etc" title="Link to this heading">#</a></h2>
<p>However, when we consider images, sound clips and many other similar kinds of data, these data  have an intrinsic
structure. More formally, they share these important properties:</p>
<ul class="simple">
<li><p>They are stored as multi-dimensional arrays (think of the pixels of a figure) .</p></li>
<li><p>They feature one or more axes for which ordering matters (e.g., width and height axes for an image, time axis for a sound clip).</p></li>
<li><p>One axis, called the channel axis, is used to access different views of the data (e.g., the red, green and blue channels of a color image, or the left and right channels of a stereo audio track).</p></li>
</ul>
<p>These properties are not exploited when an affine transformation is applied; in
fact, all the axes are treated in the same way and the topological information
is not taken into account. Still, taking advantage of the implicit structure of
the data may prove very handy in solving some tasks, like computer vision and
speech recognition, and in these cases it would be best to preserve it. This is
where discrete convolutions come into play.</p>
<p>A discrete convolution is a linear transformation that preserves this notion of
ordering. It is sparse (only a few input units contribute to a given output
unit) and reuses parameters (the same weights are applied to multiple locations
in the input).</p>
</section>
<section id="regular-nns-dont-scale-well-to-full-images">
<h2>Regular NNs don’t scale well to full images<a class="headerlink" href="#regular-nns-dont-scale-well-to-full-images" title="Link to this heading">#</a></h2>
<p>As an example, consider
an image of size <span class="math notranslate nohighlight">\(32\times 32\times 3\)</span> (32 wide, 32 high, 3 color channels), so a
single fully-connected neuron in a first hidden layer of a regular
Neural Network would have <span class="math notranslate nohighlight">\(32\times 32\times 3 = 3072\)</span> weights. This amount still
seems manageable, but clearly this fully-connected structure does not
scale to larger images. For example, an image of more respectable
size, say <span class="math notranslate nohighlight">\(200\times 200\times 3\)</span>, would lead to neurons that have
<span class="math notranslate nohighlight">\(200\times 200\times 3 = 120,000\)</span> weights.</p>
<p>We could have
several such neurons, and the parameters would add up quickly! Clearly,
this full connectivity is wasteful and the huge number of parameters
would quickly lead to possible overfitting.</p>
<!-- dom:FIGURE: [figslides/nn.jpeg, width=500 frac=0.6]  A regular 3-layer Neural Network. -->
<!-- begin figure -->
<p><img src="figslides/nn.jpeg" width="500"><p style="font-size: 0.9em"><i>Figure 1: A regular 3-layer Neural Network.</i></p></p>
<!-- end figure --></section>
<section id="d-volumes-of-neurons">
<h2>3D volumes of neurons<a class="headerlink" href="#d-volumes-of-neurons" title="Link to this heading">#</a></h2>
<p>Convolutional Neural Networks take advantage of the fact that the
input consists of images and they constrain the architecture in a more
sensible way.</p>
<p>In particular, unlike a regular Neural Network, the
layers of a CNN have neurons arranged in 3 dimensions: width,
height, depth. (Note that the word depth here refers to the third
dimension of an activation volume, not to the depth of a full Neural
Network, which can refer to the total number of layers in a network.)</p>
<p>To understand it better, the above example of an image
with an input volume of
activations has dimensions <span class="math notranslate nohighlight">\(32\times 32\times 3\)</span> (width, height,
depth respectively).</p>
<p>The neurons in a layer will
only be connected to a small region of the layer before it, instead of
all of the neurons in a fully-connected manner. Moreover, the final
output layer could  for this specific image have dimensions <span class="math notranslate nohighlight">\(1\times 1 \times 10\)</span>,
because by the
end of the CNN architecture we will reduce the full image into a
single vector of class scores, arranged along the depth
dimension.</p>
<!-- dom:FIGURE: [figslides/cnn.jpeg, width=500 frac=0.6]  A CNN arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a CNN transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels). -->
<!-- begin figure -->
<p><img src="figslides/cnn.jpeg" width="500"><p style="font-size: 0.9em"><i>Figure 1: A CNN arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a CNN transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels).</i></p></p>
<!-- end figure --></section>
<section id="more-on-dimensionalities">
<h2>More on Dimensionalities<a class="headerlink" href="#more-on-dimensionalities" title="Link to this heading">#</a></h2>
<p>In fields like signal processing (and imaging as well), one designs
so-called filters. These filters are defined by the convolutions and
are often hand-crafted. One may specify filters for smoothing, edge
detection, frequency reshaping, and similar operations. However with
neural networks the idea is to automatically learn the filters and use
many of them in conjunction with non-linear operations (activation
functions).</p>
<p>As an example consider a neural network operating on sound sequence
data.  Assume that we an input vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> of length <span class="math notranslate nohighlight">\(d=10^6\)</span>.  We
construct then a neural network with onle hidden layer only with
<span class="math notranslate nohighlight">\(10^4\)</span> nodes. This means that we will have a weight matrix with
<span class="math notranslate nohighlight">\(10^4\times 10^6=10^{10}\)</span> weights to be determined, together with <span class="math notranslate nohighlight">\(10^4\)</span> biases.</p>
<p>Assume furthermore that we have an output layer which is meant to train whether the sound sequence represents a human voice (true) or something else (false).
It means that we have only one output node. But since this output node connects to <span class="math notranslate nohighlight">\(10^4\)</span> nodes in the hidden layer, there are in total <span class="math notranslate nohighlight">\(10^4\)</span> weights to be determined for the output layer, plus one bias. In total we have</p>
<div class="math notranslate nohighlight">
\[
\mathrm{NumberParameters}=10^{10}+10^4+10^4+1 \approx 10^{10},
\]</div>
<p>that is ten billion parameters to determine.</p>
</section>
<section id="further-remarks">
<h2>Further remarks<a class="headerlink" href="#further-remarks" title="Link to this heading">#</a></h2>
<p>The main principles that justify convolutions is locality of
information and repetion of patterns within the signal. Sound samples
of the input in adjacent spots are much more likely to affect each
other than those that are very far away. Similarly, sounds are
repeated in multiple times in the signal. While slightly simplistic,
reasoning about such a sound example demonstrates this. The same
principles then apply to images and other similar data.</p>
</section>
<section id="layers-used-to-build-cnns">
<h2>Layers used to build CNNs<a class="headerlink" href="#layers-used-to-build-cnns" title="Link to this heading">#</a></h2>
<p>A simple CNN is a sequence of layers, and every layer of a CNN
transforms one volume of activations to another through a
differentiable function. We use three main types of layers to build
CNN architectures: Convolutional Layer, Pooling Layer, and
Fully-Connected Layer (exactly as seen in regular Neural Networks). We
will stack these layers to form a full CNN architecture.</p>
<p>A simple CNN for image classification could have the architecture:</p>
<ul class="simple">
<li><p><strong>INPUT</strong> (<span class="math notranslate nohighlight">\(32\times 32 \times 3\)</span>) will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.</p></li>
<li><p><strong>CONV</strong> (convolutional )layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as <span class="math notranslate nohighlight">\([32\times 32\times 12]\)</span> if we decided to use 12 filters.</p></li>
<li><p><strong>RELU</strong> layer will apply an elementwise activation function, such as the <span class="math notranslate nohighlight">\(max(0,x)\)</span> thresholding at zero. This leaves the size of the volume unchanged (<span class="math notranslate nohighlight">\([32\times 32\times 12]\)</span>).</p></li>
<li><p><strong>POOL</strong> (pooling) layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as <span class="math notranslate nohighlight">\([16\times 16\times 12]\)</span>.</p></li>
<li><p><strong>FC</strong> (i.e. fully-connected) layer will compute the class scores, resulting in volume of size <span class="math notranslate nohighlight">\([1\times 1\times 10]\)</span>, where each of the 10 numbers correspond to a class score, such as among the 10 categories of the MNIST images we considered above . As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.</p></li>
</ul>
</section>
<section id="transforming-images">
<h2>Transforming images<a class="headerlink" href="#transforming-images" title="Link to this heading">#</a></h2>
<p>CNNs transform the original image layer by layer from the original
pixel values to the final class scores.</p>
<p>Observe that some layers contain
parameters and other don’t. In particular, the CNN layers perform
transformations that are a function of not only the activations in the
input volume, but also of the parameters (the weights and biases of
the neurons). On the other hand, the RELU/POOL layers will implement a
fixed function. The parameters in the CONV/FC layers will be trained
with gradient descent so that the class scores that the CNN computes
are consistent with the labels in the training set for each image.</p>
</section>
<section id="cnns-in-brief">
<h2>CNNs in brief<a class="headerlink" href="#cnns-in-brief" title="Link to this heading">#</a></h2>
<p>In summary:</p>
<ul class="simple">
<li><p>A CNN architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)</p></li>
<li><p>There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)</p></li>
<li><p>Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function</p></li>
<li><p>Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)</p></li>
<li><p>Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t)</p></li>
</ul>
</section>
<section id="a-deep-cnn-model-from-raschka-et-al">
<h2>A deep CNN model (<a class="reference external" href="https://github.com/rasbt/machine-learning-book">From Raschka et al</a>)<a class="headerlink" href="#a-deep-cnn-model-from-raschka-et-al" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/deepcnn.png, width=500 frac=0.67]  A deep CNN -->
<!-- begin figure -->
<p><img src="figslides/deepcnn.png" width="500"><p style="font-size: 0.9em"><i>Figure 1: A deep CNN</i></p></p>
<!-- end figure --></section>
<section id="key-idea">
<h2>Key Idea<a class="headerlink" href="#key-idea" title="Link to this heading">#</a></h2>
<p>A dense neural network is representd by an affine operation (like matrix-matrix multiplication) where all parameters are included.</p>
<p>The key idea in CNNs for say imaging is that in images neighbor pixels tend to be related! So we connect
only neighboring neurons in the input instead of connecting all with the first hidden layer.</p>
<p>We say we perform a filtering (convolution is the mathematical operation).</p>
</section>
<section id="mathematics-of-cnns">
<h2>Mathematics of CNNs<a class="headerlink" href="#mathematics-of-cnns" title="Link to this heading">#</a></h2>
<p>The mathematics of CNNs is based on the mathematical operation of
<strong>convolution</strong>.  In mathematics (in particular in functional analysis),
convolution is represented by mathematical operations (integration,
summation etc) on two functions in order to produce a third function
that expresses how the shape of one gets modified by the other.
Convolution has a plethora of applications in a variety of
disciplines, spanning from statistics to signal processing, computer
vision, solutions of differential equations,linear algebra,
engineering, and yes, machine learning.</p>
<p>Mathematically, convolution is defined as follows (one-dimensional example):
Let us define a continuous function <span class="math notranslate nohighlight">\(y(t)\)</span> given by</p>
<div class="math notranslate nohighlight">
\[
y(t) = \int x(a) w(t-a) da,
\]</div>
<p>where <span class="math notranslate nohighlight">\(x(a)\)</span> represents a so-called input and <span class="math notranslate nohighlight">\(w(t-a)\)</span> is normally called the weight function or kernel.</p>
<p>The above integral is written in  a more compact form as</p>
<div class="math notranslate nohighlight">
\[
y(t) = \left(x * w\right)(t).
\]</div>
<p>The discretized version reads</p>
<div class="math notranslate nohighlight">
\[
y(t) = \sum_{a=-\infty}^{a=\infty}x(a)w(t-a).
\]</div>
<p>Computing the inverse of the above convolution operations is known as deconvolution and the process is commutative.</p>
<p>How can we use this? And what does it mean? Let us study some familiar examples first.</p>
</section>
<section id="convolution-examples-polynomial-multiplication">
<h2>Convolution Examples: Polynomial multiplication<a class="headerlink" href="#convolution-examples-polynomial-multiplication" title="Link to this heading">#</a></h2>
<p>Our first example is that of a multiplication between two polynomials,
which we will rewrite in terms of the mathematics of convolution. In
the final stage, since the problem here is a discrete one, we will
recast the final expression in terms of a matrix-vector
multiplication, where the matrix is a so-called <a class="reference external" href="https://link.springer.com/book/10.1007/978-93-86279-04-0">Toeplitz matrix
</a>.</p>
<p>Let us look a the following polynomials to second and third order, respectively:</p>
<div class="math notranslate nohighlight">
\[
p(t) = \alpha_0+\alpha_1 t+\alpha_2 t^2,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
s(t) = \beta_0+\beta_1 t+\beta_2 t^2+\beta_3 t^3.
\]</div>
<p>The polynomial multiplication gives us a new polynomial of degree <span class="math notranslate nohighlight">\(5\)</span></p>
<div class="math notranslate nohighlight">
\[
z(t) = \delta_0+\delta_1 t+\delta_2 t^2+\delta_3 t^3+\delta_4 t^4+\delta_5 t^5.
\]</div>
</section>
<section id="efficient-polynomial-multiplication">
<h2>Efficient Polynomial Multiplication<a class="headerlink" href="#efficient-polynomial-multiplication" title="Link to this heading">#</a></h2>
<p>Computing polynomial products can be implemented efficiently if we rewrite the more brute force multiplications using convolution.
We note first that the new coefficients are given as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\delta_0=&amp;\alpha_0\beta_0\\
\delta_1=&amp;\alpha_1\beta_0+\alpha_0\beta_1\\
\delta_2=&amp;\alpha_0\beta_2+\alpha_1\beta_1+\alpha_2\beta_0\\
\delta_3=&amp;\alpha_1\beta_2+\alpha_2\beta_1+\alpha_0\beta_3\\
\delta_4=&amp;\alpha_2\beta_2+\alpha_1\beta_3\\
\delta_5=&amp;\alpha_2\beta_3.\\
\end{split}
\end{split}\]</div>
<p>We note that <span class="math notranslate nohighlight">\(\alpha_i=0\)</span> except for <span class="math notranslate nohighlight">\(i\in \left\{0,1,2\right\}\)</span> and <span class="math notranslate nohighlight">\(\beta_i=0\)</span> except for <span class="math notranslate nohighlight">\(i\in\left\{0,1,2,3\right\}\)</span>.</p>
<p>We can then rewrite the coefficients <span class="math notranslate nohighlight">\(\delta_j\)</span> using a discrete convolution as</p>
<div class="math notranslate nohighlight">
\[
\delta_j = \sum_{i=-\infty}^{i=\infty}\alpha_i\beta_{j-i}=(\alpha * \beta)_j,
\]</div>
<p>or as a double sum with restriction <span class="math notranslate nohighlight">\(l=i+j\)</span></p>
<div class="math notranslate nohighlight">
\[
\delta_l = \sum_{ij}\alpha_i\beta_{j}.
\]</div>
</section>
<section id="further-simplification">
<h2>Further simplification<a class="headerlink" href="#further-simplification" title="Link to this heading">#</a></h2>
<p>Although we may have redundant operations with some few zeros for <span class="math notranslate nohighlight">\(\beta_i\)</span>, we can rewrite the above sum in a more compact way as</p>
<div class="math notranslate nohighlight">
\[
\delta_i = \sum_{k=0}^{k=m-1}\alpha_k\beta_{i-k},
\]</div>
<p>where <span class="math notranslate nohighlight">\(m=3\)</span> in our case, the maximum length of
the vector <span class="math notranslate nohighlight">\(\alpha\)</span>. Note that the vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> has length <span class="math notranslate nohighlight">\(n=4\)</span>. Below we will find an even more efficient representation.</p>
</section>
<section id="a-more-efficient-way-of-coding-the-above-convolution">
<h2>A more efficient way of coding the above Convolution<a class="headerlink" href="#a-more-efficient-way-of-coding-the-above-convolution" title="Link to this heading">#</a></h2>
<p>Since we only have a finite number of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> values
which are non-zero, we can rewrite the above convolution expressions
as a matrix-vector multiplication</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\delta}=\begin{bmatrix}\alpha_0 &amp; 0 &amp; 0 &amp; 0 \\
                            \alpha_1 &amp; \alpha_0 &amp; 0 &amp; 0 \\
			    \alpha_2 &amp; \alpha_1 &amp; \alpha_0 &amp; 0 \\
			    0 &amp; \alpha_2 &amp; \alpha_1 &amp; \alpha_0 \\
			    0 &amp; 0 &amp; \alpha_2 &amp; \alpha_1 \\
			    0 &amp; 0 &amp; 0 &amp; \alpha_2
			    \end{bmatrix}\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3\end{bmatrix}.
\end{split}\]</div>
</section>
<section id="commutative-process">
<h2>Commutative process<a class="headerlink" href="#commutative-process" title="Link to this heading">#</a></h2>
<p>The process is commutative and we can easily see that we can rewrite the multiplication in terms of  a matrix holding <span class="math notranslate nohighlight">\(\beta\)</span> and a vector holding <span class="math notranslate nohighlight">\(\alpha\)</span>.
In this case we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\delta}=\begin{bmatrix}\beta_0 &amp; 0 &amp; 0  \\
                            \beta_1 &amp; \beta_0 &amp; 0  \\
			    \beta_2 &amp; \beta_1 &amp; \beta_0  \\
			    \beta_3 &amp; \beta_2 &amp; \beta_1 \\
			    0 &amp; \beta_3 &amp; \beta_2 \\
			    0 &amp; 0 &amp; \beta_3
			    \end{bmatrix}\begin{bmatrix} \alpha_0 \\ \alpha_1 \\ \alpha_2\end{bmatrix}.
\end{split}\]</div>
<p>Note that the use of these matrices is for mathematical purposes only
and not implementation purposes.  When implementing the above equation
we do not encode (and allocate memory) the matrices explicitely.  We
rather code the convolutions in the minimal memory footprint that they
require.</p>
</section>
<section id="toeplitz-matrices">
<h2>Toeplitz matrices<a class="headerlink" href="#toeplitz-matrices" title="Link to this heading">#</a></h2>
<p>The above matrices are examples of so-called <a class="reference external" href="https://link.springer.com/book/10.1007/978-93-86279-04-0">Toeplitz
matrices</a>. A
Toeplitz matrix is a matrix in which each descending diagonal from
left to right is constant. For instance the last matrix, which we
rewrite as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{A}=\begin{bmatrix}a_0 &amp; 0 &amp; 0  \\
                            a_1 &amp; a_0 &amp; 0  \\
			    a_2 &amp; a_1 &amp; a_0  \\
			    a_3 &amp; a_2 &amp; a_1 \\
			    0 &amp; a_3 &amp; a_2 \\
			    0 &amp; 0 &amp; a_3
			    \end{bmatrix},
\end{split}\]</div>
<p>with elements <span class="math notranslate nohighlight">\(a_{ii}=a_{i+1,j+1}=a_{i-j}\)</span> is an example of a Toeplitz
matrix. Such a matrix does not need to be a square matrix.  Toeplitz
matrices are also closely connected with Fourier series, because the multiplication operator by a trigonometric
polynomial, compressed to a finite-dimensional space, can be
represented by such a matrix. The example above shows that we can
represent linear convolution as multiplication of a Toeplitz matrix by
a vector.</p>
</section>
<section id="fourier-series-and-toeplitz-matrices">
<h2>Fourier series and Toeplitz matrices<a class="headerlink" href="#fourier-series-and-toeplitz-matrices" title="Link to this heading">#</a></h2>
<p>This is an active and ogoing research area concerning CNNs. The following articles may be of interest</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.sciencedirect.com/topics/engineering/convolution-theorem#:~:text=The%20convolution%20theorem%20(together%20with,k%20)%20G%20(%20k%20)%20.">Read more about the convolution theorem and Fouriers series</a></p></li>
<li><p><a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S1568494623006257">Fourier Transform Layer</a></p></li>
</ol>
</section>
<section id="generalizing-the-above-one-dimensional-case">
<h2>Generalizing the above one-dimensional case<a class="headerlink" href="#generalizing-the-above-one-dimensional-case" title="Link to this heading">#</a></h2>
<p>In order to align the above simple case with the more general
convolution cases, we rename <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span>, whose length is <span class="math notranslate nohighlight">\(m=3\)</span>,
with <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>.  We will interpret <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> as a weight/filter function
with which we want to perform the convolution with an input variable
<span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> of length <span class="math notranslate nohighlight">\(n\)</span>.  We will assume always that the filter
<span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> has dimensionality <span class="math notranslate nohighlight">\(m \le n\)</span>.</p>
<p>We replace thus <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\delta}\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> and have</p>
<div class="math notranslate nohighlight">
\[
y(i)= \left(x*w\right)(i)= \sum_{k=0}^{k=m-1}w(k)x(i-k),
\]</div>
<p>where <span class="math notranslate nohighlight">\(m=3\)</span> in our case, the maximum length of the vector <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>.
Here the symbol <span class="math notranslate nohighlight">\(*\)</span> represents the mathematical operation of convolution.</p>
</section>
<section id="memory-considerations">
<h2>Memory considerations<a class="headerlink" href="#memory-considerations" title="Link to this heading">#</a></h2>
<p>This expression leaves us however with some terms with negative
indices, for example <span class="math notranslate nohighlight">\(x(-1)\)</span> and <span class="math notranslate nohighlight">\(x(-2)\)</span> which may not be defined. Our
vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> has components <span class="math notranslate nohighlight">\(x(0)\)</span>, <span class="math notranslate nohighlight">\(x(1)\)</span>, <span class="math notranslate nohighlight">\(x(2)\)</span> and <span class="math notranslate nohighlight">\(x(3)\)</span>.</p>
<p>The index <span class="math notranslate nohighlight">\(j\)</span> for <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> runs from <span class="math notranslate nohighlight">\(j=0\)</span> to <span class="math notranslate nohighlight">\(j=3\)</span> since <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is meant to
represent a third-order polynomial.</p>
<p>Furthermore, the index <span class="math notranslate nohighlight">\(i\)</span> runs from <span class="math notranslate nohighlight">\(i=0\)</span> to <span class="math notranslate nohighlight">\(i=5\)</span> since <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>
contains the coefficients of a fifth-order polynomial.  When <span class="math notranslate nohighlight">\(i=5\)</span> we
may also have values of <span class="math notranslate nohighlight">\(x(4)\)</span> and <span class="math notranslate nohighlight">\(x(5)\)</span> which are not defined.</p>
</section>
<section id="padding">
<h2>Padding<a class="headerlink" href="#padding" title="Link to this heading">#</a></h2>
<p>The solution to this is what is called <strong>padding</strong>!  We simply define a
new vector <span class="math notranslate nohighlight">\(x\)</span> with two added elements set to zero before <span class="math notranslate nohighlight">\(x(0)\)</span> and
two new elements after <span class="math notranslate nohighlight">\(x(3)\)</span> set to zero. That is, we augment the
length of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> from <span class="math notranslate nohighlight">\(n=4\)</span> to <span class="math notranslate nohighlight">\(n+2P=8\)</span>, where <span class="math notranslate nohighlight">\(P=2\)</span> is the padding
constant (a new hyperparameter), see discussions below as well.</p>
</section>
<section id="new-vector">
<h2>New vector<a class="headerlink" href="#new-vector" title="Link to this heading">#</a></h2>
<p>We have a new vector defined as <span class="math notranslate nohighlight">\(x(0)=0\)</span>, <span class="math notranslate nohighlight">\(x(1)=0\)</span>,
<span class="math notranslate nohighlight">\(x(2)=\beta_0\)</span>, <span class="math notranslate nohighlight">\(x(3)=\beta_1\)</span>, <span class="math notranslate nohighlight">\(x(4)=\beta_2\)</span>, <span class="math notranslate nohighlight">\(x(5)=\beta_3\)</span>,
<span class="math notranslate nohighlight">\(x(6)=0\)</span>, and <span class="math notranslate nohighlight">\(x(7)=0\)</span>.</p>
<p>We have added four new elements, which
are all zero. The benefit is that we can rewrite the equation for
<span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, with <span class="math notranslate nohighlight">\(i=0,1,\dots,5\)</span>,</p>
<div class="math notranslate nohighlight">
\[
y(i) = \sum_{k=0}^{k=m-1}w(k)x(i+(m-1)-k).
\]</div>
<p>As an example, we have</p>
<div class="math notranslate nohighlight">
\[
y(4)=x(6)w(0)+x(5)w(1)+x(4)w(2)=0\times \alpha_0+\beta_3\alpha_1+\beta_2\alpha_2,
\]</div>
<p>as before except that we have an additional term <span class="math notranslate nohighlight">\(x(6)w(0)\)</span>, which is zero.</p>
<p>Similarly, for the fifth-order term we have</p>
<div class="math notranslate nohighlight">
\[
y(5)=x(7)w(0)+x(6)w(1)+x(5)w(2)=0\times \alpha_0+0\times\alpha_1+\beta_3\alpha_2.
\]</div>
<p>The zeroth-order term is</p>
<div class="math notranslate nohighlight">
\[
y(0)=x(2)w(0)+x(1)w(1)+x(0)w(2)=\beta_0 \alpha_0+0\times\alpha_1+0\times\alpha_2=\alpha_0\beta_0.
\]</div>
</section>
<section id="rewriting-as-dot-products">
<h2>Rewriting as dot products<a class="headerlink" href="#rewriting-as-dot-products" title="Link to this heading">#</a></h2>
<p>If we now flip the filter/weight vector, with the following term as a typical example</p>
<div class="math notranslate nohighlight">
\[
y(0)=x(2)w(0)+x(1)w(1)+x(0)w(2)=x(2)\tilde{w}(2)+x(1)\tilde{w}(1)+x(0)\tilde{w}(0),
\]</div>
<p>with <span class="math notranslate nohighlight">\(\tilde{w}(0)=w(2)\)</span>, <span class="math notranslate nohighlight">\(\tilde{w}(1)=w(1)\)</span>, and <span class="math notranslate nohighlight">\(\tilde{w}(2)=w(0)\)</span>, we can then rewrite the above sum as a dot product of
<span class="math notranslate nohighlight">\(x(i:i+(m-1))\tilde{w}\)</span> for element <span class="math notranslate nohighlight">\(y(i)\)</span>, where <span class="math notranslate nohighlight">\(x(i:i+(m-1))\)</span> is simply a patch of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> of size <span class="math notranslate nohighlight">\(m-1\)</span>.</p>
<p>The padding <span class="math notranslate nohighlight">\(P\)</span> we have introduced for the convolution stage is just
another hyperparameter which is introduced as part of the
architecture. Similarly, below we will also introduce another
hyperparameter called <strong>Stride</strong> <span class="math notranslate nohighlight">\(S\)</span>.</p>
</section>
<section id="cross-correlation">
<h2>Cross correlation<a class="headerlink" href="#cross-correlation" title="Link to this heading">#</a></h2>
<p>In essentially all applications one uses what is called cross correlation instead of the standard convolution described above.
This means that multiplication is performed in the same direction and instead of the general expression we have discussed above (with infinite sums)</p>
<div class="math notranslate nohighlight">
\[
y(i) = \sum_{k=-\infty}^{k=\infty}w(k)x(i-k),
\]</div>
<p>we have now</p>
<div class="math notranslate nohighlight">
\[
y(i) = \sum_{k=-\infty}^{k=\infty}w(k)x(i+k).
\]</div>
<p>Both TensorFlow and PyTorch (as well as our own code example below),
implement the last equation, although it is normally referred to as
convolution.  The same padding rules and stride rules discussed below
apply to this expression as well.</p>
<p>We leave it as an exercise for you to convince yourself that the example we have discussed till now, gives the same final expression using the last expression.</p>
</section>
<section id="two-dimensional-objects">
<h2>Two-dimensional objects<a class="headerlink" href="#two-dimensional-objects" title="Link to this heading">#</a></h2>
<p>We are now ready to start studying the discrete convolutions relevant for convolutional neural networks.
We often use convolutions over more than one dimension at a time. If
we have a two-dimensional image <span class="math notranslate nohighlight">\(X\)</span> as input, we can have a <strong>filter</strong>
defined by a two-dimensional <strong>kernel/weight/filter</strong> <span class="math notranslate nohighlight">\(W\)</span>. This leads to an output <span class="math notranslate nohighlight">\(Y\)</span></p>
<div class="math notranslate nohighlight">
\[
Y(i,j)=(X * W)(i,j) = \sum_m\sum_n X(m,n)W(i-m,j-n).
\]</div>
<p>Convolution is a commutative process, which means we can rewrite this equation as</p>
<div class="math notranslate nohighlight">
\[
Y(i,j)=(X * W)(i,j) = \sum_m\sum_n X(i-m,j-n)W(m,n).
\]</div>
<p>Normally the latter is more straightforward to implement in a machine
larning library since there is less variation in the range of values
of <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>As mentioned above, most deep learning libraries implement
cross-correlation instead of convolution (although it is referred to as
convolution)</p>
<div class="math notranslate nohighlight">
\[
Y(i,j)=(X * W)(i,j) = \sum_m\sum_n X(i+m,j+n)W(m,n).
\]</div>
</section>
<section id="cnns-in-more-detail-simple-example">
<h2>CNNs in more detail, simple example<a class="headerlink" href="#cnns-in-more-detail-simple-example" title="Link to this heading">#</a></h2>
<p>Let assume we have an input matrix <span class="math notranslate nohighlight">\(X\)</span> of dimensionality <span class="math notranslate nohighlight">\(3\times 3\)</span>
and a <span class="math notranslate nohighlight">\(2\times 2\)</span> filter <span class="math notranslate nohighlight">\(W\)</span> given by the following matrices</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X}=\begin{bmatrix}x_{00} &amp; x_{01} &amp; x_{02}  \\
                      x_{10} &amp; x_{11} &amp; x_{12}  \\
	              x_{20} &amp; x_{21} &amp; x_{22} \end{bmatrix},
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{W}=\begin{bmatrix}w_{00} &amp; w_{01} \\
	              w_{10} &amp; w_{11}\end{bmatrix}.
\end{split}\]</div>
<p>We introduce now the hyperparameter <span class="math notranslate nohighlight">\(S\)</span> <strong>stride</strong>. Stride represents how the filter <span class="math notranslate nohighlight">\(W\)</span> moves the convolution process on the matrix <span class="math notranslate nohighlight">\(X\)</span>.
We strongly recommend the repository on <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic">Arithmetic of deep learning by Dumoulin and Visin</a></p>
<p>Here we set the stride equal to <span class="math notranslate nohighlight">\(S=1\)</span>, which means that, starting with the element <span class="math notranslate nohighlight">\(x_{00}\)</span>, the filter will act on <span class="math notranslate nohighlight">\(2\times 2\)</span> submatrices each time, starting with the upper corner and moving according to the stride value column by column.</p>
<p>Here we perform the operation</p>
<div class="math notranslate nohighlight">
\[
Y_(i,j)=(X * W)(i,j) = \sum_m\sum_n X(i-m,j-n)W(m,n),
\]</div>
<p>and obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{Y}=\begin{bmatrix}x_{00}w_{00}+x_{01}w_{01}+x_{10}w_{10}+x_{11}w_{11} &amp; x_{01}w_{00}+x_{02}w_{01}+x_{11}w_{10}+x_{12}w_{11}  \\
	              x_{10}w_{00}+x_{11}w_{01}+x_{20}w_{10}+x_{21}w_{11} &amp; x_{11}w_{00}+x_{12}w_{01}+x_{21}w_{10}+x_{22}w_{11}\end{bmatrix}.
\end{split}\]</div>
<p>We can rewrite this operation in terms of a matrix-vector multiplication by defining a new vector where we flatten out the inputs as a vector <span class="math notranslate nohighlight">\(\boldsymbol{X}'\)</span> of length <span class="math notranslate nohighlight">\(9\)</span> and
a matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}'\)</span> with dimension <span class="math notranslate nohighlight">\(4\times 9\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X}'=\begin{bmatrix}x_{00} \\ x_{01} \\ x_{02} \\ x_{10} \\ x_{11} \\ x_{12} \\ x_{20} \\ x_{21} \\ x_{22} \end{bmatrix},
\end{split}\]</div>
<p>and the new matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{W}'=\begin{bmatrix} w_{00} &amp; w_{01} &amp; 0 &amp; w_{10} &amp; w_{11} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
                        0  &amp; w_{00} &amp; w_{01} &amp; 0 &amp; w_{10} &amp; w_{11} &amp; 0 &amp; 0 &amp; 0 \\
			0 &amp; 0 &amp; 0 &amp; w_{00} &amp; w_{01} &amp; 0 &amp; w_{10} &amp; w_{11} &amp; 0  \\
                        0 &amp; 0 &amp; 0 &amp; 0 &amp; w_{00} &amp; w_{01} &amp; 0 &amp; w_{10} &amp; w_{11}\end{bmatrix}.
\end{split}\]</div>
<p>We see easily that performing the matrix-vector multiplication <span class="math notranslate nohighlight">\(\boldsymbol{W}'\boldsymbol{X}'\)</span> is the same as the above convolution with stride <span class="math notranslate nohighlight">\(S=1\)</span>, that is</p>
<div class="math notranslate nohighlight">
\[
Y=(\boldsymbol{W}*\boldsymbol{X}),
\]</div>
<p>is now given by <span class="math notranslate nohighlight">\(\boldsymbol{W}'\boldsymbol{X}'\)</span> which is a vector of length <span class="math notranslate nohighlight">\(4\)</span> instead of the originally resulting  <span class="math notranslate nohighlight">\(2\times 2\)</span> output matrix.</p>
</section>
<section id="the-convolution-stage">
<h2>The convolution stage<a class="headerlink" href="#the-convolution-stage" title="Link to this heading">#</a></h2>
<p>The convolution stage, where we apply different filters <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> in
order to reduce the dimensionality of an image, adds, in addition to
the weights and biases (to be trained by the back propagation
algorithm) that define the filters, two new hyperparameters, the so-called
<strong>padding</strong> <span class="math notranslate nohighlight">\(P\)</span> and the stride <span class="math notranslate nohighlight">\(S\)</span>.</p>
</section>
<section id="finding-the-number-of-parameters">
<h2>Finding the number of parameters<a class="headerlink" href="#finding-the-number-of-parameters" title="Link to this heading">#</a></h2>
<p>In the above example we have an input matrix of dimension <span class="math notranslate nohighlight">\(3\times
3\)</span>. In general we call the input for an input volume and it is defined
by its width <span class="math notranslate nohighlight">\(H_1\)</span>, height <span class="math notranslate nohighlight">\(H_1\)</span> and depth <span class="math notranslate nohighlight">\(D_1\)</span>. If we have the
standard three color channels <span class="math notranslate nohighlight">\(D_1=3\)</span>.</p>
<p>The above example has <span class="math notranslate nohighlight">\(W_1=H_1=3\)</span> and <span class="math notranslate nohighlight">\(D_1=1\)</span>.</p>
<p>When we introduce the filter we have the following additional hyperparameters</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(K\)</span> the number of filters. It is common to perform the convolution of the input several times since by experience shrinking the input too fast does not work well</p></li>
<li><p><span class="math notranslate nohighlight">\(F\)</span> as the filter’s spatial extent</p></li>
<li><p><span class="math notranslate nohighlight">\(S\)</span> as the stride parameter</p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span> as the padding parameter</p></li>
</ol>
<p>These parameters are defined by the architecture of the network and are not included in the training.</p>
</section>
<section id="new-image-or-volume">
<h2>New image (or volume)<a class="headerlink" href="#new-image-or-volume" title="Link to this heading">#</a></h2>
<p>Acting with the filter on the input volume produces an output volume
which is defined by its width <span class="math notranslate nohighlight">\(W_2\)</span>, its height <span class="math notranslate nohighlight">\(H_2\)</span> and its depth
<span class="math notranslate nohighlight">\(D_2\)</span>.</p>
<p>These are defined by the following relations</p>
<div class="math notranslate nohighlight">
\[
W_2 = \frac{(W_1-F+2P)}{S}+1,
\]</div>
<div class="math notranslate nohighlight">
\[
H_2 = \frac{(H_1-F+2P)}{S}+1,
\]</div>
<p>and <span class="math notranslate nohighlight">\(D_2=K\)</span>.</p>
</section>
<section id="parameters-to-train-common-settings">
<h2>Parameters to train, common settings<a class="headerlink" href="#parameters-to-train-common-settings" title="Link to this heading">#</a></h2>
<p>With parameter sharing, the convolution involves thus  for each filter  <span class="math notranslate nohighlight">\(F\times F\times D_1\)</span> weights plus one bias parameter.</p>
<p>In total we have</p>
<div class="math notranslate nohighlight">
\[
\left(F\times F\times D_1)\right) \times K+(K\mathrm{--biases}),
\]</div>
<p>parameters to train by back propagation.</p>
<p>It is common to let <span class="math notranslate nohighlight">\(K\)</span> come in powers of <span class="math notranslate nohighlight">\(2\)</span>, that is <span class="math notranslate nohighlight">\(32\)</span>, <span class="math notranslate nohighlight">\(64\)</span>, <span class="math notranslate nohighlight">\(128\)</span> etc.</p>
<p><strong>Common settings.</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\begin{array}{c} F=3 &amp; S=1 &amp; P=1 \end{array}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\begin{array}{c} F=5 &amp; S=1 &amp; P=2 \end{array}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\begin{array}{c} F=5 &amp; S=2 &amp; P=\mathrm{open} \end{array}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\begin{array}{c} F=1 &amp; S=1 &amp; P=0 \end{array}\)</span></p></li>
</ol>
</section>
<section id="examples-of-cnn-setups">
<h2>Examples of CNN setups<a class="headerlink" href="#examples-of-cnn-setups" title="Link to this heading">#</a></h2>
<p>Let us assume we have an input volume <span class="math notranslate nohighlight">\(V\)</span> given by an image of dimensionality
<span class="math notranslate nohighlight">\(32\times 32 \times 3\)</span>, that is three color channels and <span class="math notranslate nohighlight">\(32\times 32\)</span> pixels.</p>
<p>We apply a filter of dimension <span class="math notranslate nohighlight">\(5\times 5\)</span> ten times with stride <span class="math notranslate nohighlight">\(S=1\)</span> and padding <span class="math notranslate nohighlight">\(P=0\)</span>.</p>
<p>The output volume is given by <span class="math notranslate nohighlight">\((32-5)/1+1=28\)</span>, resulting in ten images
of dimensionality <span class="math notranslate nohighlight">\(28\times 28\times 3\)</span>.</p>
<p>The total number of parameters to train for each filter is then
<span class="math notranslate nohighlight">\(5\times 5\times 3+1\)</span>, where the last parameter is the bias. This
gives us <span class="math notranslate nohighlight">\(76\)</span> parameters for each filter, leading to a total of <span class="math notranslate nohighlight">\(760\)</span>
parameters for the ten filters.</p>
<p>How many parameters will a filter of dimensionality <span class="math notranslate nohighlight">\(3\times 3\)</span>
(adding color channels) result in if we produce <span class="math notranslate nohighlight">\(32\)</span> new images? Use <span class="math notranslate nohighlight">\(S=1\)</span> and <span class="math notranslate nohighlight">\(P=0\)</span>.</p>
<p>Note that strides constitute a form of <strong>subsampling</strong>. As an alternative to
being interpreted as a measure of how much the kernel/filter is translated, strides
can also be viewed as how much of the output is retained. For instance, moving
the kernel by hops of two is equivalent to moving the kernel by hops of one but
retaining only odd output elements.</p>
</section>
<section id="summarizing-performing-a-general-discrete-convolution-from-raschka-et-al">
<h2>Summarizing: Performing a general discrete convolution (<a class="reference external" href="https://github.com/rasbt/machine-learning-book">From Raschka et al</a>)<a class="headerlink" href="#summarizing-performing-a-general-discrete-convolution-from-raschka-et-al" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/discreteconv1.png, width=500 frac=0.67]  A deep CNN -->
<!-- begin figure -->
<p><img src="figslides/discreteconv1.png" width="500"><p style="font-size: 0.9em"><i>Figure 1: A deep CNN</i></p></p>
<!-- end figure --></section>
<section id="pooling">
<h2>Pooling<a class="headerlink" href="#pooling" title="Link to this heading">#</a></h2>
<p>In addition to discrete convolutions themselves, <strong>pooling</strong> operations
make up another important building block in CNNs. Pooling operations reduce
the size of feature maps by using some function to summarize subregions, such
as taking the average or the maximum value.</p>
<p>Pooling works by sliding a window across the input and feeding the content of
the window to a <strong>pooling function</strong>. In some sense, pooling works very much
like a discrete convolution, but replaces the linear combination described by
the kernel with some other function.</p>
</section>
<section id="pooling-arithmetic">
<h2>Pooling arithmetic<a class="headerlink" href="#pooling-arithmetic" title="Link to this heading">#</a></h2>
<p>In a neural network, pooling layers provide invariance to small translations of
the input. The most common kind of pooling is <strong>max pooling</strong>, which
consists in splitting the input in (usually non-overlapping) patches and
outputting the maximum value of each patch. Other kinds of pooling exist, e.g.,
mean or average pooling, which all share the same idea of aggregating the input
locally by applying a non-linearity to the content of some patches.</p>
</section>
<section id="pooling-types-from-raschka-et-al">
<h2>Pooling types (<a class="reference external" href="https://github.com/rasbt/machine-learning-book">From Raschka et al</a>)<a class="headerlink" href="#pooling-types-from-raschka-et-al" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/maxpooling.png, width=500 frac=0.67]  A deep CNN -->
<!-- begin figure -->
<p><img src="figslides/maxpooling.png" width="500"><p style="font-size: 0.9em"><i>Figure 1: A deep CNN</i></p></p>
<!-- end figure --></section>
<section id="building-convolutional-neural-networks-using-tensorflow-and-keras">
<h2>Building convolutional neural networks using Tensorflow and Keras<a class="headerlink" href="#building-convolutional-neural-networks-using-tensorflow-and-keras" title="Link to this heading">#</a></h2>
<p>As discussed above, CNNs are neural networks built from the assumption that the inputs
to the network are 2D images. This is important because the number of features or pixels in images
grows very fast with the image size, and an enormous number of weights and biases are needed in order to build an accurate network.</p>
<p>As before, we still have our input, a hidden layer and an output. What’s novel about convolutional networks
are the <strong>convolutional</strong> and <strong>pooling</strong> layers stacked in pairs between the input and the hidden layer.
In addition, the data is no longer represented as a 2D feature matrix, instead each input is a number of 2D
matrices, typically 1 for each color dimension (Red, Green, Blue).</p>
</section>
<section id="setting-it-up">
<h2>Setting it up<a class="headerlink" href="#setting-it-up" title="Link to this heading">#</a></h2>
<p>It means that to represent the entire
dataset of images, we require a 4D matrix or <strong>tensor</strong>. This tensor has the dimensions:</p>
<div class="math notranslate nohighlight">
\[
(n_{inputs},\, n_{pixels, width},\, n_{pixels, height},\, depth) .
\]</div>
</section>
<section id="the-mnist-dataset-again">
<h2>The MNIST dataset again<a class="headerlink" href="#the-mnist-dataset-again" title="Link to this heading">#</a></h2>
<p>The MNIST dataset consists of grayscale images with a pixel size of
<span class="math notranslate nohighlight">\(28\times 28\)</span>, meaning we require <span class="math notranslate nohighlight">\(28 \times 28 = 724\)</span> weights to each
neuron in the first hidden layer.</p>
<p>If we were to analyze images of size <span class="math notranslate nohighlight">\(128\times 128\)</span> we would require
<span class="math notranslate nohighlight">\(128 \times 128 = 16384\)</span> weights to each neuron. Even worse if we were
dealing with color images, as most images are, we have an image matrix
of size <span class="math notranslate nohighlight">\(128\times 128\)</span> for each color dimension (Red, Green, Blue),
meaning 3 times the number of weights <span class="math notranslate nohighlight">\(= 49152\)</span> are required for every
single neuron in the first hidden layer.</p>
</section>
<section id="strong-correlations">
<h2>Strong correlations<a class="headerlink" href="#strong-correlations" title="Link to this heading">#</a></h2>
<p>Images typically have strong local correlations, meaning that a small
part of the image varies little from its neighboring regions. If for
example we have an image of a blue car, we can roughly assume that a
small blue part of the image is surrounded by other blue regions.</p>
<p>Therefore, instead of connecting every single pixel to a neuron in the
first hidden layer, as we have previously done with deep neural
networks, we can instead connect each neuron to a small part of the
image (in all 3 RGB depth dimensions).  The size of each small area is
fixed, and known as a <a class="reference external" href="https://en.wikipedia.org/wiki/Receptive_field">receptive</a>.</p>
</section>
<section id="layers-of-a-cnn">
<h2>Layers of a CNN<a class="headerlink" href="#layers-of-a-cnn" title="Link to this heading">#</a></h2>
<p>The layers of a convolutional neural network arrange neurons in 3D: width, height and depth.<br />
The input image is typically a square matrix of depth 3.</p>
<p>A <strong>convolution</strong> is performed on the image which outputs
a 3D volume of neurons. The weights to the input are arranged in a number of 2D matrices, known as <strong>filters</strong>.</p>
<p>Each filter slides along the input image, taking the dot product
between each small part of the image and the filter, in all depth
dimensions. This is then passed through a non-linear function,
typically the <strong>Rectified Linear (ReLu)</strong> function, which serves as the
activation of the neurons in the first convolutional layer. This is
further passed through a <strong>pooling layer</strong>, which reduces the size of the
convolutional layer, e.g. by taking the maximum or average across some
small regions, and this serves as input to the next convolutional
layer.</p>
</section>
<section id="systematic-reduction">
<h2>Systematic reduction<a class="headerlink" href="#systematic-reduction" title="Link to this heading">#</a></h2>
<p>By systematically reducing the size of the input volume, through
convolution and pooling, the network should create representations of
small parts of the input, and then from them assemble representations
of larger areas.  The final pooling layer is flattened to serve as
input to a hidden layer, such that each neuron in the final pooling
layer is connected to every single neuron in the hidden layer. This
then serves as input to the output layer, e.g. a softmax output for
classification.</p>
</section>
<section id="prerequisites-collect-and-pre-process-data">
<h2>Prerequisites: Collect and pre-process data<a class="headerlink" href="#prerequisites-collect-and-pre-process-data" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>%matplotlib inline

# import necessary packages
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets


# ensure the same random numbers appear every time
np.random.seed(0)

# display images in notebook
%matplotlib inline
plt.rcParams[&#39;figure.figsize&#39;] = (12,12)


# download MNIST dataset
digits = datasets.load_digits()

# define inputs and labels
inputs = digits.images
labels = digits.target

# RGB images have a depth of 3
# our images are grayscale so they should have a depth of 1
inputs = inputs[:,:,:,np.newaxis]

print(&quot;inputs = (n_inputs, pixel_width, pixel_height, depth) = &quot; + str(inputs.shape))
print(&quot;labels = (n_inputs) = &quot; + str(labels.shape))


# choose some random images to display
n_inputs = len(inputs)
indices = np.arange(n_inputs)
random_indices = np.random.choice(indices, size=5)

for i, image in enumerate(digits.images[random_indices]):
    plt.subplot(1, 5, i+1)
    plt.axis(&#39;off&#39;)
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;)
    plt.title(&quot;Label: %d&quot; % digits.target[random_indices[i]])
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="importing-keras-and-tensorflow">
<h2>Importing Keras and Tensorflow<a class="headerlink" href="#importing-keras-and-tensorflow" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from tensorflow.keras import datasets, layers, models
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Sequential      #This allows appending layers to existing models
from tensorflow.keras.layers import Dense           #This allows defining the characteristics of a particular layer
from tensorflow.keras import optimizers             #This allows using whichever optimiser we want (sgd,adam,RMSprop)
from tensorflow.keras import regularizers           #This allows using whichever regularizer we want (l1,l2,l1_l2)
from tensorflow.keras.utils import to_categorical   #This allows using categorical cross entropy as the cost function
#from tensorflow.keras import Conv2D
#from tensorflow.keras import MaxPooling2D
#from tensorflow.keras import Flatten

from sklearn.model_selection import train_test_split

# representation of labels
labels = to_categorical(labels)

# split into train and test data
# one-liner from scikit-learn library
train_size = 0.8
test_size = 1 - train_size
X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, train_size=train_size,
                                                    test_size=test_size)
</pre></div>
</div>
</div>
</div>
</section>
<section id="running-with-keras">
<h2>Running with Keras<a class="headerlink" href="#running-with-keras" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def create_convolutional_neural_network_keras(input_shape, receptive_field,
                                              n_filters, n_neurons_connected, n_categories,
                                              eta, lmbd):
    model = Sequential()
    model.add(layers.Conv2D(n_filters, (receptive_field, receptive_field), input_shape=input_shape, padding=&#39;same&#39;,
              activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(lmbd)))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Flatten())
    model.add(layers.Dense(n_neurons_connected, activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(lmbd)))
    model.add(layers.Dense(n_categories, activation=&#39;softmax&#39;, kernel_regularizer=regularizers.l2(lmbd)))
    
    sgd = optimizers.SGD(lr=eta)
    model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=sgd, metrics=[&#39;accuracy&#39;])
    
    return model

epochs = 100
batch_size = 100
input_shape = X_train.shape[1:4]
receptive_field = 3
n_filters = 10
n_neurons_connected = 50
n_categories = 10

eta_vals = np.logspace(-5, 1, 7)
lmbd_vals = np.logspace(-5, 1, 7)
</pre></div>
</div>
</div>
</div>
</section>
<section id="final-part">
<h2>Final part<a class="headerlink" href="#final-part" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>CNN_keras = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)
        
for i, eta in enumerate(eta_vals):
    for j, lmbd in enumerate(lmbd_vals):
        CNN = create_convolutional_neural_network_keras(input_shape, receptive_field,
                                              n_filters, n_neurons_connected, n_categories,
                                              eta, lmbd)
        CNN.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=0)
        scores = CNN.evaluate(X_test, Y_test)
        
        CNN_keras[i][j] = CNN
        
        print(&quot;Learning rate = &quot;, eta)
        print(&quot;Lambda = &quot;, lmbd)
        print(&quot;Test accuracy: %.3f&quot; % scores[1])
        print()
</pre></div>
</div>
</div>
</div>
</section>
<section id="final-visualization">
<h2>Final visualization<a class="headerlink" href="#final-visualization" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># visual representation of grid search
# uses seaborn heatmap, could probably do this in matplotlib
import seaborn as sns

sns.set()

train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))
test_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))

for i in range(len(eta_vals)):
    for j in range(len(lmbd_vals)):
        CNN = CNN_keras[i][j]

        train_accuracy[i][j] = CNN.evaluate(X_train, Y_train)[1]
        test_accuracy[i][j] = CNN.evaluate(X_test, Y_test)[1]

        
fig, ax = plt.subplots(figsize = (10, 10))
sns.heatmap(train_accuracy, annot=True, ax=ax, cmap=&quot;viridis&quot;)
ax.set_title(&quot;Training Accuracy&quot;)
ax.set_ylabel(&quot;$\eta$&quot;)
ax.set_xlabel(&quot;$\lambda$&quot;)
plt.show()

fig, ax = plt.subplots(figsize = (10, 10))
sns.heatmap(test_accuracy, annot=True, ax=ax, cmap=&quot;viridis&quot;)
ax.set_title(&quot;Test Accuracy&quot;)
ax.set_ylabel(&quot;$\eta$&quot;)
ax.set_xlabel(&quot;$\lambda$&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-cifar01-data-set">
<h2>The CIFAR01 data set<a class="headerlink" href="#the-cifar01-data-set" title="Link to this heading">#</a></h2>
<p>The CIFAR10 dataset contains 60,000 color images in 10 classes, with
6,000 images in each class. The dataset is divided into 50,000
training images and 10,000 testing images. The classes are mutually
exclusive and there is no overlap between them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import tensorflow as tf

from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

# We import the data set
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Normalize pixel values to be between 0 and 1 by dividing by 255. 
train_images, test_images = train_images / 255.0, test_images / 255.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="verifying-the-data-set">
<h2>Verifying the data set<a class="headerlink" href="#verifying-the-data-set" title="Link to this heading">#</a></h2>
<p>To verify that the dataset looks correct, let’s plot the first 25 images from the training set and display the class name below each image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;,
               &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    # The CIFAR labels happen to be arrays, 
    # which is why you need the extra index
    plt.xlabel(class_names[train_labels[i][0]])
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="set-up-the-model">
<h2>Set up  the model<a class="headerlink" href="#set-up-the-model" title="Link to this heading">#</a></h2>
<p>The 6 lines of code below define the convolutional base using a common pattern: a stack of Conv2D and MaxPooling2D layers.</p>
<p>As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to these dimensions, color_channels refers to (R,G,B). In this example, you will configure our CNN to process inputs of shape (32, 32, 3), which is the format of CIFAR images. You can do this by passing the argument input_shape to our first layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;))

# Let&#39;s display the architecture of our model so far.

model.summary()
</pre></div>
</div>
</div>
</div>
<p>You can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically, as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer.</p>
</section>
<section id="add-dense-layers-on-top">
<h2>Add Dense layers on top<a class="headerlink" href="#add-dense-layers-on-top" title="Link to this heading">#</a></h2>
<p>To complete our model, you will feed the last output tensor from the
convolutional base (of shape (4, 4, 64)) into one or more Dense layers
to perform classification. Dense layers take vectors as input (which
are 1D), while the current output is a 3D tensor. First, you will
flatten (or unroll) the 3D output to 1D, then add one or more Dense
layers on top. CIFAR has 10 output classes, so you use a final Dense
layer with 10 outputs and a softmax activation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>model.add(layers.Flatten())
model.add(layers.Dense(64, activation=&#39;relu&#39;))
model.add(layers.Dense(10))
Here&#39;s the complete architecture of our model.

model.summary()
</pre></div>
</div>
</div>
</div>
<p>As you can see, our (4, 4, 64) outputs were flattened into vectors of shape (1024) before going through two Dense layers.</p>
</section>
<section id="compile-and-train-the-model">
<h2>Compile and train the model<a class="headerlink" href="#compile-and-train-the-model" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>model.compile(optimizer=&#39;adam&#39;,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=[&#39;accuracy&#39;])

history = model.fit(train_images, train_labels, epochs=10, 
                    validation_data=(test_images, test_labels))
</pre></div>
</div>
</div>
</div>
</section>
<section id="finally-evaluate-the-model">
<h2>Finally, evaluate the model<a class="headerlink" href="#finally-evaluate-the-model" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>plt.plot(history.history[&#39;accuracy&#39;], label=&#39;accuracy&#39;)
plt.plot(history.history[&#39;val_accuracy&#39;], label = &#39;val_accuracy&#39;)
plt.xlabel(&#39;Epoch&#39;)
plt.ylabel(&#39;Accuracy&#39;)
plt.ylim([0.5, 1])
plt.legend(loc=&#39;lower right&#39;)

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

print(test_acc)
</pre></div>
</div>
</div>
</div>
</section>
<section id="building-code-using-pytorch">
<h2>Building code using Pytorch<a class="headerlink" href="#building-code-using-pytorch" title="Link to this heading">#</a></h2>
<p>This code loads and normalizes the MNIST dataset. Thereafter it defines  a CNN architecture with:</p>
<ol class="arabic simple">
<li><p>Two convolutional layers</p></li>
<li><p>Max pooling</p></li>
<li><p>Dropout for regularization</p></li>
<li><p>Two fully connected layers</p></li>
</ol>
<p>It uses the Adam optimizer and for cost function it employs the
Cross-Entropy function. It trains for 10 epochs.
You can modify the architecture (number of layers, channels, dropout
rate) or training parameters (learning rate, batch size, epochs) to
experiment with different configurations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms

# Set device
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Define transforms
transform = transforms.Compose([
   transforms.ToTensor(),
   transforms.Normalize((0.1307,), (0.3081,))
])

# Load datasets
train_dataset = datasets.MNIST(root=&#39;./data&#39;, train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root=&#39;./data&#39;, train=False, download=True, transform=transform)

# Create data loaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)

# Define CNN model
class CNN(nn.Module):
   def __init__(self):
       super(CNN, self).__init__()
       self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
       self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
       self.pool = nn.MaxPool2d(2, 2)
       self.fc1 = nn.Linear(64*7*7, 1024)
       self.fc2 = nn.Linear(1024, 10)
       self.dropout = nn.Dropout(0.5)

   def forward(self, x):
       x = self.pool(F.relu(self.conv1(x)))
       x = self.pool(F.relu(self.conv2(x)))
       x = x.view(-1, 64*7*7)
       x = self.dropout(F.relu(self.fc1(x)))
       x = self.fc2(x)
       return x

# Initialize model, loss function, and optimizer
model = CNN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
   model.train()
   running_loss = 0.0
   for batch_idx, (data, target) in enumerate(train_loader):
       data, target = data.to(device), target.to(device)
       optimizer.zero_grad()
       outputs = model(data)
       loss = criterion(outputs, target)
       loss.backward()
       optimizer.step()
       running_loss += loss.item()

   print(f&#39;Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}&#39;)

# Testing the model
model.eval()
correct = 0
total = 0
with torch.no_grad():
   for data, target in test_loader:
       data, target = data.to(device), target.to(device)
       outputs = model(data)
       _, predicted = torch.max(outputs.data, 1)
       total += target.size(0)
       correct += (predicted == target).sum().item()

print(f&#39;Test Accuracy: {100 * correct / total:.2f}%&#39;)
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="exercisesweek44.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Exercises week 44</p>
      </div>
    </a>
    <a class="right-next"
       href="project1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Project 1 on Machine Learning, deadline October 6 (midnight), 2025</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plans-for-week-45">Plans for week 45</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#material-for-the-lab-sessions">Material for the lab sessions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#material-for-lecture-monday-november-3">Material for Lecture Monday November 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-recognizing-images-reminder-from-last-week">Convolutional Neural Networks (recognizing images), reminder from last week</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-difference">What is the Difference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-vs-cnns">Neural Networks vs CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-cnns-for-images-sound-files-medical-images-from-ct-scans-etc">Why CNNS for images, sound files, medical images from CT scans etc?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regular-nns-dont-scale-well-to-full-images">Regular NNs don’t scale well to full images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-volumes-of-neurons">3D volumes of neurons</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-on-dimensionalities">More on Dimensionalities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-remarks">Further remarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layers-used-to-build-cnns">Layers used to build CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transforming-images">Transforming images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnns-in-brief">CNNs in brief</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-deep-cnn-model-from-raschka-et-al">A deep CNN model (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-idea">Key Idea</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-cnns">Mathematics of CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution-examples-polynomial-multiplication">Convolution Examples: Polynomial multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#efficient-polynomial-multiplication">Efficient Polynomial Multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-simplification">Further simplification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-more-efficient-way-of-coding-the-above-convolution">A more efficient way of coding the above Convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commutative-process">Commutative process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#toeplitz-matrices">Toeplitz matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fourier-series-and-toeplitz-matrices">Fourier series and Toeplitz matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-the-above-one-dimensional-case">Generalizing the above one-dimensional case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-considerations">Memory considerations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#padding">Padding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-vector">New vector</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rewriting-as-dot-products">Rewriting as dot products</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-correlation">Cross correlation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-dimensional-objects">Two-dimensional objects</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnns-in-more-detail-simple-example">CNNs in more detail, simple example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-convolution-stage">The convolution stage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-number-of-parameters">Finding the number of parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-image-or-volume">New image (or volume)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-to-train-common-settings">Parameters to train, common settings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-cnn-setups">Examples of CNN setups</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summarizing-performing-a-general-discrete-convolution-from-raschka-et-al">Summarizing: Performing a general discrete convolution (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling">Pooling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-arithmetic">Pooling arithmetic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-types-from-raschka-et-al">Pooling types (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-convolutional-neural-networks-using-tensorflow-and-keras">Building convolutional neural networks using Tensorflow and Keras</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-it-up">Setting it up</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mnist-dataset-again">The MNIST dataset again</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#strong-correlations">Strong correlations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layers-of-a-cnn">Layers of a CNN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#systematic-reduction">Systematic reduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites-collect-and-pre-process-data">Prerequisites: Collect and pre-process data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importing-keras-and-tensorflow">Importing Keras and Tensorflow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-with-keras">Running with Keras</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-part">Final part</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-visualization">Final visualization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cifar01-data-set">The CIFAR01 data set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verifying-the-data-set">Verifying the data set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up-the-model">Set up  the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#add-dense-layers-on-top">Add Dense layers on top</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compile-and-train-the-model">Compile and train the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finally-evaluate-the-model">Finally, evaluate the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-code-using-pytorch">Building code using Pytorch</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Morten Hjorth-Jensen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>