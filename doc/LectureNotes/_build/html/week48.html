
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 48: Autoencoders and summary of course &#8212; Applied Data Analysis and Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week48';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Project 1 on Machine Learning, deadline October 6 (midnight), 2025" href="project1.html" />
    <link rel="prev" title="Exercise week 47-48" href="exercisesweek47.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Applied Data Analysis and Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Applied Data Analysis and Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Applied Data Analysis and Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About the course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Course setting</a></li>
<li class="toctree-l1"><a class="reference internal" href="teachers.html">Teachers and Grading</a></li>
<li class="toctree-l1"><a class="reference internal" href="textbooks.html">Textbooks</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Review of Statistics with Resampling Techniques and Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="statistics.html">1. Elements of Probability Theory and Statistical Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">2. Linear Algebra, Handling of Arrays and more Python Features</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Regression to Support Vector Machines</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter1.html">3. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter2.html">4. Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter3.html">5. Resampling Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter4.html">6. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapteroptimization.html">7. Optimization, the central part of any Machine Learning algortithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter5.html">8. Support Vector Machines, overarching aims</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Decision Trees, Ensemble Methods and Boosting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter6.html">9. Decision trees, overarching aims</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter7.html">10. Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Dimensionality Reduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter8.html">11. Basic ideas of the Principal Component Analysis (PCA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">12. Clustering and Unsupervised Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter9.html">13. Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter10.html">14. Building a Feed Forward Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter11.html">15. Solving Differential Equations  with Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter12.html">16. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter13.html">17. Recurrent neural networks: Overarching view</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Weekly material, notes and exercises</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="exercisesweek34.html">Exercises week 34</a></li>
<li class="toctree-l1"><a class="reference internal" href="week34.html">Week 34: Introduction to the course, Logistics and Practicalities</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek35.html">Exercises week 35</a></li>
<li class="toctree-l1"><a class="reference internal" href="week35.html">Week 35: From Ordinary Linear Regression to Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek36.html">Exercises week 36</a></li>
<li class="toctree-l1"><a class="reference internal" href="week36.html">Week 36: Linear Regression and Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek37.html">Exercises week 37</a></li>
<li class="toctree-l1"><a class="reference internal" href="week37.html">Week 37: Gradient descent methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek38.html">Exercises week 38</a></li>
<li class="toctree-l1"><a class="reference internal" href="week38.html">Week 38: Statistical analysis, bias-variance tradeoff and resampling methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek39.html">Exercises week 39</a></li>
<li class="toctree-l1"><a class="reference internal" href="week39.html">Week 39: Resampling methods and logistic regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="week40.html">Week 40: Gradient descent methods (continued) and start Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="week41.html">Week 41 Neural networks and constructing a neural network code</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek41.html">Exercises week 41</a></li>








<li class="toctree-l1"><a class="reference internal" href="week42.html">Week 42 Constructing a Neural Network code with examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek42.html">Exercises week 42</a></li>









<li class="toctree-l1"><a class="reference internal" href="week43.html">Week 43: Deep Learning: Constructing a Neural Network code and solving differential equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek43.html">Exercises week 43</a></li>

<li class="toctree-l1"><a class="reference internal" href="week44.html">Week 44,  Solving differential equations with neural networks and start Convolutional Neural Networks (CNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek44.html">Exercises week 44</a></li>

<li class="toctree-l1"><a class="reference internal" href="week45.html">Week 45,  Convolutional Neural Networks (CCNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="week46.html">Week 46: Decision Trees, Ensemble methods  and Random Forests</a></li>
<li class="toctree-l1"><a class="reference internal" href="week47.html">Week 47: Recurrent neural networks and Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek47.html">Exercise week 47-48</a></li>

<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 48: Autoencoders and summary of course</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="project1.html">Project 1 on Machine Learning, deadline October 6 (midnight), 2025</a></li>
<li class="toctree-l1"><a class="reference internal" href="project2.html">Project 2 on Machine Learning, deadline November 10 (Midnight)</a></li>
<li class="toctree-l1"><a class="reference internal" href="project3.html">Project 3 on Machine Learning, deadline December 15 (midnight), 2025</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/week48.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 48: Autoencoders and summary of course</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-week-48">Overview of week 48</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-monday-november-24">Lecture Monday, November 24</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#readings-and-videos">Readings and Videos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-sessions">Lab sessions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoders-overarching-view">Autoencoders: Overarching view</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#powerful-detectors">Powerful detectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-introduction-of-aes">First introduction of AEs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoder-structure">Autoencoder structure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#schematic-image-of-an-autoencoder">Schematic image of an Autoencoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-on-the-structure">More on the structure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-part">Decoder part</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#typical-aes">Typical AEs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-autoencoder">Feed Forward Autoencoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mirroring">Mirroring</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#output-of-middle-layer">Output of middle layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-function-of-the-output-layer">Activation Function of the Output Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relu">ReLU</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid">Sigmoid</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-loss-function">Cost/Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy">Binary Cross-Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruction-error">Reconstruction Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temporary-summary-on-autoencoders">Temporary summary on Autoencoders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-autoencoders-and-the-pca-theorem">Linear autoencoders and  the PCA theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-details">More details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-pca-theorem">The PCA Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-algorithm-before-theorem">The Algorithm before theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-steps">Further steps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-our-own-pca-code">Writing our own PCA code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-it">Implementing it</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-step">First Step</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling">Scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#centered-data">Centered Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring">Exploring</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagonalize-the-sample-covariance-matrix-to-obtain-the-principal-components">Diagonalize the sample covariance matrix to obtain the principal components</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collecting-all-steps">Collecting all Steps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classical-pca-theorem">Classical PCA Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">The PCA Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-interpretation-and-link-with-singular-value-decomposition">Geometric Interpretation and link with Singular Value Decomposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-and-scikit-learn">PCA and scikit-learn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-cancer-data">Example of Cancer Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#incremental-pca">Incremental PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#randomized-pca">Randomized PCA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-pca">Kernel PCA</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-techniques">Other techniques</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-autoencoders-linear-autoencoders">Back to Autoencoders: Linear Autoencoders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-example">PyTorch example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-course">Summary of course</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-me-worry-no-final-exam-in-this-course">What? Me worry? No final exam in this course!</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topics-we-have-covered-this-year">Topics we have covered this year</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-analysis-and-optimization-of-data">Statistical analysis and optimization of data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes-and-overarching-aims-of-this-course">Learning outcomes and overarching aims of this course</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perspective-on-machine-learning">Perspective on Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-research">Machine Learning Research</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#starting-your-machine-learning-project">Starting your Machine Learning Project</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choose-a-model-and-algorithm">Choose a Model and Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-your-data">Preparing Your Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#which-activation-and-weights-to-choose-in-neural-networks">Which activation and weights to choose in neural networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-methods-and-hyperparameters">Optimization Methods and Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling">Resampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-courses-on-data-science-and-machine-learning-at-uio">Other courses on Data science and Machine Learning  at UiO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-courses-of-interest">Additional courses of interest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-the-future-like">What’s the future like?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning-a-repetition">Types of Machine Learning, a repetition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-boltzmann-machines">Why Boltzmann machines?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boltzmann-machines">Boltzmann Machines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-similarities-and-differences-from-dnns">Some similarities and differences from DNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boltzmann-machines-bm">Boltzmann machines (BM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-standard-bm-setup">A standard BM setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-structure-of-the-rbm-network">The structure of the RBM network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-network">The network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goals">Goals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distribution">Joint distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-elements-the-energy-function">Network Elements, the energy function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-different-types-of-rbms">Defining different types of RBMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-about-rbms">More about RBMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Autoencoders: Overarching view</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-machine-learning">Bayesian Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning">Reinforcement Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">Transfer learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adversarial-learning">Adversarial learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dual-learning">Dual learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-machine-learning">Distributed machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#meta-learning">Meta learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-challenges-facing-machine-learning">The Challenges Facing Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explainable-machine-learning">Explainable machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantum-machine-learning">Quantum machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantum-machine-learning-algorithms-based-on-linear-algebra">Quantum machine learning algorithms based on linear algebra</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantum-reinforcement-learning">Quantum reinforcement learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantum-deep-learning">Quantum deep learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#social-machine-learning">Social machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-last-words">The last words?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#best-wishes-to-you-all-and-thanks-so-much-for-your-heroic-efforts-this-semester">Best wishes to you all and thanks so much for your heroic efforts this semester</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)
doconce format html week48.do.txt  -->
<!-- dom:TITLE: Week 48: Autoencoders and summary of course --><section class="tex2jax_ignore mathjax_ignore" id="week-48-autoencoders-and-summary-of-course">
<h1>Week 48: Autoencoders and summary of course<a class="headerlink" href="#week-48-autoencoders-and-summary-of-course" title="Link to this heading">#</a></h1>
<p><strong>Morten Hjorth-Jensen</strong>, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</p>
<p>Date: <strong>Nov 24, 2025</strong></p>
<p>Copyright 1999-2025, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license</p>
<section id="overview-of-week-48">
<h2>Overview of week 48<a class="headerlink" href="#overview-of-week-48" title="Link to this heading">#</a></h2>
</section>
<section id="lecture-monday-november-24">
<h2>Lecture Monday, November 24<a class="headerlink" href="#lecture-monday-november-24" title="Link to this heading">#</a></h2>
<p><strong>Plans for the lecture Monday 24 November.</strong></p>
<ol class="arabic simple">
<li><p>Discussion of Autoencoders and principal component analysis</p></li>
<li><p>Summary of course</p></li>
</ol>
</section>
<section id="readings-and-videos">
<h2>Readings and Videos<a class="headerlink" href="#readings-and-videos" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>These lecture notes at <a class="github reference external" href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week48/ipynb/week48.ipynb">CompPhysics/MachineLearning</a></p></li>
<li><p>Video of lecture at <a class="reference external" href="https://youtu.be/h2Qa49KlY50">https://youtu.be/h2Qa49KlY50</a></p></li>
<li><p>Whiteboard notes at <a class="github reference external" href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2025/FYSSTKweek48.pdf">CompPhysics/MachineLearning</a></p></li>
<li><p>Video on Autoencoders at <a class="reference external" href="https://www.youtube.com/watch?v=hZ4a4NgM3u0">https://www.youtube.com/watch?v=hZ4a4NgM3u0</a></p></li>
<li><p>Goodfellow et al chapter 14.</p></li>
<li><p>Rashcka et al. Their chapter 17 contains a brief introduction only.</p></li>
<li><p>Deep Learning Tutorial on AEs from Stanford University at <a class="reference external" href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/">http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/</a></p></li>
<li><p>Building AEs in Keras at <a class="reference external" href="https://blog.keras.io/building-autoencoders-in-keras.html">https://blog.keras.io/building-autoencoders-in-keras.html</a></p></li>
</ol>
</section>
<section id="lab-sessions">
<h2>Lab sessions<a class="headerlink" href="#lab-sessions" title="Link to this heading">#</a></h2>
<p><strong>Lab sessions on Tuesday and Wednesday.</strong></p>
<ol class="arabic simple">
<li><p>Work on and discussion of project 3.</p></li>
<li><p>See updated note on usage of LLMs at the beginning of project description</p></li>
<li><p>Last weekly exercise, see exercises week 47 and 48, deadline November 28</p></li>
</ol>
</section>
<section id="autoencoders-overarching-view">
<h2>Autoencoders: Overarching view<a class="headerlink" href="#autoencoders-overarching-view" title="Link to this heading">#</a></h2>
<p>Autoencoders are artificial neural networks capable of learning
efficient representations of the input data (these representations are called codings)  without
any supervision (i.e., the training set is unlabeled). These codings
typically have a much lower dimensionality than the input data, making
autoencoders useful for dimensionality reduction.</p>
<p>Autoencoders learn to encode the
input data into a lower-dimensional representation, and then decode it
back to the original data. The goal of autoencoders is to minimize the
reconstruction error, which measures how well the output matches the
input. Autoencoders can be seen as a way of learning the latent
features or hidden structure of the data, and they can be used for
data compression, denoising, anomaly detection, and generative
modeling.</p>
</section>
<section id="powerful-detectors">
<h2>Powerful detectors<a class="headerlink" href="#powerful-detectors" title="Link to this heading">#</a></h2>
<p>More importantly, autoencoders act as powerful feature detectors, and
they can be used for unsupervised pretraining of deep neural networks.</p>
<p>Lastly, they are capable of randomly generating new data that looks
very similar to the training data; this is called a generative
model. For example, you could train an autoencoder on pictures of
faces, and it would then be able to generate new faces.  Surprisingly,
autoencoders work by simply learning to copy their inputs to their
outputs. This may sound like a trivial task, but we will see that
constraining the network in various ways can make it rather
difficult. For example, you can limit the size of the internal
representation, or you can add noise to the inputs and train the
network to recover the original inputs. These constraints prevent the
autoencoder from trivially copying the inputs directly to the outputs,
which forces it to learn efficient ways of representing the data. In
short, the codings are byproducts of the autoencoder’s attempt to
learn the identity function under some constraints.</p>
</section>
<section id="first-introduction-of-aes">
<h2>First introduction of AEs<a class="headerlink" href="#first-introduction-of-aes" title="Link to this heading">#</a></h2>
<p>Autoencoders were first introduced by Rumelhart, Hinton, and Williams
in 1986 with the goal of learning to reconstruct the input
observations with the lowest error possible.</p>
<p>Why would one want to learn to reconstruct the input observations? If
you have problems imagining what that means, think of having a dataset
made of images. An autoencoder would be an algorithm that can give as
output an image that is as similar as possible to the input one. You
may be confused, as there is no apparent reason of doing so. To better
understand why autoencoders are useful we need a more informative
(although not yet unambiguous) definition.</p>
<p>An autoencoder is a type of algorithm with the primary purpose of learning an “informative” representation of the data that can be used for different applications (<a class="reference external" href="https://arxiv.org/abs/2003.05991">see Bank, D., Koenigstein, N., and Giryes, R., Autoencoders</a>) by learning to reconstruct a set of input observations well enough.</p>
</section>
<section id="autoencoder-structure">
<h2>Autoencoder structure<a class="headerlink" href="#autoencoder-structure" title="Link to this heading">#</a></h2>
<p>Autoencoders are neural networks where the outputs are its own
inputs. They are split into an <strong>encoder part</strong>
which maps the input <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> via a function <span class="math notranslate nohighlight">\(f(\boldsymbol{x},\boldsymbol{W})\)</span> (this
is the encoder part) to a <strong>so-called code part</strong> (or intermediate part)
with the result <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span></p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{h} = f(\boldsymbol{x},\boldsymbol{W})),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> are the weights to be determined.  The <strong>decoder</strong> parts maps, via its own parameters (weights given by the matrix <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> and its own biases) to
the final ouput</p>
<div class="math notranslate nohighlight">
\[
\tilde{\boldsymbol{x}} = g(\boldsymbol{h},\boldsymbol{V})).
\]</div>
<p>The goal is to minimize the construction error.</p>
</section>
<section id="schematic-image-of-an-autoencoder">
<h2>Schematic image of an Autoencoder<a class="headerlink" href="#schematic-image-of-an-autoencoder" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figures/ae1.png, width=700 frac=1.0] -->
<!-- begin figure -->
<p><img src="figures/ae1.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="more-on-the-structure">
<h2>More on the structure<a class="headerlink" href="#more-on-the-structure" title="Link to this heading">#</a></h2>
<p>In most typical architectures, the encoder and the decoder are neural networks
since they can be easily trained with existing software libraries such as TensorFlow or PyTorch with back propagation.</p>
<p>In general, the encoder can be written as a function <span class="math notranslate nohighlight">\(g\)</span> that will depend on some parameters</p>
<div class="math notranslate nohighlight">
\[
\mathbf{h}_{i} = g(\mathbf{x}_{i}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{h}_{i}\in\mathbb{R}^{q}\)</span>  (the latent feature representation) is the output of the encoder block where we evaluate
it using the input <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span>.</p>
</section>
<section id="decoder-part">
<h2>Decoder part<a class="headerlink" href="#decoder-part" title="Link to this heading">#</a></h2>
<p>Note that we have <span class="math notranslate nohighlight">\(g:\mathbb{R}^{n}\rightarrow\mathbb{R}^{q}\)</span>
The decoder and the output of the network <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}_{i}\)</span> can be written then as a second generic function
of the latent features</p>
<div class="math notranslate nohighlight">
\[
\tilde{\mathbf{x}}_{i} = f\left(\mathbf{h}_{i}\right) = f\left(g\left(\mathbf{x}_{i}\right)\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}_{i}\mathbf{\in }\mathbb{R}^{n}\)</span>.</p>
<p>Training an autoencoder simply means finding the functions <span class="math notranslate nohighlight">\(g(\cdot)\)</span> and <span class="math notranslate nohighlight">\(f(\cdot)\)</span>
that satisfy</p>
<div class="math notranslate nohighlight">
\[
\textrm{arg}\min_{f,g}&lt;\left[\Delta (\mathbf{x}_{i}, f(g\left(\mathbf{x}_{i}\right))\right]&gt;.
\]</div>
</section>
<section id="typical-aes">
<h2>Typical AEs<a class="headerlink" href="#typical-aes" title="Link to this heading">#</a></h2>
<p>The standard setup is done via a standard feed forward neural network (FFNN), or what is called a Feed Forward Autoencoder.</p>
<p>A typical FFNN architecture has an odd number of layers and is symmetrical with respect to the middle layer.</p>
<p>Typically, the first layer has a number of neurons <span class="math notranslate nohighlight">\(n_{1} = n\)</span> which equals the size of the input observation <span class="math notranslate nohighlight">\(\mathbf{x}_{\mathbf{i}}\)</span>.</p>
<p>As we move toward the center of the network, the number of neurons in each layer drops in some measure.
The middle layer usually has the smallest number of neurons.
The fact that the number of neurons in this layer is smaller than the size of the input, is often called the <strong>bottleneck</strong>.</p>
</section>
<section id="feed-forward-autoencoder">
<h2>Feed Forward Autoencoder<a class="headerlink" href="#feed-forward-autoencoder" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figures/ae2.png, width=700 frac=1.0] -->
<!-- begin figure -->
<p><img src="figures/ae2.png" width="700"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="mirroring">
<h2>Mirroring<a class="headerlink" href="#mirroring" title="Link to this heading">#</a></h2>
<p>In almost all practical applications,
the layers after the middle one are a mirrored version of the layers before the middle one.
For example, an autoencoder with three layers could have the following numbers of neurons:</p>
<p><span class="math notranslate nohighlight">\(n_{1} = 10\)</span>, <span class="math notranslate nohighlight">\(n_{2} = 5\)</span> and then <span class="math notranslate nohighlight">\(n_{3} = n_{1} = 10\)</span> where the input dimension is equal to ten.</p>
<p>All the layers up to and including the middle one, make what is called the encoder, and all the layers from and including
the middle one (up to the output) make what is called the decoder.</p>
<p>If the FFNN training is successful, the result will
be a good approximation of the input <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}_{i}\approx\mathbf{x}_{i}\)</span>.</p>
<p>What is essential to notice is that the decoder can reconstruct the
input by using only a much smaller number of features than the input
observations initially have.</p>
</section>
<section id="output-of-middle-layer">
<h2>Output of middle layer<a class="headerlink" href="#output-of-middle-layer" title="Link to this heading">#</a></h2>
<p>The output of the middle layer
<span class="math notranslate nohighlight">\(\mathbf{h}_{\mathbf{i}}\)</span> are also called a <strong>learned representation</strong> of the input observation <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span>.</p>
<p>The encoder can reduce the number of dimensions of the input
observation and create a learned representation of the input that has a smaller
dimension <span class="math notranslate nohighlight">\(q&lt;n\)</span>.</p>
<p>This learned representation is enough for the decoder to reconstruct
the input accurately (if the autoencoder training was successful as
intended).</p>
</section>
<section id="activation-function-of-the-output-layer">
<h2>Activation Function of the Output Layer<a class="headerlink" href="#activation-function-of-the-output-layer" title="Link to this heading">#</a></h2>
<p>In autoencoders based on neural networks, the output layer’s
activation function plays a particularly important role.  The most
used functions are ReLU and Sigmoid.</p>
</section>
<section id="relu">
<h2>ReLU<a class="headerlink" href="#relu" title="Link to this heading">#</a></h2>
<p>The  ReLU activation function can assume all values in the range <span class="math notranslate nohighlight">\(\left[0,\infty\right]\)</span>. As a remainder, its formula is</p>
<div class="math notranslate nohighlight">
\[
\textrm{ReLU}\left(x\right) = \max\left(0,x\right).
\]</div>
<p>This choice is good when the input observations (\mathbf{x}_{i}) assume a wide range of positive values.
If the input <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span> can assume negative values, the ReLU is, of course, a terrible choice, and the identity function is a much better choice. It is then common to replace to the ReLU with the so-called <strong>Leaky ReLu</strong> or just modified ReLU.</p>
<p>The ReLU activation function for the output layer is well suited for cases when the input observations (\mathbf{x}_{i}) assume a wide range of positive real values.</p>
</section>
<section id="sigmoid">
<h2>Sigmoid<a class="headerlink" href="#sigmoid" title="Link to this heading">#</a></h2>
<p>The sigmoid function <span class="math notranslate nohighlight">\(\sigma\)</span> can assume all values in the range <span class="math notranslate nohighlight">\([0,1]\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\sigma\left(x\right) =\frac{1}{1+e^{-x}}.
\]</div>
<p>This activation function can only be used if the input observations
<span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span> are all in the range <span class="math notranslate nohighlight">\([0,1]\)</span>  or if you have
normalized them to be in that range. Consider as an example the MNIST
dataset. Each value of the input observation <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span> (one
image) is the gray values of the pixels that can assume any value from
0 to 255. Normalizing the data by dividing the pixel values by 255
would make each observation (each image) have only pixel values
between 0 and 1. In this case, the sigmoid would be a good choice for
the output layer’s activation function.</p>
</section>
<section id="cost-loss-function">
<h2>Cost/Loss Function<a class="headerlink" href="#cost-loss-function" title="Link to this heading">#</a></h2>
<p>If an autoencoder is trying to solve a regression problem, the most
common choice as a loss function is the Mean Square Error</p>
<div class="math notranslate nohighlight">
\[
L_{\textrm{MSE}} = \textrm{MSE} = \frac{1}{n}\sum_{i = 1}^{n}\left\vert\vert\mathbf{x}_{i}-\tilde{\mathbf{x}}_{i}\right\vert\vert^{2}_2.
\]</div>
</section>
<section id="binary-cross-entropy">
<h2>Binary Cross-Entropy<a class="headerlink" href="#binary-cross-entropy" title="Link to this heading">#</a></h2>
<p>If the activation function of the output layer of the AE is a sigmoid
function, thus limiting neuron outputs to be between 0 and 1, and the
input features are normalized to be between 0 and 1 we can use as loss
function the binary cross-entropy. This cots/loss function is
typically used in classification problems, but it works well for
autoencoders. The formula for it is</p>
<div class="math notranslate nohighlight">
\[
L_{\textrm{CE}} = -\frac{1}{n}\sum_{i = 1}^{n}\sum_{j = 1}^{p}[x_{j,i} \log\tilde{x}_{j,i}+\left(1-x_{j,i}\right)\log (1-\tilde{x}_{j,i})].
\]</div>
</section>
<section id="reconstruction-error">
<h2>Reconstruction Error<a class="headerlink" href="#reconstruction-error" title="Link to this heading">#</a></h2>
<p>The reconstruction error (RE) is a metric that gives you an indication of how good (or bad) the autoencoder was able to reconstruct
the input observation <span class="math notranslate nohighlight">\(\mathbf{x}_{i}\)</span>. The most typical RE used is the MSE</p>
<div class="math notranslate nohighlight">
\[
\textrm{RE}\equiv \textrm{MSE} = \frac{1}{n}\sum_{i = 1}^{n}\left\vert\vert\mathbf{x}_{i}-\tilde{\mathbf{x}}_{i}\right\vert\vert^{2}_2.
\]</div>
</section>
<section id="temporary-summary-on-autoencoders">
<h2>Temporary summary on Autoencoders<a class="headerlink" href="#temporary-summary-on-autoencoders" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Understand the basic autoencoder architecture (encoder, latent space, decoder).</p></li>
<li><p>Understand  the linear autoencoder and its connection to PCA. Good for understanding; nonlinear AE (with activations) can learn more complex manifolds.</p></li>
</ol>
<p><strong>What is an Autoencoder?</strong> An autoencoder (AE) is a neural network trained to reconstruct its input: <span class="math notranslate nohighlight">\(\hat{x}=\mathrm{Dec}(\mathrm{Enc}(x))\)</span>.</p>
<p><strong>Components:</strong></p>
<ol class="arabic simple">
<li><p>\textbf{Encoder} <span class="math notranslate nohighlight">\(f_\theta:\mathbb{R}^d\to\mathbb{R}^m\)</span> compresses input to latent code <span class="math notranslate nohighlight">\(h=f_\theta(x)\)</span></p></li>
<li><p>\textbf{Decoder} <span class="math notranslate nohighlight">\(g_\phi:\mathbb{R}^m\to\mathbb{R}^d\)</span> reconstructs <span class="math notranslate nohighlight">\(\hat{x}=g_\phi(h)\)</span></p></li>
</ol>
<p><strong>Training objective:</strong> minimize reconstruction loss, e.g. MSE \mathcal{L}(\theta,\phi)=\frac{1}{N}\sum_{i=1}^N |x^{(i)}-g_\phi(f_\theta(x^{(i)}))|^2_2.</p>
</section>
<section id="linear-autoencoders-and-the-pca-theorem">
<h2>Linear autoencoders and  the PCA theorem<a class="headerlink" href="#linear-autoencoders-and-the-pca-theorem" title="Link to this heading">#</a></h2>
<p>The covariance matrix is given as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{C}[\boldsymbol{x}] = \frac{1}{n}\boldsymbol{X}^T\boldsymbol{X}= \mathbb{E}[\boldsymbol{X}^T\boldsymbol{X}].
\]</div>
<p>Let us now assume that we can perform a series of orthogonal transformations where we employ some orthogonal matrices <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span>.
These matrices are defined as <span class="math notranslate nohighlight">\(\boldsymbol{S}\in {\mathbb{R}}^{p\times p}\)</span> and obey the orthogonality requirements <span class="math notranslate nohighlight">\(\boldsymbol{S}\boldsymbol{S}^T=\boldsymbol{S}^T\boldsymbol{S}=\boldsymbol{I}\)</span>. The matrix can be written out in terms of the column vectors <span class="math notranslate nohighlight">\(\boldsymbol{s}_i\)</span> as <span class="math notranslate nohighlight">\(\boldsymbol{S}=[\boldsymbol{s}_0,\boldsymbol{s}_1,\dots,\boldsymbol{s}_{p-1}]\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{s}_i \in {\mathbb{R}}^{p}\)</span>.</p>
</section>
<section id="more-details">
<h2>More details<a class="headerlink" href="#more-details" title="Link to this heading">#</a></h2>
<p>Assume also that there is a transformation <span class="math notranslate nohighlight">\(\boldsymbol{S}^T\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{S}=\boldsymbol{C}[\boldsymbol{y}]\)</span> such that the new matrix <span class="math notranslate nohighlight">\(\boldsymbol{C}[\boldsymbol{y}]\)</span> is diagonal with elements <span class="math notranslate nohighlight">\([\lambda_0,\lambda_1,\lambda_2,\dots,\lambda_{p-1}]\)</span>.</p>
<p>That is we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{C}[\boldsymbol{y}] = \mathbb{E}[\boldsymbol{S}^T\boldsymbol{X}^T\boldsymbol{X}T\boldsymbol{S}]=\boldsymbol{S}^T\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{S},
\]</div>
<p>since the matrix <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> is not a data dependent matrix.   Multiplying with <span class="math notranslate nohighlight">\(\boldsymbol{S}\)</span> from the left we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{S}\boldsymbol{C}[\boldsymbol{y}] = \boldsymbol{C}[\boldsymbol{x}]\boldsymbol{S},
\]</div>
<p>and since <span class="math notranslate nohighlight">\(\boldsymbol{C}[\boldsymbol{y}]\)</span> is diagonal we have for a given eigenvalue <span class="math notranslate nohighlight">\(i\)</span> of the covariance matrix that</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{S}_i\lambda_i = \boldsymbol{C}[\boldsymbol{x}]\boldsymbol{S}_i.
\]</div>
</section>
<section id="the-pca-theorem">
<h2>The PCA Theorem<a class="headerlink" href="#the-pca-theorem" title="Link to this heading">#</a></h2>
<p>In the derivation of the PCA theorem we will assume that the eigenvalues are ordered in descending order, that is
<span class="math notranslate nohighlight">\(\lambda_0 &gt; \lambda_1 &gt; \dots &gt; \lambda_{p-1}\)</span>.</p>
<p>The eigenvalues tell us then how much we need to stretch the
corresponding eigenvectors. Dimensions with large eigenvalues have
thus large variations (large variance) and define therefore useful
dimensions. The data points are more spread out in the direction of
these eigenvectors.  Smaller eigenvalues mean on the other hand that
the corresponding eigenvectors are shrunk accordingly and the data
points are tightly bunched together and there is not much variation in
these specific directions. Hopefully then we could leave it out
dimensions where the eigenvalues are very small. If <span class="math notranslate nohighlight">\(p\)</span> is very large,
we could then aim at reducing <span class="math notranslate nohighlight">\(p\)</span> to <span class="math notranslate nohighlight">\(l &lt;&lt; p\)</span> and handle only <span class="math notranslate nohighlight">\(l\)</span>
features/predictors.</p>
</section>
<section id="the-algorithm-before-theorem">
<h2>The Algorithm before theorem<a class="headerlink" href="#the-algorithm-before-theorem" title="Link to this heading">#</a></h2>
<p>Here’s how we would proceed in setting up the algorithm for the PCA, see also discussion below here.
Set up the datapoints for the design/feature matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol{X}\in {\mathbb{R}}^{n\times p}\)</span>, with the predictors/features <span class="math notranslate nohighlight">\(p\)</span>  referring to the column numbers and the entries <span class="math notranslate nohighlight">\(n\)</span> being the row elements.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X}=\begin{bmatrix}
x_{0,0} &amp; x_{0,1} &amp; x_{0,2}&amp; \dots &amp; \dots x_{0,p-1}\\
x_{1,0} &amp; x_{1,1} &amp; x_{1,2}&amp; \dots &amp; \dots x_{1,p-1}\\
x_{2,0} &amp; x_{2,1} &amp; x_{2,2}&amp; \dots &amp; \dots x_{2,p-1}\\
\dots &amp; \dots &amp; \dots &amp; \dots \dots &amp; \dots \\
x_{n-2,0} &amp; x_{n-2,1} &amp; x_{n-2,2}&amp; \dots &amp; \dots x_{n-2,p-1}\\
x_{n-1,0} &amp; x_{n-1,1} &amp; x_{n-1,2}&amp; \dots &amp; \dots x_{n-1,p-1}\\
\end{bmatrix}.
\end{split}\]</div>
</section>
<section id="further-steps">
<h2>Further steps<a class="headerlink" href="#further-steps" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Center the data by subtracting the mean value for each column. This leads to a new matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\rightarrow \overline{\boldsymbol{X}}\)</span>.</p></li>
<li><p>Compute then the covariance/correlation matrix <span class="math notranslate nohighlight">\(\mathbb{E}[\overline{\boldsymbol{X}}^T\overline{\boldsymbol{X}}]\)</span>.</p></li>
<li><p>Find the eigenpairs of <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span> with eigenvalues <span class="math notranslate nohighlight">\([\lambda_0,\lambda_1,\dots,\lambda_{p-1}]\)</span> and eigenvectors <span class="math notranslate nohighlight">\([\boldsymbol{s}_0,\boldsymbol{s}_1,\dots,\boldsymbol{s}_{p-1}]\)</span>.</p></li>
<li><p>Order the eigenvalue (and the eigenvectors accordingly) in order of decreasing eigenvalues.</p></li>
<li><p>Keep only those <span class="math notranslate nohighlight">\(l\)</span> eigenvalues larger than a selected threshold value, discarding thus <span class="math notranslate nohighlight">\(p-l\)</span> features since we expect small variations in the data here.</p></li>
</ul>
</section>
<section id="writing-our-own-pca-code">
<h2>Writing our own PCA code<a class="headerlink" href="#writing-our-own-pca-code" title="Link to this heading">#</a></h2>
<p>We will use a simple example first with two-dimensional data
drawn from a multivariate normal distribution with the following mean and covariance matrix (we have fixed these quantities but will play around with them below):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mu = (-1,2) \qquad \Sigma = \begin{bmatrix} 4 &amp; 2 \\
2 &amp; 2
\end{bmatrix}
\end{split}\]</div>
<p>Note that the mean refers to each column of data.
We will generate <span class="math notranslate nohighlight">\(n = 10000\)</span> points <span class="math notranslate nohighlight">\(X = \{ x_1, \ldots, x_N \}\)</span> from
this distribution, and store them in the <span class="math notranslate nohighlight">\(1000 \times 2\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>. This is our design matrix where we have forced the covariance and mean values to take specific values.</p>
</section>
<section id="implementing-it">
<h2>Implementing it<a class="headerlink" href="#implementing-it" title="Link to this heading">#</a></h2>
<p>The following Python code aids in setting up the data and writing out the design matrix.
Note that the function <strong>multivariate</strong> returns also the covariance discussed above and that it is defined by dividing by <span class="math notranslate nohighlight">\(n-1\)</span> instead of <span class="math notranslate nohighlight">\(n\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>%matplotlib inline

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display
n = 10000
mean = (-1, 2)
cov = [[4, 2], [2, 2]]
X = np.random.multivariate_normal(mean, cov, n)
</pre></div>
</div>
</div>
</div>
<p>Now we are going to implement the PCA algorithm. We will break it down into various substeps.</p>
</section>
<section id="first-step">
<h2>First Step<a class="headerlink" href="#first-step" title="Link to this heading">#</a></h2>
<p>The first step of PCA is to compute the sample mean of the data and use it to center the data. Recall that the sample mean is</p>
<div class="math notranslate nohighlight">
\[
\mu_n = \frac{1}{n} \sum_{i=1}^n x_i
\]</div>
<p>and the mean-centered data <span class="math notranslate nohighlight">\(\bar{X} = \{ \bar{x}_1, \ldots, \bar{x}_n \}\)</span> takes the form</p>
<div class="math notranslate nohighlight">
\[
\bar{x}_i = x_i - \mu_n.
\]</div>
<p>When you are done with these steps, print out <span class="math notranslate nohighlight">\(\mu_n\)</span> to verify it is
close to <span class="math notranslate nohighlight">\(\mu\)</span> and plot your mean centered data to verify it is
centered at the origin!
The following code elements perform these operations using <strong>pandas</strong> or using our own functionality for doing so. The latter, using <strong>numpy</strong> is rather simple through the <strong>mean()</strong> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>df = pd.DataFrame(X)
# Pandas does the centering for us
df = df -df.mean()
# we center it ourselves
X_centered = X - X.mean(axis=0)
</pre></div>
</div>
</div>
</div>
</section>
<section id="scaling">
<h2>Scaling<a class="headerlink" href="#scaling" title="Link to this heading">#</a></h2>
<p>Alternatively, we could use the functions we discussed
earlier for scaling the data set.  That is, we could have used the
<strong>StandardScaler</strong> function in <strong>Scikit-Learn</strong>, a function which ensures
that for each feature/predictor we study the mean value is zero and
the variance is one (every column in the design/feature matrix).  You
would then not get the same results, since we divide by the
variance. The diagonal covariance matrix elements will then be one,
while the non-diagonal ones need to be divided by <span class="math notranslate nohighlight">\(2\sqrt{2}\)</span> for our
specific case.</p>
</section>
<section id="centered-data">
<h2>Centered Data<a class="headerlink" href="#centered-data" title="Link to this heading">#</a></h2>
<p>Now we are going to use the mean centered data to compute the sample covariance of the data by using the following equation</p>
<div class="math notranslate nohighlight">
\[
\Sigma_n = \frac{1}{n-1} \sum_{i=1}^n \bar{x}_i^T \bar{x}_i = \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu_n)^T (x_i - \mu_n)
\]</div>
<p>where the data points <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}^p\)</span> (here in this example <span class="math notranslate nohighlight">\(p = 2\)</span>) are column vectors and <span class="math notranslate nohighlight">\(x^T\)</span> is the transpose of <span class="math notranslate nohighlight">\(x\)</span>.
We can write our own code or simply use either the functionaly of <strong>numpy</strong> or that of <strong>pandas</strong>, as follows</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>print(df.cov())
print(np.cov(X_centered.T))
</pre></div>
</div>
</div>
</div>
<p>Note that the way we define the covariance matrix here has a factor <span class="math notranslate nohighlight">\(n-1\)</span> instead of <span class="math notranslate nohighlight">\(n\)</span>. This is included in the <strong>cov()</strong> function by <strong>numpy</strong> and <strong>pandas</strong>.
Our own code here is not very elegant and asks for obvious improvements. It is tailored to this specific <span class="math notranslate nohighlight">\(2\times 2\)</span> covariance matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># extract the relevant columns from the centered design matrix of dim n x 2
x = X_centered[:,0]
y = X_centered[:,1]
Cov = np.zeros((2,2))
Cov[0,1] = np.sum(x.T@y)/(n-1.0)
Cov[0,0] = np.sum(x.T@x)/(n-1.0)
Cov[1,1] = np.sum(y.T@y)/(n-1.0)
Cov[1,0]= Cov[0,1]
print(&quot;Centered covariance using own code&quot;)
print(Cov)
plt.plot(x, y, &#39;x&#39;)
plt.axis(&#39;equal&#39;)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="exploring">
<h2>Exploring<a class="headerlink" href="#exploring" title="Link to this heading">#</a></h2>
<p>Depending on the number of points <span class="math notranslate nohighlight">\(n\)</span>, we will get results that are close to the covariance values defined above.
The plot shows how the data are clustered around a line with slope close to one. Is this expected?  Try to change the covariance and the mean values. For example, try to make the variance of the first element much larger than that of the second diagonal element. Try also to shrink the covariance  (the non-diagonal elements) and see how the data points are distributed.</p>
</section>
<section id="diagonalize-the-sample-covariance-matrix-to-obtain-the-principal-components">
<h2>Diagonalize the sample covariance matrix to obtain the principal components<a class="headerlink" href="#diagonalize-the-sample-covariance-matrix-to-obtain-the-principal-components" title="Link to this heading">#</a></h2>
<p>Now we are ready to solve for the principal components! To do so we
diagonalize the sample covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>. We can use the
function <strong>np.linalg.eig</strong> to do so. It will return the eigenvalues and
eigenvectors of <span class="math notranslate nohighlight">\(\Sigma\)</span>. Once we have these we can perform the
following tasks:</p>
<ul class="simple">
<li><p>We compute the percentage of the total variance captured by the first principal component</p></li>
<li><p>We plot the mean centered data and lines along the first and second principal components</p></li>
<li><p>Then we project the mean centered data onto the first and second principal components, and plot the projected data.</p></li>
<li><p>Finally, we approximate the data as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
x_i \approx \tilde{x}_i = \mu_n + \langle x_i, v_0 \rangle v_0
\]</div>
<p>where <span class="math notranslate nohighlight">\(v_0\)</span> is the first principal component.</p>
</section>
<section id="collecting-all-steps">
<h2>Collecting all Steps<a class="headerlink" href="#collecting-all-steps" title="Link to this heading">#</a></h2>
<p>Collecting all these steps we can write our own PCA function and
compare this with the functionality included in <strong>Scikit-Learn</strong>.</p>
<p>The code here outlines some of the elements we could include in the
analysis. Feel free to extend upon this in order to address the above
questions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># diagonalize and obtain eigenvalues, not necessarily sorted
EigValues, EigVectors = np.linalg.eig(Cov)
# sort eigenvectors and eigenvalues
#permute = EigValues.argsort()
#EigValues = EigValues[permute]
#EigVectors = EigVectors[:,permute]
print(&quot;Eigenvalues of Covariance matrix&quot;)
for i in range(2):
    print(EigValues[i])
FirstEigvector = EigVectors[:,0]
SecondEigvector = EigVectors[:,1]
print(&quot;First eigenvector&quot;)
print(FirstEigvector)
print(&quot;Second eigenvector&quot;)
print(SecondEigvector)
#thereafter we do a PCA with Scikit-learn
from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
X2Dsl = pca.fit_transform(X)
print(&quot;Eigenvector of largest eigenvalue&quot;)
print(pca.components_.T[:, 0])
</pre></div>
</div>
</div>
</div>
<p>This code does not contain all the above elements, but it shows how we can use <strong>Scikit-Learn</strong> to extract the eigenvector which corresponds to the largest eigenvalue. Try to address the questions we pose before the above code.  Try also to change the values of the covariance matrix by making one of the diagonal elements much larger than the other. What do you observe then?</p>
</section>
<section id="classical-pca-theorem">
<h2>Classical PCA Theorem<a class="headerlink" href="#classical-pca-theorem" title="Link to this heading">#</a></h2>
<p>We assume now that we have a design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> which has been
centered as discussed above. For the sake of simplicity we skip the
overline symbol. The matrix is defined in terms of the various column
vectors <span class="math notranslate nohighlight">\([\boldsymbol{x}_0,\boldsymbol{x}_1,\dots, \boldsymbol{x}_{p-1}]\)</span> each with dimension
<span class="math notranslate nohighlight">\(\boldsymbol{x}\in {\mathbb{R}}^{n}\)</span>.</p>
<p>The PCA theorem states that minimizing the above reconstruction error
corresponds to setting <span class="math notranslate nohighlight">\(\boldsymbol{W}=\boldsymbol{S}\)</span>, the orthogonal matrix which
diagonalizes the empirical covariance(correlation) matrix. The optimal
low-dimensional encoding of the data is then given by a set of vectors
<span class="math notranslate nohighlight">\(\boldsymbol{z}_i\)</span> with at most <span class="math notranslate nohighlight">\(l\)</span> vectors, with <span class="math notranslate nohighlight">\(l &lt;&lt; p\)</span>, defined by the
orthogonal projection of the data onto the columns spanned by the
eigenvectors of the covariance(correlations matrix).</p>
</section>
<section id="id1">
<h2>The PCA Theorem<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>To show the PCA theorem let us start with the assumption that there is one vector <span class="math notranslate nohighlight">\(\boldsymbol{s}_0\)</span> which corresponds to a solution which minimized the reconstruction error <span class="math notranslate nohighlight">\(J\)</span>. This is an orthogonal vector. It means that we now approximate the reconstruction error in terms of <span class="math notranslate nohighlight">\(\boldsymbol{w}_0\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{z}_0\)</span> as</p>
<p>We are almost there, we have obtained a relation between minimizing
the reconstruction error and the variance and the covariance
matrix. Minimizing the error is equivalent to maximizing the variance
of the projected data.</p>
<p>We could trivially maximize the variance of the projection (and
thereby minimize the error in the reconstruction function) by letting
the norm-2 of <span class="math notranslate nohighlight">\(\boldsymbol{w}_0\)</span> go to infinity. However, this norm since we
want the matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> to be an orthogonal matrix, is constrained by
<span class="math notranslate nohighlight">\(\vert\vert \boldsymbol{w}_0 \vert\vert_2^2=1\)</span>. Imposing this condition via a
Lagrange multiplier we can then in turn maximize</p>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{w}_0)= \boldsymbol{w}_0^T\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{w}_0+\lambda_0(1-\boldsymbol{w}_0^T\boldsymbol{w}_0).
\]</div>
<p>Taking the derivative with respect to <span class="math notranslate nohighlight">\(\boldsymbol{w}_0\)</span> we obtain</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J(\boldsymbol{w}_0)}{\partial \boldsymbol{w}_0}= 2\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{w}_0-2\lambda_0\boldsymbol{w}_0=0,
\]</div>
<p>meaning that</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{w}_0=\lambda_0\boldsymbol{w}_0.
\]</div>
<p><strong>The direction that maximizes the variance (or minimizes the construction error) is an eigenvector of the covariance matrix</strong>! If we left multiply with <span class="math notranslate nohighlight">\(\boldsymbol{w}_0^T\)</span> we have the variance of the projected data is</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{w}_0^T\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{w}_0=\lambda_0.
\]</div>
<p>If we want to maximize the variance (minimize the construction error)
we simply pick the eigenvector of the covariance matrix with the
largest eigenvalue. This establishes the link between the minimization
of the reconstruction function <span class="math notranslate nohighlight">\(J\)</span> in terms of an orthogonal matrix
and the maximization of the variance and thereby the covariance of our
observations encoded in the design/feature matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p>
<p>The proof
for the other eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{w}_1,\boldsymbol{w}_2,\dots\)</span> can be
established by applying the above arguments and using the fact that
our basis of eigenvectors is orthogonal, see <a class="reference external" href="https://mitpress.mit.edu/books/machine-learning-1">Murphy chapter
12.2</a>.  The
discussion in chapter 12.2 of Murphy’s text has also a nice link with
the Singular Value Decomposition theorem. For categorical data, see
chapter 12.4 and discussion therein.</p>
<p>For more details, see for example <a class="reference external" href="https://www.springer.com/gp/book/9780387878102">Vidal, Ma and Sastry, chapter 2</a>.</p>
</section>
<section id="geometric-interpretation-and-link-with-singular-value-decomposition">
<h2>Geometric Interpretation and link with Singular Value Decomposition<a class="headerlink" href="#geometric-interpretation-and-link-with-singular-value-decomposition" title="Link to this heading">#</a></h2>
<p>For a detailed demonstration of the geometric interpretation, see <a class="reference external" href="https://www.springer.com/gp/book/9780387878102">Vidal, Ma and Sastry, section 2.1.2</a>.</p>
<p>Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm.
First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.</p>
<p>The following Python code uses NumPy’s <strong>svd()</strong> function to obtain all the principal components of the
training set, then extracts the first two principal components. First we center the data using either <strong>pandas</strong> or our own code</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
import pandas as pd
from IPython.display import display
np.random.seed(100)
# setting up a 10 x 5 vanilla matrix 
rows = 10
cols = 5
X = np.random.randn(rows,cols)
df = pd.DataFrame(X)
# Pandas does the centering for us
df = df -df.mean()
display(df)

# we center it ourselves
X_centered = X - X.mean(axis=0)
# Then check the difference between pandas and our own set up
print(X_centered-df)
#Now we do an SVD
U, s, V = np.linalg.svd(X_centered)
c1 = V.T[:, 0]
c2 = V.T[:, 1]
W2 = V.T[:, :2]
X2D = X_centered.dot(W2)
print(X2D)
</pre></div>
</div>
</div>
</div>
<p>PCA assumes that the dataset is centered around the origin. Scikit-Learn’s PCA classes take care of centering
the data for you. However, if you implement PCA yourself (as in the preceding example), or if you use other libraries, don’t
forget to center the data first.</p>
<p>Once you have identified all the principal components, you can reduce the dimensionality of the dataset
down to <span class="math notranslate nohighlight">\(d\)</span> dimensions by projecting it onto the hyperplane defined by the first <span class="math notranslate nohighlight">\(d\)</span> principal components.
Selecting this hyperplane ensures that the projection will preserve as much variance as possible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>W2 = V.T[:, :2]
X2D = X_centered.dot(W2)
</pre></div>
</div>
</div>
</div>
</section>
<section id="pca-and-scikit-learn">
<h2>PCA and scikit-learn<a class="headerlink" href="#pca-and-scikit-learn" title="Link to this heading">#</a></h2>
<p>Scikit-Learn’s PCA class implements PCA using SVD decomposition just like we did before. The
following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note
that it automatically takes care of centering the data):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#thereafter we do a PCA with Scikit-learn
from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
X2D = pca.fit_transform(X)
print(X2D)
</pre></div>
</div>
</div>
</div>
<p>After fitting the PCA transformer to the dataset, you can access the principal components using the
components variable (note that it contains the PCs as horizontal vectors, so, for example, the first
principal component is equal to</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>pca.components_.T[:, 0]
</pre></div>
</div>
</div>
</div>
<p>Another very useful piece of information is the explained variance ratio of each principal component,
available via the <span class="math notranslate nohighlight">\(explained\_variance\_ratio\)</span> variable. It indicates the proportion of the dataset’s
variance that lies along the axis of each principal component.</p>
</section>
<section id="example-of-cancer-data">
<h2>Example of Cancer Data<a class="headerlink" href="#example-of-cancer-data" title="Link to this heading">#</a></h2>
<p>We can now repeat the above but applied to real data, in this case the Wisconsin breast cancer data.
Here we compute performance scores on the training data using logistic regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import  train_test_split 
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data,cancer.target,random_state=0)

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
print(&quot;Train set accuracy from Logistic Regression: {:.2f}&quot;.format(logreg.score(X_train,y_train)))
# We scale the data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Then perform again a log reg fit
logreg.fit(X_train_scaled, y_train)
print(&quot;Train set accuracy scaled data: {:.2f}&quot;.format(logreg.score(X_train_scaled,y_train)))
#thereafter we do a PCA with Scikit-learn
from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
X2D_train = pca.fit_transform(X_train_scaled)
# and finally compute the log reg fit and the score on the training data	
logreg.fit(X2D_train,y_train)
print(&quot;Train set accuracy scaled and PCA data: {:.2f}&quot;.format(logreg.score(X2D_train,y_train)))
</pre></div>
</div>
</div>
</div>
<p>We see that our training data after the PCA decomposition has a performance similar to the non-scaled data.</p>
<p>Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to
choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%).
Unless, of course, you are reducing dimensionality for data visualization — in that case you will
generally want to reduce the dimensionality down to 2 or 3.
The following code computes PCA without reducing dimensionality, then computes the minimum number
of dimensions required to preserve 95% of the training set’s variance:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum &gt;= 0.95) + 1
</pre></div>
</div>
</div>
</div>
<p>You could then set <span class="math notranslate nohighlight">\(n\_components=d\)</span> and run PCA again. However, there is a much better option: instead
of specifying the number of principal components you want to preserve, you can set <span class="math notranslate nohighlight">\(n\_components\)</span> to be
a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_train)
</pre></div>
</div>
</div>
</div>
</section>
<section id="incremental-pca">
<h2>Incremental PCA<a class="headerlink" href="#incremental-pca" title="Link to this heading">#</a></h2>
<p>One problem with the preceding implementation of PCA is that it requires the whole training set to fit in
memory in order for the SVD algorithm to run. Fortunately, Incremental PCA (IPCA) algorithms have
been developed: you can split the training set into mini-batches and feed an IPCA algorithm one minibatch
at a time. This is useful for large training sets, and also to apply PCA online (i.e., on the fly, as new
instances arrive).</p>
<section id="randomized-pca">
<h3>Randomized PCA<a class="headerlink" href="#randomized-pca" title="Link to this heading">#</a></h3>
<p>Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This is a stochastic
algorithm that quickly finds an approximation of the first d principal components. Its computational
complexity is <span class="math notranslate nohighlight">\(O(m \times d^2)+O(d^3)\)</span>, instead of <span class="math notranslate nohighlight">\(O(m \times n^2) + O(n^3)\)</span>, so it is dramatically faster than the
previous algorithms when <span class="math notranslate nohighlight">\(d\)</span> is much smaller than <span class="math notranslate nohighlight">\(n\)</span>.</p>
</section>
<section id="kernel-pca">
<h3>Kernel PCA<a class="headerlink" href="#kernel-pca" title="Link to this heading">#</a></h3>
<p>The kernel trick is a mathematical technique that implicitly maps instances into a
very high-dimensional space (called the feature space), enabling nonlinear classification and regression
with Support Vector Machines. Recall that a linear decision boundary in the high-dimensional feature
space corresponds to a complex nonlinear decision boundary in the original space.
It turns out that the same trick can be applied to PCA, making it possible to perform complex nonlinear
projections for dimensionality reduction. This is called Kernel PCA (kPCA). It is often good at
preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a
twisted manifold.
For example, the following code uses Scikit-Learn’s KernelPCA class to perform kPCA with an</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.decomposition import KernelPCA
rbf_pca = KernelPCA(n_components = 2, kernel=&quot;rbf&quot;, gamma=0.04)
X_reduced = rbf_pca.fit_transform(X)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="other-techniques">
<h2>Other techniques<a class="headerlink" href="#other-techniques" title="Link to this heading">#</a></h2>
<p>There are many other dimensionality reduction techniques, several of which are available in Scikit-Learn.</p>
<p>Here are some of the most popular:</p>
<ul class="simple">
<li><p><strong>Multidimensional Scaling (MDS)</strong> reduces dimensionality while trying to preserve the distances between the instances.</p></li>
<li><p><strong>Isomap</strong> creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the geodesic distances between the instances.</p></li>
<li><p><strong>t-Distributed Stochastic Neighbor Embedding</strong> (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space (e.g., to visualize the MNIST images in 2D).</p></li>
<li><p>Linear Discriminant Analysis (LDA) is actually a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as a Support Vector Machine (SVM) classifier discussed in the SVM lectures.</p></li>
</ul>
</section>
<section id="back-to-autoencoders-linear-autoencoders">
<h2>Back to Autoencoders: Linear Autoencoders<a class="headerlink" href="#back-to-autoencoders-linear-autoencoders" title="Link to this heading">#</a></h2>
</section>
<section id="pytorch-example">
<h2>PyTorch example<a class="headerlink" href="#pytorch-example" title="Link to this heading">#</a></h2>
<p>We will, again,  use the MNIST database, which has <span class="math notranslate nohighlight">\(60000\)</span> training examples and a test set of 10000 handwritten numbers. The images have
only one color channel and have a size of <span class="math notranslate nohighlight">\(28\times 28\)</span> pixels.
We start by uploading the data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># import the Torch packages
# transforms are used to preprocess the images, e.g. crop, rotate, normalize, etc
import torch
from torchvision import datasets,transforms

# specify the data path in which you would like to store the downloaded files
# ToTensor() here is used to convert data type to tensor

train_dataset = datasets.MNIST(root=&#39;./mnist_data/&#39;, train=True, transform=transforms.ToTensor(), download=True)
test_dataset = datasets.MNIST(root=&#39;./mnist_data/&#39;, train=False, transform=transforms.ToTensor(), download=True)
print(train_dataset)
batchSize=128

#only after packed in DataLoader, can we feed the data into the neural network iteratively
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batchSize, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batchSize, shuffle=False)
</pre></div>
</div>
</div>
</div>
<p>We visualize the images here using the <span class="math notranslate nohighlight">\(imshow\)</span> function  function and the <span class="math notranslate nohighlight">\(make\_grid\)</span> function from PyTorch to arrange and display them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># package we used to manipulate matrix
import numpy as np
# package we used for image processing
from matplotlib import pyplot as plt
from torchvision.utils import make_grid

def imshow(img):
    npimg = img.numpy()
    #transpose: change array axis to correspond to the plt.imshow() function     
    plt.imshow(np.transpose(npimg, (1, 2, 0))) 
    plt.show()

# load the first 16 training samples from next iteration
# [:16,:,:,:] for the 4 dimension of examples, first dimension take first 16, other dimension take all data
# arrange the image in grid
examples, _ = next(iter(train_loader))
example_show=make_grid(examples[:16,:,:,:], 4)

# then display them
imshow(example_show)
</pre></div>
</div>
</div>
</div>
<p>Our autoencoder consists of two parts, see also the TensorFlow example
above. The encoder and decoder parts are represented by two fully
connected feed forward neural networks where we use the standard
Sigmoid function.  In the encoder part we reduce the dimensionality of
the image from <span class="math notranslate nohighlight">\(28\times 28=784\)</span> pixels to first <span class="math notranslate nohighlight">\(16\times 16=256\)</span>
pixels and then to 128 pixels. The 128 pixel representation is then
used to define the representation of the input and the input to the
decoder part.  The latter attempts to reconstruct the images.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Network Parameters
num_hidden_1 = 256  # 1st layer num features
num_hidden_2 = 128  # 2nd layer num features (the latent dim)
num_input = 784  # MNIST data input (img shape: 28*28)


# Building the encoder
class Autoencoder(nn.Module):
    def __init__(self, x_dim, h_dim1, h_dim2):
        super(Autoencoder, self).__init__()
        # encoder part
        self.fc1 = nn.Linear(x_dim, h_dim1)
        self.fc2 = nn.Linear(h_dim1, h_dim2)
        # decoder part
        self.fc3 = nn.Linear(h_dim2, h_dim1)
        self.fc4 = nn.Linear(h_dim1, x_dim)

    def encoder(self, x):
        x = torch.sigmoid(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        return x

    def decoder(self, x):
        x = torch.sigmoid(self.fc3(x))
        x = torch.sigmoid(self.fc4(x))
        return x

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# When initializing, it will run __init__() function as above
model = Autoencoder(num_input, num_hidden_1, num_hidden_2)
</pre></div>
</div>
</div>
</div>
<p>We define here the cost/loss function and the optimizer we employ (Adam here).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># define loss function and parameters
optimizer = optim.Adam(model.parameters())
epoch = 100
# MSE loss will calculate Mean Squared Error between the inputs 
loss_function = nn.MSELoss()

print(&#39;====Training start====&#39;)
for i in range(epoch):
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        # prepare input data
        inputs = torch.reshape(data,(-1, 784)) # -1 can be any value.
        # set gradient to zero
        optimizer.zero_grad()
        # feed inputs into model
        recon_x = model(inputs)
        # calculating loss 
        loss = loss_function(recon_x, inputs)
        # calculate gradient of each parameter
        loss.backward()
        train_loss += loss.item()
        # update the weight based on the gradient calculated
        optimizer.step()
    if i%10==0:    
        print(&#39;====&gt; Epoch: {} Average loss: {:.9f}&#39;.format(i, train_loss ))
print(&#39;====Training finish====&#39;)
</pre></div>
</div>
</div>
</div>
<p>As we have trained the network, we will now reconstruct various test samples to see if the model can generalize to data which
were not included in the training set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># load 16 images from testset
inputs, _ = next(iter(test_loader))
inputs_example = make_grid(inputs[:16,:,:,:],4)
imshow(inputs_example)

#convert from image to tensor
#inputs=inputs.cuda()
inputs=torch.reshape(inputs,(-1,784))

# get the outputs from the trained model
outputs=model(inputs)

#convert from tensor to image
outputs=torch.reshape(outputs,(-1,1,28,28))
outputs=outputs.detach().cpu()

#show the output images
outputs_example = make_grid(outputs[:16,:,:,:],4)
imshow(outputs_example)
</pre></div>
</div>
</div>
</div>
<p>After training the auto-encoder, we can now use the model to reconstruct some images.
In order to reconstruct different training images, the model
has learned to recognize how the image looks like and describe it in
the 128-dimensional  latent space. In other words, the visual information of
images is compressed and encoded in the 128-dimensional representations. As we
assume that samples from the same categories should be more visually
similar than those from different classes, the representations can
then be used for image recognition, i.e., handwritten digit images
recognition in our case.</p>
<p>One simple way to recognize images is to randomly select ten training
samples from each class and annotate them with the corresponding label.
Then given the
test data, we can predict which classes they belong to by finding the
most similar labelled training samples to them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># get 100 image-label pairs from training set
x_train, y_train = next(iter(train_loader))

# 10 classes, 10 samples per class, 100 in total
candidates = np.random.choice(batchSize, 10*10)

# randomly select 100 samples
x_train = x_train[candidates]
y_train = y_train[candidates]

# display the selected samples and print their labels

imshow(make_grid(x_train[:100,:,:,:],10))
print(y_train.reshape(10, 10))

# get 100 image-label pairs from test set
x_test, y_test = next(iter(train_loader))
candidates_test = np.random.choice(batchSize, 10*10)

x_test = x_test[candidates_test]
y_test = y_test[candidates_test]

# display the selected samples and print their labels
imshow(make_grid(x_test[:100,:,:,:],10))

print(y_test.reshape(10, 10))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># compute the representations of training and test samples
#h_train=model.encoder(torch.reshape(x_train.cuda(),(-1,784)))
#h_test=model.encoder(torch.reshape(x_test.cuda(),(-1,784)))
h_train=model.encoder(torch.reshape(x_train,(-1,784)))
h_test=model.encoder(torch.reshape(x_test,(-1,784)))

# find the nearest training samples to each test instance, in terms of MSE
MSEs = np.mean(np.power(np.expand_dims(h_test.detach().cpu(), axis=1) - np.expand_dims(h_train.detach().cpu(), axis=0), 2), axis=2)
neighbours = MSEs.argmin(axis=1)
predicts = y_train[neighbours]

# print(np.stack([y_test, predicts], axis=1))
print(&#39;Recognition accuracy according to the learned representation is %.1f%%&#39; % (100 * (y_test == predicts).numpy().astype(np.float32).mean()))
</pre></div>
</div>
</div>
</div>
</section>
<section id="summary-of-course">
<h2>Summary of course<a class="headerlink" href="#summary-of-course" title="Link to this heading">#</a></h2>
</section>
<section id="what-me-worry-no-final-exam-in-this-course">
<h2>What? Me worry? No final exam in this course!<a class="headerlink" href="#what-me-worry-no-final-exam-in-this-course" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figures/exam1.jpeg, width=500 frac=0.6] -->
<!-- begin figure -->
<p><img src="figures/exam1.jpeg" width="500"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="topics-we-have-covered-this-year">
<h2>Topics we have covered this year<a class="headerlink" href="#topics-we-have-covered-this-year" title="Link to this heading">#</a></h2>
<p>The course has two central parts</p>
<ol class="arabic simple">
<li><p>Statistical analysis and optimization of data</p></li>
<li><p>Machine learning</p></li>
</ol>
</section>
<section id="statistical-analysis-and-optimization-of-data">
<h2>Statistical analysis and optimization of data<a class="headerlink" href="#statistical-analysis-and-optimization-of-data" title="Link to this heading">#</a></h2>
<p>The following topics have been discussed:</p>
<ol class="arabic simple">
<li><p>Basic concepts, expectation values, variance, covariance, correlation functions and errors;</p></li>
<li><p>Simpler models, binomial distribution, the Poisson distribution, simple and multivariate normal distributions;</p></li>
<li><p>Central elements from linear algebra, matrix inversion and SVD</p></li>
<li><p>Gradient methods for data optimization</p></li>
<li><p>Estimation of errors using cross-validation, bootstrapping and jackknife methods;</p></li>
<li><p>Practical optimization using Singular-value decomposition and least squares for parameterizing data.</p></li>
<li><p>Not discussed: Principal Component Analysis to reduce the number of features.</p></li>
</ol>
</section>
<section id="machine-learning">
<h2>Machine learning<a class="headerlink" href="#machine-learning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Linear methods for regression and classification:</p></li>
</ul>
<p>a. Ordinary Least Squares</p>
<p>b. Ridge regression</p>
<p>c. Lasso regression</p>
<p>d. Logistic regression</p>
<ul class="simple">
<li><p>Neural networks and deep learning:</p></li>
</ul>
<p>a. Feed Forward Neural Networks</p>
<p>b. Convolutional Neural Networks</p>
<p>c. Recurrent Neural Networks</p>
<p>d. Autoencoders and PCA</p>
<ul class="simple">
<li><p>Not discussed this year Decisions trees and ensemble methods:</p></li>
</ul>
<p>a. Decision trees</p>
<p>b. Bagging and voting</p>
<p>c. Random forests</p>
<p>d. Boosting and gradient boosting</p>
<ul class="simple">
<li><p>Not discussed this year: Support vector machines</p></li>
</ul>
<p>a. Binary classification and multiclass classification</p>
<p>b. Kernel methods</p>
<p>c. Regression</p>
</section>
<section id="learning-outcomes-and-overarching-aims-of-this-course">
<h2>Learning outcomes and overarching aims of this course<a class="headerlink" href="#learning-outcomes-and-overarching-aims-of-this-course" title="Link to this heading">#</a></h2>
<p>The course introduces a variety of central algorithms and methods
essential for studies of data analysis and machine learning. The
course is project based and through the various projects, normally
three, you will be exposed to fundamental research problems
in these fields, with the aim to reproduce state of the art scientific
results. The students will learn to develop and structure large codes
for studying these systems, get acquainted with computing facilities
and learn to handle large scientific projects. A good scientific and
ethical conduct is emphasized throughout the course.</p>
<ul class="simple">
<li><p>Understand linear methods for regression and classification;</p></li>
<li><p>Learn about neural network and deep learning methods;</p></li>
</ul>
<!-- * Learn about bagging, boosting and trees -->
<!-- * Support vector machines -->
<ul class="simple">
<li><p>Learn about basic data analysis;</p></li>
<li><p>Be capable of extending the acquired knowledge to other systems and cases;</p></li>
<li><p>Have an understanding of central algorithms used in data analysis and machine learning;</p></li>
<li><p>Work on numerical projects to illustrate the theory. The projects play a central role.</p></li>
</ul>
</section>
<section id="perspective-on-machine-learning">
<h2>Perspective on Machine Learning<a class="headerlink" href="#perspective-on-machine-learning" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Rapidly emerging application area</p></li>
<li><p>Experiment AND theory are evolving in many many fields.</p></li>
<li><p>Requires education/retraining for more widespread adoption</p></li>
<li><p>A lot of “word-of-mouth” development methods</p></li>
<li><p>And LLMs are playing a big role here</p></li>
</ol>
<p>Huge amounts of data sets require automation, classical analysis tools often inadequate.
High energy physics hit this wall in the 90’s.
In 2009 single top quark production was determined via <a class="reference external" href="https://arxiv.org/pdf/0903.0850.pdf">Boosted decision trees, Bayesian
Neural Networks, etc.</a></p>
</section>
<section id="machine-learning-research">
<h2>Machine Learning Research<a class="headerlink" href="#machine-learning-research" title="Link to this heading">#</a></h2>
<p>Where to find recent results:</p>
<ol class="arabic simple">
<li><p>Conference proceedings, arXiv and blog posts!</p></li>
<li><p><strong>NIPS</strong>: <a class="reference external" href="https://papers.nips.cc">Neural Information Processing Systems</a></p></li>
<li><p><strong>ICLR</strong>: <a class="reference external" href="https://openreview.net/group?id=ICLR.cc/2018/Conference#accepted-oral-papers">International Conference on Learning Representations</a></p></li>
<li><p><strong>ICML</strong>: International Conference on Machine Learning</p></li>
<li><p><a class="reference external" href="http://www.jmlr.org/papers/v19/">Journal of Machine Learning Research</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/list/cs.LG/recent">Follow ML on ArXiv</a></p></li>
</ol>
</section>
<section id="starting-your-machine-learning-project">
<h2>Starting your Machine Learning Project<a class="headerlink" href="#starting-your-machine-learning-project" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Identify problem type: classification, regression</p></li>
<li><p>Consider your data carefully</p></li>
<li><p>Choose a simple model that fits 1 and 2</p></li>
<li><p>Consider your data carefully again! Think of data representation more carefully.</p></li>
<li><p>Based on your results, feedback loop to earliest possible point</p></li>
</ol>
</section>
<section id="choose-a-model-and-algorithm">
<h2>Choose a Model and Algorithm<a class="headerlink" href="#choose-a-model-and-algorithm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Supervised?</p></li>
<li><p>Start with the simplest model that fits your problem</p></li>
<li><p>Start with minimal processing of data</p></li>
</ul>
</section>
<section id="preparing-your-data">
<h2>Preparing Your Data<a class="headerlink" href="#preparing-your-data" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Shuffle your data</p></li>
<li><p>Mean center your data</p>
<ul>
<li><p>Why?</p></li>
</ul>
</li>
<li><p>Normalize the variance</p>
<ul>
<li><p>Why?</p></li>
</ul>
</li>
<li><p><strong>Whitening</strong></p>
<ul>
<li><p>Decorrelates data</p></li>
<li><p>Can be hit or miss</p></li>
</ul>
</li>
<li><p>When to do train/test split?</p></li>
</ul>
</section>
<section id="which-activation-and-weights-to-choose-in-neural-networks">
<h2>Which activation and weights to choose in neural networks<a class="headerlink" href="#which-activation-and-weights-to-choose-in-neural-networks" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>RELU? ELU? GELU? etc</p></li>
<li><p>Sigmoid or Tanh?</p></li>
<li><p>Set all weights to 0? Terrible idea</p></li>
<li><p>Set all weights to random values? Small random values</p></li>
</ul>
</section>
<section id="optimization-methods-and-hyperparameters">
<h2>Optimization Methods and Hyperparameters<a class="headerlink" href="#optimization-methods-and-hyperparameters" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Stochastic gradient descent</p></li>
<li><p>Stochastic gradient descent + momentum</p></li>
<li><p>State-of-the-art approaches:</p></li>
</ul>
<p>a. RMSProp</p>
<p>b. Adam</p>
<p>c. and more</p>
<p>Which regularization and hyperparameters? <span class="math notranslate nohighlight">\(L_1\)</span> or <span class="math notranslate nohighlight">\(L_2\)</span>, soft
classifiers, depths of trees and many other. Need to explore a large
set of hyperparameters and regularization methods.</p>
</section>
<section id="resampling">
<h2>Resampling<a class="headerlink" href="#resampling" title="Link to this heading">#</a></h2>
<p>When do we resample?</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.cambridge.org/core/books/bootstrap-methods-and-their-application/ED2FD043579F27952363566DC09CBD6A">Bootstrap</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=fSytzGwwBVw&amp;amp;ab_channel=StatQuestwithJoshStarmer">Cross-validation</a></p></li>
<li><p>Jackknife and many other</p></li>
</ol>
</section>
<section id="other-courses-on-data-science-and-machine-learning-at-uio">
<h2>Other courses on Data science and Machine Learning  at UiO<a class="headerlink" href="#other-courses-on-data-science-and-machine-learning-at-uio" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/fys/FYS5429/index-eng.html">FYS5429 – Advanced machine learning and data analysis for the physical sciences</a></p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/ifi/IN3050/index-eng.html">IN3050/IN4050 Introduction to Artificial Intelligence and Machine Learning</a>. Introductory course in machine learning and AI</p></li>
<li><p><a class="reference external" href="http://www.uio.no/studier/emner/matnat/math/STK-INF3000/index-eng.html">STK-INF3000/4000 Selected Topics in Data Science</a>. The course provides insight into selected contemporary relevant topics within Data Science.</p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/ifi/IN4080/index.html">IN4080 Natural Language Processing</a>. Probabilistic and machine learning techniques applied to natural language processing.</p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/math/STK-IN4300/index-eng.html">STK-IN4300 – Statistical learning methods in Data Science</a>. An advanced introduction to statistical and machine learning. For students with a good mathematics and statistics background.</p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/ifi/IN-STK5000/index-eng.html">IN-STK5000  Responsible Data Science</a>. Methods for adaptive collection and processing of data based on machine learning techniques.</p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/ifi/IN4310/index.html">IN4310 – Machine Learning for Image Analysis</a>. An introduction to deep learning with particular emphasis on applications within Image analysis, but useful for other application areas too.</p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/ifi/IN5310/index.html">IN5310 – Advanced Deep Learning for Image Analysis</a></p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/ifi/IN5490/index.html">IN5490 – Advanced Topics in Artificial Intelligence for Intelligent Systems</a></p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/its/TEK5040/">TEK5040 – Deep learning for autonomous systems</a>. The course addresses advanced algorithms and architectures for deep learning with neural networks. The course provides an introduction to how deep-learning techniques can be used in the construction of key parts of advanced autonomous systems that exist in physical environments and cyber environments.</p></li>
</ol>
</section>
<section id="additional-courses-of-interest">
<h2>Additional courses of interest<a class="headerlink" href="#additional-courses-of-interest" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/math/STK4051/index-eng.html">STK4051 Computational Statistics</a></p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/math/STK4021/index-eng.html">STK4021 Applied Bayesian Analysis and Numerical Methods</a></p></li>
</ol>
</section>
<section id="what-s-the-future-like">
<h2>What’s the future like?<a class="headerlink" href="#what-s-the-future-like" title="Link to this heading">#</a></h2>
<p>Based on multi-layer nonlinear neural networks, deep learning can
learn directly from raw data, automatically extract and abstract
features from layer to layer, and then achieve the goal of regression,
classification, or ranking. Deep learning has made breakthroughs in
computer vision, speech processing and natural language, and reached
or even surpassed human level. The success of deep learning is mainly
due to the three factors: big data, big model, and big computing.</p>
<p>In the past few decades, many different architectures of deep neural
networks have been proposed, such as</p>
<ol class="arabic simple">
<li><p>Convolutional neural networks, which are mostly used in image and video data processing, and have also been applied to sequential data such as text processing;</p></li>
<li><p>Recurrent neural networks, which can process sequential data of variable length and have been widely used in natural language understanding and speech processing;</p></li>
<li><p>Encoder-decoder framework, which is mostly used for image or sequence generation, such as machine translation, text summarization, and image captioning.</p></li>
</ol>
</section>
<section id="types-of-machine-learning-a-repetition">
<h2>Types of Machine Learning, a repetition<a class="headerlink" href="#types-of-machine-learning-a-repetition" title="Link to this heading">#</a></h2>
<p>The approaches to machine learning are many, but are often split into two main categories.
In <em>supervised learning</em> we know the answer to a problem,
and let the computer deduce the logic behind it. On the other hand, <em>unsupervised learning</em>
is a method for finding patterns and relationship in data sets without any prior knowledge of the system.
Some authours also operate with a third category, namely <em>reinforcement learning</em>. This is a paradigm
of learning inspired by behavioural psychology, where learning is achieved by trial-and-error,
solely from rewards and punishment.</p>
<p>Another way to categorize machine learning tasks is to consider the desired output of a system.
Some of the most common tasks are:</p>
<ul class="simple">
<li><p>Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.</p></li>
<li><p>Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.</p></li>
<li><p>Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.</p></li>
<li><p>Other unsupervised learning algortihms like <strong>Boltzmann machines</strong></p></li>
</ul>
</section>
<section id="why-boltzmann-machines">
<h2>Why Boltzmann machines?<a class="headerlink" href="#why-boltzmann-machines" title="Link to this heading">#</a></h2>
<p>What is known as restricted Boltzmann Machines (RMB) have received a lot of attention lately.
One of the major reasons is that they can be stacked layer-wise to build deep neural networks that capture complicated statistics.</p>
<p>The original RBMs had just one visible layer and a hidden layer, but recently so-called Gaussian-binary RBMs have gained quite some popularity in imaging since they are capable of modeling continuous data that are common to natural images.</p>
<p>Furthermore, they have been used to solve complicated <a class="reference external" href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.91.045002">quantum mechanical many-particle problems or classical statistical physics problems like the Ising and Potts classes of models</a>.</p>
</section>
<section id="boltzmann-machines">
<h2>Boltzmann Machines<a class="headerlink" href="#boltzmann-machines" title="Link to this heading">#</a></h2>
<p>Why use a generative model rather than the more well known discriminative deep neural networks (DNN)?</p>
<ul class="simple">
<li><p>Discriminitave methods have several limitations: They are mainly supervised learning methods, thus requiring labeled data. And there are tasks they cannot accomplish, like drawing new examples from an unknown probability distribution.</p></li>
<li><p>A generative model can learn to represent and sample from a probability distribution. The core idea is to learn a parametric model of the probability distribution from which the training data was drawn. As an example</p></li>
</ul>
<p>a. A model for images could learn to draw new examples of cats and dogs, given a training dataset of images of cats and dogs.</p>
<p>b. Generate a sample of an ordered or disordered phase, having been given samples of such phases.</p>
<p>c. Model the trial function for <a class="reference external" href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.91.045002">Monte Carlo calculations</a>.</p>
</section>
<section id="some-similarities-and-differences-from-dnns">
<h2>Some similarities and differences from DNNs<a class="headerlink" href="#some-similarities-and-differences-from-dnns" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Both use gradient-descent based learning procedures for minimizing cost functions</p></li>
<li><p>Energy based models don’t use backpropagation and automatic differentiation for computing gradients, instead turning to Markov Chain Monte Carlo methods.</p></li>
<li><p>DNNs often have several hidden layers. A restricted Boltzmann machine has only one hidden layer, however several RBMs can be stacked to make up Deep Belief Networks, of which they constitute the building blocks.</p></li>
</ol>
<p>History: The RBM was developed by amongst others <a class="reference external" href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Geoffrey Hinton</a>, called by some the “Godfather of Deep Learning”, working with the University of Toronto and Google.</p>
</section>
<section id="boltzmann-machines-bm">
<h2>Boltzmann machines (BM)<a class="headerlink" href="#boltzmann-machines-bm" title="Link to this heading">#</a></h2>
<p>A BM is what we would call an undirected probabilistic graphical model
with stochastic continuous or discrete units.</p>
<p>It is interpreted as a stochastic recurrent neural network where the
state of each unit(neurons/nodes) depends on the units it is connected
to. The weights in the network represent thus the strength of the
interaction between various units/nodes.</p>
<p>It turns into a Hopfield network if we choose deterministic rather
than stochastic units. In contrast to a Hopfield network, a BM is a
so-called generative model. It allows us to generate new samples from
the learned distribution.</p>
</section>
<section id="a-standard-bm-setup">
<h2>A standard BM setup<a class="headerlink" href="#a-standard-bm-setup" title="Link to this heading">#</a></h2>
<p>A standard BM network is divided into a set of observable and visible units <span class="math notranslate nohighlight">\(\hat{x}\)</span> and a set of unknown hidden units/nodes <span class="math notranslate nohighlight">\(\hat{h}\)</span>.</p>
<p>Additionally there can be bias nodes for the hidden and visible layers. These biases are normally set to <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>BMs are stackable, meaning they cwe can train a BM which serves as input to another BM. We can construct deep networks for learning complex PDFs. The layers can be trained one after another, a feature which makes them popular in deep learning</p>
<p>However, they are often hard to train. This leads to the introduction of so-called restricted BMs, or RBMS.
Here we take away all lateral connections between nodes in the visible layer as well as connections between nodes in the hidden layer. The network is illustrated in the figure below.</p>
</section>
<section id="the-structure-of-the-rbm-network">
<h2>The structure of the RBM network<a class="headerlink" href="#the-structure-of-the-rbm-network" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figures/RBM.png, width=800 frac=1.0] -->
<!-- begin figure -->
<p><img src="figures/RBM.png" width="800"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="the-network">
<h2>The network<a class="headerlink" href="#the-network" title="Link to this heading">#</a></h2>
<p><strong>The network layers</strong>:</p>
<ol class="arabic simple">
<li><p>A function <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> that represents the visible layer, a vector of <span class="math notranslate nohighlight">\(M\)</span> elements (nodes). This layer represents both what the RBM might be given as training input, and what we want it to be able to reconstruct. This might for example be given by the pixels of an image or coefficients representing speech, or the coordinates of a quantum mechanical state function.</p></li>
<li><p>The function <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> represents the hidden, or latent, layer. A vector of <span class="math notranslate nohighlight">\(N\)</span> elements (nodes). Also called “feature detectors”.</p></li>
</ol>
</section>
<section id="goals">
<h2>Goals<a class="headerlink" href="#goals" title="Link to this heading">#</a></h2>
<p>The goal of the hidden layer is to increase the model’s expressive
power. We encode complex interactions between visible variables by
introducing additional, hidden variables that interact with visible
degrees of freedom in a simple manner, yet still reproduce the complex
correlations between visible degrees in the data once marginalized
over (integrated out).</p>
<p><strong>The network parameters, to be optimized/learned</strong>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{a}\)</span> represents the visible bias, a vector of same length as <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{b}\)</span> represents the hidden bias, a vector of same lenght as <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(W\)</span> represents the interaction weights, a matrix of size <span class="math notranslate nohighlight">\(M\times N\)</span>.</p></li>
</ol>
</section>
<section id="joint-distribution">
<h2>Joint distribution<a class="headerlink" href="#joint-distribution" title="Link to this heading">#</a></h2>
<p>The restricted Boltzmann machine is described by a Boltzmann distribution</p>
<!-- Equation labels as ordinary links -->
<div id="_auto1"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
	P_{rbm}(\mathbf{x},\mathbf{h}) = \frac{1}{Z} e^{-\frac{1}{T_0}E(\mathbf{x},\mathbf{h})},
\label{_auto1} \tag{1}
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(Z\)</span> is the normalization constant or partition function, defined as</p>
<!-- Equation labels as ordinary links -->
<div id="_auto2"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
	Z = \int \int e^{-\frac{1}{T_0}E(\mathbf{x},\mathbf{h})} d\mathbf{x} d\mathbf{h}.
\label{_auto2} \tag{2}
\end{equation}
\]</div>
<p>It is common to ignore <span class="math notranslate nohighlight">\(T_0\)</span> by setting it to one.</p>
</section>
<section id="network-elements-the-energy-function">
<h2>Network Elements, the energy function<a class="headerlink" href="#network-elements-the-energy-function" title="Link to this heading">#</a></h2>
<p>The function <span class="math notranslate nohighlight">\(E(\mathbf{x},\mathbf{h})\)</span> gives the <strong>energy</strong> of a
configuration (pair of vectors) <span class="math notranslate nohighlight">\((\mathbf{x}, \mathbf{h})\)</span>. The lower
the energy of a configuration, the higher the probability of it. This
function also depends on the parameters <span class="math notranslate nohighlight">\(\mathbf{a}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> and
<span class="math notranslate nohighlight">\(W\)</span>. Thus, when we adjust them during the learning procedure, we are
adjusting the energy function to best fit our problem.</p>
<p>An expression for the energy function is</p>
<div class="math notranslate nohighlight">
\[
E(\hat{x},\hat{h}) = -\sum_{ia}^{NA}b_i^a \alpha_i^a(x_i)-\sum_{jd}^{MD}c_j^d \beta_j^d(h_j)-\sum_{ijad}^{NAMD}b_i^a \alpha_i^a(x_i)c_j^d \beta_j^d(h_j)w_{ij}^{ad}.
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\beta_j^d(h_j)\)</span> and <span class="math notranslate nohighlight">\(\alpha_i^a(x_j)\)</span> are so-called transfer functions that map a given input value to a desired feature value. The labels <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(d\)</span> denote that there can be multiple transfer functions per variable. The first sum depends only on the visible units. The second on the hidden ones. <strong>Note</strong> that there is no connection between nodes in a layer.</p>
<p>The quantities <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span> can be interpreted as the visible and hidden biases, respectively.</p>
<p>The connection between the nodes in the two layers is given by the weights <span class="math notranslate nohighlight">\(w_{ij}\)</span>.</p>
</section>
<section id="defining-different-types-of-rbms">
<h2>Defining different types of RBMs<a class="headerlink" href="#defining-different-types-of-rbms" title="Link to this heading">#</a></h2>
<p>There are different variants of RBMs, and the differences lie in the types of visible and hidden units we choose as well as in the implementation of the energy function <span class="math notranslate nohighlight">\(E(\mathbf{x},\mathbf{h})\)</span>.</p>
<p><strong>Binary-Binary RBM:</strong></p>
<p>RBMs were first developed using binary units in both the visible and hidden layer. The corresponding energy function is defined as follows:</p>
<!-- Equation labels as ordinary links -->
<div id="_auto3"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
	E(\mathbf{x}, \mathbf{h}) = - \sum_i^M x_i a_i- \sum_j^N b_j h_j - \sum_{i,j}^{M,N} x_i w_{ij} h_j,
\label{_auto3} \tag{3}
\end{equation}
\]</div>
<p>where the binary values taken on by the nodes are most commonly 0 and 1.</p>
<p><strong>Gaussian-Binary RBM:</strong></p>
<p>Another varient is the RBM where the visible units are Gaussian while the hidden units remain binary:</p>
<!-- Equation labels as ordinary links -->
<div id="_auto4"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
	E(\mathbf{x}, \mathbf{h}) = \sum_i^M \frac{(x_i - a_i)^2}{2\sigma_i^2} - \sum_j^N b_j h_j - \sum_{i,j}^{M,N} \frac{x_i w_{ij} h_j}{\sigma_i^2}. 
\label{_auto4} \tag{4}
\end{equation}
\]</div>
</section>
<section id="more-about-rbms">
<h2>More about RBMs<a class="headerlink" href="#more-about-rbms" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Useful when we model continuous data (i.e., we wish <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to be continuous)</p></li>
<li><p>Requires a smaller learning rate, since there’s no upper bound to the value a component might take in the reconstruction</p></li>
</ol>
<p>Other types of units include:</p>
<ol class="arabic simple">
<li><p>Softmax and multinomial units</p></li>
<li><p>Gaussian visible and hidden units</p></li>
<li><p>Binomial units</p></li>
<li><p>Rectified linear units</p></li>
</ol>
<p>To read more, see <a class="reference external" href="https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/pub/notebook2/ipynb/notebook2.ipynb">Lectures on Boltzmann machines in Physics</a>.</p>
</section>
<section id="id2">
<h2>Autoencoders: Overarching view<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>Autoencoders are artificial neural networks capable of learning
efficient representations of the input data (these representations are called codings)  without
any supervision (i.e., the training set is unlabeled). These codings
typically have a much lower dimensionality than the input data, making
autoencoders useful for dimensionality reduction.</p>
<p>More importantly, autoencoders act as powerful feature detectors, and
they can be used for unsupervised pretraining of deep neural networks.</p>
<p>Lastly, they are capable of randomly generating new data that looks
very similar to the training data; this is called a generative
model. For example, you could train an autoencoder on pictures of
faces, and it would then be able to generate new faces.  Surprisingly,
autoencoders work by simply learning to copy their inputs to their
outputs. This may sound like a trivial task, but we will see that
constraining the network in various ways can make it rather
difficult. For example, you can limit the size of the internal
representation, or you can add noise to the inputs and train the
network to recover the original inputs. These constraints prevent the
autoencoder from trivially copying the inputs directly to the outputs,
which forces it to learn efficient ways of representing the data. In
short, the codings are byproducts of the autoencoder’s attempt to
learn the identity function under some constraints.</p>
<p><a class="reference external" href="https://www.coursera.org/lecture/building-deep-learning-models-with-tensorflow/autoencoders-1U4L3">Video on autoencoders</a></p>
<p>See also A. Geron’s textbook, chapter 15.</p>
</section>
<section id="bayesian-machine-learning">
<h2>Bayesian Machine Learning<a class="headerlink" href="#bayesian-machine-learning" title="Link to this heading">#</a></h2>
<p>This is an important topic if we aim at extracting a probability
distribution. This gives us also a confidence interval and error
estimates.</p>
<p>Bayesian machine learning allows us to encode our prior beliefs about
what those models should look like, independent of what the data tells
us. This is especially useful when we don’t have a ton of data to
confidently learn our model.</p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=E1qhGw8QxqY&amp;amp;ab_channel=AndrewGordonWilson">Video on Bayesian deep learning</a></p>
<p>See also the <a class="reference external" href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/Articles/lec03.pdf">slides here</a>.</p>
</section>
<section id="reinforcement-learning">
<h2>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Link to this heading">#</a></h2>
<p>Reinforcement Learning (RL) is one of the most exciting fields of
Machine Learning today, and also one of the oldest. It has been around
since the 1950s, producing many interesting applications over the
years.</p>
<p>It studies
how agents take actions based on trial and error, so as to maximize
some notion of cumulative reward in a dynamic system or
environment. Due to its generality, the problem has also been studied
in many other disciplines, such as game theory, control theory,
operations research, information theory, multi-agent systems, swarm
intelligence, statistics, and genetic algorithms.</p>
<p>In March 2016, AlphaGo, a computer program that plays the board game
Go, beat Lee Sedol in a five-game match. This was the first time a
computer Go program had beaten a 9-dan (highest rank) professional
without handicaps. AlphaGo is based on deep convolutional neural
networks and reinforcement learning. AlphaGo’s victory was a major
milestone in artificial intelligence and it has also made
reinforcement learning a hot research area in the field of machine
learning.</p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=FgzM3zpZ55o&amp;amp;ab_channel=stanfordonline">Lecture on Reinforcement Learning</a>.</p>
<p>See also A. Geron’s textbook, chapter 16.</p>
</section>
<section id="transfer-learning">
<h2>Transfer learning<a class="headerlink" href="#transfer-learning" title="Link to this heading">#</a></h2>
<p>The goal of transfer learning is to transfer the model or knowledge
obtained from a source task to the target task, in order to resolve
the issues of insufficient training data in the target task. The
rationality of doing so lies in that usually the source and target
tasks have inter-correlations, and therefore either the features,
samples, or models in the source task might provide useful information
for us to better solve the target task. Transfer learning is a hot
research topic in recent years, with many problems still waiting to be studied.</p>
<p><a class="reference external" href="https://www.ias.edu/video/machinelearning/2020/0331-SamoryKpotufe">Lecture on transfer learning</a>.</p>
</section>
<section id="adversarial-learning">
<h2>Adversarial learning<a class="headerlink" href="#adversarial-learning" title="Link to this heading">#</a></h2>
<p>The conventional deep generative model has a potential problem: the
model tends to generate extreme instances to maximize the
probabilistic likelihood, which will hurt its performance. Adversarial
learning utilizes the adversarial behaviors (e.g., generating
adversarial instances or training an adversarial model) to enhance the
robustness of the model and improve the quality of the generated
data. In recent years, one of the most promising unsupervised learning
technologies, generative adversarial networks (GAN), has already been
successfully applied to image, speech, and text.</p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=CIfsB_EYsVI&amp;amp;ab_channel=StanfordUniversitySchoolofEngineering">Lecture on adversial learning</a>.</p>
</section>
<section id="dual-learning">
<h2>Dual learning<a class="headerlink" href="#dual-learning" title="Link to this heading">#</a></h2>
<p>Dual learning is a new learning paradigm, the basic idea of which is
to use the primal-dual structure between machine learning tasks to
obtain effective feedback/regularization, and guide and strengthen the
learning process, thus reducing the requirement of large-scale labeled
data for deep learning. The idea of dual learning has been applied to
many problems in machine learning, including machine translation,
image style conversion, question answering and generation, image
classification and generation, text classification and generation,
image-to-text, and text-to-image.</p>
</section>
<section id="distributed-machine-learning">
<h2>Distributed machine learning<a class="headerlink" href="#distributed-machine-learning" title="Link to this heading">#</a></h2>
<p>Distributed computation will speed up machine learning algorithms,
significantly improve their efficiency, and thus enlarge their
application. When distributed meets machine learning, more than just
implementing the machine learning algorithms in parallel is required.</p>
</section>
<section id="meta-learning">
<h2>Meta learning<a class="headerlink" href="#meta-learning" title="Link to this heading">#</a></h2>
<p>Meta learning is an emerging research direction in machine
learning. Roughly speaking, meta learning concerns learning how to
learn, and focuses on the understanding and adaptation of the learning
itself, instead of just completing a specific learning task. That is,
a meta learner needs to be able to evaluate its own learning methods
and adjust its own learning methods according to specific learning
tasks.</p>
</section>
<section id="the-challenges-facing-machine-learning">
<h2>The Challenges Facing Machine Learning<a class="headerlink" href="#the-challenges-facing-machine-learning" title="Link to this heading">#</a></h2>
<p>While there has been much progress in machine learning, there are also challenges.</p>
<p>For example, the mainstream machine learning technologies are
black-box approaches, making us concerned about their potential
risks. To tackle this challenge, we may want to make machine learning
more explainable and controllable. As another example, the
computational complexity of machine learning algorithms is usually
very high and we may want to invent lightweight algorithms or
implementations. Furthermore, in many domains such as physics,
chemistry, biology, and social sciences, people usually seek elegantly
simple equations (e.g., the Schrödinger equation) to uncover the
underlying laws behind various phenomena. In the field of machine
learning, can we reveal simple laws instead of designing more complex
models for data fitting? Although there are many challenges, we are
still very optimistic about the future of machine learning. As we look
forward to the future, here are what we think the research hotspots in
the next ten years will be.</p>
<p>See the article on <a class="reference external" href="https://www.frontiersin.org/articles/10.3389/frai.2020.00025/full">Discovery of Physics From Data: Universal Laws and Discrepancies</a></p>
</section>
<section id="explainable-machine-learning">
<h2>Explainable machine learning<a class="headerlink" href="#explainable-machine-learning" title="Link to this heading">#</a></h2>
<p>Machine learning, especially deep learning, evolves rapidly. The
ability gap between machine and human on many complex cognitive tasks
becomes narrower and narrower. However, we are still in the very early
stage in terms of explaining why those effective models work and how
they work.</p>
<p><strong>What is missing: the gap between correlation and causation</strong>. Standard Machine Learning is based on what e have called a frequentist approach.</p>
<p>Most
machine learning techniques, especially the statistical ones, depend
highly on correlations in data sets to make predictions and analyses. In
contrast, rational humans tend to reply on clear and trustworthy
causality relations obtained via logical reasoning on real and clear
facts. It is one of the core goals of explainable machine learning to
transition from solving problems by data correlation to solving
problems by logical reasoning.</p>
<p><strong>Bayesian Machine Learning is one of the exciting research directions in this field</strong>.</p>
</section>
<section id="quantum-machine-learning">
<h2>Quantum machine learning<a class="headerlink" href="#quantum-machine-learning" title="Link to this heading">#</a></h2>
<p>Quantum machine learning is an emerging interdisciplinary research
area at the intersection of quantum computing and machine learning.</p>
<p>Quantum computers use effects such as quantum coherence and quantum
entanglement to process information, which is fundamentally different
from classical computers. Quantum algorithms have surpassed the best
classical algorithms in several problems (e.g., searching for an
unsorted database, inverting a sparse matrix), which we call quantum
acceleration.</p>
<p>When quantum computing meets machine learning, it can be a mutually
beneficial and reinforcing process, as it allows us to take advantage
of quantum computing to improve the performance of classical machine
learning algorithms. In addition, we can also use the machine learning
algorithms (on classic computers) to analyze and improve quantum
computing systems.</p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=Xh9pUu3-WxM&amp;amp;ab_channel=InstituteforPure%26AppliedMathematics%28IPAM%29">Lecture on Quantum ML</a>.</p>
<p><a class="reference external" href="https://physics.aps.org/articles/v13/179?utm_campaign=weekly&amp;amp;utm_medium=email&amp;amp;utm_source=emailalert">Read interview with Maria Schuld on her work on Quantum Machine Learning</a>. See also <a class="reference external" href="https://www.springer.com/gp/book/9783319964232">her recent textbook</a>.</p>
</section>
<section id="quantum-machine-learning-algorithms-based-on-linear-algebra">
<h2>Quantum machine learning algorithms based on linear algebra<a class="headerlink" href="#quantum-machine-learning-algorithms-based-on-linear-algebra" title="Link to this heading">#</a></h2>
<p>Many quantum machine learning algorithms are based on variants of
quantum algorithms for solving linear equations, which can efficiently
solve N-variable linear equations with complexity of O(log2 N) under
certain conditions. The quantum matrix inversion algorithm can
accelerate many machine learning methods, such as least square linear
regression, least square version of support vector machine, Gaussian
process, and more. The training of these algorithms can be simplified
to solve linear equations. The key bottleneck of this type of quantum
machine learning algorithms is data input—that is, how to initialize
the quantum system with the entire data set. Although efficient
data-input algorithms exist for certain situations, how to efficiently
input data into a quantum system is as yet unknown for most cases.</p>
</section>
<section id="quantum-reinforcement-learning">
<h2>Quantum reinforcement learning<a class="headerlink" href="#quantum-reinforcement-learning" title="Link to this heading">#</a></h2>
<p>In quantum reinforcement learning, a quantum agent interacts with the
classical environment to obtain rewards from the environment, so as to
adjust and improve its behavioral strategies. In some cases, it
achieves quantum acceleration by the quantum processing capabilities
of the agent or the possibility of exploring the environment through
quantum superposition. Such algorithms have been proposed in
superconducting circuits and systems of trapped ions.</p>
</section>
<section id="quantum-deep-learning">
<h2>Quantum deep learning<a class="headerlink" href="#quantum-deep-learning" title="Link to this heading">#</a></h2>
<p>Dedicated quantum information processors, such as quantum annealers
and programmable photonic circuits, are well suited for building deep
quantum networks. The simplest deep quantum network is the Boltzmann
machine. The classical Boltzmann machine consists of bits with tunable
interactions and is trained by adjusting the interaction of these bits
so that the distribution of its expression conforms to the statistics
of the data. To quantize the Boltzmann machine, the neural network can
simply be represented as a set of interacting quantum spins that
correspond to an adjustable Ising model. Then, by initializing the
input neurons in the Boltzmann machine to a fixed state and allowing
the system to heat up, we can read out the output qubits to get the
result.</p>
</section>
<section id="social-machine-learning">
<h2>Social machine learning<a class="headerlink" href="#social-machine-learning" title="Link to this heading">#</a></h2>
<p>Machine learning aims to imitate how humans
learn. While we have developed successful machine learning algorithms,
until now we have ignored one important fact: humans are social. Each
of us is one part of the total society and it is difficult for us to
live, learn, and improve ourselves, alone and isolated. Therefore, we
should design machines with social properties. Can we let machines
evolve by imitating human society so as to achieve more effective,
intelligent, interpretable “social machine learning”?</p>
<p>And much more.</p>
</section>
<section id="the-last-words">
<h2>The last words?<a class="headerlink" href="#the-last-words" title="Link to this heading">#</a></h2>
<p>Early computer scientist Alan Kay said, <strong>The best way to predict the
future is to create it</strong>. Therefore, all machine learning
practitioners, whether scholars or engineers, professors or students,
need to work together to advance these important research
topics. Together, we will not just predict the future, but create it.</p>
</section>
<section id="best-wishes-to-you-all-and-thanks-so-much-for-your-heroic-efforts-this-semester">
<h2>Best wishes to you all and thanks so much for your heroic efforts this semester<a class="headerlink" href="#best-wishes-to-you-all-and-thanks-so-much-for-your-heroic-efforts-this-semester" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figures/Nebbdyr2.png, width=500 frac=0.6] -->
<!-- begin figure -->
<p><img src="figures/Nebbdyr2.png" width="500"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="exercisesweek47.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Exercise week 47-48</p>
      </div>
    </a>
    <a class="right-next"
       href="project1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Project 1 on Machine Learning, deadline October 6 (midnight), 2025</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-week-48">Overview of week 48</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-monday-november-24">Lecture Monday, November 24</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#readings-and-videos">Readings and Videos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-sessions">Lab sessions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoders-overarching-view">Autoencoders: Overarching view</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#powerful-detectors">Powerful detectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-introduction-of-aes">First introduction of AEs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoder-structure">Autoencoder structure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#schematic-image-of-an-autoencoder">Schematic image of an Autoencoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-on-the-structure">More on the structure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-part">Decoder part</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#typical-aes">Typical AEs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-autoencoder">Feed Forward Autoencoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mirroring">Mirroring</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#output-of-middle-layer">Output of middle layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-function-of-the-output-layer">Activation Function of the Output Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relu">ReLU</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid">Sigmoid</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-loss-function">Cost/Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy">Binary Cross-Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruction-error">Reconstruction Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temporary-summary-on-autoencoders">Temporary summary on Autoencoders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-autoencoders-and-the-pca-theorem">Linear autoencoders and  the PCA theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-details">More details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-pca-theorem">The PCA Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-algorithm-before-theorem">The Algorithm before theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-steps">Further steps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-our-own-pca-code">Writing our own PCA code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-it">Implementing it</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#first-step">First Step</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling">Scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#centered-data">Centered Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring">Exploring</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagonalize-the-sample-covariance-matrix-to-obtain-the-principal-components">Diagonalize the sample covariance matrix to obtain the principal components</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collecting-all-steps">Collecting all Steps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classical-pca-theorem">Classical PCA Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">The PCA Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-interpretation-and-link-with-singular-value-decomposition">Geometric Interpretation and link with Singular Value Decomposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-and-scikit-learn">PCA and scikit-learn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-cancer-data">Example of Cancer Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#incremental-pca">Incremental PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#randomized-pca">Randomized PCA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-pca">Kernel PCA</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-techniques">Other techniques</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-autoencoders-linear-autoencoders">Back to Autoencoders: Linear Autoencoders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-example">PyTorch example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-course">Summary of course</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-me-worry-no-final-exam-in-this-course">What? Me worry? No final exam in this course!</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topics-we-have-covered-this-year">Topics we have covered this year</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-analysis-and-optimization-of-data">Statistical analysis and optimization of data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes-and-overarching-aims-of-this-course">Learning outcomes and overarching aims of this course</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perspective-on-machine-learning">Perspective on Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-research">Machine Learning Research</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#starting-your-machine-learning-project">Starting your Machine Learning Project</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choose-a-model-and-algorithm">Choose a Model and Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-your-data">Preparing Your Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#which-activation-and-weights-to-choose-in-neural-networks">Which activation and weights to choose in neural networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-methods-and-hyperparameters">Optimization Methods and Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resampling">Resampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-courses-on-data-science-and-machine-learning-at-uio">Other courses on Data science and Machine Learning  at UiO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-courses-of-interest">Additional courses of interest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-the-future-like">What’s the future like?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning-a-repetition">Types of Machine Learning, a repetition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-boltzmann-machines">Why Boltzmann machines?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boltzmann-machines">Boltzmann Machines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-similarities-and-differences-from-dnns">Some similarities and differences from DNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boltzmann-machines-bm">Boltzmann machines (BM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-standard-bm-setup">A standard BM setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-structure-of-the-rbm-network">The structure of the RBM network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-network">The network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goals">Goals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distribution">Joint distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-elements-the-energy-function">Network Elements, the energy function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-different-types-of-rbms">Defining different types of RBMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-about-rbms">More about RBMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Autoencoders: Overarching view</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-machine-learning">Bayesian Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning">Reinforcement Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">Transfer learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adversarial-learning">Adversarial learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dual-learning">Dual learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-machine-learning">Distributed machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#meta-learning">Meta learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-challenges-facing-machine-learning">The Challenges Facing Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explainable-machine-learning">Explainable machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantum-machine-learning">Quantum machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantum-machine-learning-algorithms-based-on-linear-algebra">Quantum machine learning algorithms based on linear algebra</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantum-reinforcement-learning">Quantum reinforcement learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantum-deep-learning">Quantum deep learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#social-machine-learning">Social machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-last-words">The last words?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#best-wishes-to-you-all-and-thanks-so-much-for-your-heroic-efforts-this-semester">Best wishes to you all and thanks so much for your heroic efforts this semester</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Morten Hjorth-Jensen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>