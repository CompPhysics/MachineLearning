
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 34: Introduction to the course, Logistics and Practicalities &#8212; Applied Data Analysis and Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week34';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Exercises week 35" href="exercisesweek35.html" />
    <link rel="prev" title="Exercises week 34" href="exercisesweek34.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Applied Data Analysis and Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Applied Data Analysis and Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Applied Data Analysis and Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About the course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Course setting</a></li>
<li class="toctree-l1"><a class="reference internal" href="teachers.html">Teachers and Grading</a></li>
<li class="toctree-l1"><a class="reference internal" href="textbooks.html">Textbooks</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Review of Statistics with Resampling Techniques and Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="statistics.html">1. Elements of Probability Theory and Statistical Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">2. Linear Algebra, Handling of Arrays and more Python Features</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Regression to Support Vector Machines</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter1.html">3. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter2.html">4. Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter3.html">5. Resampling Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter4.html">6. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapteroptimization.html">7. Optimization, the central part of any Machine Learning algortithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter5.html">8. Support Vector Machines, overarching aims</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Decision Trees, Ensemble Methods and Boosting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter6.html">9. Decision trees, overarching aims</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter7.html">10. Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Dimensionality Reduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter8.html">11. Basic ideas of the Principal Component Analysis (PCA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">12. Clustering and Unsupervised Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter9.html">13. Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter10.html">14. Building a Feed Forward Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter11.html">15. Solving Differential Equations  with Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter12.html">16. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter13.html">17. Recurrent neural networks: Overarching view</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Weekly material, notes and exercises</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="exercisesweek34.html">Exercises week 34</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 34: Introduction to the course, Logistics and Practicalities</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek35.html">Exercises week 35</a></li>
<li class="toctree-l1"><a class="reference internal" href="week35.html">Week 35: From Ordinary Linear Regression to Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek36.html">Exercises week 36</a></li>
<li class="toctree-l1"><a class="reference internal" href="week36.html">Week 36: Linear Regression and Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek37.html">Exercises week 37</a></li>
<li class="toctree-l1"><a class="reference internal" href="week37.html">Week 37: Gradient descent methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek38.html">Exercises week 38</a></li>
<li class="toctree-l1"><a class="reference internal" href="week38.html">Week 38: Statistical analysis, bias-variance tradeoff and resampling methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek39.html">Exercises week 39</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="project1.html">Project 1 on Machine Learning, deadline October 6 (midnight), 2025</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/week34.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 34: Introduction to the course, Logistics and Practicalities</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-first-week">Overview of first week</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#schedule-first-week">Schedule first week</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lectures-and-computerlab">Lectures and ComputerLab</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#communication-channels">Communication channels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#course-format">Course Format</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teachers">Teachers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deadlines-for-projects-tentative">Deadlines for projects (tentative)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#grading">Grading</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-material">Reading material</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-textbooks">Main textbooks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-popular-texts">Other popular texts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-suggestions-week-34">Reading suggestions week 34</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topics-covered-in-this-course-statistical-analysis-and-optimization-of-data">Topics covered in this course: Statistical analysis and optimization of data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-analysis-and-optimization-of-data">Statistical analysis and optimization of data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-methods">Deep learning methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extremely-useful-tools-strongly-recommended">Extremely useful tools, strongly recommended</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-courses-on-data-science-and-machine-learning-at-uio">Other courses on Data science and Machine Learning  at UiO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-courses-on-data-science-and-machine-learning-at-uio-contn">Other courses on Data science and Machine Learning  at UiO, contn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning">Types of Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#essential-elements-of-ml">Essential elements of ML</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-optimization-minimization-problem">An optimization/minimization problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-plethora-of-machine-learning-algorithms-methods">The plethora  of machine learning algorithms/methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-generative-modeling">What Is Generative Modeling?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-generative-modeling-taken-from-generative-deep-learning-by-david-foster">Example of generative modeling, taken from Generative Deep Learning by David Foster</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-versus-discriminative-modeling">Generative Versus Discriminative Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-discriminative-modeling-taken-from-generative-deep-learning-by-david-foster">Example of discriminative modeling, taken from Generative Deep Learning by David Foster</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discriminative-modeling">Discriminative Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-frequentist-approach-to-data-analysis">A Frequentist approach to data analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-good-model">What is a good model?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-good-model-can-we-define-it">What is a good model? Can we define it?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#software-and-needed-installations">Software and needed installations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-installers">Python installers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-python-libraries">Useful Python libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-r-c-cython-or-julia">Installing R, C++, cython or Julia</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-r-c-cython-numba-etc">Installing R, C++, cython, Numba etc</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numpy-examples-and-important-matrix-and-vector-handling-packages">Numpy examples and Important Matrix and vector handling packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numpy-and-arrays">Numpy and arrays</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-in-python">Matrices in Python</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#meet-the-pandas">Meet the Pandas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pandas-ai">Pandas AI</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression-model-using-scikit-learn">Simple linear regression model using <strong>scikit-learn</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-our-real-data-nuclear-binding-energies-brief-reminder-on-masses-and-binding-energies">To our real data: nuclear binding energies. Brief reminder on masses and binding energies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#organizing-our-data">Organizing our data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#and-what-about-using-neural-networks">And what about using neural networks?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-first-summary">A first summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-linear-regression-aka-ordinary-least-squares-and-family">Why Linear Regression (aka Ordinary Least Squares and family)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-analysis-overarching-aims">Regression analysis, overarching aims</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-analysis-overarching-aims-ii">Regression analysis, overarching aims II</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-linear-models-and-linear-algebra">General linear models and linear algebra</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rewriting-the-fitting-procedure-as-a-linear-algebra-problem">Rewriting the fitting procedure as a linear algebra problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rewriting-the-fitting-procedure-as-a-linear-algebra-problem-more-details">Rewriting the fitting procedure as a linear algebra problem, more details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-the-fitting-procedure-as-a-linear-algebra-problem">Generalizing the fitting procedure as a linear algebra problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Generalizing the fitting procedure as a linear algebra problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-our-parameters">Optimizing our parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#our-model-for-the-nuclear-binding-energies">Our model for the nuclear binding energies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-our-parameters-more-details">Optimizing our parameters, more details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretations-and-optimizing-our-parameters">Interpretations and optimizing our parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Interpretations and optimizing our parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Interpretations and optimizing our parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#own-code-for-ordinary-least-squares">Own code for Ordinary Least Squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-error-analysis-and-training-set-up">Adding error analysis and training set up</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chi-2-function">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)
doconce format html week34.do.txt --no_mako -->
<!-- dom:TITLE: Week 34: Introduction to the course, Logistics and Practicalities --><section class="tex2jax_ignore mathjax_ignore" id="week-34-introduction-to-the-course-logistics-and-practicalities">
<h1>Week 34: Introduction to the course, Logistics and Practicalities<a class="headerlink" href="#week-34-introduction-to-the-course-logistics-and-practicalities" title="Link to this heading">#</a></h1>
<p><strong>Morten Hjorth-Jensen</strong>, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway</p>
<p>Date: <strong>Week 34, August 18-22, 2025</strong></p>
<section id="overview-of-first-week">
<h2>Overview of first week<a class="headerlink" href="#overview-of-first-week" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>The sessions on Tuesdays and Wednesdays last four hours for each group (four groups in total) and will include lectures in a flipped mode (promoting active learning) and work on exercices and projects.</p></li>
<li><p>The sessions will begin with lectures, discussions, questions and answers about the material to be covered every week. Videos and teaching material will be announced in due time.</p></li>
<li><p>There are four groups:</p></li>
</ol>
<ul class="simple">
<li><p>Tuesdays 815am-12pm and 1215pm-4pm</p></li>
<li><p>Wednesdays 815am-12pm and 1215pm-4pm.</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>On Mondays  we have a regular lecture which will be organized as a mix of active learning sessions and regular lectures. These lectures/active learning sessions start at 215pm and end at 4pm and serve the aims of giving an overview over various topics as well as solving specific problems. These lectures will also be recorded. Lectures can be attended in person or via zoom at <a class="reference external" href="https://uio.zoom.us/my/mortenhj">https://uio.zoom.us/my/mortenhj</a></p></li>
</ol>
<!-- * [Link to recording of lecture](https://youtu.be/82IPtCrzbhs) -->
<p>Videos and learning material with reading suggestions will be made available before each week starts.</p>
</section>
<section id="schedule-first-week">
<h2>Schedule first week<a class="headerlink" href="#schedule-first-week" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>August 18: Lecture: Presentation of course, Linear regression, examples and theory</p></li>
<li><p>August 19: Introduction to software and repetition of Python Programming, linear algebra and basic elements of statistics. Please select group.</p></li>
<li><p>August 20: Introduction to software and repetition of Python Programming, linear algebra and basic elements of statistics. Please select group.</p></li>
</ul>
</section>
<section id="lectures-and-computerlab">
<h2>Lectures and ComputerLab<a class="headerlink" href="#lectures-and-computerlab" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Mondays: regular lectures/active learning sessions (2.15pm-4pm)</p></li>
<li><p>The sessions on Tuesdays and Wednesdays last four hours and will include partly lectures and discussions in the beginning.</p></li>
<li><p>Weekly reading assignments and videos needed to solve projects and exercises.</p></li>
<li><p>Weekly exercises. You can hand in exercises if you want and get an extra score, see below.</p></li>
<li><p>Detailed lecture notes, exercises, all programs presented, projects etc can be found at the homepage of the course.</p></li>
<li><p>Weekly plans and all other information are on the official website. This info will also be conveyed via weekly emails.</p></li>
<li><p>No final exam, three projects that are graded and have to be approved.</p></li>
</ul>
</section>
<section id="communication-channels">
<h2>Communication channels<a class="headerlink" href="#communication-channels" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Communications (email and more) via &lt;<a class="reference external" href="http://canvas.uio.no">canvas.uio.no</a>&gt;</p></li>
</ul>
<!-- * **Discord** channel at <https://discord.gg/XBKjd4ccGq> --></section>
<section id="course-format">
<h2>Course Format<a class="headerlink" href="#course-format" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Three compulsory projects. Electronic reports only using <a class="reference external" href="https://www.uio.no/english/services/it/education/canvas/">Canvas</a> to hand in projects and <a class="reference external" href="https://git-scm.com/">git</a> as version control software and <a class="reference external" href="https://github.com/">GitHub</a> for repository (or <a class="reference external" href="https://about.gitlab.com/">GitLab</a>) of all your material.</p></li>
<li><p>Evaluation and grading: The three projects are graded and each counts 1/3 of the final mark. No final written or oral exam.</p></li>
</ul>
<p>a. For the last project each group/participant submits a proposal or works with suggested (by us) proposals for the project.</p>
<p>b. If possible, we would like to organize the last project as a workshop where each group presents this to all other participants of the course</p>
<p>c. Based on feedback etc, each group finalizes the report and submits for grading.</p>
<ul class="simple">
<li><p>Python is the default programming language, but feel free to use C/C++, Julia and/or Fortran or other programming languages. All source codes discussed during the lectures can be found at the webpage and <a class="reference external" href="https://github.com/CompPhysics/MachineLearning/tree/master/doc/Programs">github address</a> of the course.</p></li>
</ul>
</section>
<section id="teachers">
<h2>Teachers<a class="headerlink" href="#teachers" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Morten Hjorth-Jensen, <a class="reference external" href="mailto:morten&#46;hjorth-jensen&#37;&#52;&#48;fys&#46;uio&#46;no">morten<span>&#46;</span>hjorth-jensen<span>&#64;</span>fys<span>&#46;</span>uio<span>&#46;</span>no</a></p>
<ul>
<li><p><strong>Phone</strong>: +47-48257387</p></li>
<li><p><strong>Office</strong>: Department of Physics, University of Oslo, Eastern wing, room FØ470</p></li>
<li><p><strong>Office hours</strong>: <em>Anytime</em>!  Individual or group office hours can be arranged either in person or  via zoom. Feel free to send an email for planning.</p></li>
</ul>
</li>
<li><p>Ida Torkjellsdatter Storehaug, <a class="reference external" href="mailto:i&#46;t&#46;storehaug&#37;&#52;&#48;fys&#46;uio&#46;no">i<span>&#46;</span>t<span>&#46;</span>storehaug<span>&#64;</span>fys<span>&#46;</span>uio<span>&#46;</span>no</a></p></li>
<li><p>Oskar Leinonen, <a class="reference external" href="mailto:oskarlei&#37;&#52;&#48;fys&#46;uio&#46;no">oskarlei<span>&#64;</span>fys<span>&#46;</span>uio<span>&#46;</span>no</a></p></li>
<li><p>Mia-Katrin Ose Kvalsund, <a class="reference external" href="mailto:m&#46;k&#46;o&#46;kvalsund&#37;&#52;&#48;fys&#46;uio&#46;no">m<span>&#46;</span>k<span>&#46;</span>o<span>&#46;</span>kvalsund<span>&#64;</span>fys<span>&#46;</span>uio<span>&#46;</span>no</a></p></li>
<li><p>Karl Henrik Fredly, <a class="reference external" href="mailto:k&#46;h&#46;fredly&#37;&#52;&#48;fys&#46;uio&#46;no">k<span>&#46;</span>h<span>&#46;</span>fredly<span>&#64;</span>fys<span>&#46;</span>uio<span>&#46;</span>no</a></p></li>
<li><p>Eir Eline Hørlyk, <a class="reference external" href="mailto:e&#46;e&#46;horlyk&#37;&#52;&#48;fys&#46;uio&#46;no">e<span>&#46;</span>e<span>&#46;</span>horlyk<span>&#64;</span>fys<span>&#46;</span>uio<span>&#46;</span>no</a></p></li>
<li><p>Britt S. Haanen, <a class="reference external" href="mailto:b&#46;s&#46;m&#46;haanen&#37;&#52;&#48;fys&#46;uio&#46;no">b<span>&#46;</span>s<span>&#46;</span>m<span>&#46;</span>haanen<span>&#64;</span>fys<span>&#46;</span>uio<span>&#46;</span>no</a></p></li>
</ul>
</section>
<section id="deadlines-for-projects-tentative">
<h2>Deadlines for projects (tentative)<a class="headerlink" href="#deadlines-for-projects-tentative" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Project 1: October 6 (available September 1) graded with feedback)</p></li>
<li><p>Project 2: November 3 (available October 6, graded with feedback)</p></li>
<li><p>Project 3: December 8  (available November 3, graded with feedback)</p></li>
</ol>
<p>Extra Credit (not mandatory),  weekly exercise assignments, 10 in total (due Friday same week), 10% additional score. The extra credit assignments are due each Friday and can be uploaed to <strong>Canvas</strong> in your preferred format (although we prefer jupyter-notebooks). First assignment is for week 35. Each weekly exercise set gives one additional point to the final score, see below on grading.</p>
</section>
<section id="grading">
<h2>Grading<a class="headerlink" href="#grading" title="Link to this heading">#</a></h2>
<p>Grades are awarded on a scale from A to F, where A is the best grade and F is a fail. There are three projects which are graded and each project counts 1/3 of the final grade. The total score is thus the average from all three projects.</p>
<p>The final number of points is based on the average of all projects and the grade follows the following table:</p>
<ul class="simple">
<li><p>92-100 points: A</p></li>
<li><p>77-91 points: B</p></li>
<li><p>58-76 points: C</p></li>
<li><p>46-57 points: D</p></li>
<li><p>40-45 points: E</p></li>
<li><p>0-39 points: F-failed</p></li>
</ul>
<p>In addition you can get an extra score for weekly assignments (10 in total and due each Friday). Each weekly assignment counts 1 point. As an example, this means that if your average after three projects is 88 points and you have handed in and gotten approved four weekly exercises, the total score is 88+4=92, which translates into an A.</p>
</section>
<section id="reading-material">
<h2>Reading material<a class="headerlink" href="#reading-material" title="Link to this heading">#</a></h2>
<p>The lecture notes are collected as a jupyter-book at <a class="reference external" href="https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/intro.html">https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/intro.html</a>.</p>
<p>In addition to the lecture notes, we recommend the books of Rasckha et
al and Goodfellow et al. We will follow these texts closely and the
weekly reading assignments refer to these texts. The text by Hastie et
al is also widely used in the Machine Learning community. See next slide for link to textbooks.</p>
</section>
<section id="main-textbooks">
<h2>Main textbooks<a class="headerlink" href="#main-textbooks" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Goodfellow, Bengio, and Courville (GBC), Deep Learning <a class="reference external" href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a></p></li>
<li><p>Sebastian Raschka, Yuxi Lie, and Vahid Mirjalili (RLM),  Machine Learning with PyTorch and Scikit-Learn at <a class="reference external" href="https://www.packtpub.com/product/machine-learning-with-pytorch-and-scikit-learn/9781801819312">https://www.packtpub.com/product/machine-learning-with-pytorch-and-scikit-learn/9781801819312</a>, see also <a class="reference external" href="https://sebastianraschka.com/blog/2022/ml-pytorch-book.html">https://sebastianraschka.com/blog/2022/ml-pytorch-book.html</a></p></li>
</ul>
<p>The weekly reading suggestions are all from these two texts. The text by GBC can be accessed chapter by chapter from the abovementioned URL.
Each chapter of RLM gives access to the pertinent notebooks. These notebooks are highly recommended.</p>
</section>
<section id="other-popular-texts">
<h2>Other popular texts<a class="headerlink" href="#other-popular-texts" title="Link to this heading">#</a></h2>
<p><strong>Other texts.</strong></p>
<ul class="simple">
<li><p>Christopher M. Bishop (CB), Pattern Recognition and Machine Learning</p></li>
<li><p><a class="reference external" href="https://www.springer.com/gp/book/9780387848570">Hastie, Tibshirani, and Friedman (HTF), The Elements of Statistical Learning, Springer</a>.</p></li>
<li><p><a class="reference external" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/">Aurelien Geron (AG), Hands‑On Machine Learning with Scikit‑Learn and TensorFlow, O’Reilly</a>. This text is very useful since it contains many code examples and hands-on applications of all algorithms discussed in this course.</p></li>
<li><p><a class="reference external" href="https://probml.github.io/pml-book/book1.html">Kevin Murphy (KM), Probabilistic Machine Learning, an Introduction</a></p></li>
<li><p>David Foster (DF), Generative Deep Learning, <a class="reference external" href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/">https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/</a></p></li>
<li><p>Babcock and Gavras (BG), Generative AI with Python and TensorFlow, <a class="github reference external" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2">PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2</a></p></li>
</ul>
</section>
<section id="reading-suggestions-week-34">
<h2>Reading suggestions week 34<a class="headerlink" href="#reading-suggestions-week-34" title="Link to this heading">#</a></h2>
<p>This week: Refresh linear algebra, GBC chapter 2. Install scikit-learn. See lecture notes for week 34 at <a class="reference external" href="https://compphysics.github.io/MachineLearning/doc/web/course.html">https://compphysics.github.io/MachineLearning/doc/web/course.html</a> (these notes).</p>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">#</a></h2>
<p>Basic knowledge in programming and mathematics, with an emphasis on
linear algebra. Knowledge of Python or/and C++ as programming
languages is strongly recommended and experience with Jupiter notebook
is recommended. Required courses are the equivalents to the University
of Oslo mathematics courses MAT1100, MAT1110, MAT1120 and at least one
of the corresponding computing and programming courses INF1000/INF1110
or MAT-INF1100/MAT-INF1100L/BIOS1100/KJM-INF1100. Most universities
offer nowadays a basic programming course (often compulsory) where
Python is the recurring programming language.</p>
</section>
<section id="topics-covered-in-this-course-statistical-analysis-and-optimization-of-data">
<h2>Topics covered in this course: Statistical analysis and optimization of data<a class="headerlink" href="#topics-covered-in-this-course-statistical-analysis-and-optimization-of-data" title="Link to this heading">#</a></h2>
<p>The course has two central parts</p>
<ol class="arabic simple">
<li><p>Statistical analysis and optimization of data</p></li>
<li><p>Machine learning</p></li>
</ol>
<p>These topics will be scattered thorughout the course and may not  necessarily be taught separately. Rather, we will often take an approach (during the lectures and project/exercise sessions) where say elements from statistical data analysis are mixed with specific Machine Learning algorithms.</p>
</section>
<section id="statistical-analysis-and-optimization-of-data">
<h2>Statistical analysis and optimization of data<a class="headerlink" href="#statistical-analysis-and-optimization-of-data" title="Link to this heading">#</a></h2>
<p>We plan to cover the following topics:</p>
<ul class="simple">
<li><p>Basic concepts, expectation values, variance, covariance, correlation functions and errors;</p></li>
<li><p>Simpler models, binomial distribution, the Poisson distribution, simple and multivariate normal distributions;</p></li>
<li><p>Central elements of Bayesian statistics and modeling;</p></li>
<li><p>Gradient methods for data optimization;</p></li>
<li><p>Monte Carlo methods, Markov chains, Gibbs sampling and Metropolis-Hastings sampling (tentative);</p></li>
<li><p>Estimation of errors and resampling techniques such as the cross-validation, blocking, bootstrapping and jackknife methods;</p></li>
<li><p>Principal Component Analysis (PCA) and its mathematical foundation;</p></li>
</ul>
</section>
<section id="machine-learning">
<h2>Machine Learning<a class="headerlink" href="#machine-learning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Pre deep-learning revolution (2008 approx)</p>
<ul>
<li><p>Linear Regression and Logistic Regression, classification and regression problems;</p></li>
<li><p>Bayesian linear and logistic regression, kernel regression;</p></li>
<li><p>Decisions trees, Random Forests, Bagging and Boosting methods;</p></li>
<li><p>Support vector machines (only survey);</p></li>
<li><p>Unsupervised learning and  dimensionality reduction, from PCA to clustering;</p></li>
</ul>
</li>
</ul>
</section>
<section id="deep-learning-methods">
<h2>Deep learning methods<a class="headerlink" href="#deep-learning-methods" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Deep learning</p>
<ul>
<li><p>Neural networks and deep learning;</p></li>
<li><p>Convolutional neural networks;</p></li>
<li><p>Recurrent neural networks;</p></li>
<li><p>Autoencoders</p></li>
<li><p>Generative methods with an emphasis on Boltzmann Machines, Variational Autoencoders and Generalized Adversarial Networks(covered by FYS5429);</p></li>
</ul>
</li>
</ul>
<p>Hands-on demonstrations, exercises and projects aim at deepening your understanding of these topics.</p>
</section>
<section id="extremely-useful-tools-strongly-recommended">
<h2>Extremely useful tools, strongly recommended<a class="headerlink" href="#extremely-useful-tools-strongly-recommended" title="Link to this heading">#</a></h2>
<p><strong>and discussed at the lab sessions.</strong></p>
<ul class="simple">
<li><p>GIT for version control, and GitHub or GitLab as repositories, highly recommended. This will be discussed during the first exercise session</p></li>
<li><p>Anaconda and other Python environments, see intro slides and links to programming resources at <a class="reference external" href="https://computationalscienceuio.github.io/RefreshProgrammingSkills/intro.html">https://computationalscienceuio.github.io/RefreshProgrammingSkills/intro.html</a></p></li>
</ul>
</section>
<section id="other-courses-on-data-science-and-machine-learning-at-uio">
<h2>Other courses on Data science and Machine Learning  at UiO<a class="headerlink" href="#other-courses-on-data-science-and-machine-learning-at-uio" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/fys/FYS5419/index-eng.html">FYS5419 Quantum Computing and Quantum Machine Learning</a></p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/fys/FYS5429/index-eng.html">FYS5429 Advanced Machine Learning for the Physical Sciences</a></p></li>
<li><p><a class="reference external" href="http://www.uio.no/studier/emner/matnat/math/STK2100/index-eng.html">STK2100 Machine learning and statistical methods for prediction and classification</a>.</p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/ifi/IN3050/index-eng.html">IN3050/4050 Introduction to Artificial Intelligence and Machine Learning</a>. Introductory course in machine learning and AI with an algorithmic approach.</p></li>
<li><p><a class="reference external" href="http://www.uio.no/studier/emner/matnat/math/STK-INF3000/index-eng.html">STK-INF3000/4000 Selected Topics in Data Science</a>. The course provides insight into selected contemporary relevant topics within Data Science.</p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/ifi/IN4080/index.html">IN4080 Natural Language Processing</a>. Probabilistic and machine learning techniques applied to natural language processing.</p></li>
</ul>
</section>
<section id="other-courses-on-data-science-and-machine-learning-at-uio-contn">
<h2>Other courses on Data science and Machine Learning  at UiO, contn<a class="headerlink" href="#other-courses-on-data-science-and-machine-learning-at-uio-contn" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/math/STK-IN4300/index-eng.html">STK-IN4300 Statistical learning methods in Data Science</a>. An advanced introduction to statistical and machine learning. For students with a good mathematics and statistics background.</p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/ifi/IN4310/index.html">IN3310/4310 Deep Learnig for Image Analysis</a></p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/math/STK4051/index-eng.html">STK4051 Computational Statistics</a></p></li>
<li><p><a class="reference external" href="https://www.uio.no/studier/emner/matnat/math/STK4021/index-eng.html">STK4021 Applied Bayesian Analysis and Numerical Methods</a></p></li>
</ul>
</section>
<section id="learning-outcomes">
<h2>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Learn about basic data analysis, statistical analysis, Bayesian statistics, Monte Carlo sampling, data optimization and machine learning;</p></li>
<li><p>Be capable of extending the acquired knowledge to other systems and cases;</p></li>
<li><p>Have an understanding of central algorithms used in data analysis and machine learning;</p></li>
<li><p>Understand linear methods for regression and classification, from ordinary least squares, via Lasso and Ridge to Logistic regression;</p></li>
<li><p>Learn about neural networks and deep  learning methods for supervised and unsupervised learning. Emphasis on feed forward neural networks, convolutional and recurrent neural networks;</p></li>
<li><p>Learn about about decision trees, random forests, bagging and boosting methods;</p></li>
<li><p>Learn about support vector machines and kernel transformations;</p></li>
<li><p>Reduction of data sets, from PCA to clustering;</p></li>
<li><p>Generative models</p></li>
<li><p>Work on numerical projects to illustrate the theory. The projects play a central role and you are expected to know modern programming languages like Python or C++ and/or Fortran (Fortran2003 or later) or Julia or other.</p></li>
</ul>
</section>
<section id="types-of-machine-learning">
<h2>Types of Machine Learning<a class="headerlink" href="#types-of-machine-learning" title="Link to this heading">#</a></h2>
<p>The approaches to machine learning are many, but are often split into
two main categories.  In <em>supervised learning</em> we know the answer to a
problem, and let the computer deduce the logic behind it. On the other
hand, <em>unsupervised learning</em> is a method for finding patterns and
relationship in data sets without any prior knowledge of the system.
Some authours also operate with a third category, namely
<em>reinforcement learning</em>. This is a paradigm of learning inspired by
behavioral psychology, where learning is achieved by trial-and-error,
solely from rewards and punishment.</p>
<p>Another way to categorize machine learning tasks is to consider the
desired output of a system.  Some of the most common tasks are:</p>
<ul class="simple">
<li><p>Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.</p></li>
<li><p>Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.</p></li>
<li><p>Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.</p></li>
</ul>
</section>
<section id="essential-elements-of-ml">
<h2>Essential elements of ML<a class="headerlink" href="#essential-elements-of-ml" title="Link to this heading">#</a></h2>
<p>The methods we cover have three main topics in common, irrespective of
whether we deal with supervised or unsupervised learning.</p>
<ul class="simple">
<li><p>The first ingredient is normally our data set (which can be subdivided into training, validation  and test data). Many find the most difficult part of using Machine Learning to be the set up of your data in a meaningful way.</p></li>
<li><p>The second item is a model which is normally a function of some parameters.  The model reflects our knowledge of the system (or lack thereof). As an example, if we know that our data show a behavior similar to what would be predicted by a polynomial, fitting our data to a polynomial of some degree would then determin our model.</p></li>
<li><p>The last ingredient is a so-called <strong>cost/loss</strong> function (or error or risk function) which allows us to present an estimate on how good our model is in reproducing the data it is supposed to train.</p></li>
</ul>
</section>
<section id="an-optimization-minimization-problem">
<h2>An optimization/minimization problem<a class="headerlink" href="#an-optimization-minimization-problem" title="Link to this heading">#</a></h2>
<p>At the heart of basically all Machine Learning algorithms we will encounter so-called minimization or optimization algorithms. A large family of such methods are so-called <strong>gradient methods</strong>.</p>
</section>
<section id="the-plethora-of-machine-learning-algorithms-methods">
<h2>The plethora  of machine learning algorithms/methods<a class="headerlink" href="#the-plethora-of-machine-learning-algorithms-methods" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Deep learning: Neural Networks (NNs), Convolutional NNs, Recurrent NNs, Transformers, Boltzmann machines, autoencoders and variational autoencoders  and generative adversarial networks and other generative models</p></li>
<li><p>Bayesian statistics and Bayesian Machine Learning, Bayesian experimental design, Bayesian Regression models, Bayesian neural networks, Gaussian processes and much more</p></li>
<li><p>Dimensionality reduction (Principal component analysis), Clustering Methods and more</p></li>
<li><p>Ensemble Methods, Random forests, bagging and voting methods, gradient boosting approaches</p></li>
<li><p>Linear and logistic regression, Kernel methods, support vector machines and more</p></li>
<li><p>Reinforcement Learning; Transfer Learning and more</p></li>
</ol>
</section>
<section id="what-is-generative-modeling">
<h2>What Is Generative Modeling?<a class="headerlink" href="#what-is-generative-modeling" title="Link to this heading">#</a></h2>
<p>Generative modeling can be broadly defined as follows:</p>
<p>Generative modeling is a branch of machine learning that involves
training a model to produce new data that is similar to a given
dataset.</p>
<p>What does this mean in practice? Suppose we have a dataset containing
photos of horses. We can train a generative model on this dataset to
capture the rules that govern the complex relationships between pixels
in images of horses. Then we can sample from this model to create
novel, realistic images of horses that did not exist in the original
dataset.</p>
</section>
<section id="example-of-generative-modeling-taken-from-generative-deep-learning-by-david-foster">
<h2>Example of generative modeling, <a class="reference external" href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html">taken from Generative Deep Learning by David Foster</a><a class="headerlink" href="#example-of-generative-modeling-taken-from-generative-deep-learning-by-david-foster" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figures/generativelearning.png, width=900 frac=1.0] -->
<!-- begin figure -->
<p><img src="figures/generativelearning.png" width="900"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="generative-versus-discriminative-modeling">
<h2>Generative Versus Discriminative Modeling<a class="headerlink" href="#generative-versus-discriminative-modeling" title="Link to this heading">#</a></h2>
<p>In order to truly understand what generative modeling aims to achieve
and why this is important, it is useful to compare it to its
counterpart, discriminative modeling. If you have studied machine
learning, most problems you will have faced will have most likely been
discriminative in nature.</p>
</section>
<section id="example-of-discriminative-modeling-taken-from-generative-deep-learning-by-david-foster">
<h2>Example of discriminative modeling, <a class="reference external" href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html">taken from Generative Deep Learning by David Foster</a><a class="headerlink" href="#example-of-discriminative-modeling-taken-from-generative-deep-learning-by-david-foster" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figures/standarddeeplearning.png, width=900 frac=1.0] -->
<!-- begin figure -->
<p><img src="figures/standarddeeplearning.png" width="900"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure --></section>
<section id="discriminative-modeling">
<h2>Discriminative Modeling<a class="headerlink" href="#discriminative-modeling" title="Link to this heading">#</a></h2>
<p>When performing discriminative modeling, each observation in the
training data has a label. For a binary classification problem such as
our data could be labeled as ones and zeros. Our model then learns how to
discriminate between these two groups and outputs the probability that
a new observation has label 1 or 0</p>
<p>In contrast, generative modeling doesn’t require the dataset to be
labeled because it concerns itself with generating entirely new
data (for example an image), rather than trying to predict a label for say  a given image.</p>
</section>
<section id="a-frequentist-approach-to-data-analysis">
<h2>A Frequentist approach to data analysis<a class="headerlink" href="#a-frequentist-approach-to-data-analysis" title="Link to this heading">#</a></h2>
<p>When you hear phrases like <strong>predictions and estimations</strong> and
<strong>correlations and causations</strong>, what do you think of?  May be you think
of the difference between classifying new data points and generating
new data points.
Or perhaps you consider that correlations represent some kind of symmetric statements like
if <span class="math notranslate nohighlight">\(A\)</span> is correlated with <span class="math notranslate nohighlight">\(B\)</span>, then <span class="math notranslate nohighlight">\(B\)</span> is correlated with
<span class="math notranslate nohighlight">\(A\)</span>. Causation on the other hand is directional, that is if <span class="math notranslate nohighlight">\(A\)</span> causes <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(B\)</span> does not
necessarily cause <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>These concepts are in some sense the difference between machine
learning and statistics. In machine learning and prediction based
tasks, we are often interested in developing algorithms that are
capable of learning patterns from given data in an automated fashion,
and then using these learned patterns to make predictions or
assessments of newly given data. In many cases, our primary concern
is the quality of the predictions or assessments, and we are less
concerned about the underlying patterns that were learned in order
to make these predictions.</p>
<p>In machine learning we normally use <a class="reference external" href="https://en.wikipedia.org/wiki/Frequentist_inference">a so-called frequentist approach</a>,
where the aim is to make predictions and find correlations. We focus
less on for example extracting a probability distribution function (PDF). The PDF can be
used in turn to make estimations and find causations such as given <span class="math notranslate nohighlight">\(A\)</span>
what is the likelihood of finding <span class="math notranslate nohighlight">\(B\)</span>.</p>
</section>
<section id="what-is-a-good-model">
<h2>What is a good model?<a class="headerlink" href="#what-is-a-good-model" title="Link to this heading">#</a></h2>
<p>In science and engineering we often end up in situations where we want to infer (or learn) a
quantitative model <span class="math notranslate nohighlight">\(M\)</span> for a given set of sample points <span class="math notranslate nohighlight">\(\boldsymbol{X} \in [x_1, x_2,\dots x_N]\)</span>.</p>
<p>As we will see repeatedely in these lectures, we could try to fit these data points to a model given by a
straight line, or if we wish to be more sophisticated to a more complex
function.</p>
<p>The reason for inferring such a model is that it
serves many useful purposes. On the one hand, the model can reveal information
encoded in the data or underlying mechanisms from which the data were generated. For instance, we could discover important
corelations that relate interesting physics interpretations.</p>
<p>In addition, it can simplify the representation of the given data set and help
us in making predictions about  future data samples.</p>
<p>A first important consideration to keep in mind is that inferring the <em>correct</em> model
for a given data set is an elusive, if not impossible, task. The fundamental difficulty
is that if we are not specific about what we mean by a <em>correct</em> model, there
could easily be many different models that fit the given data set <em>equally well</em>.</p>
</section>
<section id="what-is-a-good-model-can-we-define-it">
<h2>What is a good model? Can we define it?<a class="headerlink" href="#what-is-a-good-model-can-we-define-it" title="Link to this heading">#</a></h2>
<p>The central question is this: what leads us to say that a model is correct or
optimal for a given data set? To make the model inference problem well posed, i.e.,
to guarantee that there is a unique optimal model for the given data, we need to
impose additional assumptions or restrictions on the class of models considered. To
this end, we should not be looking for just any model that can describe the data.
Instead, we should look for a <strong>model</strong> <span class="math notranslate nohighlight">\(M\)</span> that is the best among a restricted class
of models. In addition, to make the model inference problem computationally
tractable, we need to specify how restricted the class of models needs to be. A
common strategy is to start
with the simplest possible class of models that is just necessary to describe the data
or solve the problem at hand. More precisely, the model class should be rich enough
to contain at least one model that can fit the data to a desired accuracy and yet be
restricted enough that it is relatively simple to find the best model for the given data.</p>
<p>Thus, the most popular strategy is to start from the
simplest class of models and increase the complexity of the models only when the
simpler models become inadequate. For instance, if we work with a regression problem to fit a set of sample points, one
may first try the simplest class of models, namely linear models, followed obviously by more complex models.</p>
<p>How to evaluate which model fits best the data is something we will come back to over and over again in these sets of lectures.</p>
</section>
<section id="software-and-needed-installations">
<h2>Software and needed installations<a class="headerlink" href="#software-and-needed-installations" title="Link to this heading">#</a></h2>
<p>We will make extensive use of Python as programming language and its
myriad of available libraries.  You will find
Jupyter notebooks invaluable in your work.  You can run <strong>R</strong>
codes in the Jupyter/IPython notebooks, with the immediate benefit of
visualizing your data. You can also use compiled languages like C++,
Rust, Julia, Fortran etc if you prefer. The focus in these lectures will be
on Python.</p>
<p>If you have Python installed (we strongly recommend Python3) and you feel
pretty familiar with installing different packages, we recommend that
you install the following Python packages via <strong>pip</strong> as</p>
<ol class="arabic simple">
<li><p>pip install numpy scipy matplotlib ipython scikit-learn mglearn sympy pandas pillow</p></li>
</ol>
<p>For Python3, replace <strong>pip</strong> with <strong>pip3</strong>.</p>
<p>For OSX users we recommend, after having installed Xcode, to
install <strong>brew</strong>. Brew allows for a seamless installation of additional
software via for example</p>
<ol class="arabic simple">
<li><p>brew install python3</p></li>
</ol>
<p>For Linux users, with its variety of distributions like for example the widely popular Ubuntu distribution,
you can use <strong>pip</strong> as well and simply install Python as</p>
<ol class="arabic simple">
<li><p>sudo apt-get install python3  (or python for pyhton2.7)</p></li>
</ol>
<p>etc etc.</p>
</section>
<section id="python-installers">
<h2>Python installers<a class="headerlink" href="#python-installers" title="Link to this heading">#</a></h2>
<p>If you don’t want to perform these operations separately and venture
into the hassle of exploring how to set up dependencies and paths, we
recommend two widely used distrubutions which set up all relevant
dependencies for Python, namely</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.anaconda.com/">Anaconda</a>,</p></li>
</ul>
<p>which is an open source
distribution of the Python and R programming languages for large-scale
data processing, predictive analytics, and scientific computing, that
aims to simplify package management and deployment. Package versions
are managed by the package management system <strong>conda</strong>.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.enthought.com/product/canopy/">Enthought canopy</a></p></li>
</ul>
<p>is a Python
distribution for scientific and analytic computing distribution and
analysis environment, available for free and under a commercial
license.</p>
<p>Furthermore, <a class="reference external" href="https://colab.research.google.com/notebooks/welcome.ipynb">Google’s Colab</a> is a free Jupyter notebook environment that requires
no setup and runs entirely in the cloud. Try it out!</p>
</section>
<section id="useful-python-libraries">
<h2>Useful Python libraries<a class="headerlink" href="#useful-python-libraries" title="Link to this heading">#</a></h2>
<p>Here we list several useful Python libraries we strongly recommend (if you use anaconda many of these are already there)</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.numpy.org/">NumPy</a> is a highly popular library for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays</p></li>
<li><p><a class="reference external" href="https://pandas.pydata.org/">The pandas</a> library provides high-performance, easy-to-use data structures and data analysis tools</p></li>
<li><p><a class="reference external" href="http://xarray.pydata.org/en/stable/">Xarray</a> is a Python package that makes working with labelled multi-dimensional arrays simple, efficient, and fun!</p></li>
<li><p><a class="reference external" href="https://www.scipy.org/">Scipy</a> (pronounced “Sigh Pie”) is a Python-based ecosystem of open-source software for mathematics, science, and engineering.</p></li>
<li><p><a class="reference external" href="https://matplotlib.org/">Matplotlib</a> is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.</p></li>
<li><p><a class="reference external" href="https://github.com/HIPS/autograd">Autograd</a> can automatically differentiate native Python and Numpy code. It can handle a large subset of Python’s features, including loops, ifs, recursion and closures, and it can even take derivatives of derivatives of derivatives</p></li>
<li><p><a class="reference external" href="https://jax.readthedocs.io/en/latest/index.html">JAX</a> has now more or less replaced <strong>Autograd</strong>. JAX is Autograd and XLA, brought together for high-performance numerical computing and machine learning research. It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more.</p></li>
<li><p><a class="reference external" href="https://www.sympy.org/en/index.html">SymPy</a> is a Python library for symbolic mathematics.</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a> has simple and efficient tools for machine learning, data mining and data analysis</p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a> is a Python library for fast numerical computing created and released by Google</p></li>
<li><p><a class="reference external" href="https://keras.io/">Keras</a> is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano</p></li>
<li><p><a class="reference external" href="https://pytorch.org/">Pytorch</a>, highly recommened</p></li>
<li><p><a class="reference external" href="https://pypi.org/project/Theano/">Theano</a> and many other</p></li>
</ul>
</section>
<section id="installing-r-c-cython-or-julia">
<h2>Installing R, C++, cython or Julia<a class="headerlink" href="#installing-r-c-cython-or-julia" title="Link to this heading">#</a></h2>
<p>You will also find it convenient to utilize <strong>R</strong>. We will mainly
use Python during our lectures and in various projects and exercises.
Those of you
already familiar with <strong>R</strong> should feel free to continue using <strong>R</strong>, keeping
however an eye on the parallel Python set ups. Similarly, if you are a
Python afecionado, feel free to explore <strong>R</strong> as well.  Jupyter(Julia, Python and R) /Ipython
notebook allows you to run <strong>R</strong> codes and <strong>Julia</strong> codes interactively in your
browser. The software library <strong>R</strong> is really tailored  for statistical data analysis
and allows for an easy usage of the tools and algorithms we will discuss in these
lectures.</p>
<p>To install <strong>R</strong> with Jupyter notebook
<a class="reference external" href="https://mpacer.org/maths/r-kernel-for-ipython-notebook">follow the link here</a></p>
</section>
<section id="installing-r-c-cython-numba-etc">
<h2>Installing R, C++, cython, Numba etc<a class="headerlink" href="#installing-r-c-cython-numba-etc" title="Link to this heading">#</a></h2>
<p>For the C++ aficionados, Jupyter/IPython notebook allows you also to
install C++ and run codes written in this language interactively in
the browser. Since we will emphasize writing many of the algorithms
yourself, you can thus opt for either Python or C++ (or Fortran or other compiled languages) as programming
languages.</p>
<p>To add more entropy, <strong>cython</strong> can also be used when running your
notebooks. It means that Python with the jupyter notebook
setup allows you to integrate widely popular softwares and tools for
scientific computing. Similarly, the
<a class="reference external" href="https://numba.pydata.org/">Numba Python package</a> delivers increased performance
capabilities with minimal rewrites of your codes.  With its
versatility, including symbolic operations, Python offers a unique
computational environment. Your jupyter notebook can easily be
converted into a nicely rendered <strong>PDF</strong> file or a Latex file for
further processing. For example, convert to latex as</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    pycod jupyter nbconvert filename.ipynb --to latex 
</pre></div>
</div>
<p>And to add more versatility, the Python package <a class="reference external" href="http://www.sympy.org/en/index.html">SymPy</a> is a Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS)  and is entirely written in Python.</p>
<p>Finally, we recommend strongly using Autograd or JAX for automatic differentiation.</p>
</section>
<section id="numpy-examples-and-important-matrix-and-vector-handling-packages">
<h2>Numpy examples and Important Matrix and vector handling packages<a class="headerlink" href="#numpy-examples-and-important-matrix-and-vector-handling-packages" title="Link to this heading">#</a></h2>
<p>There are several central software libraries for linear algebra and eigenvalue problems. Several of the more
popular ones have been wrapped into ofter software packages like those from the widely used text <strong>Numerical Recipes</strong>. The original source codes in many of the available packages are often taken from the widely used
software package LAPACK, which follows two other popular packages
developed in the 1970s, namely EISPACK and LINPACK.  We describe them shortly here.</p>
<ul class="simple">
<li><p>LINPACK: package for linear equations and least square problems.</p></li>
<li><p>LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK’s website <a class="reference external" href="http://www.netlib.org">http://www.netlib.org</a> it is possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.</p></li>
<li><p>BLAS (I, II and III): (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations. Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations. Highly parallelized and efficient codes, all available for download from <a class="reference external" href="http://www.netlib.org">http://www.netlib.org</a>.</p></li>
</ul>
</section>
<section id="numpy-and-arrays">
<h2>Numpy and arrays<a class="headerlink" href="#numpy-and-arrays" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="http://www.numpy.org/">Numpy</a> provides an easy way to handle arrays in Python. The standard way to import this library is as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
</pre></div>
</div>
</div>
</div>
<p>Here follows a simple example where we set up an array of ten elements, all determined by random numbers drawn according to the normal distribution,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>n = 10
x = np.random.normal(size=n)
print(x)
</pre></div>
</div>
</div>
</div>
<p>We defined a vector <span class="math notranslate nohighlight">\(x\)</span> with <span class="math notranslate nohighlight">\(n=10\)</span> elements with its values given by the Normal distribution <span class="math notranslate nohighlight">\(N(0,1)\)</span>.
Another alternative is to declare a vector as follows</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
x = np.array([1, 2, 3])
print(x)
</pre></div>
</div>
</div>
</div>
<p>Here we have defined a vector with three elements, with <span class="math notranslate nohighlight">\(x_0=1\)</span>, <span class="math notranslate nohighlight">\(x_1=2\)</span> and <span class="math notranslate nohighlight">\(x_2=3\)</span>. Note that both Python and C++
start numbering array elements from <span class="math notranslate nohighlight">\(0\)</span> and on. This means that a vector with <span class="math notranslate nohighlight">\(n\)</span> elements has a sequence of entities <span class="math notranslate nohighlight">\(x_0, x_1, x_2, \dots, x_{n-1}\)</span>. We could also let (recommended) Numpy to compute the logarithms of a specific array as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
x = np.log(np.array([4, 7, 8]))
print(x)
</pre></div>
</div>
</div>
</div>
<p>In the last example we used Numpy’s unary function <span class="math notranslate nohighlight">\(np.log\)</span>. This function is
highly tuned to compute array elements since the code is vectorized
and does not require looping. We normaly recommend that you use the
Numpy intrinsic functions instead of the corresponding <strong>log</strong> function
from Python’s <strong>math</strong> module. The looping is done explicitely by the
<strong>np.log</strong> function. The alternative, and slower way to compute the
logarithms of a vector would be to write</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
from math import log
x = np.array([4, 7, 8])
for i in range(0, len(x)):
    x[i] = log(x[i])
print(x)
</pre></div>
</div>
</div>
</div>
<p>We note that our code is much longer already and we need to import the <strong>log</strong> function from the <strong>math</strong> module.
The attentive reader will also notice that the output is <span class="math notranslate nohighlight">\([1, 1, 2]\)</span>. Python interprets automagically our numbers as integers (like the <strong>automatic</strong> keyword in C++). To change this we could define our array elements to be double precision numbers as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
x = np.log(np.array([4, 7, 8], dtype = np.float64))
print(x)
</pre></div>
</div>
</div>
</div>
<p>or simply write them as double precision numbers (Python uses 64 bits as default for floating point type variables), that is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
x = np.log(np.array([4.0, 7.0, 8.0]))
print(x)
</pre></div>
</div>
</div>
</div>
<p>To check the number of bytes (remember that one byte contains eight bits for double precision variables), you can use simple use the <strong>itemsize</strong> functionality (the array <span class="math notranslate nohighlight">\(x\)</span> is actually an object which inherits the functionalities defined in Numpy) as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
x = np.log(np.array([4.0, 7.0, 8.0]))
print(x.itemsize)
</pre></div>
</div>
</div>
</div>
</section>
<section id="matrices-in-python">
<h2>Matrices in Python<a class="headerlink" href="#matrices-in-python" title="Link to this heading">#</a></h2>
<p>Having defined vectors, we are now ready to try out matrices. We can
define a <span class="math notranslate nohighlight">\(3 \times 3 \)</span> real matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> as (recall that we user
lowercase letters for vectors and uppercase letters for matrices)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
A = np.log(np.array([ [4.0, 7.0, 8.0], [3.0, 10.0, 11.0], [4.0, 5.0, 7.0] ]))
print(A)
</pre></div>
</div>
</div>
</div>
<p>If we use the <strong>shape</strong> function we would get <span class="math notranslate nohighlight">\((3, 3)\)</span> as output, that is verifying that our matrix is a <span class="math notranslate nohighlight">\(3\times 3\)</span> matrix. We can slice the matrix and print for example the first column (Python organized matrix elements in a row-major order, see below) as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
A = np.log(np.array([ [4.0, 7.0, 8.0], [3.0, 10.0, 11.0], [4.0, 5.0, 7.0] ]))
# print the first column, row-major order and elements start with 0
print(A[:,0])
</pre></div>
</div>
</div>
</div>
<p>We can continue this was by printing out other columns or rows. The example here prints out the second column</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
A = np.log(np.array([ [4.0, 7.0, 8.0], [3.0, 10.0, 11.0], [4.0, 5.0, 7.0] ]))
# print the first column, row-major order and elements start with 0
print(A[1,:])
</pre></div>
</div>
</div>
</div>
<p>Numpy contains many other functionalities that allow us to slice, subdivide etc etc arrays. We strongly recommend that you look up the <a class="reference external" href="http://www.numpy.org/">Numpy website for more details</a>. Useful functions when defining a matrix are the <strong>np.zeros</strong> function which declares a matrix of a given dimension and sets all elements to zero</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
n = 10
# define a matrix of dimension 10 x 10 and set all elements to zero
A = np.zeros( (n, n) )
print(A)
</pre></div>
</div>
</div>
</div>
<p>or initializing all elements to</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
n = 10
# define a matrix of dimension 10 x 10 and set all elements to one
A = np.ones( (n, n) )
print(A)
</pre></div>
</div>
</div>
</div>
<p>or as unitarily distributed random numbers (see the material on random number generators in the statistics part)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
n = 10
# define a matrix of dimension 10 x 10 and set all elements to random numbers with x \in [0, 1]
A = np.random.rand(n, n)
print(A)
</pre></div>
</div>
</div>
</div>
<p>As we will see throughout these lectures, there are several extremely useful functionalities in Numpy.
As an example, consider the discussion of the covariance matrix. Suppose we have defined three vectors
<span class="math notranslate nohighlight">\(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{z}\)</span> with <span class="math notranslate nohighlight">\(n\)</span> elements each. The covariance matrix is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\Sigma} = \begin{bmatrix} \sigma_{xx} &amp; \sigma_{xy} &amp; \sigma_{xz} \\
                              \sigma_{yx} &amp; \sigma_{yy} &amp; \sigma_{yz} \\
                              \sigma_{zx} &amp; \sigma_{zy} &amp; \sigma_{zz} 
             \end{bmatrix},
\end{split}\]</div>
<p>where for example</p>
<div class="math notranslate nohighlight">
\[
\sigma_{xy} =\frac{1}{n} \sum_{i=0}^{n-1}(x_i- \overline{x})(y_i- \overline{y}).
\]</div>
<p>The Numpy function <strong>np.cov</strong> calculates the covariance elements using the factor <span class="math notranslate nohighlight">\(1/(n-1)\)</span> instead of <span class="math notranslate nohighlight">\(1/n\)</span> since it assumes we do not have the exact mean values.
The following simple function uses the <strong>np.vstack</strong> function which takes each vector of dimension <span class="math notranslate nohighlight">\(1\times n\)</span> and produces a <span class="math notranslate nohighlight">\(3\times n\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{W} = \begin{bmatrix} x_0 &amp; x_1 &amp; x_2 &amp; \dots &amp; x_{n-2} &amp; x_{n-1} \\
                         y_0 &amp; y_1 &amp; y_2 &amp; \dots &amp; y_{n-2} &amp; y_{n-1} \\
			 z_0 &amp; z_1 &amp; z_2 &amp; \dots &amp; z_{n-2} &amp; z_{n-1} \\
             \end{bmatrix},
\end{split}\]</div>
<p>which in turn is converted into into the <span class="math notranslate nohighlight">\(3\times 3\)</span> covariance matrix
<span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> via the Numpy function <strong>np.cov()</strong>. We note that we can also calculate
the mean value of each set of samples <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> etc using the Numpy
function <strong>np.mean(x)</strong>. We can also extract the eigenvalues of the
covariance matrix through the <strong>np.linalg.eig()</strong> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Importing various packages
import numpy as np

n = 100
x = np.random.normal(size=n)
print(np.mean(x))
y = 4+3*x+np.random.normal(size=n)
print(np.mean(y))
z = x**3+np.random.normal(size=n)
print(np.mean(z))
W = np.vstack((x, y, z))
Sigma = np.cov(W)
print(Sigma)
Eigvals, Eigvecs = np.linalg.eig(Sigma)
print(Eigvals)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>%matplotlib inline

import numpy as np
import matplotlib.pyplot as plt
from scipy import sparse
eye = np.eye(4)
print(eye)
sparse_mtx = sparse.csr_matrix(eye)
print(sparse_mtx)
x = np.linspace(-10,10,100)
y = np.sin(x)
plt.plot(x,y,marker=&#39;x&#39;)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="meet-the-pandas">
<h2>Meet the Pandas<a class="headerlink" href="#meet-the-pandas" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [fig/pandas.jpg, width=600 frac=0.8] -->
<!-- begin figure -->
<p><img src="fig/pandas.jpg" width="600"><p style="font-size: 0.9em"><i>Figure 1: </i></p></p>
<!-- end figure -->
<p>Another useful Python package is
<a class="reference external" href="https://pandas.pydata.org/">pandas</a>, which is an open source library
providing high-performance, easy-to-use data structures and data
analysis tools for Python. <strong>pandas</strong> stands for panel data, a term borrowed from econometrics and is an efficient library for data analysis with an emphasis on tabular data.
<strong>pandas</strong> has two major classes, the <strong>DataFrame</strong> class with two-dimensional data objects and tabular data organized in columns and the class <strong>Series</strong> with a focus on one-dimensional data objects. Both classes allow you to index data easily as we will see in the examples below.
<strong>pandas</strong> allows you also to perform mathematical operations on the data, spanning from simple reshapings of vectors and matrices to statistical operations.</p>
<p>The following simple example shows how we can, in an easy way make tables of our data. Here we define a data set which includes names, place of birth and date of birth, and displays the data in an easy to read way. We will see repeated use of <strong>pandas</strong>, in particular in connection with classification of data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import pandas as pd
from IPython.display import display
data = {&#39;First Name&#39;: [&quot;Frodo&quot;, &quot;Bilbo&quot;, &quot;Aragorn II&quot;, &quot;Samwise&quot;],
        &#39;Last Name&#39;: [&quot;Baggins&quot;, &quot;Baggins&quot;,&quot;Elessar&quot;,&quot;Gamgee&quot;],
        &#39;Place of birth&#39;: [&quot;Shire&quot;, &quot;Shire&quot;, &quot;Eriador&quot;, &quot;Shire&quot;],
        &#39;Date of Birth T.A.&#39;: [2968, 2890, 2931, 2980]
        }
data_pandas = pd.DataFrame(data)
display(data_pandas)
</pre></div>
</div>
</div>
</div>
<p>In the above we have imported <strong>pandas</strong> with the shorthand <strong>pd</strong>, the latter has become the standard way we import <strong>pandas</strong>. We make then a list of various variables
and reorganize the aboves lists into a <strong>DataFrame</strong> and then print out  a neat table with specific column labels as <em>Name</em>, <em>place of birth</em> and <em>date of birth</em>.
Displaying these results, we see that the indices are given by the default numbers from zero to three.
<strong>pandas</strong> is extremely flexible and we can easily change the above indices by defining a new type of indexing as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>data_pandas = pd.DataFrame(data,index=[&#39;Frodo&#39;,&#39;Bilbo&#39;,&#39;Aragorn&#39;,&#39;Sam&#39;])
display(data_pandas)
</pre></div>
</div>
</div>
</div>
<p>Thereafter we display the content of the row which begins with the index <strong>Aragorn</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>display(data_pandas.loc[&#39;Aragorn&#39;])
</pre></div>
</div>
</div>
</div>
<p>We can easily append data to this, for example</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>new_hobbit = {&#39;First Name&#39;: [&quot;Peregrin&quot;],
              &#39;Last Name&#39;: [&quot;Took&quot;],
              &#39;Place of birth&#39;: [&quot;Shire&quot;],
              &#39;Date of Birth T.A.&#39;: [2990]
              }
data_pandas=data_pandas.append(pd.DataFrame(new_hobbit, index=[&#39;Pippin&#39;]))
display(data_pandas)
</pre></div>
</div>
</div>
</div>
<p>Here are other examples where we use the <strong>DataFrame</strong> functionality to handle arrays, now with more interesting features for us, namely numbers. We set up a matrix
of dimensionality <span class="math notranslate nohighlight">\(10\times 5\)</span> and compute the mean value and standard deviation of each column. Similarly, we can perform mathematial operations like squaring the matrix elements and many other operations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
import pandas as pd
from IPython.display import display
np.random.seed(100)
# setting up a 10 x 5 matrix
rows = 10
cols = 5
a = np.random.randn(rows,cols)
df = pd.DataFrame(a)
display(df)
print(df.mean())
print(df.std())
display(df**2)
</pre></div>
</div>
</div>
</div>
<p>Thereafter we can select specific columns only and plot final results</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>df.columns = [&#39;First&#39;, &#39;Second&#39;, &#39;Third&#39;, &#39;Fourth&#39;, &#39;Fifth&#39;]
df.index = np.arange(10)

display(df)
print(df[&#39;Second&#39;].mean() )
print(df.info())
print(df.describe())

from pylab import plt, mpl
plt.style.use(&#39;seaborn&#39;)
mpl.rcParams[&#39;font.family&#39;] = &#39;serif&#39;

df.cumsum().plot(lw=2.0, figsize=(10,6))
plt.show()


df.plot.bar(figsize=(10,6), rot=15)
plt.show()
</pre></div>
</div>
</div>
</div>
<p>We can produce a <span class="math notranslate nohighlight">\(4\times 4\)</span> matrix</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>b = np.arange(16).reshape((4,4))
print(b)
df1 = pd.DataFrame(b)
print(df1)
</pre></div>
</div>
</div>
</div>
<p>and many other operations.</p>
<p>The <strong>Series</strong> class is another important class included in
<strong>pandas</strong>. You can view it as a specialization of <strong>DataFrame</strong> but where
we have just a single column of data. It shares many of the same features as <strong>DataFrame</strong>. As with <strong>DataFrame</strong>,
most operations are vectorized, achieving thereby a high performance when dealing with computations of arrays, in particular labeled arrays.
As we will see below it leads also to a very concice code close to the mathematical operations we may be interested in.
For multidimensional arrays, we recommend strongly <a class="reference external" href="http://xarray.pydata.org/en/stable/">xarray</a>. <strong>xarray</strong> has much of the same flexibility as <strong>pandas</strong>, but allows for the extension to higher dimensions than two. We will see examples later of the usage of both <strong>pandas</strong> and <strong>xarray</strong>.</p>
</section>
<section id="pandas-ai">
<h2>Pandas AI<a class="headerlink" href="#pandas-ai" title="Link to this heading">#</a></h2>
<p>Try out <a class="reference external" href="https://pandas-ai.com/">Pandas AI</a></p>
<section id="simple-linear-regression-model-using-scikit-learn">
<h3>Simple linear regression model using <strong>scikit-learn</strong><a class="headerlink" href="#simple-linear-regression-model-using-scikit-learn" title="Link to this heading">#</a></h3>
<p>We start with perhaps our simplest possible example, using <strong>Scikit-Learn</strong> to perform linear regression analysis on a data set produced by us.</p>
<p>What follows is a simple Python code where we have defined a function
<span class="math notranslate nohighlight">\(y\)</span> in terms of the variable <span class="math notranslate nohighlight">\(x\)</span>. Both are defined as vectors with  <span class="math notranslate nohighlight">\(100\)</span> entries.
The numbers in the vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> are given
by random numbers generated with a uniform distribution with entries
<span class="math notranslate nohighlight">\(x_i \in [0,1]\)</span> (more about probability distribution functions
later). These values are then used to define a function <span class="math notranslate nohighlight">\(y(x)\)</span>
(tabulated again as a vector) with a linear dependence on <span class="math notranslate nohighlight">\(x\)</span> plus a
random noise added via the normal distribution.</p>
<p>The Numpy functions are imported used the <strong>import numpy as np</strong>
statement and the random number generator for the uniform distribution
is called using the function <strong>np.random.rand()</strong>, where we specificy
that we want <span class="math notranslate nohighlight">\(100\)</span> random variables.  Using Numpy we define
automatically an array with the specified number of elements, <span class="math notranslate nohighlight">\(100\)</span> in
our case.  With the Numpy function <strong>randn()</strong> we can compute random
numbers with the normal distribution (mean value <span class="math notranslate nohighlight">\(\mu\)</span> equal to zero and
variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> set to one) and produce the values of <span class="math notranslate nohighlight">\(y\)</span> assuming a linear
dependence as function of <span class="math notranslate nohighlight">\(x\)</span></p>
<div class="math notranslate nohighlight">
\[
y = 2x+N(0,1),
\]</div>
<p>where <span class="math notranslate nohighlight">\(N(0,1)\)</span> represents random numbers generated by the normal
distribution.  From <strong>Scikit-Learn</strong> we import then the
<strong>LinearRegression</strong> functionality and make a prediction <span class="math notranslate nohighlight">\(\tilde{y} =
\alpha + \beta x\)</span> using the function <strong>fit(x,y)</strong>. We call the set of
data <span class="math notranslate nohighlight">\((\boldsymbol{x},\boldsymbol{y})\)</span> for our training data. The Python package
<strong>scikit-learn</strong> has also a functionality which extracts the above
fitting parameters <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> (see below). Later we will
distinguish between training data and test data.</p>
<p>For plotting we use the Python package
<a class="reference external" href="https://matplotlib.org/">matplotlib</a> which produces publication
quality figures. Feel free to explore the extensive
<a class="reference external" href="https://matplotlib.org/gallery/index.html">gallery</a> of examples. In
this example we plot our original values of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> as well as the
prediction <strong>ypredict</strong> (<span class="math notranslate nohighlight">\(\tilde{y}\)</span>), which attempts at fitting our
data with a straight line.</p>
<p>The Python code follows here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Importing various packages
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

x = np.random.rand(100,1)
y = 2*x+np.random.randn(100,1)
linreg = LinearRegression()
linreg.fit(x,y)
xnew = np.array([[0],[1]])
ypredict = linreg.predict(xnew)

plt.plot(xnew, ypredict, &quot;r-&quot;)
plt.plot(x, y ,&#39;ro&#39;)
plt.axis([0,1.0,0, 5.0])
plt.xlabel(r&#39;$x$&#39;)
plt.ylabel(r&#39;$y$&#39;)
plt.title(r&#39;Simple Linear Regression&#39;)
plt.show()
</pre></div>
</div>
</div>
</div>
<p>This example serves several aims. It allows us to demonstrate several
aspects of data analysis and later machine learning algorithms. The
immediate visualization shows that our linear fit is not
impressive. It goes through the data points, but there are many
outliers which are not reproduced by our linear regression.  We could
now play around with this small program and change for example the
factor in front of <span class="math notranslate nohighlight">\(x\)</span> and the normal distribution.  Try to change the
function <span class="math notranslate nohighlight">\(y\)</span> to</p>
<div class="math notranslate nohighlight">
\[
y = 10x+0.01 \times N(0,1),
\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is defined as before.  Does the fit look better? Indeed, by
reducing the role of the noise given by the normal distribution we see immediately that
our linear prediction seemingly reproduces better the training
set. However, this testing ‘by the eye’ is obviouly not satisfactory in the
long run. Here we have only defined the training data and our model, and
have not discussed a more rigorous approach to the <strong>cost</strong> function.</p>
<p>We need more rigorous criteria in defining whether we have succeeded or
not in modeling our training data.  You will be surprised to see that
many scientists seldomly venture beyond this ‘by the eye’ approach. A
standard approach for the <em>cost</em> function is the so-called <span class="math notranslate nohighlight">\(\chi^2\)</span>
function (a variant of the mean-squared error (MSE))</p>
<div class="math notranslate nohighlight">
\[
\chi^2 = \frac{1}{n}
\sum_{i=0}^{n-1}\frac{(y_i-\tilde{y}_i)^2}{\sigma_i^2},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_i^2\)</span> is the variance (to be defined later) of the entry
<span class="math notranslate nohighlight">\(y_i\)</span>.  We may not know the explicit value of <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>, it serves
however the aim of scaling the equations and make the cost function
dimensionless.</p>
<p>Minimizing the cost function is a central aspect of
our discussions to come. Finding its minima as function of the model
parameters (<span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> in our case) will be a recurring
theme in these series of lectures. Essentially all machine learning
algorithms we will discuss center around the minimization of the
chosen cost function. This depends in turn on our specific
model for describing the data, a typical situation in supervised
learning. Automatizing the search for the minima of the cost function is a
central ingredient in all algorithms. Typical methods which are
employed are various variants of <strong>gradient</strong> methods. These will be
discussed in more detail later. Again, you’ll be surprised to hear that
many practitioners minimize the above function ‘’by the eye’, popularly dubbed as
‘chi by the eye’. That is, change a parameter and see (visually and numerically) that
the  <span class="math notranslate nohighlight">\(\chi^2\)</span> function becomes smaller.</p>
<p>There are many ways to define the cost function. A simpler approach is to look at the relative difference between the training data and the predicted data, that is we define
the relative error (why would we prefer the MSE instead of the relative error?) as</p>
<div class="math notranslate nohighlight">
\[
\epsilon_{\mathrm{relative}}= \frac{\vert \boldsymbol{y} -\boldsymbol{\tilde{y}}\vert}{\vert \boldsymbol{y}\vert}.
\]</div>
<p>The squared cost function results in an arithmetic mean-unbiased
estimator, and the absolute-value cost function results in a
median-unbiased estimator (in the one-dimensional case, and a
geometric median-unbiased estimator for the multi-dimensional
case). The squared cost function has the disadvantage that it has the tendency
to be dominated by outliers.</p>
<p>We can modify easily the above Python code and plot the relative error instead</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

x = np.random.rand(100,1)
y = 5*x+0.01*np.random.randn(100,1)
linreg = LinearRegression()
linreg.fit(x,y)
ypredict = linreg.predict(x)

plt.plot(x, np.abs(ypredict-y)/abs(y), &quot;ro&quot;)
plt.axis([0,1.0,0.0, 0.5])
plt.xlabel(r&#39;$x$&#39;)
plt.ylabel(r&#39;$\epsilon_{\mathrm{relative}}$&#39;)
plt.title(r&#39;Relative error&#39;)
plt.show()
</pre></div>
</div>
</div>
</div>
<p>Depending on the parameter in front of the normal distribution, we may
have a small or larger relative error. Try to play around with
different training data sets and study (graphically) the value of the
relative error.</p>
<p>As mentioned above, <strong>Scikit-Learn</strong> has an impressive functionality.
We can for example extract the values of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> and
their error estimates, or the variance and standard deviation and many
other properties from the statistical data analysis.</p>
<p>Here we show an
example of the functionality of <strong>Scikit-Learn</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.linear_model import LinearRegression 
from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error, mean_absolute_error

x = np.random.rand(100,1)
y = 2.0+ 5*x+0.5*np.random.randn(100,1)
linreg = LinearRegression()
linreg.fit(x,y)
ypredict = linreg.predict(x)
print(&#39;The intercept alpha: \n&#39;, linreg.intercept_)
print(&#39;Coefficient beta : \n&#39;, linreg.coef_)
# The mean squared error                               
print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(y, ypredict))
# Explained variance score: 1 is perfect prediction                                 
print(&#39;Variance score: %.2f&#39; % r2_score(y, ypredict))
# Mean squared log error                                                        
print(&#39;Mean squared log error: %.2f&#39; % mean_squared_log_error(y, ypredict) )
# Mean absolute error                                                           
print(&#39;Mean absolute error: %.2f&#39; % mean_absolute_error(y, ypredict))
plt.plot(x, ypredict, &quot;r-&quot;)
plt.plot(x, y ,&#39;ro&#39;)
plt.axis([0.0,1.0,1.5, 7.0])
plt.xlabel(r&#39;$x$&#39;)
plt.ylabel(r&#39;$y$&#39;)
plt.title(r&#39;Linear Regression fit &#39;)
plt.show()
</pre></div>
</div>
</div>
</div>
<p>The function <strong>coef</strong> gives us the parameter <span class="math notranslate nohighlight">\(\beta\)</span> of our fit while <strong>intercept</strong> yields
<span class="math notranslate nohighlight">\(\alpha\)</span>. Depending on the constant in front of the normal distribution, we get values near or far from <span class="math notranslate nohighlight">\(\alpha =2\)</span> and <span class="math notranslate nohighlight">\(\beta =5\)</span>. Try to play around with different parameters in front of the normal distribution. The function <strong>meansquarederror</strong> gives us the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss defined as</p>
<div class="math notranslate nohighlight">
\[
MSE(\boldsymbol{y},\boldsymbol{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2,
\]</div>
<p>The smaller the value, the better the fit. Ideally we would like to
have an MSE equal zero.  The attentive reader has probably recognized
this function as being similar to the <span class="math notranslate nohighlight">\(\chi^2\)</span> function defined above.</p>
<p>The <strong>r2score</strong> function computes <span class="math notranslate nohighlight">\(R^2\)</span>, the coefficient of
determination. It provides a measure of how well future samples are
likely to be predicted by the model. Best possible score is 1.0 and it
can be negative (because the model can be arbitrarily worse). A
constant model that always predicts the expected value of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>,
disregarding the input features, would get a <span class="math notranslate nohighlight">\(R^2\)</span> score of <span class="math notranslate nohighlight">\(0.0\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{y}}_i\)</span> is the predicted value of the <span class="math notranslate nohighlight">\(i-th\)</span> sample and <span class="math notranslate nohighlight">\(y_i\)</span> is the corresponding true value, then the score <span class="math notranslate nohighlight">\(R^2\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
R^2(\boldsymbol{y}, \tilde{\boldsymbol{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
\]</div>
<p>where we have defined the mean value  of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
\]</div>
<p>Another quantity taht we will meet again in our discussions of regression analysis is
the mean absolute error (MAE), a risk metric corresponding to the expected value of the absolute error loss or what we call the <span class="math notranslate nohighlight">\(l1\)</span>-norm loss. In our discussion above we presented the relative error.
The MAE is defined as follows</p>
<div class="math notranslate nohighlight">
\[
\text{MAE}(\boldsymbol{y}, \boldsymbol{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n-1} \left| y_i - \tilde{y}_i \right|.
\]</div>
<p>We present the
squared logarithmic (quadratic) error</p>
<div class="math notranslate nohighlight">
\[
\text{MSLE}(\boldsymbol{y}, \boldsymbol{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n - 1} (\log_e (1 + y_i) - \log_e (1 + \tilde{y}_i) )^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\log_e (x)\)</span> stands for the natural logarithm of <span class="math notranslate nohighlight">\(x\)</span>. This error
estimate is best to use when targets having exponential growth, such
as population counts, average sales of a commodity over a span of
years etc.</p>
<p>Finally, another cost function is the Huber cost function used in robust regression.</p>
<p>The rationale behind this possible cost function is its reduced
sensitivity to outliers in the data set. In our discussions on
dimensionality reduction and normalization of data we will meet other
ways of dealing with outliers.</p>
<p>The Huber cost function is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H_{\delta}(\boldsymbol{a})=\left\{\begin{array}{cc}\frac{1}{2} \boldsymbol{a}^{2}&amp; \text{for }|\boldsymbol{a}|\leq \delta\\ \delta (|\boldsymbol{a}|-\frac{1}{2}\delta ),&amp;\text{otherwise}.\end{array}\right.
\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\boldsymbol{a}=\boldsymbol{y} - \boldsymbol{\tilde{y}}\)</span>.</p>
<p>We will discuss in more detail these and other functions in the
various lectures and lab sessions.</p>
</section>
<section id="to-our-real-data-nuclear-binding-energies-brief-reminder-on-masses-and-binding-energies">
<h3>To our real data: nuclear binding energies. Brief reminder on masses and binding energies<a class="headerlink" href="#to-our-real-data-nuclear-binding-energies-brief-reminder-on-masses-and-binding-energies" title="Link to this heading">#</a></h3>
<p>Let us now dive into  nuclear physics and remind ourselves briefly about some basic features about binding
energies.  A basic quantity which can be measured for the ground
states of nuclei is the atomic mass <span class="math notranslate nohighlight">\(M(N, Z)\)</span> of the neutral atom with
atomic mass number <span class="math notranslate nohighlight">\(A\)</span> and charge <span class="math notranslate nohighlight">\(Z\)</span>. The number of neutrons is <span class="math notranslate nohighlight">\(N\)</span>. There are indeed several sophisticated experiments worldwide which allow us to measure this quantity to high precision (parts per million even).</p>
<p>Atomic masses are usually tabulated in terms of the mass excess defined by</p>
<div class="math notranslate nohighlight">
\[
\Delta M(N, Z) =  M(N, Z) - uA,
\]</div>
<p>where <span class="math notranslate nohighlight">\(u\)</span> is the Atomic Mass Unit</p>
<div class="math notranslate nohighlight">
\[
u = M(^{12}\mathrm{C})/12 = 931.4940954(57) \hspace{0.1cm} \mathrm{MeV}/c^2.
\]</div>
<p>The nucleon masses are</p>
<div class="math notranslate nohighlight">
\[
m_p =  1.00727646693(9)u,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
m_n = 939.56536(8)\hspace{0.1cm} \mathrm{MeV}/c^2 = 1.0086649156(6)u.
\]</div>
<p>In the <a class="reference external" href="http://nuclearmasses.org/resources_folder/Wang_2017_Chinese_Phys_C_41_030003.pdf">2016 mass evaluation of by W.J.Huang, G.Audi, M.Wang, F.G.Kondev, S.Naimi and X.Xu</a>
there are data on masses and decays of 3437 nuclei.</p>
<p>The nuclear binding energy is defined as the energy required to break
up a given nucleus into its constituent parts of <span class="math notranslate nohighlight">\(N\)</span> neutrons and <span class="math notranslate nohighlight">\(Z\)</span>
protons. In terms of the atomic masses <span class="math notranslate nohighlight">\(M(N, Z)\)</span> the binding energy is
defined by</p>
<div class="math notranslate nohighlight">
\[
BE(N, Z) = ZM_H c^2 + Nm_n c^2 - M(N, Z)c^2 ,
\]</div>
<p>where <span class="math notranslate nohighlight">\(M_H\)</span> is the mass of the hydrogen atom and <span class="math notranslate nohighlight">\(m_n\)</span> is the mass of the neutron.
In terms of the mass excess the binding energy is given by</p>
<div class="math notranslate nohighlight">
\[
BE(N, Z) = Z\Delta_H c^2 + N\Delta_n c^2 -\Delta(N, Z)c^2 ,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta_H c^2 = 7.2890\)</span> MeV and <span class="math notranslate nohighlight">\(\Delta_n c^2 = 8.0713\)</span> MeV.</p>
<p>A popular and physically intuitive model which can be used to parametrize
the experimental binding energies as function of <span class="math notranslate nohighlight">\(A\)</span>, is the so-called
<strong>liquid drop model</strong>. The ansatz is based on the following expression</p>
<div class="math notranslate nohighlight">
\[
BE(N,Z) = a_1A-a_2A^{2/3}-a_3\frac{Z^2}{A^{1/3}}-a_4\frac{(N-Z)^2}{A},
\]</div>
<p>where <span class="math notranslate nohighlight">\(A\)</span> stands for the number of nucleons and the <span class="math notranslate nohighlight">\(a_i\)</span>s are parameters which are determined by a fit
to the experimental data.</p>
<p>To arrive at the above expression we have assumed that we can make the following assumptions:</p>
<ul class="simple">
<li><p>There is a volume term <span class="math notranslate nohighlight">\(a_1A\)</span> proportional with the number of nucleons (the energy is also an extensive quantity). When an assembly of nucleons of the same size is packed together into the smallest volume, each interior nucleon has a certain number of other nucleons in contact with it. This contribution is proportional to the volume.</p></li>
<li><p>There is a surface energy term <span class="math notranslate nohighlight">\(a_2A^{2/3}\)</span>. The assumption here is that a nucleon at the surface of a nucleus interacts with fewer other nucleons than one in the interior of the nucleus and hence its binding energy is less. This surface energy term takes that into account and is therefore negative and is proportional to the surface area.</p></li>
<li><p>There is a Coulomb energy term <span class="math notranslate nohighlight">\(a_3\frac{Z^2}{A^{1/3}}\)</span>. The electric repulsion between each pair of protons in a nucleus yields less binding.</p></li>
<li><p>There is an asymmetry term <span class="math notranslate nohighlight">\(a_4\frac{(N-Z)^2}{A}\)</span>. This term is associated with the Pauli exclusion principle and reflects the fact that the proton-neutron interaction is more attractive on the average than the neutron-neutron and proton-proton interactions.</p></li>
</ul>
<p>We could also add a so-called pairing term, which is a correction term that
arises from the tendency of proton pairs and neutron pairs to
occur. An even number of particles is more stable than an odd number.</p>
</section>
<section id="organizing-our-data">
<h3>Organizing our data<a class="headerlink" href="#organizing-our-data" title="Link to this heading">#</a></h3>
<p>Let us start with reading and organizing our data.
We start with the compilation of masses and binding energies from 2016.
After having downloaded this file to our own computer, we are now ready to read the file and start structuring our data.</p>
<p>We start with preparing folders for storing our calculations and the data file over masses and binding energies. We import also various modules that we will find useful in order to present various Machine Learning methods. Here we focus mainly on the functionality of <strong>scikit-learn</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Common imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.linear_model as skl
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import os

# Where to save the figures and data files
PROJECT_ROOT_DIR = &quot;Results&quot;
FIGURE_ID = &quot;Results/FigureFiles&quot;
DATA_ID = &quot;DataFiles/&quot;

if not os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

if not os.path.exists(FIGURE_ID):
    os.makedirs(FIGURE_ID)

if not os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

def image_path(fig_id):
    return os.path.join(FIGURE_ID, fig_id)

def data_path(dat_id):
    return os.path.join(DATA_ID, dat_id)

def save_fig(fig_id):
    plt.savefig(image_path(fig_id) + &quot;.png&quot;, format=&#39;png&#39;)

infile = open(data_path(&quot;MassEval2016.dat&quot;),&#39;r&#39;)
</pre></div>
</div>
</div>
</div>
<p>Our next step is to read the data on experimental binding energies and
reorganize them as functions of the mass number <span class="math notranslate nohighlight">\(A\)</span>, the number of
protons <span class="math notranslate nohighlight">\(Z\)</span> and neutrons <span class="math notranslate nohighlight">\(N\)</span> using <strong>pandas</strong>.  Before we do this it is
always useful (unless you have a binary file or other types of compressed
data) to actually open the file and simply take a look at it!</p>
<p>In particular, the program that outputs the final nuclear masses is written in Fortran with a specific format. It means that we need to figure out the format and which columns contain the data we are interested in. Pandas comes with a function that reads formatted output. After having admired the file, we are now ready to start massaging it with <strong>pandas</strong>. The file begins with some basic format information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&quot;&quot;&quot;                                                                                                                         
This is taken from the data file of the mass 2016 evaluation.                                                               
All files are 3436 lines long with 124 character per line.                                                                  
       Headers are 39 lines long.                                                                                           
   col 1     :  Fortran character control: 1 = page feed  0 = line feed                                                     
   format    :  a1,i3,i5,i5,i5,1x,a3,a4,1x,f13.5,f11.5,f11.3,f9.3,1x,a2,f11.3,f9.3,1x,i3,1x,f12.5,f11.5                     
   These formats are reflected in the pandas widths variable below, see the statement                                       
   widths=(1,3,5,5,5,1,3,4,1,13,11,11,9,1,2,11,9,1,3,1,12,11,1),                                                            
   Pandas has also a variable header, with length 39 in this case.                                                          
&quot;&quot;&quot;
</pre></div>
</div>
</div>
</div>
<p>The data we are interested in are in columns 2, 3, 4 and 11, giving us
the number of neutrons, protons, mass numbers and binding energies,
respectively. We add also for the sake of completeness the element name. The data are in fixed-width formatted lines and we will
covert them into the <strong>pandas</strong> DataFrame structure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Read the experimental data with Pandas
Masses = pd.read_fwf(infile, usecols=(2,3,4,6,11),
              names=(&#39;N&#39;, &#39;Z&#39;, &#39;A&#39;, &#39;Element&#39;, &#39;Ebinding&#39;),
              widths=(1,3,5,5,5,1,3,4,1,13,11,11,9,1,2,11,9,1,3,1,12,11,1),
              header=39,
              index_col=False)

# Extrapolated values are indicated by &#39;#&#39; in place of the decimal place, so
# the Ebinding column won&#39;t be numeric. Coerce to float and drop these entries.
Masses[&#39;Ebinding&#39;] = pd.to_numeric(Masses[&#39;Ebinding&#39;], errors=&#39;coerce&#39;)
Masses = Masses.dropna()
# Convert from keV to MeV.
Masses[&#39;Ebinding&#39;] /= 1000

# Group the DataFrame by nucleon number, A.
Masses = Masses.groupby(&#39;A&#39;)
# Find the rows of the grouped DataFrame with the maximum binding energy.
Masses = Masses.apply(lambda t: t[t.Ebinding==t.Ebinding.max()])
</pre></div>
</div>
</div>
</div>
<p>We have now read in the data, grouped them according to the variables we are interested in.
We see how easy it is to reorganize the data using <strong>pandas</strong>. If we
were to do these operations in C/C++ or Fortran, we would have had to
write various functions/subroutines which perform the above
reorganizations for us.  Having reorganized the data, we can now start
to make some simple fits using both the functionalities in <strong>numpy</strong> and
<strong>Scikit-Learn</strong> afterwards.</p>
<p>Now we define five variables which contain
the number of nucleons <span class="math notranslate nohighlight">\(A\)</span>, the number of protons <span class="math notranslate nohighlight">\(Z\)</span> and the number of neutrons <span class="math notranslate nohighlight">\(N\)</span>, the element name and finally the energies themselves.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A = Masses[&#39;A&#39;]
Z = Masses[&#39;Z&#39;]
N = Masses[&#39;N&#39;]
Element = Masses[&#39;Element&#39;]
Energies = Masses[&#39;Ebinding&#39;]
print(Masses)
</pre></div>
</div>
</div>
</div>
<p>The next step, and we will define this mathematically later, is to set up the so-called <strong>design matrix</strong>. We will throughout call this matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.
It has dimensionality <span class="math notranslate nohighlight">\(p\times n\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of data points and <span class="math notranslate nohighlight">\(p\)</span> are the so-called predictors. In our case here they are given by the number of polynomials in <span class="math notranslate nohighlight">\(A\)</span> we wish to include in the fit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Now we set up the design matrix X
X = np.zeros((len(A),5))
X[:,0] = 1
X[:,1] = A
X[:,2] = A**(2.0/3.0)
X[:,3] = A**(-1.0/3.0)
X[:,4] = A**(-1.0)
</pre></div>
</div>
</div>
</div>
<p>With <strong>scikitlearn</strong> we are now ready to use linear regression and fit our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>clf = skl.LinearRegression().fit(X, Energies)
fity = clf.predict(X)
</pre></div>
</div>
</div>
</div>
<p>Pretty simple!<br />
Now we can print measures of how our fit is doing, the coefficients from the fits and plot the final fit together with our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># The mean squared error                               
print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(Energies, fity))
# Explained variance score: 1 is perfect prediction                                 
print(&#39;Variance score: %.2f&#39; % r2_score(Energies, fity))
# Mean absolute error                                                           
print(&#39;Mean absolute error: %.2f&#39; % mean_absolute_error(Energies, fity))
print(clf.coef_, clf.intercept_)

Masses[&#39;Eapprox&#39;]  = fity
# Generate a plot comparing the experimental with the fitted values values.
fig, ax = plt.subplots()
ax.set_xlabel(r&#39;$A = N + Z$&#39;)
ax.set_ylabel(r&#39;$E_\mathrm{bind}\,/\mathrm{MeV}$&#39;)
ax.plot(Masses[&#39;A&#39;], Masses[&#39;Ebinding&#39;], alpha=0.7, lw=2,
            label=&#39;Ame2016&#39;)
ax.plot(Masses[&#39;A&#39;], Masses[&#39;Eapprox&#39;], alpha=0.7, lw=2, c=&#39;m&#39;,
            label=&#39;Fit&#39;)
ax.legend()
save_fig(&quot;Masses2016&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="and-what-about-using-neural-networks">
<h3>And what about using neural networks?<a class="headerlink" href="#and-what-about-using-neural-networks" title="Link to this heading">#</a></h3>
<p>The <strong>seaborn</strong> package allows us to visualize data in an efficient way. Note that we use <strong>scikit-learn</strong>’s multi-layer perceptron (or feed forward neural network)
functionality.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.neural_network import MLPRegressor
from sklearn.metrics import accuracy_score
import seaborn as sns


X_train = X
Y_train = Energies
n_hidden_neurons = 50
epochs = 100
# store models for later use
eta_vals = np.logspace(-3, 0, 4)
lmbd_vals = np.logspace(-3, 0, 4)
# store the models for later use
DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)
train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))
sns.set()
for i, eta in enumerate(eta_vals):
    for j, lmbd in enumerate(lmbd_vals):
        dnn = MLPRegressor(hidden_layer_sizes=(n_hidden_neurons), activation=&#39;relu&#39;, solver=&#39;adam&#39;,
                            alpha=lmbd, learning_rate_init=eta, max_iter=epochs)
        dnn.fit(X_train, Y_train)
        DNN_scikit[i][j] = dnn
        train_accuracy[i][j] = dnn.score(X_train, Y_train)
        fity = dnn.predict(X_train)
        MSE = mean_squared_error(Y_train, fity)
        print(&quot;Mean squared error: %.2f&quot; % mean_squared_error(Y_train, fity))
        train_accuracy[i][j] = MSE
fig, ax = plt.subplots(figsize = (10, 10))
sns.heatmap(train_accuracy, annot=True, ax=ax, cmap=&quot;viridis&quot;)
ax.set_title(&quot;Training Accuracy&quot;)
ax.set_ylabel(&quot;$\eta$&quot;)
ax.set_xlabel(&quot;$\lambda$&quot;)
plt.show()
print(train_accuracy)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="a-first-summary">
<h2>A first summary<a class="headerlink" href="#a-first-summary" title="Link to this heading">#</a></h2>
<p>The aim behind these introductory words was to present to you various
Python libraries and their functionalities, in particular libraries like
<strong>numpy</strong>, <strong>pandas</strong>, <strong>xarray</strong> and <strong>matplotlib</strong> and other that make our life much easier
in handling various data sets and visualizing data.</p>
<p>Furthermore,
<strong>Scikit-Learn</strong> allows us with few lines of code to implement popular
Machine Learning algorithms for supervised learning. Later we will meet <strong>Tensorflow</strong>, a powerful library for deep learning.
Now it is time to dive more into the details of various methods. We will start with linear regression and try to take a deeper look at what it entails.</p>
</section>
<section id="why-linear-regression-aka-ordinary-least-squares-and-family">
<h2>Why Linear Regression (aka Ordinary Least Squares and family)<a class="headerlink" href="#why-linear-regression-aka-ordinary-least-squares-and-family" title="Link to this heading">#</a></h2>
<p>Fitting a continuous function with linear parameterization in terms of the parameters  <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<ul class="simple">
<li><p>Method of choice for fitting a continuous function!</p></li>
<li><p>Gives an excellent introduction to central Machine Learning features with <strong>understandable pedagogical</strong> links to other methods like <strong>Neural Networks</strong>, <strong>Support Vector Machines</strong> etc</p></li>
<li><p>Analytical expression for the fitting parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></p></li>
<li><p>Analytical expressions for statistical propertiers like mean values, variances, confidence intervals and more</p></li>
<li><p>Analytical relation with probabilistic interpretations</p></li>
<li><p>Easy to introduce basic concepts like bias-variance tradeoff, cross-validation, resampling and regularization techniques and many other ML topics</p></li>
<li><p>Easy to code! And links well with classification problems and logistic regression and neural networks</p></li>
<li><p>Allows for <strong>easy</strong> hands-on understanding of gradient descent methods</p></li>
<li><p>and many more features</p></li>
</ul>
<p>For more discussions of Ridge and Lasso regression, <a class="reference external" href="https://arxiv.org/abs/1509.09169">Wessel van Wieringen’s</a> article is highly recommended.
Similarly, <a class="reference external" href="https://arxiv.org/abs/1803.08823">Mehta et al’s article</a> is also recommended.</p>
</section>
<section id="regression-analysis-overarching-aims">
<h2>Regression analysis, overarching aims<a class="headerlink" href="#regression-analysis-overarching-aims" title="Link to this heading">#</a></h2>
<p>Regression modeling deals with the description of  the sampling distribution of a given random variable <span class="math notranslate nohighlight">\(y\)</span> and how it varies as function of another variable or a set of such variables <span class="math notranslate nohighlight">\(\boldsymbol{x} =[x_0, x_1,\dots, x_{n-1}]^T\)</span>.
The first variable <span class="math notranslate nohighlight">\(y\)</span> is called the the <strong>outcome</strong> or the <strong>response</strong> variable, or simply just the <strong>outputs</strong>.</p>
<p>The set of variables <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is called the independent variable, or the predictor variable or the explanatory variable, or simply just the <strong>inputs</strong>. <strong>We will throughout the course just use inputs and outputs as names</strong>.</p>
<p>A regression model aims at finding a likelihood function <span class="math notranslate nohighlight">\(p(\boldsymbol{y}\vert \boldsymbol{x})\)</span> or in the more traditional sense a function <span class="math notranslate nohighlight">\(\boldsymbol{y}(\boldsymbol{x})\)</span>, that is the conditional distribution for <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> with a given <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>. The estimation of  <span class="math notranslate nohighlight">\(p(\boldsymbol{y}\vert \boldsymbol{x})\)</span> is made using a data set with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> cases <span class="math notranslate nohighlight">\(i = 0, 1, 2, \dots, n-1\)</span></p></li>
<li><p>Response (our output) variable <span class="math notranslate nohighlight">\(y_i\)</span> with <span class="math notranslate nohighlight">\(i = 0, 1, 2, \dots, n-1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span> so-called explanatory (independent or predictor or feature) variables <span class="math notranslate nohighlight">\(\boldsymbol{x}_i=[x_{i0}, x_{i1}, \dots, x_{ip-1}]\)</span> with <span class="math notranslate nohighlight">\(i = 0, 1, 2, \dots, n-1\)</span> and explanatory variables running from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(p-1\)</span>. These are the inputs. See below for more explicit examples.</p></li>
</ul>
<p>The goal of the regression analysis is to extract/exploit relationship between <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> in order to infer specific dependencies, approximations to the likelihood functions, functional relationships and to make predictions, making fits and many other things.</p>
</section>
<section id="regression-analysis-overarching-aims-ii">
<h2>Regression analysis, overarching aims II<a class="headerlink" href="#regression-analysis-overarching-aims-ii" title="Link to this heading">#</a></h2>
<p>Consider an experiment in which <span class="math notranslate nohighlight">\(p\)</span> characteristics/features of <span class="math notranslate nohighlight">\(n\)</span> samples are
measured. The data from this experiment, for various explanatory variables <span class="math notranslate nohighlight">\(p\)</span> are normally represented by a matrix<br />
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p>The matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is called the <em>design
matrix</em>. Additional information of the samples is available in the
form of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> (also as above). The variable <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> is
generally referred to as the <em>response variable</em>. The aim of
regression analysis is to explain <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> in terms of
<span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> through a functional relationship like <span class="math notranslate nohighlight">\(y_i =
f(\mathbf{X}_{i,\ast})\)</span>. When no prior knowledge on the form of
<span class="math notranslate nohighlight">\(f(\cdot)\)</span> is available, it is common to assume a linear relationship
between <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>. This assumption gives rise to
the <em>linear regression model</em> where <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = [\theta_0, \ldots,
\theta_{p-1}]^{T}\)</span> are the <em>regression parameters</em>.</p>
<p>Linear regression gives us a set of analytical equations for the parameters <span class="math notranslate nohighlight">\(\theta_j\)</span>.</p>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h2>
<p>In order to understand the relation among the predictors (or features or properties) <span class="math notranslate nohighlight">\(p\)</span>, the set of data <span class="math notranslate nohighlight">\(n\)</span> and the target (outcome, output etc) <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>,
consider the model we discussed for describing nuclear binding energies.</p>
<p>There we assumed that we could parametrize the data using a polynomial approximation based on the liquid drop model.
Assuming</p>
<div class="math notranslate nohighlight">
\[
BE(A) = a_0+a_1A+a_2A^{2/3}+a_3A^{-1/3}+a_4A^{-1},
\]</div>
<p>we have five predictors, that is the intercept, the <span class="math notranslate nohighlight">\(A\)</span> dependent term, the <span class="math notranslate nohighlight">\(A^{2/3}\)</span> term and the <span class="math notranslate nohighlight">\(A^{-1/3}\)</span> and <span class="math notranslate nohighlight">\(A^{-1}\)</span> terms.
This gives <span class="math notranslate nohighlight">\(p=0,1,2,3,4\)</span>. Furthermore we have <span class="math notranslate nohighlight">\(n\)</span> entries for each predictor. It means that our design matrix is a
<span class="math notranslate nohighlight">\(p\times n\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p>
<p>Here the predictors are based on a model we have made. A popular data set which is widely encountered in ML applications is the
so-called <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0957417407006719?via%3Dihub">credit card default data from Taiwan</a>. The data set contains data on <span class="math notranslate nohighlight">\(n=30000\)</span> credit card holders with predictors like gender, marital status, age, profession, education, etc. In total there are <span class="math notranslate nohighlight">\(24\)</span> such predictors or attributes leading to a design matrix of dimensionality <span class="math notranslate nohighlight">\(24 \times 30000\)</span>. This is however a classification problem and we will come back to it when we discuss Logistic Regression.</p>
</section>
<section id="general-linear-models-and-linear-algebra">
<h2>General linear models and linear algebra<a class="headerlink" href="#general-linear-models-and-linear-algebra" title="Link to this heading">#</a></h2>
<p>Before we proceed let us study a case where we aim at fitting a set of data <span class="math notranslate nohighlight">\(\boldsymbol{y}=[y_0,y_1,\dots,y_{n-1}]\)</span>. We could think of these data as a result of an experiment or a complicated numerical experiment. These data are functions of a series of variables <span class="math notranslate nohighlight">\(\boldsymbol{x}=[x_0,x_1,\dots,x_{n-1}]\)</span>, that is <span class="math notranslate nohighlight">\(y_i = y(x_i)\)</span> with <span class="math notranslate nohighlight">\(i=0,1,2,\dots,n-1\)</span>. The variables <span class="math notranslate nohighlight">\(x_i\)</span> could represent physical quantities like time, temperature, position etc. We assume that <span class="math notranslate nohighlight">\(y(x)\)</span> is a smooth function.</p>
<p>Since obtaining these data points may not be trivial, we want to use these data to fit a function which can allow us to make predictions for values of <span class="math notranslate nohighlight">\(y\)</span> which are not in the present set. The perhaps simplest approach is to assume we can parametrize our function in terms of a polynomial of degree <span class="math notranslate nohighlight">\(n-1\)</span> with <span class="math notranslate nohighlight">\(n\)</span> points, that is</p>
<div class="math notranslate nohighlight">
\[
y=y(x) \rightarrow y(x_i)=\tilde{y}_i+\epsilon_i=\sum_{j=0}^{n-1} \theta_j x_i^j+\epsilon_i,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is the error in our approximation.</p>
</section>
<section id="rewriting-the-fitting-procedure-as-a-linear-algebra-problem">
<h2>Rewriting the fitting procedure as a linear algebra problem<a class="headerlink" href="#rewriting-the-fitting-procedure-as-a-linear-algebra-problem" title="Link to this heading">#</a></h2>
<p>For every set of values <span class="math notranslate nohighlight">\(y_i,x_i\)</span> we have thus the corresponding set of equations</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
y_0&amp;=\theta_0+\theta_1x_0^1+\theta_2x_0^2+\dots+\theta_{n-1}x_0^{n-1}+\epsilon_0\\
y_1&amp;=\theta_0+\theta_1x_1^1+\theta_2x_1^2+\dots+\theta_{n-1}x_1^{n-1}+\epsilon_1\\
y_2&amp;=\theta_0+\theta_1x_2^1+\theta_2x_2^2+\dots+\theta_{n-1}x_2^{n-1}+\epsilon_2\\
\dots &amp; \dots \\
y_{n-1}&amp;=\theta_0+\theta_1x_{n-1}^1+\theta_2x_{n-1}^2+\dots+\theta_{n-1}x_{n-1}^{n-1}+\epsilon_{n-1}.\\
\end{align*}
\end{split}\]</div>
</section>
<section id="rewriting-the-fitting-procedure-as-a-linear-algebra-problem-more-details">
<h2>Rewriting the fitting procedure as a linear algebra problem, more details<a class="headerlink" href="#rewriting-the-fitting-procedure-as-a-linear-algebra-problem-more-details" title="Link to this heading">#</a></h2>
<p>Defining the vectors</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = [y_0,y_1, y_2,\dots, y_{n-1}]^T,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta} = [\theta_0,\theta_1, \theta_2,\dots, \theta_{n-1}]^T,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\epsilon} = [\epsilon_0,\epsilon_1, \epsilon_2,\dots, \epsilon_{n-1}]^T,
\]</div>
<p>and the design matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X}=
\begin{bmatrix} 
1&amp; x_{0}^1 &amp;x_{0}^2&amp; \dots &amp; \dots &amp;x_{0}^{n-1}\\
1&amp; x_{1}^1 &amp;x_{1}^2&amp; \dots &amp; \dots &amp;x_{1}^{n-1}\\
1&amp; x_{2}^1 &amp;x_{2}^2&amp; \dots &amp; \dots &amp;x_{2}^{n-1}\\                      
\dots&amp; \dots &amp;\dots&amp; \dots &amp; \dots &amp;\dots\\
1&amp; x_{n-1}^1 &amp;x_{n-1}^2&amp; \dots &amp; \dots &amp;x_{n-1}^{n-1}\\
\end{bmatrix}
\end{split}\]</div>
<p>we can rewrite our equations as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\epsilon}.
\]</div>
<p>The above design matrix is called a <a class="reference external" href="https://en.wikipedia.org/wiki/Vandermonde_matrix">Vandermonde matrix</a>.</p>
</section>
<section id="generalizing-the-fitting-procedure-as-a-linear-algebra-problem">
<h2>Generalizing the fitting procedure as a linear algebra problem<a class="headerlink" href="#generalizing-the-fitting-procedure-as-a-linear-algebra-problem" title="Link to this heading">#</a></h2>
<p>We are obviously not limited to the above polynomial expansions.  We
could replace the various powers of <span class="math notranslate nohighlight">\(x\)</span> with elements of Fourier
series or instead of <span class="math notranslate nohighlight">\(x_i^j\)</span> we could have <span class="math notranslate nohighlight">\(\cos{(j x_i)}\)</span> or <span class="math notranslate nohighlight">\(\sin{(j
x_i)}\)</span>, or time series or other orthogonal functions.  For every set
of values <span class="math notranslate nohighlight">\(y_i,x_i\)</span> we can then generalize the equations to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
y_0&amp;=\theta_0x_{00}+\theta_1x_{01}+\theta_2x_{02}+\dots+\theta_{n-1}x_{0n-1}+\epsilon_0\\
y_1&amp;=\theta_0x_{10}+\theta_1x_{11}+\theta_2x_{12}+\dots+\theta_{n-1}x_{1n-1}+\epsilon_1\\
y_2&amp;=\theta_0x_{20}+\theta_1x_{21}+\theta_2x_{22}+\dots+\theta_{n-1}x_{2n-1}+\epsilon_2\\
\dots &amp; \dots \\
y_{i}&amp;=\theta_0x_{i0}+\theta_1x_{i1}+\theta_2x_{i2}+\dots+\theta_{n-1}x_{in-1}+\epsilon_i\\
\dots &amp; \dots \\
y_{n-1}&amp;=\theta_0x_{n-1,0}+\theta_1x_{n-1,2}+\theta_2x_{n-1,2}+\dots+\theta_{n-1}x_{n-1,n-1}+\epsilon_{n-1}.\\
\end{align*}
\end{split}\]</div>
<p><strong>Note that we have <span class="math notranslate nohighlight">\(p=n\)</span> here. The matrix is symmetric. This is generally not the case!</strong></p>
</section>
<section id="id1">
<h2>Generalizing the fitting procedure as a linear algebra problem<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>We redefine in turn the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X}=
\begin{bmatrix} 
x_{00}&amp; x_{01} &amp;x_{02}&amp; \dots &amp; \dots &amp;x_{0,n-1}\\
x_{10}&amp; x_{11} &amp;x_{12}&amp; \dots &amp; \dots &amp;x_{1,n-1}\\
x_{20}&amp; x_{21} &amp;x_{22}&amp; \dots &amp; \dots &amp;x_{2,n-1}\\                      
\dots&amp; \dots &amp;\dots&amp; \dots &amp; \dots &amp;\dots\\
x_{n-1,0}&amp; x_{n-1,1} &amp;x_{n-1,2}&amp; \dots &amp; \dots &amp;x_{n-1,n-1}\\
\end{bmatrix}
\end{split}\]</div>
<p>and without loss of generality we rewrite again  our equations as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\epsilon}.
\]</div>
<p>The left-hand side of this equation is kwown. Our error vector <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> and the parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> are our unknow quantities. How can we obtain the optimal set of <span class="math notranslate nohighlight">\(\theta_i\)</span> values?</p>
</section>
<section id="optimizing-our-parameters">
<h2>Optimizing our parameters<a class="headerlink" href="#optimizing-our-parameters" title="Link to this heading">#</a></h2>
<p>We have defined the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> via the equations</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
y_0&amp;=\theta_0x_{00}+\theta_1x_{01}+\theta_2x_{02}+\dots+\theta_{n-1}x_{0n-1}+\epsilon_0\\
y_1&amp;=\theta_0x_{10}+\theta_1x_{11}+\theta_2x_{12}+\dots+\theta_{n-1}x_{1n-1}+\epsilon_1\\
y_2&amp;=\theta_0x_{20}+\theta_1x_{21}+\theta_2x_{22}+\dots+\theta_{n-1}x_{2n-1}+\epsilon_1\\
\dots &amp; \dots \\
y_{i}&amp;=\theta_0x_{i0}+\theta_1x_{i1}+\theta_2x_{i2}+\dots+\theta_{n-1}x_{in-1}+\epsilon_1\\
\dots &amp; \dots \\
y_{n-1}&amp;=\theta_0x_{n-1,0}+\theta_1x_{n-1,2}+\theta_2x_{n-1,2}+\dots+\theta_{n-1}x_{n-1,n-1}+\epsilon_{n-1}.\\
\end{align*}
\end{split}\]</div>
<p>As we noted above, we stayed with a system with the design matrix
<span class="math notranslate nohighlight">\(\boldsymbol{X}\in {\mathbb{R}}^{n\times n}\)</span>, that is we have <span class="math notranslate nohighlight">\(p=n\)</span>. For reasons to come later (algorithmic arguments) we will hereafter define
our matrix as <span class="math notranslate nohighlight">\(\boldsymbol{X}\in {\mathbb{R}}^{n\times p}\)</span>, with the predictors refering to the column numbers and the entries <span class="math notranslate nohighlight">\(n\)</span> being the row elements.</p>
</section>
<section id="our-model-for-the-nuclear-binding-energies">
<h2>Our model for the nuclear binding energies<a class="headerlink" href="#our-model-for-the-nuclear-binding-energies" title="Link to this heading">#</a></h2>
<p>In our <a class="reference external" href="https://compphysics.github.io/MachineLearning/doc/pub/How2ReadData/html/How2ReadData.html">introductory notes</a> we looked at the so-called <a class="reference external" href="https://en.wikipedia.org/wiki/Semi-empirical_mass_formula">liquid drop model</a>. Let us remind ourselves about what we did by looking at the code.</p>
<p>We restate the parts of the code we are most interested in.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Common imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display
import os

# Where to save the figures and data files
PROJECT_ROOT_DIR = &quot;Results&quot;
FIGURE_ID = &quot;Results/FigureFiles&quot;
DATA_ID = &quot;DataFiles/&quot;

if not os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

if not os.path.exists(FIGURE_ID):
    os.makedirs(FIGURE_ID)

if not os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

def image_path(fig_id):
    return os.path.join(FIGURE_ID, fig_id)

def data_path(dat_id):
    return os.path.join(DATA_ID, dat_id)

def save_fig(fig_id):
    plt.savefig(image_path(fig_id) + &quot;.png&quot;, format=&#39;png&#39;)

infile = open(data_path(&quot;MassEval2016.dat&quot;),&#39;r&#39;)


# Read the experimental data with Pandas
Masses = pd.read_fwf(infile, usecols=(2,3,4,6,11),
              names=(&#39;N&#39;, &#39;Z&#39;, &#39;A&#39;, &#39;Element&#39;, &#39;Ebinding&#39;),
              widths=(1,3,5,5,5,1,3,4,1,13,11,11,9,1,2,11,9,1,3,1,12,11,1),
              header=39,
              index_col=False)

# Extrapolated values are indicated by &#39;#&#39; in place of the decimal place, so
# the Ebinding column won&#39;t be numeric. Coerce to float and drop these entries.
Masses[&#39;Ebinding&#39;] = pd.to_numeric(Masses[&#39;Ebinding&#39;], errors=&#39;coerce&#39;)
Masses = Masses.dropna()
# Convert from keV to MeV.
Masses[&#39;Ebinding&#39;] /= 1000

# Group the DataFrame by nucleon number, A.
Masses = Masses.groupby(&#39;A&#39;)
# Find the rows of the grouped DataFrame with the maximum binding energy.
Masses = Masses.apply(lambda t: t[t.Ebinding==t.Ebinding.max()])
A = Masses[&#39;A&#39;]
Z = Masses[&#39;Z&#39;]
N = Masses[&#39;N&#39;]
Element = Masses[&#39;Element&#39;]
Energies = Masses[&#39;Ebinding&#39;]

# Now we set up the design matrix X
X = np.zeros((len(A),5))
X[:,0] = 1
X[:,1] = A
X[:,2] = A**(2.0/3.0)
X[:,3] = A**(-1.0/3.0)
X[:,4] = A**(-1.0)
# Then nice printout using pandas
DesignMatrix = pd.DataFrame(X)
DesignMatrix.index = A
DesignMatrix.columns = [&#39;1&#39;, &#39;A&#39;, &#39;A^(2/3)&#39;, &#39;A^(-1/3)&#39;, &#39;1/A&#39;]
display(DesignMatrix)
</pre></div>
</div>
</div>
</div>
<p>With <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\in {\mathbb{R}}^{p\times 1}\)</span>, it means that we will hereafter write our equations for the approximation as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\tilde{y}}= \boldsymbol{X}\boldsymbol{\theta},
\]</div>
<p>throughout these lectures.</p>
</section>
<section id="optimizing-our-parameters-more-details">
<h2>Optimizing our parameters, more details<a class="headerlink" href="#optimizing-our-parameters-more-details" title="Link to this heading">#</a></h2>
<p>With the above we use the design matrix to define the approximation <span class="math notranslate nohighlight">\(\boldsymbol{\tilde{y}}\)</span> via the unknown quantity <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\tilde{y}}= \boldsymbol{X}\boldsymbol{\theta},
\]</div>
<p>and in order to find the optimal parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> instead of solving the above linear algebra problem, we define a function which gives a measure of the spread between the values <span class="math notranslate nohighlight">\(y_i\)</span> (which represent hopefully the exact values) and the parameterized values <span class="math notranslate nohighlight">\(\tilde{y}_i\)</span>, namely</p>
<div class="math notranslate nohighlight">
\[
C(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right\},
\]</div>
<p>or using the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and in a more compact matrix-vector notation as</p>
<div class="math notranslate nohighlight">
\[
C(\boldsymbol{\theta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\}.
\]</div>
<p>This function is one possible way to define the so-called cost function.</p>
<p>It is also common to define
the function <span class="math notranslate nohighlight">\(C\)</span> as</p>
<div class="math notranslate nohighlight">
\[
C(\boldsymbol{\theta})=\frac{1}{2n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2,
\]</div>
<p>since when taking the first derivative with respect to the unknown parameters <span class="math notranslate nohighlight">\(\theta\)</span>, the factor of <span class="math notranslate nohighlight">\(2\)</span> cancels out.</p>
</section>
<section id="interpretations-and-optimizing-our-parameters">
<h2>Interpretations and optimizing our parameters<a class="headerlink" href="#interpretations-and-optimizing-our-parameters" title="Link to this heading">#</a></h2>
<p>The function</p>
<div class="math notranslate nohighlight">
\[
C(\boldsymbol{\theta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\},
\]</div>
<p>can be linked to the variance of the quantity <span class="math notranslate nohighlight">\(y_i\)</span> if we interpret the latter as the mean value.
When linking (see the discussion below) with the maximum likelihood approach below, we will indeed interpret <span class="math notranslate nohighlight">\(y_i\)</span> as a mean value</p>
<div class="math notranslate nohighlight">
\[
y_{i}=\langle y_i \rangle = \theta_0x_{i,0}+\theta_1x_{i,1}+\theta_2x_{i,2}+\dots+\theta_{n-1}x_{i,n-1}+\epsilon_i,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\langle y_i \rangle\)</span> is the mean value. Keep in mind also that
till now we have treated <span class="math notranslate nohighlight">\(y_i\)</span> as the exact value. Normally, the
response (dependent or outcome) variable <span class="math notranslate nohighlight">\(y_i\)</span> the outcome of a
numerical experiment or another type of experiment and is thus only an
approximation to the true value. It is then always accompanied by an
error estimate, often limited to a statistical error estimate given by
the standard deviation discussed earlier. In the discussion here we
will treat <span class="math notranslate nohighlight">\(y_i\)</span> as our exact value for the response variable.</p>
<p>In order to find the parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> we will then minimize the spread of <span class="math notranslate nohighlight">\(C(\boldsymbol{\theta})\)</span>, that is we are going to solve the problem</p>
<div class="math notranslate nohighlight">
\[
{\displaystyle \min_{\boldsymbol{\theta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\}.
\]</div>
<p>In practical terms it means we will require</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial C(\boldsymbol{\theta})}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}\right)^2\right]=0,
\]</div>
<p>which results in</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial C(\boldsymbol{\theta})}{\partial \theta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}x_{ij}\left(y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}\right)\right]=0,
\]</div>
<p>or in a matrix-vector form as</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial C(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right).
\]</div>
</section>
<section id="id2">
<h2>Interpretations and optimizing our parameters<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>We can rewrite</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial C(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right),
\]</div>
<p>as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X}^T\boldsymbol{y} = \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\theta},
\]</div>
<p>and if the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}^T\boldsymbol{X}\)</span> is invertible we have the solution</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta} =\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.
\]</div>
<p>We note also that since our design matrix is defined as <span class="math notranslate nohighlight">\(\boldsymbol{X}\in
{\mathbb{R}}^{n\times p}\)</span>, the product <span class="math notranslate nohighlight">\(\boldsymbol{X}^T\boldsymbol{X} \in
{\mathbb{R}}^{p\times p}\)</span>.  In the above case we have that <span class="math notranslate nohighlight">\(p \ll n\)</span>,
in our case <span class="math notranslate nohighlight">\(p=5\)</span> meaning that we end up with inverting a small
<span class="math notranslate nohighlight">\(5\times 5\)</span> matrix. This is a rather common situation, in many cases we end up with low-dimensional
matrices to invert. The methods discussed here and for many other
supervised learning algorithms like classification with logistic
regression or support vector machines, exhibit dimensionalities which
allow for the usage of direct linear algebra methods such as <strong>LU</strong> decomposition or <strong>Singular Value Decomposition</strong> (SVD) for finding the inverse of the matrix
<span class="math notranslate nohighlight">\(\boldsymbol{X}^T\boldsymbol{X}\)</span>.</p>
<p><strong>Small question</strong>: Do you think the example we have at hand here (the nuclear binding energies) can lead to problems in inverting the matrix  <span class="math notranslate nohighlight">\(\boldsymbol{X}^T\boldsymbol{X}\)</span>? What kind of problems can we expect?</p>
</section>
<section id="id3">
<h2>Interpretations and optimizing our parameters<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>The residuals <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> are in turn given by</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\epsilon} = \boldsymbol{y}-\boldsymbol{\tilde{y}} = \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta},
\]</div>
<p>and with</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)= 0,
\]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X}^T\boldsymbol{\epsilon}=\boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)= 0,
\]</div>
<p>meaning that the solution for <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is the one which minimizes the residuals.  Later we will link this with the maximum likelihood approach.</p>
<p>Let us now return to our nuclear binding energies and simply code the above equations.</p>
</section>
<section id="own-code-for-ordinary-least-squares">
<h2>Own code for Ordinary Least Squares<a class="headerlink" href="#own-code-for-ordinary-least-squares" title="Link to this heading">#</a></h2>
<p>It is rather straightforward to implement the matrix inversion and obtain the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. After having defined the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> we simply need to
write</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># matrix inversion to find beta
beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Energies)
# and then make the prediction
ytilde = X @ beta
</pre></div>
</div>
</div>
</div>
<p>Alternatively, you can use the least squares functionality in <strong>Numpy</strong> as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>fit = np.linalg.lstsq(X, Energies, rcond =None)[0]
ytildenp = np.dot(fit,X.T)
</pre></div>
</div>
</div>
</div>
<p>And finally we plot our fit with and compare with data</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Masses[&#39;Eapprox&#39;]  = ytilde
# Generate a plot comparing the experimental with the fitted values values.
fig, ax = plt.subplots()
ax.set_xlabel(r&#39;$A = N + Z$&#39;)
ax.set_ylabel(r&#39;$E_\mathrm{bind}\,/\mathrm{MeV}$&#39;)
ax.plot(Masses[&#39;A&#39;], Masses[&#39;Ebinding&#39;], alpha=0.7, lw=2,
            label=&#39;Ame2016&#39;)
ax.plot(Masses[&#39;A&#39;], Masses[&#39;Eapprox&#39;], alpha=0.7, lw=2, c=&#39;m&#39;,
            label=&#39;Fit&#39;)
ax.legend()
save_fig(&quot;Masses2016OLS&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="adding-error-analysis-and-training-set-up">
<h2>Adding error analysis and training set up<a class="headerlink" href="#adding-error-analysis-and-training-set-up" title="Link to this heading">#</a></h2>
<p>We can easily test our fit by computing the <span class="math notranslate nohighlight">\(R2\)</span> score that we discussed in connection with the functionality of <strong>Scikit-Learn</strong> in the introductory slides.
Since we are not using <strong>Scikit-Learn</strong> here we can define our own <span class="math notranslate nohighlight">\(R2\)</span> function as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def R2(y_data, y_model):
    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)
</pre></div>
</div>
</div>
</div>
<p>and we would be using it as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>print(R2(Energies,ytilde))
</pre></div>
</div>
</div>
</div>
<p>We can easily add our <strong>MSE</strong> score as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def MSE(y_data,y_model):
    n = np.size(y_model)
    return np.sum((y_data-y_model)**2)/n

print(MSE(Energies,ytilde))
</pre></div>
</div>
</div>
</div>
<p>and finally the relative error as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def RelativeError(y_data,y_model):
    return abs((y_data-y_model)/y_data)
print(RelativeError(Energies, ytilde))
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-chi-2-function">
<h2>The <span class="math notranslate nohighlight">\(\chi^2\)</span> function<a class="headerlink" href="#the-chi-2-function" title="Link to this heading">#</a></h2>
<p>Normally, the response (dependent or outcome) variable <span class="math notranslate nohighlight">\(y_i\)</span> is the
outcome of a numerical experiment or another type of experiment and is
thus only an approximation to the true value. It is then always
accompanied by an error estimate, often limited to a statistical error
estimate given by the standard deviation discussed earlier. In the
discussion here we will treat <span class="math notranslate nohighlight">\(y_i\)</span> as our exact value for the
response variable.</p>
<p>Introducing the standard deviation <span class="math notranslate nohighlight">\(\sigma_i\)</span> for each measurement
<span class="math notranslate nohighlight">\(y_i\)</span>, we define now the <span class="math notranslate nohighlight">\(\chi^2\)</span> function (omitting the <span class="math notranslate nohighlight">\(1/n\)</span> term)
as</p>
<div class="math notranslate nohighlight">
\[
\chi^2(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=0}^{n-1}\frac{\left(y_i-\tilde{y}_i\right)^2}{\sigma_i^2}=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T\frac{1}{\boldsymbol{\Sigma^2}}\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right\},
\]</div>
<p>where the matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is a diagonal matrix with <span class="math notranslate nohighlight">\(\sigma_i\)</span> as matrix elements.</p>
</section>
<section id="id4">
<h2>The <span class="math notranslate nohighlight">\(\chi^2\)</span> function<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<p>In order to find the parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> we will then minimize the spread of <span class="math notranslate nohighlight">\(\chi^2(\boldsymbol{\theta})\)</span> by requiring</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(\frac{y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}}{\sigma_i}\right)^2\right]=0,
\]</div>
<p>which results in</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}\frac{x_{ij}}{\sigma_i}\left(\frac{y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}}{\sigma_i}\right)\right]=0,
\]</div>
<p>or in a matrix-vector form as</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{A}^T\left( \boldsymbol{b}-\boldsymbol{A}\boldsymbol{\theta}\right).
\]</div>
<p>where we have defined the matrix <span class="math notranslate nohighlight">\(\boldsymbol{A} =\boldsymbol{X}/\boldsymbol{\Sigma}\)</span> with matrix elements <span class="math notranslate nohighlight">\(a_{ij} = x_{ij}/\sigma_i\)</span> and the vector <span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span> with elements <span class="math notranslate nohighlight">\(b_i = y_i/\sigma_i\)</span>.</p>
</section>
<section id="id5">
<h2>The <span class="math notranslate nohighlight">\(\chi^2\)</span> function<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>We can rewrite</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{A}^T\left( \boldsymbol{b}-\boldsymbol{A}\boldsymbol{\theta}\right),
\]</div>
<p>as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{A}^T\boldsymbol{b} = \boldsymbol{A}^T\boldsymbol{A}\boldsymbol{\theta},
\]</div>
<p>and if the matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}^T\boldsymbol{A}\)</span> is invertible we have the solution</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta} =\left(\boldsymbol{A}^T\boldsymbol{A}\right)^{-1}\boldsymbol{A}^T\boldsymbol{b}.
\]</div>
</section>
<section id="id6">
<h2>The <span class="math notranslate nohighlight">\(\chi^2\)</span> function<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<p>If we then introduce the matrix</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{H} =  \left(\boldsymbol{A}^T\boldsymbol{A}\right)^{-1},
\]</div>
<p>we have then the following expression for the parameters <span class="math notranslate nohighlight">\(\theta_j\)</span> (the matrix elements of <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span> are <span class="math notranslate nohighlight">\(h_{ij}\)</span>)</p>
<div class="math notranslate nohighlight">
\[
\theta_j = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}\frac{y_i}{\sigma_i}\frac{x_{ik}}{\sigma_i} = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}b_ia_{ik}
\]</div>
<p>We state without proof the expression for the uncertainty  in the parameters <span class="math notranslate nohighlight">\(\theta_j\)</span> as (we leave this as an exercise)</p>
<div class="math notranslate nohighlight">
\[
\sigma^2(\theta_j) = \sum_{i=0}^{n-1}\sigma_i^2\left( \frac{\partial \theta_j}{\partial y_i}\right)^2,
\]</div>
<p>resulting in</p>
<div class="math notranslate nohighlight">
\[
\sigma^2(\theta_j) = \left(\sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}a_{ik}\right)\left(\sum_{l=0}^{p-1}h_{jl}\sum_{m=0}^{n-1}a_{ml}\right) = h_{jj}!
\]</div>
</section>
<section id="id7">
<h2>The <span class="math notranslate nohighlight">\(\chi^2\)</span> function<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<p>The first step here is to approximate the function <span class="math notranslate nohighlight">\(y\)</span> with a first-order polynomial, that is we write</p>
<div class="math notranslate nohighlight">
\[
y=y(x) \rightarrow y(x_i) \approx \theta_0+\theta_1 x_i.
\]</div>
<p>By computing the derivatives of <span class="math notranslate nohighlight">\(\chi^2\)</span> with respect to <span class="math notranslate nohighlight">\(\theta_0\)</span> and <span class="math notranslate nohighlight">\(\theta_1\)</span> show that these are given by</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_0} = -2\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(\frac{y_i-\theta_0-\theta_1x_{i}}{\sigma_i^2}\right)\right]=0,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_1} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}x_i\left(\frac{y_i-\theta_0-\theta_1x_{i}}{\sigma_i^2}\right)\right]=0.
\]</div>
</section>
<section id="id8">
<h2>The <span class="math notranslate nohighlight">\(\chi^2\)</span> function<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<p>For a linear fit (a first-order polynomial) we don’t need to invert a matrix!!<br />
Defining</p>
<div class="math notranslate nohighlight">
\[
\gamma =  \sum_{i=0}^{n-1}\frac{1}{\sigma_i^2},
\]</div>
<div class="math notranslate nohighlight">
\[
\gamma_x =  \sum_{i=0}^{n-1}\frac{x_{i}}{\sigma_i^2},
\]</div>
<div class="math notranslate nohighlight">
\[
\gamma_y = \sum_{i=0}^{n-1}\left(\frac{y_i}{\sigma_i^2}\right),
\]</div>
<div class="math notranslate nohighlight">
\[
\gamma_{xx} =  \sum_{i=0}^{n-1}\frac{x_ix_{i}}{\sigma_i^2},
\]</div>
<div class="math notranslate nohighlight">
\[
\gamma_{xy} = \sum_{i=0}^{n-1}\frac{y_ix_{i}}{\sigma_i^2},
\]</div>
<p>we obtain</p>
<div class="math notranslate nohighlight">
\[
\theta_0 = \frac{\gamma_{xx}\gamma_y-\gamma_x\gamma_y}{\gamma\gamma_{xx}-\gamma_x^2},
\]</div>
<div class="math notranslate nohighlight">
\[
\theta_1 = \frac{\gamma_{xy}\gamma-\gamma_x\gamma_y}{\gamma\gamma_{xx}-\gamma_x^2}.
\]</div>
<p>This approach (different linear and non-linear regression) suffers
often from both being underdetermined and overdetermined in the
unknown coefficients <span class="math notranslate nohighlight">\(\theta_i\)</span>.  A better approach is to use the
Singular Value Decomposition (SVD) method discussed next week.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="exercisesweek34.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Exercises week 34</p>
      </div>
    </a>
    <a class="right-next"
       href="exercisesweek35.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Exercises week 35</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview-of-first-week">Overview of first week</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#schedule-first-week">Schedule first week</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lectures-and-computerlab">Lectures and ComputerLab</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#communication-channels">Communication channels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#course-format">Course Format</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teachers">Teachers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deadlines-for-projects-tentative">Deadlines for projects (tentative)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#grading">Grading</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-material">Reading material</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-textbooks">Main textbooks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-popular-texts">Other popular texts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reading-suggestions-week-34">Reading suggestions week 34</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topics-covered-in-this-course-statistical-analysis-and-optimization-of-data">Topics covered in this course: Statistical analysis and optimization of data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-analysis-and-optimization-of-data">Statistical analysis and optimization of data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-methods">Deep learning methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extremely-useful-tools-strongly-recommended">Extremely useful tools, strongly recommended</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-courses-on-data-science-and-machine-learning-at-uio">Other courses on Data science and Machine Learning  at UiO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-courses-on-data-science-and-machine-learning-at-uio-contn">Other courses on Data science and Machine Learning  at UiO, contn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning">Types of Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#essential-elements-of-ml">Essential elements of ML</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-optimization-minimization-problem">An optimization/minimization problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-plethora-of-machine-learning-algorithms-methods">The plethora  of machine learning algorithms/methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-generative-modeling">What Is Generative Modeling?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-generative-modeling-taken-from-generative-deep-learning-by-david-foster">Example of generative modeling, taken from Generative Deep Learning by David Foster</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-versus-discriminative-modeling">Generative Versus Discriminative Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-discriminative-modeling-taken-from-generative-deep-learning-by-david-foster">Example of discriminative modeling, taken from Generative Deep Learning by David Foster</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discriminative-modeling">Discriminative Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-frequentist-approach-to-data-analysis">A Frequentist approach to data analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-good-model">What is a good model?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-good-model-can-we-define-it">What is a good model? Can we define it?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#software-and-needed-installations">Software and needed installations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-installers">Python installers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-python-libraries">Useful Python libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-r-c-cython-or-julia">Installing R, C++, cython or Julia</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-r-c-cython-numba-etc">Installing R, C++, cython, Numba etc</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numpy-examples-and-important-matrix-and-vector-handling-packages">Numpy examples and Important Matrix and vector handling packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numpy-and-arrays">Numpy and arrays</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-in-python">Matrices in Python</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#meet-the-pandas">Meet the Pandas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pandas-ai">Pandas AI</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression-model-using-scikit-learn">Simple linear regression model using <strong>scikit-learn</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#to-our-real-data-nuclear-binding-energies-brief-reminder-on-masses-and-binding-energies">To our real data: nuclear binding energies. Brief reminder on masses and binding energies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#organizing-our-data">Organizing our data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#and-what-about-using-neural-networks">And what about using neural networks?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-first-summary">A first summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-linear-regression-aka-ordinary-least-squares-and-family">Why Linear Regression (aka Ordinary Least Squares and family)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-analysis-overarching-aims">Regression analysis, overarching aims</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-analysis-overarching-aims-ii">Regression analysis, overarching aims II</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-linear-models-and-linear-algebra">General linear models and linear algebra</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rewriting-the-fitting-procedure-as-a-linear-algebra-problem">Rewriting the fitting procedure as a linear algebra problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rewriting-the-fitting-procedure-as-a-linear-algebra-problem-more-details">Rewriting the fitting procedure as a linear algebra problem, more details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-the-fitting-procedure-as-a-linear-algebra-problem">Generalizing the fitting procedure as a linear algebra problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Generalizing the fitting procedure as a linear algebra problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-our-parameters">Optimizing our parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#our-model-for-the-nuclear-binding-energies">Our model for the nuclear binding energies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-our-parameters-more-details">Optimizing our parameters, more details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretations-and-optimizing-our-parameters">Interpretations and optimizing our parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Interpretations and optimizing our parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Interpretations and optimizing our parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#own-code-for-ordinary-least-squares">Own code for Ordinary Least Squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-error-analysis-and-training-set-up">Adding error analysis and training set up</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chi-2-function">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">The <span class="math notranslate nohighlight">\(\chi^2\)</span> function</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Morten Hjorth-Jensen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>