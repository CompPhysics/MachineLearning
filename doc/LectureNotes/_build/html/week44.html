
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 44, Solving differential equations with neural networks and start Convolutional Neural Networks (CNN) &#8212; Applied Data Analysis and Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week44';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Exercises week 44" href="exercisesweek44.html" />
    <link rel="prev" title="Exercises week 43" href="exercisesweek43.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Applied Data Analysis and Machine Learning - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Applied Data Analysis and Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Applied Data Analysis and Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About the course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="schedule.html">Course setting</a></li>
<li class="toctree-l1"><a class="reference internal" href="teachers.html">Teachers and Grading</a></li>
<li class="toctree-l1"><a class="reference internal" href="textbooks.html">Textbooks</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Review of Statistics with Resampling Techniques and Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="statistics.html">1. Elements of Probability Theory and Statistical Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">2. Linear Algebra, Handling of Arrays and more Python Features</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">From Regression to Support Vector Machines</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter1.html">3. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter2.html">4. Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter3.html">5. Resampling Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter4.html">6. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapteroptimization.html">7. Optimization, the central part of any Machine Learning algortithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter5.html">8. Support Vector Machines, overarching aims</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Decision Trees, Ensemble Methods and Boosting</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter6.html">9. Decision trees, overarching aims</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter7.html">10. Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Dimensionality Reduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter8.html">11. Basic ideas of the Principal Component Analysis (PCA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">12. Clustering and Unsupervised Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter9.html">13. Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter10.html">14. Building a Feed Forward Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter11.html">15. Solving Differential Equations  with Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter12.html">16. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter13.html">17. Recurrent neural networks: Overarching view</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Weekly material, notes and exercises</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="exercisesweek34.html">Exercises week 34</a></li>
<li class="toctree-l1"><a class="reference internal" href="week34.html">Week 34: Introduction to the course, Logistics and Practicalities</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek35.html">Exercises week 35</a></li>
<li class="toctree-l1"><a class="reference internal" href="week35.html">Week 35: From Ordinary Linear Regression to Ridge and Lasso Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek36.html">Exercises week 36</a></li>
<li class="toctree-l1"><a class="reference internal" href="week36.html">Week 36: Linear Regression and Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek37.html">Exercises week 37</a></li>
<li class="toctree-l1"><a class="reference internal" href="week37.html">Week 37: Gradient descent methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek38.html">Exercises week 38</a></li>
<li class="toctree-l1"><a class="reference internal" href="week38.html">Week 38: Statistical analysis, bias-variance tradeoff and resampling methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek39.html">Exercises week 39</a></li>
<li class="toctree-l1"><a class="reference internal" href="week39.html">Week 39: Resampling methods and logistic regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="week40.html">Week 40: Gradient descent methods (continued) and start Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="week41.html">Week 41 Neural networks and constructing a neural network code</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek41.html">Exercises week 41</a></li>








<li class="toctree-l1"><a class="reference internal" href="week42.html">Week 42 Constructing a Neural Network code with examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek42.html">Exercises week 42</a></li>









<li class="toctree-l1"><a class="reference internal" href="week43.html">Week 43: Deep Learning: Constructing a Neural Network code and solving differential equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek43.html">Exercises week 43</a></li>

<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 44,  Solving differential equations with neural networks and start Convolutional Neural Networks (CNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek44.html">Exercises week 44</a></li>

<li class="toctree-l1"><a class="reference internal" href="week45.html">Week 45,  Convolutional Neural Networks (CCNs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="week46.html">Week 46: Decision Trees, Ensemble methods  and Random Forests</a></li>
<li class="toctree-l1"><a class="reference internal" href="week47.html">Week 47: Recurrent neural networks and Autoencoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercisesweek47.html">Exercise week 47-48</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="project1.html">Project 1 on Machine Learning, deadline October 6 (midnight), 2025</a></li>
<li class="toctree-l1"><a class="reference internal" href="project2.html">Project 2 on Machine Learning, deadline November 10 (Midnight)</a></li>
<li class="toctree-l1"><a class="reference internal" href="project3.html">Project 3 on Machine Learning, deadline December 15 (midnight), 2025</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/week44.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 44,  Solving differential equations with neural networks and start Convolutional Neural Networks (CNN)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plan-for-week-44">Plan for week 44</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-sessions-on-tuesday-and-wednesday">Lab  sessions on Tuesday and Wednesday</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#material-for-lecture-monday-october-27">Material for Lecture Monday October 27</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-differential-equations-with-deep-learning">Solving differential equations  with Deep Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-differential-equations-first">Ordinary Differential Equations first</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-trial-solution">The trial solution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimization-process">Minimization process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-the-cost-function-using-gradient-descent-and-automatic-differentiation">Minimizing the cost function using gradient descent and automatic differentiation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-exponential-decay">Example: Exponential decay</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-function-to-solve-for">The function to solve for</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">The trial solution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-of-network">Setup of Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reformulating-the-problem">Reformulating the problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-technicalities">More technicalities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-details">More details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-possible-implementation-of-a-neural-network">A possible implementation of a neural network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#technicalities">Technicalities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-technicalities-i">Final technicalities I</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-technicalities-ii">Final technicalities II</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-technicalities-iii">Final technicalities III</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-technicalities-iv">Final technicalities IV</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation">Back propagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-code-for-solving-the-ode">The code for solving the ODE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-network-with-one-input-layer-specified-number-of-hidden-layers-and-one-output-layer">The network with one input layer, specified number of hidden layers, and one output layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-population-growth">Example: Population growth</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-problem">Setting up the problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">The trial solution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-program-using-autograd">The program using Autograd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-forward-euler-to-solve-the-ode">Using forward Euler to solve the ODE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-solving-the-one-dimensional-poisson-equation">Example: Solving the one dimensional Poisson equation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-specific-equation-to-solve-for">The specific equation to solve for</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-equation-using-autograd">Solving the equation using Autograd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-with-a-numerical-scheme">Comparing with a numerical scheme</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-code">Setting up the code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-differential-equations">Partial Differential Equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#type-of-problem">Type of problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-requirements">Network requirements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">More details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-the-diffusion-equation">Example: The diffusion equation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-problem">Defining the problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-network-using-autograd">Setting up the network using Autograd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-network-using-autograd-the-trial-solution">Setting up the network using Autograd; The trial solution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-the-jacobian">Why the Jacobian?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-network-using-autograd-the-full-program">Setting up the network using Autograd; The full program</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resources-on-differential-equations-and-deep-learning">Resources on differential equations and deep learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-recognizing-images">Convolutional Neural Networks (recognizing images)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-difference">What is the Difference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-vs-cnns">Neural Networks vs CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-cnns-for-images-sound-files-medical-images-from-ct-scans-etc">Why CNNS for images, sound files, medical images from CT scans etc?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regular-nns-dont-scale-well-to-full-images">Regular NNs don’t scale well to full images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-volumes-of-neurons">3D volumes of neurons</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-on-dimensionalities">More on Dimensionalities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-remarks">Further remarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layers-used-to-build-cnns">Layers used to build CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transforming-images">Transforming images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnns-in-brief">CNNs in brief</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-deep-cnn-model-from-raschka-et-al">A deep CNN model (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-idea">Key Idea</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-do-image-compression-before-the-era-of-deep-learning">How to do image compression before the era of deep learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-svd-example">The SVD example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-cnns">Mathematics of CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution-examples-polynomial-multiplication">Convolution Examples: Polynomial multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#efficient-polynomial-multiplication">Efficient Polynomial Multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-simplification">Further simplification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-more-efficient-way-of-coding-the-above-convolution">A more efficient way of coding the above Convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commutative-process">Commutative process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#toeplitz-matrices">Toeplitz matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fourier-series-and-toeplitz-matrices">Fourier series and Toeplitz matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-the-above-one-dimensional-case">Generalizing the above one-dimensional case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-considerations">Memory considerations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#padding">Padding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-vector">New vector</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rewriting-as-dot-products">Rewriting as dot products</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-correlation">Cross correlation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-dimensional-objects">Two-dimensional objects</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnns-in-more-detail-simple-example">CNNs in more detail, simple example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-convolution-stage">The convolution stage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-number-of-parameters">Finding the number of parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-image-or-volume">New image (or volume)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-to-train-common-settings">Parameters to train, common settings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-cnn-setups">Examples of CNN setups</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summarizing-performing-a-general-discrete-convolution-from-raschka-et-al">Summarizing: Performing a general discrete convolution (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling">Pooling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-arithmetic">Pooling arithmetic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-types-from-raschka-et-al">Pooling types (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-convolutional-neural-networks-in-tensorflow-keras-and-pytorch">Building convolutional neural networks in Tensorflow/Keras and PyTorch</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)
doconce format html week44.do.txt --no_mako -->
<!-- dom:TITLE: Week 44,  Solving differential equations with neural networks and start Convolutional Neural Networks (CNN) --><section class="tex2jax_ignore mathjax_ignore" id="week-44-solving-differential-equations-with-neural-networks-and-start-convolutional-neural-networks-cnn">
<h1>Week 44,  Solving differential equations with neural networks and start Convolutional Neural Networks (CNN)<a class="headerlink" href="#week-44-solving-differential-equations-with-neural-networks-and-start-convolutional-neural-networks-cnn" title="Link to this heading">#</a></h1>
<p><strong>Morten Hjorth-Jensen</strong>, Department of Physics, University of Oslo, Norway</p>
<p>Date: <strong>Week 44</strong></p>
<section id="plan-for-week-44">
<h2>Plan for week 44<a class="headerlink" href="#plan-for-week-44" title="Link to this heading">#</a></h2>
<p><strong>Material for the lecture Monday October 27, 2025.</strong></p>
<ol class="arabic simple">
<li><p>Solving differential equations, continuation from last week, first lecture</p></li>
<li><p>Convolutional  Neural Networks, second lecture</p></li>
<li><p>Readings and Videos:</p></li>
</ol>
<ul class="simple">
<li><p>These lecture notes at <a class="github reference external" href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week44/ipynb/week44.ipynb">CompPhysics/MachineLearning</a></p></li>
<li><p>For a more in depth discussion on  neural networks we recommend Goodfellow et al chapter 9. See also chapter 11 and 12 on practicalities and applications</p></li>
<li><p>Reading suggestions for implementation of CNNs see Rashcka et al.’s chapter 14 at <a class="github reference external" href="https://github.com/rasbt/machine-learning-book/tree/main/ch14">rasbt/machine-learning-book</a>.</p></li>
<li><p>Video on Deep Learning at <a class="reference external" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a></p></li>
<li><p>Video  on Convolutional Neural Networks from MIT at <a class="reference external" href="https://www.youtube.com/watch?v=iaSUYvmCekI&amp;amp;ab_channel=AlexanderAmini">https://www.youtube.com/watch?v=iaSUYvmCekI&amp;ab_channel=AlexanderAmini</a></p></li>
<li><p>Video on CNNs from Stanford at <a class="reference external" href="https://www.youtube.com/watch?v=bNb2fEVKeEo&amp;amp;list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&amp;amp;index=6&amp;amp;ab_channel=StanfordUniversitySchoolofEngineering">https://www.youtube.com/watch?v=bNb2fEVKeEo&amp;list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&amp;index=6&amp;ab_channel=StanfordUniversitySchoolofEngineering</a></p></li>
<li><p>Video of lecture October 27 at <a class="reference external" href="https://youtu.be/QqOGhLgkig0">https://youtu.be/QqOGhLgkig0</a></p></li>
<li><p>Whiteboard notes at <a class="github reference external" href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2025/FYSSTKweek44">CompPhysics/MachineLearning</a></p></li>
</ul>
</section>
<section id="lab-sessions-on-tuesday-and-wednesday">
<h2>Lab  sessions on Tuesday and Wednesday<a class="headerlink" href="#lab-sessions-on-tuesday-and-wednesday" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Main focus is discussion of and work on project 2</p></li>
<li><p>If you did not get time to finish the exercises from weeks 41-42, you can also keep working on them and hand in this coming Friday</p></li>
</ul>
</section>
<section id="material-for-lecture-monday-october-27">
<h2>Material for Lecture Monday October 27<a class="headerlink" href="#material-for-lecture-monday-october-27" title="Link to this heading">#</a></h2>
</section>
<section id="solving-differential-equations-with-deep-learning">
<h2>Solving differential equations  with Deep Learning<a class="headerlink" href="#solving-differential-equations-with-deep-learning" title="Link to this heading">#</a></h2>
<p>The Universal Approximation Theorem states that a neural network can
approximate any function at a single hidden layer along with one input
and output layer to any given precision.</p>
<p><strong>Book on solving differential equations with ML methods.</strong></p>
<p><a class="reference external" href="https://www.springer.com/gp/book/9789401798150">An Introduction to Neural Network Methods for Differential Equations</a>, by Yadav and Kumar.</p>
<p><strong>Physics informed neural networks.</strong></p>
<p><a class="reference external" href="https://link.springer.com/article/10.1007/s10915-022-01939-z">Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next</a>, by Cuomo et al</p>
<p><strong>Thanks to Kristine Baluka Hein.</strong></p>
<p>The lectures on differential equations were developed by Kristine Baluka Hein, now PhD student at IFI.
A great thanks to Kristine.</p>
</section>
<section id="ordinary-differential-equations-first">
<h2>Ordinary Differential Equations first<a class="headerlink" href="#ordinary-differential-equations-first" title="Link to this heading">#</a></h2>
<p>An ordinary differential equation (ODE) is an equation involving functions having one variable.</p>
<p>In general, an ordinary differential equation looks like</p>
<!-- Equation labels as ordinary links -->
<div id="ode"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{ode} \tag{1}
f\left(x, \, g(x), \, g'(x), \, g''(x), \, \dots \, , \, g^{(n)}(x)\right) = 0
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(g(x)\)</span> is the function to find, and <span class="math notranslate nohighlight">\(g^{(n)}(x)\)</span> is the <span class="math notranslate nohighlight">\(n\)</span>-th derivative of <span class="math notranslate nohighlight">\(g(x)\)</span>.</p>
<p>The <span class="math notranslate nohighlight">\(f\left(x, g(x), g'(x), g''(x), \, \dots \, , g^{(n)}(x)\right)\)</span> is just a way to write that there is an expression involving <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(g(x), \ g'(x), \ g''(x), \, \dots \, , \text{ and } g^{(n)}(x)\)</span> on the left side of the equality sign in (<a class="reference internal" href="#ode"><span class="xref myst">1</span></a>).
The highest order of derivative, that is the value of <span class="math notranslate nohighlight">\(n\)</span>, determines to the order of the equation.
The equation is referred to as a <span class="math notranslate nohighlight">\(n\)</span>-th order ODE.
Along with (<a class="reference internal" href="#ode"><span class="xref myst">1</span></a>), some additional conditions of the function <span class="math notranslate nohighlight">\(g(x)\)</span> are typically given
for the solution to be unique.</p>
</section>
<section id="the-trial-solution">
<h2>The trial solution<a class="headerlink" href="#the-trial-solution" title="Link to this heading">#</a></h2>
<p>Let the trial solution <span class="math notranslate nohighlight">\(g_t(x)\)</span> be</p>
<!-- Equation labels as ordinary links -->
<div id="_auto1"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
	g_t(x) = h_1(x) + h_2(x,N(x,P))
\label{_auto1} \tag{2}
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(h_1(x)\)</span> is a function that makes <span class="math notranslate nohighlight">\(g_t(x)\)</span> satisfy a given set
of conditions, <span class="math notranslate nohighlight">\(N(x,P)\)</span> a neural network with weights and biases
described by <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(h_2(x, N(x,P))\)</span> some expression involving the
neural network.  The role of the function <span class="math notranslate nohighlight">\(h_2(x, N(x,P))\)</span>, is to
ensure that the output from <span class="math notranslate nohighlight">\(N(x,P)\)</span> is zero when <span class="math notranslate nohighlight">\(g_t(x)\)</span> is
evaluated at the values of <span class="math notranslate nohighlight">\(x\)</span> where the given conditions must be
satisfied.  The function <span class="math notranslate nohighlight">\(h_1(x)\)</span> should alone make <span class="math notranslate nohighlight">\(g_t(x)\)</span> satisfy
the conditions.</p>
<p>But what about the network <span class="math notranslate nohighlight">\(N(x,P)\)</span>?</p>
<p>As described previously, an optimization method could be used to minimize the parameters of a neural network, that being its weights and biases, through backward propagation.</p>
</section>
<section id="minimization-process">
<h2>Minimization process<a class="headerlink" href="#minimization-process" title="Link to this heading">#</a></h2>
<p>For the minimization to be defined, we need to have a cost function at hand to minimize.</p>
<p>It is given that <span class="math notranslate nohighlight">\(f\left(x, \, g(x), \, g'(x), \, g''(x), \, \dots \, , \, g^{(n)}(x)\right)\)</span> should be equal to zero in (<a class="reference internal" href="#ode"><span class="xref myst">1</span></a>).
We can choose to consider the mean squared error as the cost function for an input <span class="math notranslate nohighlight">\(x\)</span>.
Since we are looking at one input, the cost function is just <span class="math notranslate nohighlight">\(f\)</span> squared.
The cost function <span class="math notranslate nohighlight">\(c\left(x, P \right)\)</span> can therefore be expressed as</p>
<div class="math notranslate nohighlight">
\[
C\left(x, P\right) = \big(f\left(x, \, g(x), \, g'(x), \, g''(x), \, \dots \, , \, g^{(n)}(x)\right)\big)^2
\]</div>
<p>If <span class="math notranslate nohighlight">\(N\)</span> inputs are given as a vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> with elements <span class="math notranslate nohighlight">\(x_i\)</span> for <span class="math notranslate nohighlight">\(i = 1,\dots,N\)</span>,
the cost function becomes</p>
<!-- Equation labels as ordinary links -->
<div id="cost"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{cost} \tag{3}
	C\left(\boldsymbol{x}, P\right) = \frac{1}{N} \sum_{i=1}^N \big(f\left(x_i, \, g(x_i), \, g'(x_i), \, g''(x_i), \, \dots \, , \, g^{(n)}(x_i)\right)\big)^2
\end{equation}
\]</div>
<p>The neural net should then find the parameters <span class="math notranslate nohighlight">\(P\)</span> that minimizes the cost function in
(<a class="reference internal" href="#cost"><span class="xref myst">3</span></a>) for a set of <span class="math notranslate nohighlight">\(N\)</span> training samples <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
</section>
<section id="minimizing-the-cost-function-using-gradient-descent-and-automatic-differentiation">
<h2>Minimizing the cost function using gradient descent and automatic differentiation<a class="headerlink" href="#minimizing-the-cost-function-using-gradient-descent-and-automatic-differentiation" title="Link to this heading">#</a></h2>
<p>To perform the minimization using gradient descent, the gradient of <span class="math notranslate nohighlight">\(C\left(\boldsymbol{x}, P\right)\)</span> is needed.
It might happen so that finding an analytical expression of the gradient of <span class="math notranslate nohighlight">\(C(\boldsymbol{x}, P)\)</span> from (<a class="reference internal" href="#cost"><span class="xref myst">3</span></a>) gets too messy, depending on which cost function one desires to use.</p>
<p>Luckily, there exists libraries that makes the job for us through automatic differentiation.
Automatic differentiation is a method of finding the derivatives numerically with very high precision.</p>
</section>
<section id="example-exponential-decay">
<h2>Example: Exponential decay<a class="headerlink" href="#example-exponential-decay" title="Link to this heading">#</a></h2>
<p>An exponential decay of a quantity <span class="math notranslate nohighlight">\(g(x)\)</span> is described by the equation</p>
<!-- Equation labels as ordinary links -->
<div id="solve_expdec"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{solve_expdec} \tag{4}
  g'(x) = -\gamma g(x)
\end{equation}
\]</div>
<p>with <span class="math notranslate nohighlight">\(g(0) = g_0\)</span> for some chosen initial value <span class="math notranslate nohighlight">\(g_0\)</span>.</p>
<p>The analytical solution of (<a class="reference internal" href="#solve_expdec"><span class="xref myst">4</span></a>) is</p>
<!-- Equation labels as ordinary links -->
<div id="_auto2"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
  g(x) = g_0 \exp\left(-\gamma x\right)
\label{_auto2} \tag{5}
\end{equation}
\]</div>
<p>Having an analytical solution at hand, it is possible to use it to compare how well a neural network finds a solution of (<a class="reference internal" href="#solve_expdec"><span class="xref myst">4</span></a>).</p>
</section>
<section id="the-function-to-solve-for">
<h2>The function to solve for<a class="headerlink" href="#the-function-to-solve-for" title="Link to this heading">#</a></h2>
<p>The program will use a neural network to solve</p>
<!-- Equation labels as ordinary links -->
<div id="solveode"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{solveode} \tag{6}
g'(x) = -\gamma g(x)
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(g(0) = g_0\)</span> with <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(g_0\)</span> being some chosen values.</p>
<p>In this example, <span class="math notranslate nohighlight">\(\gamma = 2\)</span> and <span class="math notranslate nohighlight">\(g_0 = 10\)</span>.</p>
</section>
<section id="id1">
<h2>The trial solution<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>To begin with, a trial solution <span class="math notranslate nohighlight">\(g_t(t)\)</span> must be chosen. A general trial solution for ordinary differential equations could be</p>
<div class="math notranslate nohighlight">
\[
g_t(x, P) = h_1(x) + h_2(x, N(x, P))
\]</div>
<p>with <span class="math notranslate nohighlight">\(h_1(x)\)</span> ensuring that <span class="math notranslate nohighlight">\(g_t(x)\)</span> satisfies some conditions and <span class="math notranslate nohighlight">\(h_2(x,N(x, P))\)</span> an expression involving <span class="math notranslate nohighlight">\(x\)</span> and the output from the neural network <span class="math notranslate nohighlight">\(N(x,P)\)</span> with <span class="math notranslate nohighlight">\(P \)</span> being the collection of the weights and biases for each layer. For now, it is assumed that the network consists of one input layer, one hidden layer, and one output layer.</p>
</section>
<section id="setup-of-network">
<h2>Setup of Network<a class="headerlink" href="#setup-of-network" title="Link to this heading">#</a></h2>
<p>In this network, there are no weights and bias at the input layer, so <span class="math notranslate nohighlight">\(P = \{ P_{\text{hidden}},  P_{\text{output}} \}\)</span>.
If there are <span class="math notranslate nohighlight">\(N_{\text{hidden} }\)</span> neurons in the hidden layer, then <span class="math notranslate nohighlight">\(P_{\text{hidden}}\)</span> is a <span class="math notranslate nohighlight">\(N_{\text{hidden} } \times (1 + N_{\text{input}})\)</span> matrix, given that there are <span class="math notranslate nohighlight">\(N_{\text{input}}\)</span> neurons in the input layer.</p>
<p>The first column in <span class="math notranslate nohighlight">\(P_{\text{hidden} }\)</span> represents the bias for each neuron in the hidden layer and the second column represents the weights for each neuron in the hidden layer from the input layer.
If there are <span class="math notranslate nohighlight">\(N_{\text{output} }\)</span> neurons in the output layer, then <span class="math notranslate nohighlight">\(P_{\text{output}} \)</span> is a <span class="math notranslate nohighlight">\(N_{\text{output} } \times (1 + N_{\text{hidden} })\)</span> matrix.</p>
<p>Its first column represents the bias of each neuron and the remaining columns represents the weights to each neuron.</p>
<p>It is given that <span class="math notranslate nohighlight">\(g(0) = g_0\)</span>. The trial solution must fulfill this condition to be a proper solution of (<a class="reference internal" href="#solveode"><span class="xref myst">6</span></a>). A possible way to ensure that <span class="math notranslate nohighlight">\(g_t(0, P) = g_0\)</span>, is to let <span class="math notranslate nohighlight">\(F(N(x,P)) = x \cdot N(x,P)\)</span> and <span class="math notranslate nohighlight">\(h_1(x) = g_0\)</span>. This gives the following trial solution:</p>
<!-- Equation labels as ordinary links -->
<div id="trial"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{trial} \tag{7}
g_t(x, P) = g_0 + x \cdot N(x, P)
\end{equation}
\]</div>
</section>
<section id="reformulating-the-problem">
<h2>Reformulating the problem<a class="headerlink" href="#reformulating-the-problem" title="Link to this heading">#</a></h2>
<p>We wish that our neural network manages to minimize a given cost function.</p>
<p>A reformulation of out equation, (<a class="reference internal" href="#solveode"><span class="xref myst">6</span></a>), must therefore be done,
such that it describes the problem a neural network can solve for.</p>
<p>The neural network must find the set of weights and biases <span class="math notranslate nohighlight">\(P\)</span> such that the trial solution in (<a class="reference internal" href="#trial"><span class="xref myst">7</span></a>) satisfies (<a class="reference internal" href="#solveode"><span class="xref myst">6</span></a>).</p>
<p>The trial solution</p>
<div class="math notranslate nohighlight">
\[
g_t(x, P) = g_0 + x \cdot N(x, P)
\]</div>
<p>has been chosen such that it already solves the condition <span class="math notranslate nohighlight">\(g(0) = g_0\)</span>. What remains, is to find <span class="math notranslate nohighlight">\(P\)</span> such that</p>
<!-- Equation labels as ordinary links -->
<div id="nnmin"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{nnmin} \tag{8}
g_t'(x, P) = - \gamma g_t(x, P)
\end{equation}
\]</div>
<p>is fulfilled as <em>best as possible</em>.</p>
</section>
<section id="more-technicalities">
<h2>More technicalities<a class="headerlink" href="#more-technicalities" title="Link to this heading">#</a></h2>
<p>The left hand side and right hand side of (<a class="reference internal" href="#nnmin"><span class="xref myst">8</span></a>) must be computed separately, and then the neural network must choose weights and biases, contained in <span class="math notranslate nohighlight">\(P\)</span>, such that the sides are equal as best as possible.
This means that the absolute or squared difference between the sides must be as close to zero, ideally equal to zero.
In this case, the difference squared shows to be an appropriate measurement of how erroneous the trial solution is with respect to <span class="math notranslate nohighlight">\(P\)</span> of the neural network.</p>
<p>This gives the following cost function our neural network must solve for:</p>
<div class="math notranslate nohighlight">
\[
\min_{P}\Big\{ \big(g_t'(x, P) - ( -\gamma g_t(x, P) \big)^2 \Big\}
\]</div>
<p>(the notation <span class="math notranslate nohighlight">\(\min_{P}\{ f(x, P) \}\)</span> means that we desire to find <span class="math notranslate nohighlight">\(P\)</span> that yields the minimum of <span class="math notranslate nohighlight">\(f(x, P)\)</span>)</p>
<p>or, in terms of weights and biases for the hidden and output layer in our network:</p>
<div class="math notranslate nohighlight">
\[
\min_{P_{\text{hidden} }, \ P_{\text{output} }}\Big\{ \big(g_t'(x, \{ P_{\text{hidden} }, P_{\text{output} }\}) - ( -\gamma g_t(x, \{ P_{\text{hidden} }, P_{\text{output} }\}) \big)^2 \Big\}
\]</div>
<p>for an input value <span class="math notranslate nohighlight">\(x\)</span>.</p>
</section>
<section id="more-details">
<h2>More details<a class="headerlink" href="#more-details" title="Link to this heading">#</a></h2>
<p>If the neural network evaluates <span class="math notranslate nohighlight">\(g_t(x, P)\)</span> at more values for <span class="math notranslate nohighlight">\(x\)</span>, say <span class="math notranslate nohighlight">\(N\)</span> values <span class="math notranslate nohighlight">\(x_i\)</span> for <span class="math notranslate nohighlight">\(i = 1, \dots, N\)</span>, then the <em>total</em> error to minimize becomes</p>
<!-- Equation labels as ordinary links -->
<div id="min"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{min} \tag{9}
\min_{P}\Big\{\frac{1}{N} \sum_{i=1}^N  \big(g_t'(x_i, P) - ( -\gamma g_t(x_i, P) \big)^2 \Big\}
\end{equation}
\]</div>
<p>Letting <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> be a vector with elements <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(C(\boldsymbol{x}, P) = \frac{1}{N} \sum_i  \big(g_t'(x_i, P) - ( -\gamma g_t(x_i, P) \big)^2\)</span> denote the cost function, the minimization problem that our network must solve, becomes</p>
<div class="math notranslate nohighlight">
\[
\min_{P} C(\boldsymbol{x}, P)
\]</div>
<p>In terms of <span class="math notranslate nohighlight">\(P_{\text{hidden} }\)</span> and <span class="math notranslate nohighlight">\(P_{\text{output} }\)</span>, this could also be expressed as</p>
<div class="math notranslate nohighlight">
\[
\min_{P_{\text{hidden} }, \ P_{\text{output} }} C(\boldsymbol{x}, \{P_{\text{hidden} }, P_{\text{output} }\})
\]</div>
</section>
<section id="a-possible-implementation-of-a-neural-network">
<h2>A possible implementation of a neural network<a class="headerlink" href="#a-possible-implementation-of-a-neural-network" title="Link to this heading">#</a></h2>
<p>For simplicity, it is assumed that the input is an array <span class="math notranslate nohighlight">\(\boldsymbol{x} = (x_1, \dots, x_N)\)</span> with <span class="math notranslate nohighlight">\(N\)</span> elements. It is at these points the neural network should find <span class="math notranslate nohighlight">\(P\)</span> such that it fulfills (<a class="reference internal" href="#min"><span class="xref myst">9</span></a>).</p>
<p>First, the neural network must feed forward the inputs.
This means that <span class="math notranslate nohighlight">\(\boldsymbol{x}s\)</span> must be passed through an input layer, a hidden layer and a output layer. The input layer in this case, does not need to process the data any further.
The input layer will consist of <span class="math notranslate nohighlight">\(N_{\text{input} }\)</span> neurons, passing its element to each neuron in the hidden layer.  The number of neurons in the hidden layer will be <span class="math notranslate nohighlight">\(N_{\text{hidden} }\)</span>.</p>
</section>
<section id="technicalities">
<h2>Technicalities<a class="headerlink" href="#technicalities" title="Link to this heading">#</a></h2>
<p>For the <span class="math notranslate nohighlight">\(i\)</span>-th in the hidden layer with weight <span class="math notranslate nohighlight">\(w_i^{\text{hidden} }\)</span> and bias <span class="math notranslate nohighlight">\(b_i^{\text{hidden} }\)</span>, the weighting from the <span class="math notranslate nohighlight">\(j\)</span>-th neuron at the input layer is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
z_{i,j}^{\text{hidden}} &amp;= b_i^{\text{hidden}} + w_i^{\text{hidden}}x_j \\
&amp;=
\begin{pmatrix}
b_i^{\text{hidden}} &amp; w_i^{\text{hidden}}
\end{pmatrix}
\begin{pmatrix}
1 \\
x_j
\end{pmatrix}
\end{aligned}
\end{split}\]</div>
</section>
<section id="final-technicalities-i">
<h2>Final technicalities I<a class="headerlink" href="#final-technicalities-i" title="Link to this heading">#</a></h2>
<p>The result after weighting the inputs at the <span class="math notranslate nohighlight">\(i\)</span>-th hidden neuron can be written as a vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{z}_{i}^{\text{hidden}} &amp;= \Big( b_i^{\text{hidden}} + w_i^{\text{hidden}}x_1 , \ b_i^{\text{hidden}} + w_i^{\text{hidden}} x_2, \ \dots \, , \ b_i^{\text{hidden}} + w_i^{\text{hidden}} x_N\Big)  \\
&amp;=
\begin{pmatrix}
 b_i^{\text{hidden}}  &amp; w_i^{\text{hidden}}
\end{pmatrix}
\begin{pmatrix}
1  &amp; 1 &amp; \dots &amp; 1 \\
x_1 &amp; x_2 &amp; \dots &amp; x_N
\end{pmatrix} \\
&amp;= \boldsymbol{p}_{i, \text{hidden}}^T X
\end{aligned}
\end{split}\]</div>
</section>
<section id="final-technicalities-ii">
<h2>Final technicalities II<a class="headerlink" href="#final-technicalities-ii" title="Link to this heading">#</a></h2>
<p>The vector <span class="math notranslate nohighlight">\(\boldsymbol{p}_{i, \text{hidden}}^T\)</span> constitutes each row in <span class="math notranslate nohighlight">\(P_{\text{hidden} }\)</span>, which contains the weights for the neural network to minimize according to (<a class="reference internal" href="#min"><span class="xref myst">9</span></a>).</p>
<p>After having found <span class="math notranslate nohighlight">\(\boldsymbol{z}_{i}^{\text{hidden}} \)</span> for every <span class="math notranslate nohighlight">\(i\)</span>-th neuron within the hidden layer, the vector will be sent to an activation function <span class="math notranslate nohighlight">\(a_i(\boldsymbol{z})\)</span>.</p>
<p>In this example, the sigmoid function has been chosen to be the activation function for each hidden neuron:</p>
<div class="math notranslate nohighlight">
\[
f(z) = \frac{1}{1 + \exp{(-z)}}
\]</div>
<p>It is possible to use other activations functions for the hidden layer also.</p>
<p>The output <span class="math notranslate nohighlight">\(\boldsymbol{x}_i^{\text{hidden}}\)</span> from each <span class="math notranslate nohighlight">\(i\)</span>-th hidden neuron is:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x}_i^{\text{hidden} } = f\big(  \boldsymbol{z}_{i}^{\text{hidden}} \big)
\]</div>
<p>The outputs <span class="math notranslate nohighlight">\(\boldsymbol{x}_i^{\text{hidden} } \)</span> are then sent to the output layer.</p>
<p>The output layer consists of one neuron in this case, and combines the
output from each of the neurons in the hidden layers. The output layer
combines the results from the hidden layer using some weights <span class="math notranslate nohighlight">\(w_i^{\text{output}}\)</span>
and biases <span class="math notranslate nohighlight">\(b_i^{\text{output}}\)</span>. In this case,
it is assumes that the number of neurons in the output layer is one.</p>
</section>
<section id="final-technicalities-iii">
<h2>Final technicalities III<a class="headerlink" href="#final-technicalities-iii" title="Link to this heading">#</a></h2>
<p>The procedure of weighting the output neuron <span class="math notranslate nohighlight">\(j\)</span> in the hidden layer to the <span class="math notranslate nohighlight">\(i\)</span>-th neuron in the output layer is similar as for the hidden layer described previously.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
z_{1,j}^{\text{output}} &amp; =
\begin{pmatrix}
b_1^{\text{output}} &amp; \boldsymbol{w}_1^{\text{output}}
\end{pmatrix}
\begin{pmatrix}
1 \\
\boldsymbol{x}_j^{\text{hidden}}
\end{pmatrix}
\end{aligned}
\end{split}\]</div>
</section>
<section id="final-technicalities-iv">
<h2>Final technicalities IV<a class="headerlink" href="#final-technicalities-iv" title="Link to this heading">#</a></h2>
<p>Expressing <span class="math notranslate nohighlight">\(z_{1,j}^{\text{output}}\)</span> as a vector gives the following way of weighting the inputs from the hidden layer:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{z}_{1}^{\text{output}} =
\begin{pmatrix}
b_1^{\text{output}} &amp; \boldsymbol{w}_1^{\text{output}}
\end{pmatrix}
\begin{pmatrix}
1  &amp; 1 &amp; \dots &amp; 1 \\
\boldsymbol{x}_1^{\text{hidden}} &amp; \boldsymbol{x}_2^{\text{hidden}} &amp; \dots &amp; \boldsymbol{x}_N^{\text{hidden}}
\end{pmatrix}
\end{split}\]</div>
<p>In this case we seek a continuous range of values since we are approximating a function. This means that after computing <span class="math notranslate nohighlight">\(\boldsymbol{z}_{1}^{\text{output}}\)</span> the neural network has finished its feed forward step, and <span class="math notranslate nohighlight">\(\boldsymbol{z}_{1}^{\text{output}}\)</span> is the final output of the network.</p>
</section>
<section id="back-propagation">
<h2>Back propagation<a class="headerlink" href="#back-propagation" title="Link to this heading">#</a></h2>
<p>The next step is to decide how the parameters should be changed such that they minimize the cost function.</p>
<p>The chosen cost function for this problem is</p>
<div class="math notranslate nohighlight">
\[
C(\boldsymbol{x}, P) = \frac{1}{N} \sum_i  \big(g_t'(x_i, P) - ( -\gamma g_t(x_i, P) \big)^2
\]</div>
<p>In order to minimize the cost function, an optimization method must be chosen.</p>
<p>Here, gradient descent with a constant step size has been chosen.</p>
</section>
<section id="gradient-descent">
<h2>Gradient descent<a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h2>
<p>The idea of the gradient descent algorithm is to update parameters in
a direction where the cost function decreases goes to a minimum.</p>
<p>In general, the update of some parameters <span class="math notranslate nohighlight">\(\boldsymbol{\omega}\)</span> given a cost
function defined by some weights <span class="math notranslate nohighlight">\(\boldsymbol{\omega}\)</span>, <span class="math notranslate nohighlight">\(C(\boldsymbol{x},
\boldsymbol{\omega})\)</span>, goes as follows:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\omega}_{\text{new} } = \boldsymbol{\omega} - \lambda \nabla_{\boldsymbol{\omega}} C(\boldsymbol{x}, \boldsymbol{\omega})
\]</div>
<p>for a number of iterations or until <span class="math notranslate nohighlight">\( \big|\big| \boldsymbol{\omega}_{\text{new} } - \boldsymbol{\omega} \big|\big|\)</span> becomes smaller than some given tolerance.</p>
<p>The value of <span class="math notranslate nohighlight">\(\lambda\)</span> decides how large steps the algorithm must take
in the direction of <span class="math notranslate nohighlight">\( \nabla_{\boldsymbol{\omega}} C(\boldsymbol{x}, \boldsymbol{\omega})\)</span>.
The notation <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{\omega}}\)</span> express the gradient with respect
to the elements in <span class="math notranslate nohighlight">\(\boldsymbol{\omega}\)</span>.</p>
<p>In our case, we have to minimize the cost function <span class="math notranslate nohighlight">\(C(\boldsymbol{x}, P)\)</span> with
respect to the two sets of weights and biases, that is for the hidden
layer <span class="math notranslate nohighlight">\(P_{\text{hidden} }\)</span> and for the output layer <span class="math notranslate nohighlight">\(P_{\text{output}
}\)</span> .</p>
<p>This means that <span class="math notranslate nohighlight">\(P_{\text{hidden} }\)</span> and <span class="math notranslate nohighlight">\(P_{\text{output} }\)</span> is updated by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
P_{\text{hidden},\text{new}} &amp;= P_{\text{hidden}} - \lambda \nabla_{P_{\text{hidden}}} C(\boldsymbol{x}, P)  \\
P_{\text{output},\text{new}} &amp;= P_{\text{output}} - \lambda \nabla_{P_{\text{output}}} C(\boldsymbol{x}, P)
\end{aligned}
\end{split}\]</div>
</section>
<section id="the-code-for-solving-the-ode">
<h2>The code for solving the ODE<a class="headerlink" href="#the-code-for-solving-the-ode" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>%matplotlib inline

import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

def sigmoid(z):
    return 1/(1 + np.exp(-z))

# Assuming one input, hidden, and output layer
def neural_network(params, x):

    # Find the weights (including and biases) for the hidden and output layer.
    # Assume that params is a list of parameters for each layer.
    # The biases are the first element for each array in params,
    # and the weights are the remaning elements in each array in params.

    w_hidden = params[0]
    w_output = params[1]

    # Assumes input x being an one-dimensional array
    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    # Assume that the input layer does nothing to the input x
    x_input = x

    ## Hidden layer:

    # Add a row of ones to include bias
    x_input = np.concatenate((np.ones((1,num_values)), x_input ), axis = 0)

    z_hidden = np.matmul(w_hidden, x_input)
    x_hidden = sigmoid(z_hidden)

    ## Output layer:

    # Include bias:
    x_hidden = np.concatenate((np.ones((1,num_values)), x_hidden ), axis = 0)

    z_output = np.matmul(w_output, x_hidden)
    x_output = z_output

    return x_output

# The trial solution using the deep neural network:
def g_trial(x,params, g0 = 10):
    return g0 + x*neural_network(params,x)

# The right side of the ODE:
def g(x, g_trial, gamma = 2):
    return -gamma*g_trial

# The cost function:
def cost_function(P, x):

    # Evaluate the trial function with the current parameters P
    g_t = g_trial(x,P)

    # Find the derivative w.r.t x of the neural network
    d_net_out = elementwise_grad(neural_network,1)(P,x)

    # Find the derivative w.r.t x of the trial function
    d_g_t = elementwise_grad(g_trial,0)(x,P)

    # The right side of the ODE
    func = g(x, g_t)

    err_sqr = (d_g_t - func)**2
    cost_sum = np.sum(err_sqr)

    return cost_sum / np.size(err_sqr)

# Solve the exponential decay ODE using neural network with one input, hidden, and output layer
def solve_ode_neural_network(x, num_neurons_hidden, num_iter, lmb):
    ## Set up initial weights and biases

    # For the hidden layer
    p0 = npr.randn(num_neurons_hidden, 2 )

    # For the output layer
    p1 = npr.randn(1, num_neurons_hidden + 1 ) # +1 since bias is included

    P = [p0, p1]

    print(&#39;Initial cost: %g&#39;%cost_function(P, x))

    ## Start finding the optimal weights using gradient descent

    # Find the Python function that represents the gradient of the cost function
    # w.r.t the 0-th input argument -- that is the weights and biases in the hidden and output layer
    cost_function_grad = grad(cost_function,0)

    # Let the update be done num_iter times
    for i in range(num_iter):
        # Evaluate the gradient at the current weights and biases in P.
        # The cost_grad consist now of two arrays;
        # one for the gradient w.r.t P_hidden and
        # one for the gradient w.r.t P_output
        cost_grad =  cost_function_grad(P, x)

        P[0] = P[0] - lmb * cost_grad[0]
        P[1] = P[1] - lmb * cost_grad[1]

    print(&#39;Final cost: %g&#39;%cost_function(P, x))

    return P

def g_analytic(x, gamma = 2, g0 = 10):
    return g0*np.exp(-gamma*x)

# Solve the given problem
if __name__ == &#39;__main__&#39;:
    # Set seed such that the weight are initialized
    # with same weights and biases for every run.
    npr.seed(15)

    ## Decide the vales of arguments to the function to solve
    N = 10
    x = np.linspace(0, 1, N)

    ## Set up the initial parameters
    num_hidden_neurons = 10
    num_iter = 10000
    lmb = 0.001

    # Use the network
    P = solve_ode_neural_network(x, num_hidden_neurons, num_iter, lmb)

    # Print the deviation from the trial solution and true solution
    res = g_trial(x,P)
    res_analytical = g_analytic(x)

    print(&#39;Max absolute difference: %g&#39;%np.max(np.abs(res - res_analytical)))

    # Plot the results
    plt.figure(figsize=(10,10))

    plt.title(&#39;Performance of neural network solving an ODE compared to the analytical solution&#39;)
    plt.plot(x, res_analytical)
    plt.plot(x, res[0,:])
    plt.legend([&#39;analytical&#39;,&#39;nn&#39;])
    plt.xlabel(&#39;x&#39;)
    plt.ylabel(&#39;g(x)&#39;)
    plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-network-with-one-input-layer-specified-number-of-hidden-layers-and-one-output-layer">
<h2>The network with one input layer, specified number of hidden layers, and one output layer<a class="headerlink" href="#the-network-with-one-input-layer-specified-number-of-hidden-layers-and-one-output-layer" title="Link to this heading">#</a></h2>
<p>It is also possible to extend the construction of our network into a more general one, allowing the network to contain more than one hidden layers.</p>
<p>The number of neurons within each hidden layer are given as a list of integers in the program below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

def sigmoid(z):
    return 1/(1 + np.exp(-z))

# The neural network with one input layer and one output layer,
# but with number of hidden layers specified by the user.
def deep_neural_network(deep_params, x):
    # N_hidden is the number of hidden layers
    # deep_params is a list, len() should be used
    N_hidden = len(deep_params) - 1 # -1 since params consists of
                                        # parameters to all the hidden
                                        # layers AND the output layer.

    # Assumes input x being an one-dimensional array
    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    # Assume that the input layer does nothing to the input x
    x_input = x

    # Due to multiple hidden layers, define a variable referencing to the
    # output of the previous layer:
    x_prev = x_input

    ## Hidden layers:

    for l in range(N_hidden):
        # From the list of parameters P; find the correct weigths and bias for this layer
        w_hidden = deep_params[l]

        # Add a row of ones to include bias
        x_prev = np.concatenate((np.ones((1,num_values)), x_prev ), axis = 0)

        z_hidden = np.matmul(w_hidden, x_prev)
        x_hidden = sigmoid(z_hidden)

        # Update x_prev such that next layer can use the output from this layer
        x_prev = x_hidden

    ## Output layer:

    # Get the weights and bias for this layer
    w_output = deep_params[-1]

    # Include bias:
    x_prev = np.concatenate((np.ones((1,num_values)), x_prev), axis = 0)

    z_output = np.matmul(w_output, x_prev)
    x_output = z_output

    return x_output

# The trial solution using the deep neural network:
def g_trial_deep(x,params, g0 = 10):
    return g0 + x*deep_neural_network(params, x)

# The right side of the ODE:
def g(x, g_trial, gamma = 2):
    return -gamma*g_trial

# The same cost function as before, but calls deep_neural_network instead.
def cost_function_deep(P, x):

    # Evaluate the trial function with the current parameters P
    g_t = g_trial_deep(x,P)

    # Find the derivative w.r.t x of the neural network
    d_net_out = elementwise_grad(deep_neural_network,1)(P,x)

    # Find the derivative w.r.t x of the trial function
    d_g_t = elementwise_grad(g_trial_deep,0)(x,P)

    # The right side of the ODE
    func = g(x, g_t)

    err_sqr = (d_g_t - func)**2
    cost_sum = np.sum(err_sqr)

    return cost_sum / np.size(err_sqr)

# Solve the exponential decay ODE using neural network with one input and one output layer,
# but with specified number of hidden layers from the user.
def solve_ode_deep_neural_network(x, num_neurons, num_iter, lmb):
    # num_hidden_neurons is now a list of number of neurons within each hidden layer

    # The number of elements in the list num_hidden_neurons thus represents
    # the number of hidden layers.

    # Find the number of hidden layers:
    N_hidden = np.size(num_neurons)

    ## Set up initial weights and biases

    # Initialize the list of parameters:
    P = [None]*(N_hidden + 1) # + 1 to include the output layer

    P[0] = npr.randn(num_neurons[0], 2 )
    for l in range(1,N_hidden):
        P[l] = npr.randn(num_neurons[l], num_neurons[l-1] + 1) # +1 to include bias

    # For the output layer
    P[-1] = npr.randn(1, num_neurons[-1] + 1 ) # +1 since bias is included

    print(&#39;Initial cost: %g&#39;%cost_function_deep(P, x))

    ## Start finding the optimal weights using gradient descent

    # Find the Python function that represents the gradient of the cost function
    # w.r.t the 0-th input argument -- that is the weights and biases in the hidden and output layer
    cost_function_deep_grad = grad(cost_function_deep,0)

    # Let the update be done num_iter times
    for i in range(num_iter):
        # Evaluate the gradient at the current weights and biases in P.
        # The cost_grad consist now of N_hidden + 1 arrays; the gradient w.r.t the weights and biases
        # in the hidden layers and output layers evaluated at x.
        cost_deep_grad =  cost_function_deep_grad(P, x)

        for l in range(N_hidden+1):
            P[l] = P[l] - lmb * cost_deep_grad[l]

    print(&#39;Final cost: %g&#39;%cost_function_deep(P, x))

    return P

def g_analytic(x, gamma = 2, g0 = 10):
    return g0*np.exp(-gamma*x)

# Solve the given problem
if __name__ == &#39;__main__&#39;:
    npr.seed(15)

    ## Decide the vales of arguments to the function to solve
    N = 10
    x = np.linspace(0, 1, N)

    ## Set up the initial parameters
    num_hidden_neurons = np.array([10,10])
    num_iter = 10000
    lmb = 0.001

    P = solve_ode_deep_neural_network(x, num_hidden_neurons, num_iter, lmb)

    res = g_trial_deep(x,P)
    res_analytical = g_analytic(x)

    plt.figure(figsize=(10,10))

    plt.title(&#39;Performance of a deep neural network solving an ODE compared to the analytical solution&#39;)
    plt.plot(x, res_analytical)
    plt.plot(x, res[0,:])
    plt.legend([&#39;analytical&#39;,&#39;dnn&#39;])
    plt.ylabel(&#39;g(x)&#39;)
    plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="example-population-growth">
<h2>Example: Population growth<a class="headerlink" href="#example-population-growth" title="Link to this heading">#</a></h2>
<p>A logistic model of population growth assumes that a population converges toward an equilibrium.
The population growth can be modeled by</p>
<!-- Equation labels as ordinary links -->
<div id="log"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{log} \tag{10}
	g'(t) = \alpha g(t)(A - g(t))
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(g(t)\)</span> is the population density at time <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> the growth rate and <span class="math notranslate nohighlight">\(A &gt; 0\)</span> is the maximum population number in the environment.
Also, at <span class="math notranslate nohighlight">\(t = 0\)</span> the population has the size <span class="math notranslate nohighlight">\(g(0) = g_0\)</span>, where <span class="math notranslate nohighlight">\(g_0\)</span> is some chosen constant.</p>
<p>In this example, similar network as for the exponential decay using Autograd has been used to solve the equation. However, as the implementation might suffer from e.g numerical instability
and high execution time (this might be more apparent in the examples solving PDEs),
using a library like  TensorFlow is recommended.
Here, we stay with a more simple approach and implement for comparison, the simple forward Euler method.</p>
</section>
<section id="setting-up-the-problem">
<h2>Setting up the problem<a class="headerlink" href="#setting-up-the-problem" title="Link to this heading">#</a></h2>
<p>Here, we will model a population <span class="math notranslate nohighlight">\(g(t)\)</span> in an environment having carrying capacity <span class="math notranslate nohighlight">\(A\)</span>.
The population follows the model</p>
<!-- Equation labels as ordinary links -->
<div id="solveode_population"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{solveode_population} \tag{11}
g'(t) = \alpha g(t)(A - g(t))
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(g(0) = g_0\)</span>.</p>
<p>In this example, we let <span class="math notranslate nohighlight">\(\alpha = 2\)</span>, <span class="math notranslate nohighlight">\(A = 1\)</span>, and <span class="math notranslate nohighlight">\(g_0 = 1.2\)</span>.</p>
</section>
<section id="id2">
<h2>The trial solution<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>We will get a slightly different trial solution, as the boundary conditions are different
compared to the case for exponential decay.</p>
<p>A possible trial solution satisfying the condition <span class="math notranslate nohighlight">\(g(0) = g_0\)</span> could be</p>
<div class="math notranslate nohighlight">
\[
h_1(t) = g_0 + t \cdot N(t,P)
\]</div>
<p>with <span class="math notranslate nohighlight">\(N(t,P)\)</span> being the output from the neural network with weights and biases for each layer collected in the set <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>The analytical solution is</p>
<div class="math notranslate nohighlight">
\[
g(t) = \frac{Ag_0}{g_0 + (A - g_0)\exp(-\alpha A t)}
\]</div>
</section>
<section id="the-program-using-autograd">
<h2>The program using Autograd<a class="headerlink" href="#the-program-using-autograd" title="Link to this heading">#</a></h2>
<p>The network will be the similar as for the exponential decay example, but with some small modifications for our problem.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

def sigmoid(z):
    return 1/(1 + np.exp(-z))

# Function to get the parameters.
# Done such that one can easily change the paramaters after one&#39;s liking.
def get_parameters():
    alpha = 2
    A = 1
    g0 = 1.2
    return alpha, A, g0

def deep_neural_network(deep_params, x):
    # N_hidden is the number of hidden layers
    # deep_params is a list, len() should be used
    N_hidden = len(deep_params) - 1 # -1 since params consists of
                                        # parameters to all the hidden
                                        # layers AND the output layer.

    # Assumes input x being an one-dimensional array
    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    # Assume that the input layer does nothing to the input x
    x_input = x

    # Due to multiple hidden layers, define a variable referencing to the
    # output of the previous layer:
    x_prev = x_input

    ## Hidden layers:

    for l in range(N_hidden):
        # From the list of parameters P; find the correct weigths and bias for this layer
        w_hidden = deep_params[l]

        # Add a row of ones to include bias
        x_prev = np.concatenate((np.ones((1,num_values)), x_prev ), axis = 0)

        z_hidden = np.matmul(w_hidden, x_prev)
        x_hidden = sigmoid(z_hidden)

        # Update x_prev such that next layer can use the output from this layer
        x_prev = x_hidden

    ## Output layer:

    # Get the weights and bias for this layer
    w_output = deep_params[-1]

    # Include bias:
    x_prev = np.concatenate((np.ones((1,num_values)), x_prev), axis = 0)

    z_output = np.matmul(w_output, x_prev)
    x_output = z_output

    return x_output




def cost_function_deep(P, x):

    # Evaluate the trial function with the current parameters P
    g_t = g_trial_deep(x,P)

    # Find the derivative w.r.t x of the trial function
    d_g_t = elementwise_grad(g_trial_deep,0)(x,P)

    # The right side of the ODE
    func = f(x, g_t)

    err_sqr = (d_g_t - func)**2
    cost_sum = np.sum(err_sqr)

    return cost_sum / np.size(err_sqr)

# The right side of the ODE:
def f(x, g_trial):
    alpha,A, g0 = get_parameters()
    return alpha*g_trial*(A - g_trial)

# The trial solution using the deep neural network:
def g_trial_deep(x, params):
    alpha,A, g0 = get_parameters()
    return g0 + x*deep_neural_network(params,x)

# The analytical solution:
def g_analytic(t):
    alpha,A, g0 = get_parameters()
    return A*g0/(g0 + (A - g0)*np.exp(-alpha*A*t))

def solve_ode_deep_neural_network(x, num_neurons, num_iter, lmb):
    # num_hidden_neurons is now a list of number of neurons within each hidden layer

    # Find the number of hidden layers:
    N_hidden = np.size(num_neurons)

    ## Set up initial weigths and biases

    # Initialize the list of parameters:
    P = [None]*(N_hidden + 1) # + 1 to include the output layer

    P[0] = npr.randn(num_neurons[0], 2 )
    for l in range(1,N_hidden):
        P[l] = npr.randn(num_neurons[l], num_neurons[l-1] + 1) # +1 to include bias

    # For the output layer
    P[-1] = npr.randn(1, num_neurons[-1] + 1 ) # +1 since bias is included

    print(&#39;Initial cost: %g&#39;%cost_function_deep(P, x))

    ## Start finding the optimal weigths using gradient descent

    # Find the Python function that represents the gradient of the cost function
    # w.r.t the 0-th input argument -- that is the weights and biases in the hidden and output layer
    cost_function_deep_grad = grad(cost_function_deep,0)

    # Let the update be done num_iter times
    for i in range(num_iter):
        # Evaluate the gradient at the current weights and biases in P.
        # The cost_grad consist now of N_hidden + 1 arrays; the gradient w.r.t the weights and biases
        # in the hidden layers and output layers evaluated at x.
        cost_deep_grad =  cost_function_deep_grad(P, x)

        for l in range(N_hidden+1):
            P[l] = P[l] - lmb * cost_deep_grad[l]

    print(&#39;Final cost: %g&#39;%cost_function_deep(P, x))

    return P

if __name__ == &#39;__main__&#39;:
    npr.seed(4155)

    ## Decide the vales of arguments to the function to solve
    Nt = 10
    T = 1
    t = np.linspace(0,T, Nt)

    ## Set up the initial parameters
    num_hidden_neurons = [100, 50, 25]
    num_iter = 1000
    lmb = 1e-3

    P = solve_ode_deep_neural_network(t, num_hidden_neurons, num_iter, lmb)

    g_dnn_ag = g_trial_deep(t,P)
    g_analytical = g_analytic(t)

    # Find the maximum absolute difference between the solutons:
    diff_ag = np.max(np.abs(g_dnn_ag - g_analytical))
    print(&quot;The max absolute difference between the solutions is: %g&quot;%diff_ag)

    plt.figure(figsize=(10,10))

    plt.title(&#39;Performance of neural network solving an ODE compared to the analytical solution&#39;)
    plt.plot(t, g_analytical)
    plt.plot(t, g_dnn_ag[0,:])
    plt.legend([&#39;analytical&#39;,&#39;nn&#39;])
    plt.xlabel(&#39;t&#39;)
    plt.ylabel(&#39;g(t)&#39;)

    plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="using-forward-euler-to-solve-the-ode">
<h2>Using forward Euler to solve the ODE<a class="headerlink" href="#using-forward-euler-to-solve-the-ode" title="Link to this heading">#</a></h2>
<p>A straightforward way of solving an ODE numerically, is to use Euler’s method.</p>
<p>Euler’s method uses Taylor series to approximate the value at a function <span class="math notranslate nohighlight">\(f\)</span> at a step <span class="math notranslate nohighlight">\(\Delta x\)</span> from <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[
f(x + \Delta x) \approx f(x) + \Delta x f'(x)
\]</div>
<p>In our case, using Euler’s method to approximate the value of <span class="math notranslate nohighlight">\(g\)</span> at a step <span class="math notranslate nohighlight">\(\Delta t\)</span> from <span class="math notranslate nohighlight">\(t\)</span> yields</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
  g(t + \Delta t) &amp;\approx g(t) + \Delta t g'(t) \\
  &amp;= g(t) + \Delta t \big(\alpha g(t)(A - g(t))\big)
\end{aligned}
\end{split}\]</div>
<p>along with the condition that <span class="math notranslate nohighlight">\(g(0) = g_0\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(t_i = i \cdot \Delta t\)</span> where <span class="math notranslate nohighlight">\(\Delta t = \frac{T}{N_t-1}\)</span> where <span class="math notranslate nohighlight">\(T\)</span> is the final time our solver must solve for and <span class="math notranslate nohighlight">\(N_t\)</span> the number of values for <span class="math notranslate nohighlight">\(t \in [0, T]\)</span> for <span class="math notranslate nohighlight">\(i = 0, \dots, N_t-1\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(i \geq 1\)</span>, we have that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
t_i &amp;= i\Delta t \\
&amp;= (i - 1)\Delta t + \Delta t \\
&amp;= t_{i-1} + \Delta t
\end{aligned}
\end{split}\]</div>
<p>Now, if <span class="math notranslate nohighlight">\(g_i = g(t_i)\)</span> then</p>
<!-- Equation labels as ordinary links -->
<div id="odenum"></div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
  \begin{aligned}
  g_i &amp;= g(t_i) \\
  &amp;= g(t_{i-1} + \Delta t) \\
  &amp;\approx g(t_{i-1}) + \Delta t \big(\alpha g(t_{i-1})(A - g(t_{i-1}))\big) \\
  &amp;= g_{i-1} + \Delta t \big(\alpha g_{i-1}(A - g_{i-1})\big)
  \end{aligned}
\end{equation} \label{odenum} \tag{12}
\end{split}\]</div>
<p>for <span class="math notranslate nohighlight">\(i \geq 1\)</span> and <span class="math notranslate nohighlight">\(g_0 = g(t_0) = g(0) = g_0\)</span>.</p>
<p>Equation (<a class="reference internal" href="#odenum"><span class="xref myst">12</span></a>) could be implemented in the following way,
extending the program that uses the network using Autograd:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Assume that all function definitions from the example program using Autograd
# are located here.

if __name__ == &#39;__main__&#39;:
    npr.seed(4155)

    ## Decide the vales of arguments to the function to solve
    Nt = 10
    T = 1
    t = np.linspace(0,T, Nt)

    ## Set up the initial parameters
    num_hidden_neurons = [100,50,25]
    num_iter = 1000
    lmb = 1e-3

    P = solve_ode_deep_neural_network(t, num_hidden_neurons, num_iter, lmb)

    g_dnn_ag = g_trial_deep(t,P)
    g_analytical = g_analytic(t)

    # Find the maximum absolute difference between the solutons:
    diff_ag = np.max(np.abs(g_dnn_ag - g_analytical))
    print(&quot;The max absolute difference between the solutions is: %g&quot;%diff_ag)

    plt.figure(figsize=(10,10))

    plt.title(&#39;Performance of neural network solving an ODE compared to the analytical solution&#39;)
    plt.plot(t, g_analytical)
    plt.plot(t, g_dnn_ag[0,:])
    plt.legend([&#39;analytical&#39;,&#39;nn&#39;])
    plt.xlabel(&#39;t&#39;)
    plt.ylabel(&#39;g(t)&#39;)

    ## Find an approximation to the funtion using forward Euler

    alpha, A, g0 = get_parameters()
    dt = T/(Nt - 1)

    # Perform forward Euler to solve the ODE
    g_euler = np.zeros(Nt)
    g_euler[0] = g0

    for i in range(1,Nt):
        g_euler[i] = g_euler[i-1] + dt*(alpha*g_euler[i-1]*(A - g_euler[i-1]))

    # Print the errors done by each method
    diff1 = np.max(np.abs(g_euler - g_analytical))
    diff2 = np.max(np.abs(g_dnn_ag[0,:] - g_analytical))

    print(&#39;Max absolute difference between Euler method and analytical: %g&#39;%diff1)
    print(&#39;Max absolute difference between deep neural network and analytical: %g&#39;%diff2)

    # Plot results
    plt.figure(figsize=(10,10))

    plt.plot(t,g_euler)
    plt.plot(t,g_analytical)
    plt.plot(t,g_dnn_ag[0,:])

    plt.legend([&#39;euler&#39;,&#39;analytical&#39;,&#39;dnn&#39;])
    plt.xlabel(&#39;Time t&#39;)
    plt.ylabel(&#39;g(t)&#39;)

    plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="example-solving-the-one-dimensional-poisson-equation">
<h2>Example: Solving the one dimensional Poisson equation<a class="headerlink" href="#example-solving-the-one-dimensional-poisson-equation" title="Link to this heading">#</a></h2>
<p>The Poisson equation for <span class="math notranslate nohighlight">\(g(x)\)</span> in one dimension is</p>
<!-- Equation labels as ordinary links -->
<div id="poisson"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{poisson} \tag{13}
  -g''(x) = f(x)
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(x)\)</span> is a given function for <span class="math notranslate nohighlight">\(x \in (0,1)\)</span>.</p>
<p>The conditions that <span class="math notranslate nohighlight">\(g(x)\)</span> is chosen to fulfill, are</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
  g(0) &amp;= 0 \\
  g(1) &amp;= 0
\end{align*}
\end{split}\]</div>
<p>This equation can be solved numerically using programs where e.g Autograd and TensorFlow are used.
The results from the networks can then be compared to the analytical solution.
In addition, it could be interesting to see how a typical method for numerically solving second order ODEs compares to the neural networks.</p>
</section>
<section id="the-specific-equation-to-solve-for">
<h2>The specific equation to solve for<a class="headerlink" href="#the-specific-equation-to-solve-for" title="Link to this heading">#</a></h2>
<p>Here, the function <span class="math notranslate nohighlight">\(g(x)\)</span> to solve for follows the equation</p>
<div class="math notranslate nohighlight">
\[
-g''(x) = f(x),\qquad x \in (0,1)
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(x)\)</span> is a given function, along with the chosen conditions</p>
<!-- Equation labels as ordinary links -->
<div id="cond"></div>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
g(0) = g(1) = 0
\end{aligned}\label{cond} \tag{14}
\]</div>
<p>In this example, we consider the case when <span class="math notranslate nohighlight">\(f(x) = (3x + x^2)\exp(x)\)</span>.</p>
<p>For this case, a possible trial solution satisfying the conditions could be</p>
<div class="math notranslate nohighlight">
\[
g_t(x) = x \cdot (1-x) \cdot N(P,x)
\]</div>
<p>The analytical solution for this problem is</p>
<div class="math notranslate nohighlight">
\[
g(x) = x(1 - x)\exp(x)
\]</div>
</section>
<section id="solving-the-equation-using-autograd">
<h2>Solving the equation using Autograd<a class="headerlink" href="#solving-the-equation-using-autograd" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

def sigmoid(z):
    return 1/(1 + np.exp(-z))

def deep_neural_network(deep_params, x):
    # N_hidden is the number of hidden layers
    # deep_params is a list, len() should be used
    N_hidden = len(deep_params) - 1 # -1 since params consists of
                                        # parameters to all the hidden
                                        # layers AND the output layer.

    # Assumes input x being an one-dimensional array
    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    # Assume that the input layer does nothing to the input x
    x_input = x

    # Due to multiple hidden layers, define a variable referencing to the
    # output of the previous layer:
    x_prev = x_input

    ## Hidden layers:

    for l in range(N_hidden):
        # From the list of parameters P; find the correct weigths and bias for this layer
        w_hidden = deep_params[l]

        # Add a row of ones to include bias
        x_prev = np.concatenate((np.ones((1,num_values)), x_prev ), axis = 0)

        z_hidden = np.matmul(w_hidden, x_prev)
        x_hidden = sigmoid(z_hidden)

        # Update x_prev such that next layer can use the output from this layer
        x_prev = x_hidden

    ## Output layer:

    # Get the weights and bias for this layer
    w_output = deep_params[-1]

    # Include bias:
    x_prev = np.concatenate((np.ones((1,num_values)), x_prev), axis = 0)

    z_output = np.matmul(w_output, x_prev)
    x_output = z_output

    return x_output


def solve_ode_deep_neural_network(x, num_neurons, num_iter, lmb):
    # num_hidden_neurons is now a list of number of neurons within each hidden layer

    # Find the number of hidden layers:
    N_hidden = np.size(num_neurons)

    ## Set up initial weigths and biases

    # Initialize the list of parameters:
    P = [None]*(N_hidden + 1) # + 1 to include the output layer

    P[0] = npr.randn(num_neurons[0], 2 )
    for l in range(1,N_hidden):
        P[l] = npr.randn(num_neurons[l], num_neurons[l-1] + 1) # +1 to include bias

    # For the output layer
    P[-1] = npr.randn(1, num_neurons[-1] + 1 ) # +1 since bias is included

    print(&#39;Initial cost: %g&#39;%cost_function_deep(P, x))

    ## Start finding the optimal weigths using gradient descent

    # Find the Python function that represents the gradient of the cost function
    # w.r.t the 0-th input argument -- that is the weights and biases in the hidden and output layer
    cost_function_deep_grad = grad(cost_function_deep,0)

    # Let the update be done num_iter times
    for i in range(num_iter):
        # Evaluate the gradient at the current weights and biases in P.
        # The cost_grad consist now of N_hidden + 1 arrays; the gradient w.r.t the weights and biases
        # in the hidden layers and output layers evaluated at x.
        cost_deep_grad =  cost_function_deep_grad(P, x)

        for l in range(N_hidden+1):
            P[l] = P[l] - lmb * cost_deep_grad[l]

    print(&#39;Final cost: %g&#39;%cost_function_deep(P, x))

    return P

## Set up the cost function specified for this Poisson equation:

# The right side of the ODE
def f(x):
    return (3*x + x**2)*np.exp(x)

def cost_function_deep(P, x):

    # Evaluate the trial function with the current parameters P
    g_t = g_trial_deep(x,P)

    # Find the derivative w.r.t x of the trial function
    d2_g_t = elementwise_grad(elementwise_grad(g_trial_deep,0))(x,P)

    right_side = f(x)

    err_sqr = (-d2_g_t - right_side)**2
    cost_sum = np.sum(err_sqr)

    return cost_sum/np.size(err_sqr)

# The trial solution:
def g_trial_deep(x,P):
    return x*(1-x)*deep_neural_network(P,x)

# The analytic solution;
def g_analytic(x):
    return x*(1-x)*np.exp(x)

if __name__ == &#39;__main__&#39;:
    npr.seed(4155)

    ## Decide the vales of arguments to the function to solve
    Nx = 10
    x = np.linspace(0,1, Nx)

    ## Set up the initial parameters
    num_hidden_neurons = [200,100]
    num_iter = 1000
    lmb = 1e-3

    P = solve_ode_deep_neural_network(x, num_hidden_neurons, num_iter, lmb)

    g_dnn_ag = g_trial_deep(x,P)
    g_analytical = g_analytic(x)

    # Find the maximum absolute difference between the solutons:
    max_diff = np.max(np.abs(g_dnn_ag - g_analytical))
    print(&quot;The max absolute difference between the solutions is: %g&quot;%max_diff)

    plt.figure(figsize=(10,10))

    plt.title(&#39;Performance of neural network solving an ODE compared to the analytical solution&#39;)
    plt.plot(x, g_analytical)
    plt.plot(x, g_dnn_ag[0,:])
    plt.legend([&#39;analytical&#39;,&#39;nn&#39;])
    plt.xlabel(&#39;x&#39;)
    plt.ylabel(&#39;g(x)&#39;)
    plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="comparing-with-a-numerical-scheme">
<h2>Comparing with a numerical scheme<a class="headerlink" href="#comparing-with-a-numerical-scheme" title="Link to this heading">#</a></h2>
<p>The Poisson equation is possible to solve using Taylor series to approximate the second derivative.</p>
<p>Using Taylor series, the second derivative can be expressed as</p>
<div class="math notranslate nohighlight">
\[
g''(x) = \frac{g(x + \Delta x) - 2g(x) + g(x-\Delta x)}{\Delta x^2} + E_{\Delta x}(x)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta x\)</span> is a small step size and <span class="math notranslate nohighlight">\(E_{\Delta x}(x)\)</span> being the error term.</p>
<p>Looking away from the error terms gives an approximation to the second derivative:</p>
<!-- Equation labels as ordinary links -->
<div id="approx"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{approx} \tag{15}
g''(x) \approx \frac{g(x + \Delta x) - 2g(x) + g(x-\Delta x)}{\Delta x^2}
\end{equation}
\]</div>
<p>If <span class="math notranslate nohighlight">\(x_i = i \Delta x = x_{i-1} + \Delta x\)</span> and <span class="math notranslate nohighlight">\(g_i = g(x_i)\)</span> for <span class="math notranslate nohighlight">\(i = 1,\dots N_x - 2\)</span> with <span class="math notranslate nohighlight">\(N_x\)</span> being the number of values for <span class="math notranslate nohighlight">\(x\)</span>, (<a class="reference internal" href="#approx"><span class="xref myst">15</span></a>) becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
g''(x_i) &amp;\approx \frac{g(x_i + \Delta x) - 2g(x_i) + g(x_i -\Delta x)}{\Delta x^2} \\
&amp;= \frac{g_{i+1} - 2g_i + g_{i-1}}{\Delta x^2}
\end{aligned}
\end{split}\]</div>
<p>Since we know from our problem that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-g''(x) &amp;= f(x) \\
&amp;= (3x + x^2)\exp(x)
\end{aligned}
\end{split}\]</div>
<p>along with the conditions <span class="math notranslate nohighlight">\(g(0) = g(1) = 0\)</span>,
the following scheme can be used to find an approximate solution for <span class="math notranslate nohighlight">\(g(x)\)</span> numerically:</p>
<!-- Equation labels as ordinary links -->
<div id="odesys"></div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
  \begin{aligned}
  -\Big( \frac{g_{i+1} - 2g_i + g_{i-1}}{\Delta x^2} \Big) &amp;= f(x_i) \\
  -g_{i+1} + 2g_i - g_{i-1} &amp;= \Delta x^2 f(x_i)
  \end{aligned}
\end{equation} \label{odesys} \tag{16}
\end{split}\]</div>
<p>for <span class="math notranslate nohighlight">\(i = 1, \dots, N_x - 2\)</span> where <span class="math notranslate nohighlight">\(g_0 = g_{N_x - 1} = 0\)</span> and <span class="math notranslate nohighlight">\(f(x_i) = (3x_i + x_i^2)\exp(x_i)\)</span>, which is given for our specific problem.</p>
<p>The equation can be rewritten into a matrix equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\begin{pmatrix}
2 &amp; -1 &amp; 0 &amp; \dots &amp; 0 \\
-1 &amp; 2 &amp; -1 &amp; \dots &amp; 0 \\
\vdots &amp; &amp; \ddots &amp; &amp; \vdots \\
0 &amp; \dots &amp; -1 &amp; 2 &amp; -1  \\
0 &amp; \dots &amp; 0 &amp; -1 &amp; 2\\
\end{pmatrix}
\begin{pmatrix}
g_1 \\
g_2 \\
\vdots \\
g_{N_x - 3} \\
g_{N_x - 2}
\end{pmatrix}
&amp;=
\Delta x^2
\begin{pmatrix}
f(x_1) \\
f(x_2) \\
\vdots \\
f(x_{N_x - 3}) \\
f(x_{N_x - 2})
\end{pmatrix} \\
\boldsymbol{A}\boldsymbol{g} &amp;= \boldsymbol{f},
\end{aligned}
\end{split}\]</div>
<p>which makes it possible to solve for the vector <span class="math notranslate nohighlight">\(\boldsymbol{g}\)</span>.</p>
</section>
<section id="setting-up-the-code">
<h2>Setting up the code<a class="headerlink" href="#setting-up-the-code" title="Link to this heading">#</a></h2>
<p>We can then compare the result from this numerical scheme with the output from our network using Autograd:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

def sigmoid(z):
    return 1/(1 + np.exp(-z))

def deep_neural_network(deep_params, x):
    # N_hidden is the number of hidden layers
    # deep_params is a list, len() should be used
    N_hidden = len(deep_params) - 1 # -1 since params consists of
                                        # parameters to all the hidden
                                        # layers AND the output layer.

    # Assumes input x being an one-dimensional array
    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    # Assume that the input layer does nothing to the input x
    x_input = x

    # Due to multiple hidden layers, define a variable referencing to the
    # output of the previous layer:
    x_prev = x_input

    ## Hidden layers:

    for l in range(N_hidden):
        # From the list of parameters P; find the correct weigths and bias for this layer
        w_hidden = deep_params[l]

        # Add a row of ones to include bias
        x_prev = np.concatenate((np.ones((1,num_values)), x_prev ), axis = 0)

        z_hidden = np.matmul(w_hidden, x_prev)
        x_hidden = sigmoid(z_hidden)

        # Update x_prev such that next layer can use the output from this layer
        x_prev = x_hidden

    ## Output layer:

    # Get the weights and bias for this layer
    w_output = deep_params[-1]

    # Include bias:
    x_prev = np.concatenate((np.ones((1,num_values)), x_prev), axis = 0)

    z_output = np.matmul(w_output, x_prev)
    x_output = z_output

    return x_output


def solve_ode_deep_neural_network(x, num_neurons, num_iter, lmb):
    # num_hidden_neurons is now a list of number of neurons within each hidden layer

    # Find the number of hidden layers:
    N_hidden = np.size(num_neurons)

    ## Set up initial weigths and biases

    # Initialize the list of parameters:
    P = [None]*(N_hidden + 1) # + 1 to include the output layer

    P[0] = npr.randn(num_neurons[0], 2 )
    for l in range(1,N_hidden):
        P[l] = npr.randn(num_neurons[l], num_neurons[l-1] + 1) # +1 to include bias

    # For the output layer
    P[-1] = npr.randn(1, num_neurons[-1] + 1 ) # +1 since bias is included

    print(&#39;Initial cost: %g&#39;%cost_function_deep(P, x))

    ## Start finding the optimal weigths using gradient descent

    # Find the Python function that represents the gradient of the cost function
    # w.r.t the 0-th input argument -- that is the weights and biases in the hidden and output layer
    cost_function_deep_grad = grad(cost_function_deep,0)

    # Let the update be done num_iter times
    for i in range(num_iter):
        # Evaluate the gradient at the current weights and biases in P.
        # The cost_grad consist now of N_hidden + 1 arrays; the gradient w.r.t the weights and biases
        # in the hidden layers and output layers evaluated at x.
        cost_deep_grad =  cost_function_deep_grad(P, x)

        for l in range(N_hidden+1):
            P[l] = P[l] - lmb * cost_deep_grad[l]

    print(&#39;Final cost: %g&#39;%cost_function_deep(P, x))

    return P

## Set up the cost function specified for this Poisson equation:

# The right side of the ODE
def f(x):
    return (3*x + x**2)*np.exp(x)

def cost_function_deep(P, x):

    # Evaluate the trial function with the current parameters P
    g_t = g_trial_deep(x,P)

    # Find the derivative w.r.t x of the trial function
    d2_g_t = elementwise_grad(elementwise_grad(g_trial_deep,0))(x,P)

    right_side = f(x)

    err_sqr = (-d2_g_t - right_side)**2
    cost_sum = np.sum(err_sqr)

    return cost_sum/np.size(err_sqr)

# The trial solution:
def g_trial_deep(x,P):
    return x*(1-x)*deep_neural_network(P,x)

# The analytic solution;
def g_analytic(x):
    return x*(1-x)*np.exp(x)

if __name__ == &#39;__main__&#39;:
    npr.seed(4155)

    ## Decide the vales of arguments to the function to solve
    Nx = 10
    x = np.linspace(0,1, Nx)

    ## Set up the initial parameters
    num_hidden_neurons = [200,100]
    num_iter = 1000
    lmb = 1e-3

    P = solve_ode_deep_neural_network(x, num_hidden_neurons, num_iter, lmb)

    g_dnn_ag = g_trial_deep(x,P)
    g_analytical = g_analytic(x)

    # Find the maximum absolute difference between the solutons:

    plt.figure(figsize=(10,10))

    plt.title(&#39;Performance of neural network solving an ODE compared to the analytical solution&#39;)
    plt.plot(x, g_analytical)
    plt.plot(x, g_dnn_ag[0,:])
    plt.legend([&#39;analytical&#39;,&#39;nn&#39;])
    plt.xlabel(&#39;x&#39;)
    plt.ylabel(&#39;g(x)&#39;)

    ## Perform the computation using the numerical scheme

    dx = 1/(Nx - 1)

    # Set up the matrix A
    A = np.zeros((Nx-2,Nx-2))

    A[0,0] = 2
    A[0,1] = -1

    for i in range(1,Nx-3):
        A[i,i-1] = -1
        A[i,i] = 2
        A[i,i+1] = -1

    A[Nx - 3, Nx - 4] = -1
    A[Nx - 3, Nx - 3] = 2

    # Set up the vector f
    f_vec = dx**2 * f(x[1:-1])

    # Solve the equation
    g_res = np.linalg.solve(A,f_vec)

    g_vec = np.zeros(Nx)
    g_vec[1:-1] = g_res

    # Print the differences between each method
    max_diff1 = np.max(np.abs(g_dnn_ag - g_analytical))
    max_diff2 = np.max(np.abs(g_vec - g_analytical))
    print(&quot;The max absolute difference between the analytical solution and DNN Autograd: %g&quot;%max_diff1)
    print(&quot;The max absolute difference between the analytical solution and numerical scheme: %g&quot;%max_diff2)

    # Plot the results
    plt.figure(figsize=(10,10))

    plt.plot(x,g_vec)
    plt.plot(x,g_analytical)
    plt.plot(x,g_dnn_ag[0,:])

    plt.legend([&#39;numerical scheme&#39;,&#39;analytical&#39;,&#39;dnn&#39;])
    plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="partial-differential-equations">
<h2>Partial Differential Equations<a class="headerlink" href="#partial-differential-equations" title="Link to this heading">#</a></h2>
<p>A partial differential equation (PDE) has a solution here the function
is defined by multiple variables.  The equation may involve all kinds
of combinations of which variables the function is differentiated with
respect to.</p>
<p>In general, a partial differential equation for a function <span class="math notranslate nohighlight">\(g(x_1,\dots,x_N)\)</span> with <span class="math notranslate nohighlight">\(N\)</span> variables may be expressed as</p>
<!-- Equation labels as ordinary links -->
<div id="PDE"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation} \label{PDE} \tag{17}
  f\left(x_1, \, \dots \, , x_N, \frac{\partial g(x_1,\dots,x_N) }{\partial x_1}, \dots , \frac{\partial g(x_1,\dots,x_N) }{\partial x_N}, \frac{\partial g(x_1,\dots,x_N) }{\partial x_1\partial x_2}, \, \dots \, , \frac{\partial^n g(x_1,\dots,x_N) }{\partial x_N^n} \right) = 0
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is an expression involving all kinds of possible mixed derivatives of <span class="math notranslate nohighlight">\(g(x_1,\dots,x_N)\)</span> up to an order <span class="math notranslate nohighlight">\(n\)</span>. In order for the solution to be unique, some additional conditions must also be given.</p>
</section>
<section id="type-of-problem">
<h2>Type of problem<a class="headerlink" href="#type-of-problem" title="Link to this heading">#</a></h2>
<p>The problem our network must solve for, is similar to the ODE case.
We must have a trial solution <span class="math notranslate nohighlight">\(g_t\)</span> at hand.</p>
<p>For instance, the trial solution could be expressed as</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
  g_t(x_1,\dots,x_N) = h_1(x_1,\dots,x_N) + h_2(x_1,\dots,x_N,N(x_1,\dots,x_N,P))
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(h_1(x_1,\dots,x_N)\)</span> is a function that ensures <span class="math notranslate nohighlight">\(g_t(x_1,\dots,x_N)\)</span> satisfies some given conditions.
The neural network <span class="math notranslate nohighlight">\(N(x_1,\dots,x_N,P)\)</span> has weights and biases described by <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(h_2(x_1,\dots,x_N,N(x_1,\dots,x_N,P))\)</span> is an expression using the output from the neural network in some way.</p>
<p>The role of the function <span class="math notranslate nohighlight">\(h_2(x_1,\dots,x_N,N(x_1,\dots,x_N,P))\)</span>, is to ensure that the output of <span class="math notranslate nohighlight">\(N(x_1,\dots,x_N,P)\)</span> is zero when <span class="math notranslate nohighlight">\(g_t(x_1,\dots,x_N)\)</span> is evaluated at the values of <span class="math notranslate nohighlight">\(x_1,\dots,x_N\)</span> where the given conditions must be satisfied. The function <span class="math notranslate nohighlight">\(h_1(x_1,\dots,x_N)\)</span> should alone make <span class="math notranslate nohighlight">\(g_t(x_1,\dots,x_N)\)</span> satisfy the conditions.</p>
</section>
<section id="network-requirements">
<h2>Network requirements<a class="headerlink" href="#network-requirements" title="Link to this heading">#</a></h2>
<p>The network tries then the minimize the cost function following the
same ideas as described for the ODE case, but now with more than one
variables to consider.  The concept still remains the same; find a set
of parameters <span class="math notranslate nohighlight">\(P\)</span> such that the expression <span class="math notranslate nohighlight">\(f\)</span> in (<a class="reference internal" href="#PDE"><span class="xref myst">17</span></a>) is as
close to zero as possible.</p>
<p>As for the ODE case, the cost function is the mean squared error that
the network must try to minimize. The cost function for the network to
minimize is</p>
<div class="math notranslate nohighlight">
\[
C\left(x_1, \dots, x_N, P\right) = \left(  f\left(x_1, \, \dots \, , x_N, \frac{\partial g(x_1,\dots,x_N) }{\partial x_1}, \dots , \frac{\partial g(x_1,\dots,x_N) }{\partial x_N}, \frac{\partial g(x_1,\dots,x_N) }{\partial x_1\partial x_2}, \, \dots \, , \frac{\partial^n g(x_1,\dots,x_N) }{\partial x_N^n} \right) \right)^2
\]</div>
</section>
<section id="id3">
<h2>More details<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>If we let <span class="math notranslate nohighlight">\(\boldsymbol{x} = \big( x_1, \dots, x_N \big)\)</span> be an array containing the values for <span class="math notranslate nohighlight">\(x_1, \dots, x_N\)</span> respectively, the cost function can be reformulated into the following:</p>
<div class="math notranslate nohighlight">
\[
C\left(\boldsymbol{x}, P\right) = f\left( \left( \boldsymbol{x}, \frac{\partial g(\boldsymbol{x}) }{\partial x_1}, \dots , \frac{\partial g(\boldsymbol{x}) }{\partial x_N}, \frac{\partial g(\boldsymbol{x}) }{\partial x_1\partial x_2}, \, \dots \, , \frac{\partial^n g(\boldsymbol{x}) }{\partial x_N^n} \right) \right)^2
\]</div>
<p>If we also have <span class="math notranslate nohighlight">\(M\)</span> different sets of values for <span class="math notranslate nohighlight">\(x_1, \dots, x_N\)</span>, that is <span class="math notranslate nohighlight">\(\boldsymbol{x}_i = \big(x_1^{(i)}, \dots, x_N^{(i)}\big)\)</span> for <span class="math notranslate nohighlight">\(i = 1,\dots,M\)</span> being the rows in matrix <span class="math notranslate nohighlight">\(X\)</span>, the cost function can be generalized into</p>
<div class="math notranslate nohighlight">
\[
C\left(X, P \right) = \sum_{i=1}^M f\left( \left( \boldsymbol{x}_i, \frac{\partial g(\boldsymbol{x}_i) }{\partial x_1}, \dots , \frac{\partial g(\boldsymbol{x}_i) }{\partial x_N}, \frac{\partial g(\boldsymbol{x}_i) }{\partial x_1\partial x_2}, \, \dots \, , \frac{\partial^n g(\boldsymbol{x}_i) }{\partial x_N^n} \right) \right)^2.
\]</div>
</section>
<section id="example-the-diffusion-equation">
<h2>Example: The diffusion equation<a class="headerlink" href="#example-the-diffusion-equation" title="Link to this heading">#</a></h2>
<p>In one spatial dimension, the equation reads</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial g(x,t)}{\partial t} = \frac{\partial^2 g(x,t)}{\partial x^2}
\]</div>
<p>where a possible choice of conditions are</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
g(0,t) &amp;= 0 ,\qquad t \geq 0 \\
g(1,t) &amp;= 0, \qquad t \geq 0 \\
g(x,0) &amp;= u(x),\qquad x\in [0,1]
\end{align*}
\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(u(x)\)</span> being some given function.</p>
</section>
<section id="defining-the-problem">
<h2>Defining the problem<a class="headerlink" href="#defining-the-problem" title="Link to this heading">#</a></h2>
<p>For this case, we want to find <span class="math notranslate nohighlight">\(g(x,t)\)</span> such that</p>
<!-- Equation labels as ordinary links -->
<div id="diffonedim"></div>
<div class="math notranslate nohighlight">
\[
\begin{equation}
  \frac{\partial g(x,t)}{\partial t} = \frac{\partial^2 g(x,t)}{\partial x^2}
\end{equation} \label{diffonedim} \tag{18}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
g(0,t) &amp;= 0 ,\qquad t \geq 0 \\
g(1,t) &amp;= 0, \qquad t \geq 0 \\
g(x,0) &amp;= u(x),\qquad x\in [0,1]
\end{align*}
\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(u(x) = \sin(\pi x)\)</span>.</p>
<p>First, let us set up the deep neural network.
The deep neural network will follow the same structure as discussed in the examples solving the ODEs.
First, we will look into how Autograd could be used in a network tailored to solve for bivariate functions.</p>
</section>
<section id="setting-up-the-network-using-autograd">
<h2>Setting up the network using Autograd<a class="headerlink" href="#setting-up-the-network-using-autograd" title="Link to this heading">#</a></h2>
<p>The only change to do here, is to extend our network such that
functions of multiple parameters are correctly handled.  In this case
we have two variables in our function to solve for, that is time <span class="math notranslate nohighlight">\(t\)</span>
and position <span class="math notranslate nohighlight">\(x\)</span>.  The variables will be represented by a
one-dimensional array in the program.  The program will evaluate the
network at each possible pair <span class="math notranslate nohighlight">\((x,t)\)</span>, given an array for the desired
<span class="math notranslate nohighlight">\(x\)</span>-values and <span class="math notranslate nohighlight">\(t\)</span>-values to approximate the solution at.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def sigmoid(z):
    return 1/(1 + np.exp(-z))

def deep_neural_network(deep_params, x):
    # x is now a point and a 1D numpy array; make it a column vector
    num_coordinates = np.size(x,0)
    x = x.reshape(num_coordinates,-1)

    num_points = np.size(x,1)

    # N_hidden is the number of hidden layers
    N_hidden = len(deep_params) - 1 # -1 since params consist of parameters to all the hidden layers AND the output layer

    # Assume that the input layer does nothing to the input x
    x_input = x
    x_prev = x_input

    ## Hidden layers:

    for l in range(N_hidden):
        # From the list of parameters P; find the correct weigths and bias for this layer
        w_hidden = deep_params[l]

        # Add a row of ones to include bias
        x_prev = np.concatenate((np.ones((1,num_points)), x_prev ), axis = 0)

        z_hidden = np.matmul(w_hidden, x_prev)
        x_hidden = sigmoid(z_hidden)

        # Update x_prev such that next layer can use the output from this layer
        x_prev = x_hidden

    ## Output layer:

    # Get the weights and bias for this layer
    w_output = deep_params[-1]

    # Include bias:
    x_prev = np.concatenate((np.ones((1,num_points)), x_prev), axis = 0)

    z_output = np.matmul(w_output, x_prev)
    x_output = z_output

    return x_output[0][0]
</pre></div>
</div>
</div>
</div>
</section>
<section id="setting-up-the-network-using-autograd-the-trial-solution">
<h2>Setting up the network using Autograd; The trial solution<a class="headerlink" href="#setting-up-the-network-using-autograd-the-trial-solution" title="Link to this heading">#</a></h2>
<p>The cost function must then iterate through the given arrays
containing values for <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(t\)</span>, defines a point <span class="math notranslate nohighlight">\((x,t)\)</span> the deep
neural network and the trial solution is evaluated at, and then finds
the Jacobian of the trial solution.</p>
<p>A possible trial solution for this PDE is</p>
<div class="math notranslate nohighlight">
\[
g_t(x,t) = h_1(x,t) + x(1-x)tN(x,t,P)
\]</div>
<p>with <span class="math notranslate nohighlight">\(h_1(x,t)\)</span> being a function ensuring that <span class="math notranslate nohighlight">\(g_t(x,t)\)</span> satisfies our given conditions, and <span class="math notranslate nohighlight">\(N(x,t,P)\)</span> being the output from the deep neural network using weights and biases for each layer from <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>To fulfill the conditions, <span class="math notranslate nohighlight">\(h_1(x,t)\)</span> could be:</p>
<div class="math notranslate nohighlight">
\[
h_1(x,t) = (1-t)\Big(u(x) - \big((1-x)u(0) + x u(1)\big)\Big) = (1-t)u(x) = (1-t)\sin(\pi x)
\]</div>
<p>since <span class="math notranslate nohighlight">\((0) = u(1) = 0\)</span> and <span class="math notranslate nohighlight">\(u(x) = \sin(\pi x)\)</span>.</p>
</section>
<section id="why-the-jacobian">
<h2>Why the Jacobian?<a class="headerlink" href="#why-the-jacobian" title="Link to this heading">#</a></h2>
<p>The Jacobian is used because the program must find the derivative of
the trial solution with respect to <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>This gives the necessity of computing the Jacobian matrix, as we want
to evaluate the gradient with respect to <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(t\)</span> (note that the
Jacobian of a scalar-valued multivariate function is simply its
gradient).</p>
<p>In Autograd, the differentiation is by default done with respect to
the first input argument of your Python function. Since the points is
an array representing <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(t\)</span>, the Jacobian is calculated using
the values of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>To find the second derivative with respect to <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(t\)</span>, the
Jacobian can be found for the second time. The result is a Hessian
matrix, which is the matrix containing all the possible second order
mixed derivatives of <span class="math notranslate nohighlight">\(g(x,t)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Set up the trial function:
def u(x):
    return np.sin(np.pi*x)

def g_trial(point,P):
    x,t = point
    return (1-t)*u(x) + x*(1-x)*t*deep_neural_network(P,point)

# The right side of the ODE:
def f(point):
    return 0.

# The cost function:
def cost_function(P, x, t):
    cost_sum = 0

    g_t_jacobian_func = jacobian(g_trial)
    g_t_hessian_func = hessian(g_trial)

    for x_ in x:
        for t_ in t:
            point = np.array([x_,t_])

            g_t = g_trial(point,P)
            g_t_jacobian = g_t_jacobian_func(point,P)
            g_t_hessian = g_t_hessian_func(point,P)

            g_t_dt = g_t_jacobian[1]
            g_t_d2x = g_t_hessian[0][0]

            func = f(point)

            err_sqr = ( (g_t_dt - g_t_d2x) - func)**2
            cost_sum += err_sqr

    return cost_sum
</pre></div>
</div>
</div>
</div>
</section>
<section id="setting-up-the-network-using-autograd-the-full-program">
<h2>Setting up the network using Autograd; The full program<a class="headerlink" href="#setting-up-the-network-using-autograd-the-full-program" title="Link to this heading">#</a></h2>
<p>Having set up the network, along with the trial solution and cost function, we can now see how the deep neural network performs by comparing the results to the analytical solution.</p>
<p>The analytical solution of our problem is</p>
<div class="math notranslate nohighlight">
\[
g(x,t) = \exp(-\pi^2 t)\sin(\pi x)
\]</div>
<p>A possible way to implement a neural network solving the PDE, is given below.
Be aware, though, that it is fairly slow for the parameters used.
A better result is possible, but requires more iterations, and thus longer time to complete.</p>
<p>Indeed, the program below is not optimal in its implementation, but rather serves as an example on how to implement and use a neural network to solve a PDE.
Using TensorFlow results in a much better execution time. Try it!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import autograd.numpy as np
from autograd import jacobian,hessian,grad
import autograd.numpy.random as npr
from matplotlib import cm
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import axes3d

## Set up the network

def sigmoid(z):
    return 1/(1 + np.exp(-z))

def deep_neural_network(deep_params, x):
    # x is now a point and a 1D numpy array; make it a column vector
    num_coordinates = np.size(x,0)
    x = x.reshape(num_coordinates,-1)

    num_points = np.size(x,1)

    # N_hidden is the number of hidden layers
    N_hidden = len(deep_params) - 1 # -1 since params consist of parameters to all the hidden layers AND the output layer

    # Assume that the input layer does nothing to the input x
    x_input = x
    x_prev = x_input

    ## Hidden layers:

    for l in range(N_hidden):
        # From the list of parameters P; find the correct weigths and bias for this layer
        w_hidden = deep_params[l]

        # Add a row of ones to include bias
        x_prev = np.concatenate((np.ones((1,num_points)), x_prev ), axis = 0)

        z_hidden = np.matmul(w_hidden, x_prev)
        x_hidden = sigmoid(z_hidden)

        # Update x_prev such that next layer can use the output from this layer
        x_prev = x_hidden

    ## Output layer:

    # Get the weights and bias for this layer
    w_output = deep_params[-1]

    # Include bias:
    x_prev = np.concatenate((np.ones((1,num_points)), x_prev), axis = 0)

    z_output = np.matmul(w_output, x_prev)
    x_output = z_output

    return x_output[0][0]

## Define the trial solution and cost function
def u(x):
    return np.sin(np.pi*x)

def g_trial(point,P):
    x,t = point
    return (1-t)*u(x) + x*(1-x)*t*deep_neural_network(P,point)

# The right side of the ODE:
def f(point):
    return 0.

# The cost function:
def cost_function(P, x, t):
    cost_sum = 0

    g_t_jacobian_func = jacobian(g_trial)
    g_t_hessian_func = hessian(g_trial)

    for x_ in x:
        for t_ in t:
            point = np.array([x_,t_])

            g_t = g_trial(point,P)
            g_t_jacobian = g_t_jacobian_func(point,P)
            g_t_hessian = g_t_hessian_func(point,P)

            g_t_dt = g_t_jacobian[1]
            g_t_d2x = g_t_hessian[0][0]

            func = f(point)

            err_sqr = ( (g_t_dt - g_t_d2x) - func)**2
            cost_sum += err_sqr

    return cost_sum /( np.size(x)*np.size(t) )

## For comparison, define the analytical solution
def g_analytic(point):
    x,t = point
    return np.exp(-np.pi**2*t)*np.sin(np.pi*x)

## Set up a function for training the network to solve for the equation
def solve_pde_deep_neural_network(x,t, num_neurons, num_iter, lmb):
    ## Set up initial weigths and biases
    N_hidden = np.size(num_neurons)

    ## Set up initial weigths and biases

    # Initialize the list of parameters:
    P = [None]*(N_hidden + 1) # + 1 to include the output layer

    P[0] = npr.randn(num_neurons[0], 2 + 1 ) # 2 since we have two points, +1 to include bias
    for l in range(1,N_hidden):
        P[l] = npr.randn(num_neurons[l], num_neurons[l-1] + 1) # +1 to include bias

    # For the output layer
    P[-1] = npr.randn(1, num_neurons[-1] + 1 ) # +1 since bias is included

    print(&#39;Initial cost: &#39;,cost_function(P, x, t))

    cost_function_grad = grad(cost_function,0)

    # Let the update be done num_iter times
    for i in range(num_iter):
        cost_grad =  cost_function_grad(P, x , t)

        for l in range(N_hidden+1):
            P[l] = P[l] - lmb * cost_grad[l]

    print(&#39;Final cost: &#39;,cost_function(P, x, t))

    return P

if __name__ == &#39;__main__&#39;:
    ### Use the neural network:
    npr.seed(15)

    ## Decide the vales of arguments to the function to solve
    Nx = 10; Nt = 10
    x = np.linspace(0, 1, Nx)
    t = np.linspace(0,1,Nt)

    ## Set up the parameters for the network
    num_hidden_neurons = [100, 25]
    num_iter = 250
    lmb = 0.01

    P = solve_pde_deep_neural_network(x,t, num_hidden_neurons, num_iter, lmb)

    ## Store the results
    g_dnn_ag = np.zeros((Nx, Nt))
    G_analytical = np.zeros((Nx, Nt))
    for i,x_ in enumerate(x):
        for j, t_ in enumerate(t):
            point = np.array([x_, t_])
            g_dnn_ag[i,j] = g_trial(point,P)

            G_analytical[i,j] = g_analytic(point)

    # Find the map difference between the analytical and the computed solution
    diff_ag = np.abs(g_dnn_ag - G_analytical)
    print(&#39;Max absolute difference between the analytical solution and the network: %g&#39;%np.max(diff_ag))

    ## Plot the solutions in two dimensions, that being in position and time

    T,X = np.meshgrid(t,x)

    fig = plt.figure(figsize=(10,10))
    ax = fig.add_suplot(projection=&#39;3d&#39;)
    ax.set_title(&#39;Solution from the deep neural network w/ %d layer&#39;%len(num_hidden_neurons))
    s = ax.plot_surface(T,X,g_dnn_ag,linewidth=0,antialiased=False,cmap=cm.viridis)
    ax.set_xlabel(&#39;Time $t$&#39;)
    ax.set_ylabel(&#39;Position $x$&#39;);


    fig = plt.figure(figsize=(10,10))
    ax = fig.add_suplot(projection=&#39;3d&#39;)
    ax.set_title(&#39;Analytical solution&#39;)
    s = ax.plot_surface(T,X,G_analytical,linewidth=0,antialiased=False,cmap=cm.viridis)
    ax.set_xlabel(&#39;Time $t$&#39;)
    ax.set_ylabel(&#39;Position $x$&#39;);

    fig = plt.figure(figsize=(10,10))
    ax = fig.add_suplot(projection=&#39;3d&#39;)
    ax.set_title(&#39;Difference&#39;)
    s = ax.plot_surface(T,X,diff_ag,linewidth=0,antialiased=False,cmap=cm.viridis)
    ax.set_xlabel(&#39;Time $t$&#39;)
    ax.set_ylabel(&#39;Position $x$&#39;);

    ## Take some slices of the 3D plots just to see the solutions at particular times
    indx1 = 0
    indx2 = int(Nt/2)
    indx3 = Nt-1

    t1 = t[indx1]
    t2 = t[indx2]
    t3 = t[indx3]

    # Slice the results from the DNN
    res1 = g_dnn_ag[:,indx1]
    res2 = g_dnn_ag[:,indx2]
    res3 = g_dnn_ag[:,indx3]

    # Slice the analytical results
    res_analytical1 = G_analytical[:,indx1]
    res_analytical2 = G_analytical[:,indx2]
    res_analytical3 = G_analytical[:,indx3]

    # Plot the slices
    plt.figure(figsize=(10,10))
    plt.title(&quot;Computed solutions at time = %g&quot;%t1)
    plt.plot(x, res1)
    plt.plot(x,res_analytical1)
    plt.legend([&#39;dnn&#39;,&#39;analytical&#39;])

    plt.figure(figsize=(10,10))
    plt.title(&quot;Computed solutions at time = %g&quot;%t2)
    plt.plot(x, res2)
    plt.plot(x,res_analytical2)
    plt.legend([&#39;dnn&#39;,&#39;analytical&#39;])

    plt.figure(figsize=(10,10))
    plt.title(&quot;Computed solutions at time = %g&quot;%t3)
    plt.plot(x, res3)
    plt.plot(x,res_analytical3)
    plt.legend([&#39;dnn&#39;,&#39;analytical&#39;])

    plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="resources-on-differential-equations-and-deep-learning">
<h2>Resources on differential equations and deep learning<a class="headerlink" href="#resources-on-differential-equations-and-deep-learning" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://pdfs.semanticscholar.org/d061/df393e0e8fbfd0ea24976458b7d42419040d.pdf">Artificial neural networks for solving ordinary and partial differential equations by I.E. Lagaris et al</a></p></li>
<li><p><a class="reference external" href="https://becominghuman.ai/neural-networks-for-solving-differential-equations-fa230ac5e04c">Neural networks for solving differential equations by A. Honchar</a></p></li>
<li><p><a class="reference external" href="http://cs229.stanford.edu/proj2013/ChiaramonteKiener-SolvingDifferentialEquationsUsingNeuralNetworks.pdf">Solving differential equations using neural networks by M.M Chiaramonte and M. Kiener</a></p></li>
<li><p><a class="reference external" href="https://www.springer.com/us/book/9783540225515">Introduction to Partial Differential Equations by A. Tveito, R. Winther</a></p></li>
</ol>
</section>
<section id="convolutional-neural-networks-recognizing-images">
<h2>Convolutional Neural Networks (recognizing images)<a class="headerlink" href="#convolutional-neural-networks-recognizing-images" title="Link to this heading">#</a></h2>
<p>Convolutional neural networks (CNNs) were developed during the last
decade of the previous century, with a focus on character recognition
tasks. Nowadays, CNNs are a central element in the spectacular success
of deep learning methods. The success in for example image
classifications have made them a central tool for most machine
learning practitioners.</p>
<p>CNNs are very similar to ordinary Neural Networks.
They are made up of neurons that have learnable weights and
biases. Each neuron receives some inputs, performs a dot product and
optionally follows it with a non-linearity. The whole network still
expresses a single differentiable score function: from the raw image
pixels on one end to class scores at the other. And they still have a
loss function (for example Softmax) on the last (fully-connected) layer
and all the tips/tricks we developed for learning regular Neural
Networks still apply (back propagation, gradient descent etc etc).</p>
</section>
<section id="what-is-the-difference">
<h2>What is the Difference<a class="headerlink" href="#what-is-the-difference" title="Link to this heading">#</a></h2>
<p><strong>CNN architectures make the explicit assumption that
the inputs are images, which allows us to encode certain properties
into the architecture. These then make the forward function more
efficient to implement and vastly reduce the amount of parameters in
the network.</strong></p>
</section>
<section id="neural-networks-vs-cnns">
<h2>Neural Networks vs CNNs<a class="headerlink" href="#neural-networks-vs-cnns" title="Link to this heading">#</a></h2>
<p>Neural networks are defined as <strong>affine transformations</strong>, that is
a vector is received as input and is multiplied with a matrix of so-called weights (our unknown paramters) to produce an
output (to which a bias vector is usually added before passing the result
through a nonlinear activation function). This is applicable to any type of input, be it an
image, a sound clip or an unordered collection of features: whatever their
dimensionality, their representation can always be flattened into a vector
before the transformation.</p>
</section>
<section id="why-cnns-for-images-sound-files-medical-images-from-ct-scans-etc">
<h2>Why CNNS for images, sound files, medical images from CT scans etc?<a class="headerlink" href="#why-cnns-for-images-sound-files-medical-images-from-ct-scans-etc" title="Link to this heading">#</a></h2>
<p>However, when we consider images, sound clips and many other similar kinds of data, these data  have an intrinsic
structure. More formally, they share these important properties:</p>
<ul class="simple">
<li><p>They are stored as multi-dimensional arrays (think of the pixels of a figure) .</p></li>
<li><p>They feature one or more axes for which ordering matters (e.g., width and height axes for an image, time axis for a sound clip).</p></li>
<li><p>One axis, called the channel axis, is used to access different views of the data (e.g., the red, green and blue channels of a color image, or the left and right channels of a stereo audio track).</p></li>
</ul>
<p>These properties are not exploited when an affine transformation is applied; in
fact, all the axes are treated in the same way and the topological information
is not taken into account. Still, taking advantage of the implicit structure of
the data may prove very handy in solving some tasks, like computer vision and
speech recognition, and in these cases it would be best to preserve it. This is
where discrete convolutions come into play.</p>
<p>A discrete convolution is a linear transformation that preserves this notion of
ordering. It is sparse (only a few input units contribute to a given output
unit) and reuses parameters (the same weights are applied to multiple locations
in the input).</p>
</section>
<section id="regular-nns-dont-scale-well-to-full-images">
<h2>Regular NNs don’t scale well to full images<a class="headerlink" href="#regular-nns-dont-scale-well-to-full-images" title="Link to this heading">#</a></h2>
<p>As an example, consider
an image of size <span class="math notranslate nohighlight">\(32\times 32\times 3\)</span> (32 wide, 32 high, 3 color channels), so a
single fully-connected neuron in a first hidden layer of a regular
Neural Network would have <span class="math notranslate nohighlight">\(32\times 32\times 3 = 3072\)</span> weights. This amount still
seems manageable, but clearly this fully-connected structure does not
scale to larger images. For example, an image of more respectable
size, say <span class="math notranslate nohighlight">\(200\times 200\times 3\)</span>, would lead to neurons that have
<span class="math notranslate nohighlight">\(200\times 200\times 3 = 120,000\)</span> weights.</p>
<p>We could have
several such neurons, and the parameters would add up quickly! Clearly,
this full connectivity is wasteful and the huge number of parameters
would quickly lead to possible overfitting.</p>
<!-- dom:FIGURE: [figslides/nn.jpeg, width=500 frac=0.6]  A regular 3-layer Neural Network. -->
<!-- begin figure -->
<p><img src="figslides/nn.jpeg" width="500"><p style="font-size: 0.9em"><i>Figure 1: A regular 3-layer Neural Network.</i></p></p>
<!-- end figure --></section>
<section id="d-volumes-of-neurons">
<h2>3D volumes of neurons<a class="headerlink" href="#d-volumes-of-neurons" title="Link to this heading">#</a></h2>
<p>Convolutional Neural Networks take advantage of the fact that the
input consists of images and they constrain the architecture in a more
sensible way.</p>
<p>In particular, unlike a regular Neural Network, the
layers of a CNN have neurons arranged in 3 dimensions: width,
height, depth. (Note that the word depth here refers to the third
dimension of an activation volume, not to the depth of a full Neural
Network, which can refer to the total number of layers in a network.)</p>
<p>To understand it better, the above example of an image
with an input volume of
activations has dimensions <span class="math notranslate nohighlight">\(32\times 32\times 3\)</span> (width, height,
depth respectively).</p>
<p>The neurons in a layer will
only be connected to a small region of the layer before it, instead of
all of the neurons in a fully-connected manner. Moreover, the final
output layer could  for this specific image have dimensions <span class="math notranslate nohighlight">\(1\times 1 \times 10\)</span>,
because by the
end of the CNN architecture we will reduce the full image into a
single vector of class scores, arranged along the depth
dimension.</p>
<!-- dom:FIGURE: [figslides/cnn.jpeg, width=500 frac=0.6]  A CNN arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a CNN transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels). -->
<!-- begin figure -->
<p><img src="figslides/cnn.jpeg" width="500"><p style="font-size: 0.9em"><i>Figure 1: A CNN arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a CNN transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels).</i></p></p>
<!-- end figure --></section>
<section id="more-on-dimensionalities">
<h2>More on Dimensionalities<a class="headerlink" href="#more-on-dimensionalities" title="Link to this heading">#</a></h2>
<p>In fields like signal processing (and imaging as well), one designs
so-called filters. These filters are defined by the convolutions and
are often hand-crafted. One may specify filters for smoothing, edge
detection, frequency reshaping, and similar operations. However with
neural networks the idea is to automatically learn the filters and use
many of them in conjunction with non-linear operations (activation
functions).</p>
<p>As an example consider a neural network operating on sound sequence
data.  Assume that we an input vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> of length <span class="math notranslate nohighlight">\(d=10^6\)</span>.  We
construct then a neural network with onle hidden layer only with
<span class="math notranslate nohighlight">\(10^4\)</span> nodes. This means that we will have a weight matrix with
<span class="math notranslate nohighlight">\(10^4\times 10^6=10^{10}\)</span> weights to be determined, together with <span class="math notranslate nohighlight">\(10^4\)</span> biases.</p>
<p>Assume furthermore that we have an output layer which is meant to train whether the sound sequence represents a human voice (true) or something else (false).
It means that we have only one output node. But since this output node connects to <span class="math notranslate nohighlight">\(10^4\)</span> nodes in the hidden layer, there are in total <span class="math notranslate nohighlight">\(10^4\)</span> weights to be determined for the output layer, plus one bias. In total we have</p>
<div class="math notranslate nohighlight">
\[
\mathrm{NumberParameters}=10^{10}+10^4+10^4+1 \approx 10^{10},
\]</div>
<p>that is ten billion parameters to determine.</p>
</section>
<section id="further-remarks">
<h2>Further remarks<a class="headerlink" href="#further-remarks" title="Link to this heading">#</a></h2>
<p>The main principles that justify convolutions is locality of
information and repetion of patterns within the signal. Sound samples
of the input in adjacent spots are much more likely to affect each
other than those that are very far away. Similarly, sounds are
repeated in multiple times in the signal. While slightly simplistic,
reasoning about such a sound example demonstrates this. The same
principles then apply to images and other similar data.</p>
</section>
<section id="layers-used-to-build-cnns">
<h2>Layers used to build CNNs<a class="headerlink" href="#layers-used-to-build-cnns" title="Link to this heading">#</a></h2>
<p>A simple CNN is a sequence of layers, and every layer of a CNN
transforms one volume of activations to another through a
differentiable function. We use three main types of layers to build
CNN architectures: Convolutional Layer, Pooling Layer, and
Fully-Connected Layer (exactly as seen in regular Neural Networks). We
will stack these layers to form a full CNN architecture.</p>
<p>A simple CNN for image classification could have the architecture:</p>
<ul class="simple">
<li><p><strong>INPUT</strong> (<span class="math notranslate nohighlight">\(32\times 32 \times 3\)</span>) will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.</p></li>
<li><p><strong>CONV</strong> (convolutional )layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as <span class="math notranslate nohighlight">\([32\times 32\times 12]\)</span> if we decided to use 12 filters.</p></li>
<li><p><strong>RELU</strong> layer will apply an elementwise activation function, such as the <span class="math notranslate nohighlight">\(max(0,x)\)</span> thresholding at zero. This leaves the size of the volume unchanged (<span class="math notranslate nohighlight">\([32\times 32\times 12]\)</span>).</p></li>
<li><p><strong>POOL</strong> (pooling) layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as <span class="math notranslate nohighlight">\([16\times 16\times 12]\)</span>.</p></li>
<li><p><strong>FC</strong> (i.e. fully-connected) layer will compute the class scores, resulting in volume of size <span class="math notranslate nohighlight">\([1\times 1\times 10]\)</span>, where each of the 10 numbers correspond to a class score, such as among the 10 categories of the MNIST images we considered above . As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.</p></li>
</ul>
</section>
<section id="transforming-images">
<h2>Transforming images<a class="headerlink" href="#transforming-images" title="Link to this heading">#</a></h2>
<p>CNNs transform the original image layer by layer from the original
pixel values to the final class scores.</p>
<p>Observe that some layers contain
parameters and other don’t. In particular, the CNN layers perform
transformations that are a function of not only the activations in the
input volume, but also of the parameters (the weights and biases of
the neurons). On the other hand, the RELU/POOL layers will implement a
fixed function. The parameters in the CONV/FC layers will be trained
with gradient descent so that the class scores that the CNN computes
are consistent with the labels in the training set for each image.</p>
</section>
<section id="cnns-in-brief">
<h2>CNNs in brief<a class="headerlink" href="#cnns-in-brief" title="Link to this heading">#</a></h2>
<p>In summary:</p>
<ul class="simple">
<li><p>A CNN architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)</p></li>
<li><p>There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)</p></li>
<li><p>Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function</p></li>
<li><p>Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)</p></li>
<li><p>Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t)</p></li>
</ul>
</section>
<section id="a-deep-cnn-model-from-raschka-et-al">
<h2>A deep CNN model (<a class="reference external" href="https://github.com/rasbt/machine-learning-book">From Raschka et al</a>)<a class="headerlink" href="#a-deep-cnn-model-from-raschka-et-al" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/deepcnn.png, width=500 frac=0.67]  A deep CNN -->
<!-- begin figure -->
<p><img src="figslides/deepcnn.png" width="500"><p style="font-size: 0.9em"><i>Figure 1: A deep CNN</i></p></p>
<!-- end figure --></section>
<section id="key-idea">
<h2>Key Idea<a class="headerlink" href="#key-idea" title="Link to this heading">#</a></h2>
<p>A dense neural network is representd by an affine operation (like
matrix-matrix multiplication) where all parameters are included.</p>
<p>The key idea in CNNs for say imaging is that in images neighbor pixels tend to be related! So we connect
only neighboring neurons in the input instead of connecting all with the first hidden layer.</p>
<p>We say we perform a filtering (convolution is the mathematical operation).</p>
</section>
<section id="how-to-do-image-compression-before-the-era-of-deep-learning">
<h2>How to do image compression before the era of deep learning<a class="headerlink" href="#how-to-do-image-compression-before-the-era-of-deep-learning" title="Link to this heading">#</a></h2>
<p>The singular-value decomposition (SVD) algorithm has been for decades one of the standard ways of compressing images.
The <a class="reference external" href="https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapter2.html#the-singular-value-decomposition">lectures on the SVD</a> give many of the essential details concerning the SVD.</p>
<p>The orthogonal vectors which are obtained from the SVD, can be used to
project down the dimensionality of a given image. In the example here
we gray-scale an image and downsize it.</p>
<p>This recipe relies on us being able to actually perform the SVD. For
large images, and in particular with many images to reconstruct, using the SVD
may quickly become an overwhelming task. With the advent of efficient deep
learning methods like CNNs and later generative methods, these methods
have become in the last years the premier way of performing image
analysis. In particular for classification problems with labelled images.</p>
</section>
<section id="the-svd-example">
<h2>The SVD example<a class="headerlink" href="#the-svd-example" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from matplotlib.image import imread
import matplotlib.pyplot as plt
import scipy.linalg as ln
import numpy as np
import os
from PIL import Image
from math import log10, sqrt 
plt.rcParams[&#39;figure.figsize&#39;] = [16, 8]
# Import image
A = imread(os.path.join(&quot;figslides/photo1.jpg&quot;))
X = A.dot([0.299, 0.5870, 0.114]) # Convert RGB to grayscale
img = plt.imshow(X)
# convert to gray
img.set_cmap(&#39;gray&#39;)
plt.axis(&#39;off&#39;)
plt.show()
# Call image size
print(&#39;: %s&#39;%str(X.shape))


# split the matrix into U, S, VT
U, S, VT = np.linalg.svd(X,full_matrices=False)
S = np.diag(S)
m = 800 # Image&#39;s width
n = 1200 # Image&#39;s height
j = 0
# Try compression with different k vectors (these represent projections):
for k in (5,10, 20, 100,200,400,500):
    # Original size of the image
    originalSize = m * n 
    # Size after compressed
    compressedSize = k * (1 + m + n) 
    # The projection of the original image
    Xapprox = U[:,:k] @ S[0:k,:k] @ VT[:k,:]
    plt.figure(j+1)
    j += 1
    img = plt.imshow(Xapprox)
    img.set_cmap(&#39;gray&#39;)
    
    plt.axis(&#39;off&#39;)
    plt.title(&#39;k = &#39; + str(k))
    plt.show() 
    print(&#39;Original size of image:&#39;)
    print(originalSize)
    print(&#39;Compression rate as Compressed image / Original size:&#39;)
    ratio = compressedSize * 1.0 / originalSize
    print(ratio)
    print(&#39;Compression rate is &#39; + str( round(ratio * 100 ,2)) + &#39;%&#39; )  
    # Estimate MQA
    x= X.astype(&quot;float&quot;)
    y=Xapprox.astype(&quot;float&quot;)
    err = np.sum((x - y) ** 2)
    err /= float(X.shape[0] * Xapprox.shape[1])
    print(&#39;The mean-square deviation &#39;+ str(round( err)))
    max_pixel = 255.0
    # Estimate Signal Noise Ratio
    srv = 20 * (log10(max_pixel / sqrt(err)))
    print(&#39;Signa to noise ratio &#39;+ str(round(srv)) +&#39;dB&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="mathematics-of-cnns">
<h2>Mathematics of CNNs<a class="headerlink" href="#mathematics-of-cnns" title="Link to this heading">#</a></h2>
<p>The mathematics of CNNs is based on the mathematical operation of
<strong>convolution</strong>.  In mathematics (in particular in functional analysis),
convolution is represented by mathematical operations (integration,
summation etc) on two functions in order to produce a third function
that expresses how the shape of one gets modified by the other.
Convolution has a plethora of applications in a variety of
disciplines, spanning from statistics to signal processing, computer
vision, solutions of differential equations,linear algebra,
engineering, and yes, machine learning.</p>
<p>Mathematically, convolution is defined as follows (one-dimensional example):
Let us define a continuous function <span class="math notranslate nohighlight">\(y(t)\)</span> given by</p>
<div class="math notranslate nohighlight">
\[
y(t) = \int x(a) w(t-a) da,
\]</div>
<p>where <span class="math notranslate nohighlight">\(x(a)\)</span> represents a so-called input and <span class="math notranslate nohighlight">\(w(t-a)\)</span> is normally called the weight function or kernel.</p>
<p>The above integral is written in  a more compact form as</p>
<div class="math notranslate nohighlight">
\[
y(t) = \left(x * w\right)(t).
\]</div>
<p>The discretized version reads</p>
<div class="math notranslate nohighlight">
\[
y(t) = \sum_{a=-\infty}^{a=\infty}x(a)w(t-a).
\]</div>
<p>Computing the inverse of the above convolution operations is known as deconvolution and the process is commutative.</p>
<p>How can we use this? And what does it mean? Let us study some familiar examples first.</p>
</section>
<section id="convolution-examples-polynomial-multiplication">
<h2>Convolution Examples: Polynomial multiplication<a class="headerlink" href="#convolution-examples-polynomial-multiplication" title="Link to this heading">#</a></h2>
<p>Our first example is that of a multiplication between two polynomials,
which we will rewrite in terms of the mathematics of convolution. In
the final stage, since the problem here is a discrete one, we will
recast the final expression in terms of a matrix-vector
multiplication, where the matrix is a so-called <a class="reference external" href="https://link.springer.com/book/10.1007/978-93-86279-04-0">Toeplitz matrix
</a>.</p>
<p>Let us look a the following polynomials to second and third order, respectively:</p>
<div class="math notranslate nohighlight">
\[
p(t) = \alpha_0+\alpha_1 t+\alpha_2 t^2,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
s(t) = \beta_0+\beta_1 t+\beta_2 t^2+\beta_3 t^3.
\]</div>
<p>The polynomial multiplication gives us a new polynomial of degree <span class="math notranslate nohighlight">\(5\)</span></p>
<div class="math notranslate nohighlight">
\[
z(t) = \delta_0+\delta_1 t+\delta_2 t^2+\delta_3 t^3+\delta_4 t^4+\delta_5 t^5.
\]</div>
</section>
<section id="efficient-polynomial-multiplication">
<h2>Efficient Polynomial Multiplication<a class="headerlink" href="#efficient-polynomial-multiplication" title="Link to this heading">#</a></h2>
<p>Computing polynomial products can be implemented efficiently if we rewrite the more brute force multiplications using convolution.
We note first that the new coefficients are given as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\delta_0=&amp;\alpha_0\beta_0\\
\delta_1=&amp;\alpha_1\beta_0+\alpha_0\beta_1\\
\delta_2=&amp;\alpha_0\beta_2+\alpha_1\beta_1+\alpha_2\beta_0\\
\delta_3=&amp;\alpha_1\beta_2+\alpha_2\beta_1+\alpha_0\beta_3\\
\delta_4=&amp;\alpha_2\beta_2+\alpha_1\beta_3\\
\delta_5=&amp;\alpha_2\beta_3.\\
\end{split}
\end{split}\]</div>
<p>We note that <span class="math notranslate nohighlight">\(\alpha_i=0\)</span> except for <span class="math notranslate nohighlight">\(i\in \left\{0,1,2\right\}\)</span> and <span class="math notranslate nohighlight">\(\beta_i=0\)</span> except for <span class="math notranslate nohighlight">\(i\in\left\{0,1,2,3\right\}\)</span>.</p>
<p>We can then rewrite the coefficients <span class="math notranslate nohighlight">\(\delta_j\)</span> using a discrete convolution as</p>
<div class="math notranslate nohighlight">
\[
\delta_j = \sum_{i=-\infty}^{i=\infty}\alpha_i\beta_{j-i}=(\alpha * \beta)_j,
\]</div>
<p>or as a double sum with restriction <span class="math notranslate nohighlight">\(l=i+j\)</span></p>
<div class="math notranslate nohighlight">
\[
\delta_l = \sum_{ij}\alpha_i\beta_{j}.
\]</div>
</section>
<section id="further-simplification">
<h2>Further simplification<a class="headerlink" href="#further-simplification" title="Link to this heading">#</a></h2>
<p>Although we may have redundant operations with some few zeros for <span class="math notranslate nohighlight">\(\beta_i\)</span>, we can rewrite the above sum in a more compact way as</p>
<div class="math notranslate nohighlight">
\[
\delta_i = \sum_{k=0}^{k=m-1}\alpha_k\beta_{i-k},
\]</div>
<p>where <span class="math notranslate nohighlight">\(m=3\)</span> in our case, the maximum length of
the vector <span class="math notranslate nohighlight">\(\alpha\)</span>. Note that the vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> has length <span class="math notranslate nohighlight">\(n=4\)</span>. Below we will find an even more efficient representation.</p>
</section>
<section id="a-more-efficient-way-of-coding-the-above-convolution">
<h2>A more efficient way of coding the above Convolution<a class="headerlink" href="#a-more-efficient-way-of-coding-the-above-convolution" title="Link to this heading">#</a></h2>
<p>Since we only have a finite number of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> values
which are non-zero, we can rewrite the above convolution expressions
as a matrix-vector multiplication</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\delta}=\begin{bmatrix}\alpha_0 &amp; 0 &amp; 0 &amp; 0 \\
                            \alpha_1 &amp; \alpha_0 &amp; 0 &amp; 0 \\
			    \alpha_2 &amp; \alpha_1 &amp; \alpha_0 &amp; 0 \\
			    0 &amp; \alpha_2 &amp; \alpha_1 &amp; \alpha_0 \\
			    0 &amp; 0 &amp; \alpha_2 &amp; \alpha_1 \\
			    0 &amp; 0 &amp; 0 &amp; \alpha_2
			    \end{bmatrix}\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3\end{bmatrix}.
\end{split}\]</div>
</section>
<section id="commutative-process">
<h2>Commutative process<a class="headerlink" href="#commutative-process" title="Link to this heading">#</a></h2>
<p>The process is commutative and we can easily see that we can rewrite the multiplication in terms of  a matrix holding <span class="math notranslate nohighlight">\(\beta\)</span> and a vector holding <span class="math notranslate nohighlight">\(\alpha\)</span>.
In this case we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\delta}=\begin{bmatrix}\beta_0 &amp; 0 &amp; 0  \\
                            \beta_1 &amp; \beta_0 &amp; 0  \\
			    \beta_2 &amp; \beta_1 &amp; \beta_0  \\
			    \beta_3 &amp; \beta_2 &amp; \beta_1 \\
			    0 &amp; \beta_3 &amp; \beta_2 \\
			    0 &amp; 0 &amp; \beta_3
			    \end{bmatrix}\begin{bmatrix} \alpha_0 \\ \alpha_1 \\ \alpha_2\end{bmatrix}.
\end{split}\]</div>
<p>Note that the use of these matrices is for mathematical purposes only
and not implementation purposes.  When implementing the above equation
we do not encode (and allocate memory) the matrices explicitely.  We
rather code the convolutions in the minimal memory footprint that they
require.</p>
</section>
<section id="toeplitz-matrices">
<h2>Toeplitz matrices<a class="headerlink" href="#toeplitz-matrices" title="Link to this heading">#</a></h2>
<p>The above matrices are examples of so-called <a class="reference external" href="https://link.springer.com/book/10.1007/978-93-86279-04-0">Toeplitz
matrices</a>. A
Toeplitz matrix is a matrix in which each descending diagonal from
left to right is constant. For instance the last matrix, which we
rewrite as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{A}=\begin{bmatrix}a_0 &amp; 0 &amp; 0  \\
                            a_1 &amp; a_0 &amp; 0  \\
			    a_2 &amp; a_1 &amp; a_0  \\
			    a_3 &amp; a_2 &amp; a_1 \\
			    0 &amp; a_3 &amp; a_2 \\
			    0 &amp; 0 &amp; a_3
			    \end{bmatrix},
\end{split}\]</div>
<p>with elements <span class="math notranslate nohighlight">\(a_{ii}=a_{i+1,j+1}=a_{i-j}\)</span> is an example of a Toeplitz
matrix. Such a matrix does not need to be a square matrix.  Toeplitz
matrices are also closely connected with Fourier series, because the multiplication operator by a trigonometric
polynomial, compressed to a finite-dimensional space, can be
represented by such a matrix. The example above shows that we can
represent linear convolution as multiplication of a Toeplitz matrix by
a vector.</p>
</section>
<section id="fourier-series-and-toeplitz-matrices">
<h2>Fourier series and Toeplitz matrices<a class="headerlink" href="#fourier-series-and-toeplitz-matrices" title="Link to this heading">#</a></h2>
<p>This is an active and ogoing research area concerning CNNs. The following articles may be of interest</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.sciencedirect.com/topics/engineering/convolution-theorem#:~:text=The%20convolution%20theorem%20(together%20with,k%20)%20G%20(%20k%20)%20.">Read more about the convolution theorem and Fouriers series</a></p></li>
<li><p><a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S1568494623006257">Fourier Transform Layer</a></p></li>
</ol>
</section>
<section id="generalizing-the-above-one-dimensional-case">
<h2>Generalizing the above one-dimensional case<a class="headerlink" href="#generalizing-the-above-one-dimensional-case" title="Link to this heading">#</a></h2>
<p>In order to align the above simple case with the more general
convolution cases, we rename <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span>, whose length is <span class="math notranslate nohighlight">\(m=3\)</span>,
with <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>.  We will interpret <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> as a weight/filter function
with which we want to perform the convolution with an input variable
<span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> of length <span class="math notranslate nohighlight">\(n\)</span>.  We will assume always that the filter
<span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> has dimensionality <span class="math notranslate nohighlight">\(m \le n\)</span>.</p>
<p>We replace thus <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\delta}\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> and have</p>
<div class="math notranslate nohighlight">
\[
y(i)= \left(x*w\right)(i)= \sum_{k=0}^{k=m-1}w(k)x(i-k),
\]</div>
<p>where <span class="math notranslate nohighlight">\(m=3\)</span> in our case, the maximum length of the vector <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>.
Here the symbol <span class="math notranslate nohighlight">\(*\)</span> represents the mathematical operation of convolution.</p>
</section>
<section id="memory-considerations">
<h2>Memory considerations<a class="headerlink" href="#memory-considerations" title="Link to this heading">#</a></h2>
<p>This expression leaves us however with some terms with negative
indices, for example <span class="math notranslate nohighlight">\(x(-1)\)</span> and <span class="math notranslate nohighlight">\(x(-2)\)</span> which may not be defined. Our
vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> has components <span class="math notranslate nohighlight">\(x(0)\)</span>, <span class="math notranslate nohighlight">\(x(1)\)</span>, <span class="math notranslate nohighlight">\(x(2)\)</span> and <span class="math notranslate nohighlight">\(x(3)\)</span>.</p>
<p>The index <span class="math notranslate nohighlight">\(j\)</span> for <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> runs from <span class="math notranslate nohighlight">\(j=0\)</span> to <span class="math notranslate nohighlight">\(j=3\)</span> since <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is meant to
represent a third-order polynomial.</p>
<p>Furthermore, the index <span class="math notranslate nohighlight">\(i\)</span> runs from <span class="math notranslate nohighlight">\(i=0\)</span> to <span class="math notranslate nohighlight">\(i=5\)</span> since <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>
contains the coefficients of a fifth-order polynomial.  When <span class="math notranslate nohighlight">\(i=5\)</span> we
may also have values of <span class="math notranslate nohighlight">\(x(4)\)</span> and <span class="math notranslate nohighlight">\(x(5)\)</span> which are not defined.</p>
</section>
<section id="padding">
<h2>Padding<a class="headerlink" href="#padding" title="Link to this heading">#</a></h2>
<p>The solution to this is what is called <strong>padding</strong>!  We simply define a
new vector <span class="math notranslate nohighlight">\(x\)</span> with two added elements set to zero before <span class="math notranslate nohighlight">\(x(0)\)</span> and
two new elements after <span class="math notranslate nohighlight">\(x(3)\)</span> set to zero. That is, we augment the
length of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> from <span class="math notranslate nohighlight">\(n=4\)</span> to <span class="math notranslate nohighlight">\(n+2P=8\)</span>, where <span class="math notranslate nohighlight">\(P=2\)</span> is the padding
constant (a new hyperparameter), see discussions below as well.</p>
</section>
<section id="new-vector">
<h2>New vector<a class="headerlink" href="#new-vector" title="Link to this heading">#</a></h2>
<p>We have a new vector defined as <span class="math notranslate nohighlight">\(x(0)=0\)</span>, <span class="math notranslate nohighlight">\(x(1)=0\)</span>,
<span class="math notranslate nohighlight">\(x(2)=\beta_0\)</span>, <span class="math notranslate nohighlight">\(x(3)=\beta_1\)</span>, <span class="math notranslate nohighlight">\(x(4)=\beta_2\)</span>, <span class="math notranslate nohighlight">\(x(5)=\beta_3\)</span>,
<span class="math notranslate nohighlight">\(x(6)=0\)</span>, and <span class="math notranslate nohighlight">\(x(7)=0\)</span>.</p>
<p>We have added four new elements, which
are all zero. The benefit is that we can rewrite the equation for
<span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, with <span class="math notranslate nohighlight">\(i=0,1,\dots,5\)</span>,</p>
<div class="math notranslate nohighlight">
\[
y(i) = \sum_{k=0}^{k=m-1}w(k)x(i+(m-1)-k).
\]</div>
<p>As an example, we have</p>
<div class="math notranslate nohighlight">
\[
y(4)=x(6)w(0)+x(5)w(1)+x(4)w(2)=0\times \alpha_0+\beta_3\alpha_1+\beta_2\alpha_2,
\]</div>
<p>as before except that we have an additional term <span class="math notranslate nohighlight">\(x(6)w(0)\)</span>, which is zero.</p>
<p>Similarly, for the fifth-order term we have</p>
<div class="math notranslate nohighlight">
\[
y(5)=x(7)w(0)+x(6)w(1)+x(5)w(2)=0\times \alpha_0+0\times\alpha_1+\beta_3\alpha_2.
\]</div>
<p>The zeroth-order term is</p>
<div class="math notranslate nohighlight">
\[
y(0)=x(2)w(0)+x(1)w(1)+x(0)w(2)=\beta_0 \alpha_0+0\times\alpha_1+0\times\alpha_2=\alpha_0\beta_0.
\]</div>
</section>
<section id="rewriting-as-dot-products">
<h2>Rewriting as dot products<a class="headerlink" href="#rewriting-as-dot-products" title="Link to this heading">#</a></h2>
<p>If we now flip the filter/weight vector, with the following term as a typical example</p>
<div class="math notranslate nohighlight">
\[
y(0)=x(2)w(0)+x(1)w(1)+x(0)w(2)=x(2)\tilde{w}(2)+x(1)\tilde{w}(1)+x(0)\tilde{w}(0),
\]</div>
<p>with <span class="math notranslate nohighlight">\(\tilde{w}(0)=w(2)\)</span>, <span class="math notranslate nohighlight">\(\tilde{w}(1)=w(1)\)</span>, and <span class="math notranslate nohighlight">\(\tilde{w}(2)=w(0)\)</span>, we can then rewrite the above sum as a dot product of
<span class="math notranslate nohighlight">\(x(i:i+(m-1))\tilde{w}\)</span> for element <span class="math notranslate nohighlight">\(y(i)\)</span>, where <span class="math notranslate nohighlight">\(x(i:i+(m-1))\)</span> is simply a patch of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> of size <span class="math notranslate nohighlight">\(m-1\)</span>.</p>
<p>The padding <span class="math notranslate nohighlight">\(P\)</span> we have introduced for the convolution stage is just
another hyperparameter which is introduced as part of the
architecture. Similarly, below we will also introduce another
hyperparameter called <strong>Stride</strong> <span class="math notranslate nohighlight">\(S\)</span>.</p>
</section>
<section id="cross-correlation">
<h2>Cross correlation<a class="headerlink" href="#cross-correlation" title="Link to this heading">#</a></h2>
<p>In essentially all applications one uses what is called cross correlation instead of the standard convolution described above.
This means that multiplication is performed in the same direction and instead of the general expression we have discussed above (with infinite sums)</p>
<div class="math notranslate nohighlight">
\[
y(i) = \sum_{k=-\infty}^{k=\infty}w(k)x(i-k),
\]</div>
<p>we have now</p>
<div class="math notranslate nohighlight">
\[
y(i) = \sum_{k=-\infty}^{k=\infty}w(k)x(i+k).
\]</div>
<p>Both TensorFlow and PyTorch (as well as our own code example below),
implement the last equation, although it is normally referred to as
convolution.  The same padding rules and stride rules discussed below
apply to this expression as well.</p>
<p>We leave it as an exercise for you to convince yourself that the example we have discussed till now, gives the same final expression using the last expression.</p>
</section>
<section id="two-dimensional-objects">
<h2>Two-dimensional objects<a class="headerlink" href="#two-dimensional-objects" title="Link to this heading">#</a></h2>
<p>We are now ready to start studying the discrete convolutions relevant for convolutional neural networks.
We often use convolutions over more than one dimension at a time. If
we have a two-dimensional image <span class="math notranslate nohighlight">\(X\)</span> as input, we can have a <strong>filter</strong>
defined by a two-dimensional <strong>kernel/weight/filter</strong> <span class="math notranslate nohighlight">\(W\)</span>. This leads to an output <span class="math notranslate nohighlight">\(Y\)</span></p>
<div class="math notranslate nohighlight">
\[
Y(i,j)=(X * W)(i,j) = \sum_m\sum_n X(m,n)W(i-m,j-n).
\]</div>
<p>Convolution is a commutative process, which means we can rewrite this equation as</p>
<div class="math notranslate nohighlight">
\[
Y(i,j)=(X * W)(i,j) = \sum_m\sum_n X(i-m,j-n)W(m,n).
\]</div>
<p>Normally the latter is more straightforward to implement in a machine
larning library since there is less variation in the range of values
of <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>As mentioned above, most deep learning libraries implement
cross-correlation instead of convolution (although it is referred to as
convolution)</p>
<div class="math notranslate nohighlight">
\[
Y(i,j)=(X * W)(i,j) = \sum_m\sum_n X(i+m,j+n)W(m,n).
\]</div>
</section>
<section id="cnns-in-more-detail-simple-example">
<h2>CNNs in more detail, simple example<a class="headerlink" href="#cnns-in-more-detail-simple-example" title="Link to this heading">#</a></h2>
<p>Let assume we have an input matrix <span class="math notranslate nohighlight">\(X\)</span> of dimensionality <span class="math notranslate nohighlight">\(3\times 3\)</span>
and a <span class="math notranslate nohighlight">\(2\times 2\)</span> filter <span class="math notranslate nohighlight">\(W\)</span> given by the following matrices</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X}=\begin{bmatrix}x_{00} &amp; x_{01} &amp; x_{02}  \\
                      x_{10} &amp; x_{11} &amp; x_{12}  \\
	              x_{20} &amp; x_{21} &amp; x_{22} \end{bmatrix},
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{W}=\begin{bmatrix}w_{00} &amp; w_{01} \\
	              w_{10} &amp; w_{11}\end{bmatrix}.
\end{split}\]</div>
<p>We introduce now the hyperparameter <span class="math notranslate nohighlight">\(S\)</span> <strong>stride</strong>. Stride represents how the filter <span class="math notranslate nohighlight">\(W\)</span> moves the convolution process on the matrix <span class="math notranslate nohighlight">\(X\)</span>.
We strongly recommend the repository on <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic">Arithmetic of deep learning by Dumoulin and Visin</a></p>
<p>Here we set the stride equal to <span class="math notranslate nohighlight">\(S=1\)</span>, which means that, starting with the element <span class="math notranslate nohighlight">\(x_{00}\)</span>, the filter will act on <span class="math notranslate nohighlight">\(2\times 2\)</span> submatrices each time, starting with the upper corner and moving according to the stride value column by column.</p>
<p>Here we perform the operation</p>
<div class="math notranslate nohighlight">
\[
Y_(i,j)=(X * W)(i,j) = \sum_m\sum_n X(i-m,j-n)W(m,n),
\]</div>
<p>and obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{Y}=\begin{bmatrix}x_{00}w_{00}+x_{01}w_{01}+x_{10}w_{10}+x_{11}w_{11} &amp; x_{01}w_{00}+x_{02}w_{01}+x_{11}w_{10}+x_{12}w_{11}  \\
	              x_{10}w_{00}+x_{11}w_{01}+x_{20}w_{10}+x_{21}w_{11} &amp; x_{11}w_{00}+x_{12}w_{01}+x_{21}w_{10}+x_{22}w_{11}\end{bmatrix}.
\end{split}\]</div>
<p>We can rewrite this operation in terms of a matrix-vector multiplication by defining a new vector where we flatten out the inputs as a vector <span class="math notranslate nohighlight">\(\boldsymbol{X}'\)</span> of length <span class="math notranslate nohighlight">\(9\)</span> and
a matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}'\)</span> with dimension <span class="math notranslate nohighlight">\(4\times 9\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X}'=\begin{bmatrix}x_{00} \\ x_{01} \\ x_{02} \\ x_{10} \\ x_{11} \\ x_{12} \\ x_{20} \\ x_{21} \\ x_{22} \end{bmatrix},
\end{split}\]</div>
<p>and the new matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{W}'=\begin{bmatrix} w_{00} &amp; w_{01} &amp; 0 &amp; w_{10} &amp; w_{11} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
                        0  &amp; w_{00} &amp; w_{01} &amp; 0 &amp; w_{10} &amp; w_{11} &amp; 0 &amp; 0 &amp; 0 \\
			0 &amp; 0 &amp; 0 &amp; w_{00} &amp; w_{01} &amp; 0 &amp; w_{10} &amp; w_{11} &amp; 0  \\
                        0 &amp; 0 &amp; 0 &amp; 0 &amp; w_{00} &amp; w_{01} &amp; 0 &amp; w_{10} &amp; w_{11}\end{bmatrix}.
\end{split}\]</div>
<p>We see easily that performing the matrix-vector multiplication <span class="math notranslate nohighlight">\(\boldsymbol{W}'\boldsymbol{X}'\)</span> is the same as the above convolution with stride <span class="math notranslate nohighlight">\(S=1\)</span>, that is</p>
<div class="math notranslate nohighlight">
\[
Y=(\boldsymbol{W}*\boldsymbol{X}),
\]</div>
<p>is now given by <span class="math notranslate nohighlight">\(\boldsymbol{W}'\boldsymbol{X}'\)</span> which is a vector of length <span class="math notranslate nohighlight">\(4\)</span> instead of the originally resulting  <span class="math notranslate nohighlight">\(2\times 2\)</span> output matrix.</p>
</section>
<section id="the-convolution-stage">
<h2>The convolution stage<a class="headerlink" href="#the-convolution-stage" title="Link to this heading">#</a></h2>
<p>The convolution stage, where we apply different filters <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> in
order to reduce the dimensionality of an image, adds, in addition to
the weights and biases (to be trained by the back propagation
algorithm) that define the filters, two new hyperparameters, the so-called
<strong>padding</strong> <span class="math notranslate nohighlight">\(P\)</span> and the stride <span class="math notranslate nohighlight">\(S\)</span>.</p>
</section>
<section id="finding-the-number-of-parameters">
<h2>Finding the number of parameters<a class="headerlink" href="#finding-the-number-of-parameters" title="Link to this heading">#</a></h2>
<p>In the above example we have an input matrix of dimension <span class="math notranslate nohighlight">\(3\times
3\)</span>. In general we call the input for an input volume and it is defined
by its width <span class="math notranslate nohighlight">\(H_1\)</span>, height <span class="math notranslate nohighlight">\(H_1\)</span> and depth <span class="math notranslate nohighlight">\(D_1\)</span>. If we have the
standard three color channels <span class="math notranslate nohighlight">\(D_1=3\)</span>.</p>
<p>The above example has <span class="math notranslate nohighlight">\(W_1=H_1=3\)</span> and <span class="math notranslate nohighlight">\(D_1=1\)</span>.</p>
<p>When we introduce the filter we have the following additional hyperparameters</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(K\)</span> the number of filters. It is common to perform the convolution of the input several times since by experience shrinking the input too fast does not work well</p></li>
<li><p><span class="math notranslate nohighlight">\(F\)</span> as the filter’s spatial extent</p></li>
<li><p><span class="math notranslate nohighlight">\(S\)</span> as the stride parameter</p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span> as the padding parameter</p></li>
</ol>
<p>These parameters are defined by the architecture of the network and are not included in the training.</p>
</section>
<section id="new-image-or-volume">
<h2>New image (or volume)<a class="headerlink" href="#new-image-or-volume" title="Link to this heading">#</a></h2>
<p>Acting with the filter on the input volume produces an output volume
which is defined by its width <span class="math notranslate nohighlight">\(W_2\)</span>, its height <span class="math notranslate nohighlight">\(H_2\)</span> and its depth
<span class="math notranslate nohighlight">\(D_2\)</span>.</p>
<p>These are defined by the following relations</p>
<div class="math notranslate nohighlight">
\[
W_2 = \frac{(W_1-F+2P)}{S}+1,
\]</div>
<div class="math notranslate nohighlight">
\[
H_2 = \frac{(H_1-F+2P)}{S}+1,
\]</div>
<p>and <span class="math notranslate nohighlight">\(D_2=K\)</span>.</p>
</section>
<section id="parameters-to-train-common-settings">
<h2>Parameters to train, common settings<a class="headerlink" href="#parameters-to-train-common-settings" title="Link to this heading">#</a></h2>
<p>With parameter sharing, the convolution involves thus  for each filter  <span class="math notranslate nohighlight">\(F\times F\times D_1\)</span> weights plus one bias parameter.</p>
<p>In total we have</p>
<div class="math notranslate nohighlight">
\[
\left(F\times F\times D_1)\right) \times K+(K\mathrm{--biases}),
\]</div>
<p>parameters to train by back propagation.</p>
<p>It is common to let <span class="math notranslate nohighlight">\(K\)</span> come in powers of <span class="math notranslate nohighlight">\(2\)</span>, that is <span class="math notranslate nohighlight">\(32\)</span>, <span class="math notranslate nohighlight">\(64\)</span>, <span class="math notranslate nohighlight">\(128\)</span> etc.</p>
<p><strong>Common settings.</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\begin{array}{c} F=3 &amp; S=1 &amp; P=1 \end{array}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\begin{array}{c} F=5 &amp; S=1 &amp; P=2 \end{array}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\begin{array}{c} F=5 &amp; S=2 &amp; P=\mathrm{open} \end{array}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\begin{array}{c} F=1 &amp; S=1 &amp; P=0 \end{array}\)</span></p></li>
</ol>
</section>
<section id="examples-of-cnn-setups">
<h2>Examples of CNN setups<a class="headerlink" href="#examples-of-cnn-setups" title="Link to this heading">#</a></h2>
<p>Let us assume we have an input volume <span class="math notranslate nohighlight">\(V\)</span> given by an image of dimensionality
<span class="math notranslate nohighlight">\(32\times 32 \times 3\)</span>, that is three color channels and <span class="math notranslate nohighlight">\(32\times 32\)</span> pixels.</p>
<p>We apply a filter of dimension <span class="math notranslate nohighlight">\(5\times 5\)</span> ten times with stride <span class="math notranslate nohighlight">\(S=1\)</span> and padding <span class="math notranslate nohighlight">\(P=0\)</span>.</p>
<p>The output volume is given by <span class="math notranslate nohighlight">\((32-5)/1+1=28\)</span>, resulting in ten images
of dimensionality <span class="math notranslate nohighlight">\(28\times 28\times 3\)</span>.</p>
<p>The total number of parameters to train for each filter is then
<span class="math notranslate nohighlight">\(5\times 5\times 3+1\)</span>, where the last parameter is the bias. This
gives us <span class="math notranslate nohighlight">\(76\)</span> parameters for each filter, leading to a total of <span class="math notranslate nohighlight">\(760\)</span>
parameters for the ten filters.</p>
<p>How many parameters will a filter of dimensionality <span class="math notranslate nohighlight">\(3\times 3\)</span>
(adding color channels) result in if we produce <span class="math notranslate nohighlight">\(32\)</span> new images? Use <span class="math notranslate nohighlight">\(S=1\)</span> and <span class="math notranslate nohighlight">\(P=0\)</span>.</p>
<p>Note that strides constitute a form of <strong>subsampling</strong>. As an alternative to
being interpreted as a measure of how much the kernel/filter is translated, strides
can also be viewed as how much of the output is retained. For instance, moving
the kernel by hops of two is equivalent to moving the kernel by hops of one but
retaining only odd output elements.</p>
</section>
<section id="summarizing-performing-a-general-discrete-convolution-from-raschka-et-al">
<h2>Summarizing: Performing a general discrete convolution (<a class="reference external" href="https://github.com/rasbt/machine-learning-book">From Raschka et al</a>)<a class="headerlink" href="#summarizing-performing-a-general-discrete-convolution-from-raschka-et-al" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/discreteconv1.png, width=500 frac=0.67]  A deep CNN -->
<!-- begin figure -->
<p><img src="figslides/discreteconv1.png" width="500"><p style="font-size: 0.9em"><i>Figure 1: A deep CNN</i></p></p>
<!-- end figure --></section>
<section id="pooling">
<h2>Pooling<a class="headerlink" href="#pooling" title="Link to this heading">#</a></h2>
<p>In addition to discrete convolutions themselves, <strong>pooling</strong> operations
make up another important building block in CNNs. Pooling operations reduce
the size of feature maps by using some function to summarize subregions, such
as taking the average or the maximum value.</p>
<p>Pooling works by sliding a window across the input and feeding the content of
the window to a <strong>pooling function</strong>. In some sense, pooling works very much
like a discrete convolution, but replaces the linear combination described by
the kernel with some other function.</p>
</section>
<section id="pooling-arithmetic">
<h2>Pooling arithmetic<a class="headerlink" href="#pooling-arithmetic" title="Link to this heading">#</a></h2>
<p>In a neural network, pooling layers provide invariance to small translations of
the input. The most common kind of pooling is <strong>max pooling</strong>, which
consists in splitting the input in (usually non-overlapping) patches and
outputting the maximum value of each patch. Other kinds of pooling exist, e.g.,
mean or average pooling, which all share the same idea of aggregating the input
locally by applying a non-linearity to the content of some patches.</p>
</section>
<section id="pooling-types-from-raschka-et-al">
<h2>Pooling types (<a class="reference external" href="https://github.com/rasbt/machine-learning-book">From Raschka et al</a>)<a class="headerlink" href="#pooling-types-from-raschka-et-al" title="Link to this heading">#</a></h2>
<!-- dom:FIGURE: [figslides/maxpooling.png, width=500 frac=0.67]  A deep CNN -->
<!-- begin figure -->
<p><img src="figslides/maxpooling.png" width="500"><p style="font-size: 0.9em"><i>Figure 1: A deep CNN</i></p></p>
<!-- end figure --></section>
<section id="building-convolutional-neural-networks-in-tensorflow-keras-and-pytorch">
<h2>Building convolutional neural networks in Tensorflow/Keras and PyTorch<a class="headerlink" href="#building-convolutional-neural-networks-in-tensorflow-keras-and-pytorch" title="Link to this heading">#</a></h2>
<p>As discussed above, CNNs are neural networks built from the assumption
that the inputs to the network are 2D images. This is important
because the number of features or pixels in images grows very fast
with the image size, and an enormous number of weights and biases are
needed in order to build an accurate network.  Next week we will
discuss in more detail how we can build a CNN using either TensorFlow
with Keras and PyTorch.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="exercisesweek43.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Exercises week 43</p>
      </div>
    </a>
    <a class="right-next"
       href="exercisesweek44.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Exercises week 44</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plan-for-week-44">Plan for week 44</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-sessions-on-tuesday-and-wednesday">Lab  sessions on Tuesday and Wednesday</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#material-for-lecture-monday-october-27">Material for Lecture Monday October 27</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-differential-equations-with-deep-learning">Solving differential equations  with Deep Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-differential-equations-first">Ordinary Differential Equations first</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-trial-solution">The trial solution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimization-process">Minimization process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimizing-the-cost-function-using-gradient-descent-and-automatic-differentiation">Minimizing the cost function using gradient descent and automatic differentiation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-exponential-decay">Example: Exponential decay</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-function-to-solve-for">The function to solve for</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">The trial solution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-of-network">Setup of Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reformulating-the-problem">Reformulating the problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-technicalities">More technicalities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-details">More details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-possible-implementation-of-a-neural-network">A possible implementation of a neural network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#technicalities">Technicalities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-technicalities-i">Final technicalities I</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-technicalities-ii">Final technicalities II</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-technicalities-iii">Final technicalities III</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-technicalities-iv">Final technicalities IV</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation">Back propagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-code-for-solving-the-ode">The code for solving the ODE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-network-with-one-input-layer-specified-number-of-hidden-layers-and-one-output-layer">The network with one input layer, specified number of hidden layers, and one output layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-population-growth">Example: Population growth</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-problem">Setting up the problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">The trial solution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-program-using-autograd">The program using Autograd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-forward-euler-to-solve-the-ode">Using forward Euler to solve the ODE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-solving-the-one-dimensional-poisson-equation">Example: Solving the one dimensional Poisson equation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-specific-equation-to-solve-for">The specific equation to solve for</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-equation-using-autograd">Solving the equation using Autograd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-with-a-numerical-scheme">Comparing with a numerical scheme</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-code">Setting up the code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-differential-equations">Partial Differential Equations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#type-of-problem">Type of problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-requirements">Network requirements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">More details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-the-diffusion-equation">Example: The diffusion equation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-problem">Defining the problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-network-using-autograd">Setting up the network using Autograd</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-network-using-autograd-the-trial-solution">Setting up the network using Autograd; The trial solution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-the-jacobian">Why the Jacobian?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-the-network-using-autograd-the-full-program">Setting up the network using Autograd; The full program</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resources-on-differential-equations-and-deep-learning">Resources on differential equations and deep learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-recognizing-images">Convolutional Neural Networks (recognizing images)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-difference">What is the Difference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-vs-cnns">Neural Networks vs CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-cnns-for-images-sound-files-medical-images-from-ct-scans-etc">Why CNNS for images, sound files, medical images from CT scans etc?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regular-nns-dont-scale-well-to-full-images">Regular NNs don’t scale well to full images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-volumes-of-neurons">3D volumes of neurons</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-on-dimensionalities">More on Dimensionalities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-remarks">Further remarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layers-used-to-build-cnns">Layers used to build CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transforming-images">Transforming images</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnns-in-brief">CNNs in brief</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-deep-cnn-model-from-raschka-et-al">A deep CNN model (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-idea">Key Idea</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-do-image-compression-before-the-era-of-deep-learning">How to do image compression before the era of deep learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-svd-example">The SVD example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-cnns">Mathematics of CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution-examples-polynomial-multiplication">Convolution Examples: Polynomial multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#efficient-polynomial-multiplication">Efficient Polynomial Multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-simplification">Further simplification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-more-efficient-way-of-coding-the-above-convolution">A more efficient way of coding the above Convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#commutative-process">Commutative process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#toeplitz-matrices">Toeplitz matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fourier-series-and-toeplitz-matrices">Fourier series and Toeplitz matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-the-above-one-dimensional-case">Generalizing the above one-dimensional case</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-considerations">Memory considerations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#padding">Padding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-vector">New vector</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rewriting-as-dot-products">Rewriting as dot products</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-correlation">Cross correlation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-dimensional-objects">Two-dimensional objects</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnns-in-more-detail-simple-example">CNNs in more detail, simple example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-convolution-stage">The convolution stage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-number-of-parameters">Finding the number of parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-image-or-volume">New image (or volume)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-to-train-common-settings">Parameters to train, common settings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-cnn-setups">Examples of CNN setups</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summarizing-performing-a-general-discrete-convolution-from-raschka-et-al">Summarizing: Performing a general discrete convolution (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling">Pooling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-arithmetic">Pooling arithmetic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-types-from-raschka-et-al">Pooling types (From Raschka et al)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-convolutional-neural-networks-in-tensorflow-keras-and-pytorch">Building convolutional neural networks in Tensorflow/Keras and PyTorch</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Morten Hjorth-Jensen
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>