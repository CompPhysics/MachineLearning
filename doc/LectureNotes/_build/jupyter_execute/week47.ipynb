{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395dc9ca",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week47.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Week 47: Recurrent neural networks and Autoencoders -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe96292",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Week 47: Recurrent neural networks and Autoencoders\n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo, Norway\n",
    "\n",
    "Date: **November 17-21, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb567380",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plan for week 47\n",
    "\n",
    "**Plans for the lecture Monday 17 November, with video suggestions etc.**\n",
    "\n",
    "1. Recurrent neural networks, code examples and long-short-term memory\n",
    "\n",
    "2. Autoencoders (last topic this semester)\n",
    "\n",
    "3. Last lecture: November 24, note error in time planner.\n",
    "<!-- o Video of lecture at <https://youtu.be/RIHzmLv05DA> -->\n",
    "<!-- o Whiteboard notes at <https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2024/NotesNovember18.pdf> -->\n",
    "<!-- * [Video of Lecture](https://youtu.be/SpWXsvn5I9E) -->\n",
    "\n",
    "**Lab sessions on Tuesday and Wednesday.**\n",
    "\n",
    "1. Work and Discussion of project 3\n",
    "\n",
    "2. Last weekly exercise with deadline November 28, available from (early morning) Tuesday November 18.\n",
    "\n",
    "3. Last lab sessions: November 25 and 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e7533",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reading recommendations RNNs\n",
    "\n",
    "1. These lecture notes at <https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week47/ipynb/week47.ipynb>\n",
    "\n",
    "2. See also lecture notes from week 46 at <https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week46/ipynb/week46.ipynb>. The lecture on Monday starts with a repetition on recurrent neural networks. The second lecture starts with basics of autoenconders.\n",
    "\n",
    "3. For RNNs, see Goodfellow et al chapter 10, see <https://www.deeplearningbook.org/contents/rnn.html>.\n",
    "\n",
    "4. Reading suggestions for implementation of RNNs in PyTorch: see Rashcka et al.'s chapter 15 and GitHub site at <https://github.com/rasbt/machine-learning-book/tree/main/ch15>.\n",
    "\n",
    "5. RNN video at <https://youtu.be/PCgrgHgy26c?feature=shared>\n",
    "\n",
    "6. New xLSTM, see Beck et al <https://arxiv.org/abs/2405.04517>. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f2cc5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## TensorFlow examples\n",
    "For TensorFlow (using Keras) implementations, we recommend\n",
    "1. David Foster, Generative Deep Learning with TensorFlow, see chapter 5 at <https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html>\n",
    "\n",
    "2. Joseph Babcock and Raghav Bali Generative AI with Python and their GitHub link, chapters 2 and  3 at <https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d798c4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reading recommendations: Autoencoders (AE)\n",
    "\n",
    "1. Goodfellow et al chapter 14, see <https://www.deeplearningbook.org/contents/autoencoders.html>\n",
    "\n",
    "2. Rashcka et al. Their chapter 17 contains a brief introduction only.\n",
    "\n",
    "3. Building AEs in Keras at <https://blog.keras.io/building-autoencoders-in-keras.html>\n",
    "\n",
    "4. Introduction to AEs in TensorFlow at <https://www.tensorflow.org/tutorials/generative/autoencoder>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b15ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What is a recurrent NN?\n",
    "\n",
    "A recurrent neural network (RNN), as opposed to a regular fully\n",
    "connected neural network (FCNN) or just neural network (NN), has\n",
    "layers that are connected to themselves.\n",
    "\n",
    "In an FCNN there are no connections between nodes in a single\n",
    "layer. For instance, $(h_1^1$ is not connected to $(h_2^1$. In\n",
    "addition, the input and output are always of a fixed length.\n",
    "\n",
    "In an RNN, however, this is no longer the case. Nodes in the hidden\n",
    "layers are connected to themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8943067",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Why RNNs?\n",
    "\n",
    "Recurrent neural networks work very well when working with\n",
    "sequential data, that is data where the order matters. In a regular\n",
    "fully connected network, the order of input doesn't really matter.\n",
    "\n",
    "Another property of  RNNs is that they can handle variable input\n",
    "and output. Consider again the simplified breast cancer dataset. If you\n",
    "have trained a regular FCNN on the dataset with the two features, it\n",
    "makes no sense to suddenly add a third feature. The network would not\n",
    "know what to do with it, and would reject such inputs with three\n",
    "features (or any other number of features that isn't two, for that\n",
    "matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cd65e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More whys\n",
    "1. Traditional feedforward networks process fixed-size inputs and ignore temporal order. RNNs incorporate recurrence to handle sequential data like time series or language ￼.\n",
    "\n",
    "2. At each time step, an RNN cell processes input x_t and a hidden state h_{t-1} from the previous step, producing a new hidden state h_t and (optionally) an output y_t.\n",
    "\n",
    "3. This hidden state acts as a “memory” carrying information forward. For example, predicting stock prices or words in a sentence relies on past inputs ￼ ￼.\n",
    "\n",
    "4. RNNs share parameters across time steps, so they can generalize patterns regardless of sequence length ￼."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f062c7f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN2.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN2.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b269c67",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 2\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN3.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN3.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3647a4e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 3\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN4.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN4.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ffd6e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 4\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN5.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN5.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d8f6a0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 5\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN6.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN6.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb95999",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 6\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN7.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN7.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2442076e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNNs in more detail, part 7\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/RNN8.png, width=700 frac=0.9] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/RNN8.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753a10c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RNN Forward Pass Equations\n",
    "\n",
    "For a simple (vanilla) RNN with one hidden layer and no bias, the state update and output are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3503490",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{h}_t = \\sigma(\\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{W}_{hh}\\mathbf{h}_{t-1})\\,,\\quad \\mathbf{y}_t = \\mathbf{W}_{yh}\\mathbf{h}_t,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694565a5",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\sigma$ is an activation (e.g. tanh or ReLU) ￼.\n",
    "\n",
    "In matrix form,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9e9c3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{W}_{xh}\\in\\mathbb{R}^{h\\times d}, \\mathbf{W}_{hh}\\in\\mathbb{R}^{h\\times h}, \\mathbf{W}_{yh}\\in\\mathbb{R}^{q\\times h},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4c25d",
   "metadata": {
    "editable": true
   },
   "source": [
    "for input dimension  $d$, hidden dimension $h$, output dimension $q$.\n",
    "\n",
    "Because the same $\\mathbf{W}$ are used each step, gradients during training will propagate through time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d008a208",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Unrolled RNN in Time\n",
    "\n",
    "1. Input $x_1,x_2,x_3,\\dots$ feed sequentially; the hidden state flows from one step to the next, capturing past context.\n",
    "\n",
    "2. After processing the final input $x_T$, the network can make a prediction (many-to-one) or outputs can be produced at each step (many-to-many).\n",
    "\n",
    "3. Unrolling clarifies that training an RNN is like training a deep feedforward network of depth T, with recurrent connections tying layers together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ea40a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Example Task: Character-level RNN Classification\n",
    "1. A classic example: feed a name (sequence of characters) one char at a time, and classify its language of origin.\n",
    "\n",
    "2. At each step, the RNN outputs a hidden state; we use the final hidden state to predict the class of the entire sequence.\n",
    "\n",
    "3. A character-level RNN reads words as a series of characters—outputting a prediction and ‘hidden state’ at each step, feeding the previous hidden state into the next step. We take the final prediction to be the output” ￼.\n",
    "\n",
    "4. This illustrates sequence-to-one modeling: every output depends on all previous inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b7da7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## PyTorch: Defining a Simple RNN, using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed3cd07d",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# 1. Hyperparameters\n",
    "# -----------------------\n",
    "input_size = 10        # Dimensionality of each time step\n",
    "hidden_size = 20       # Number of recurrent units\n",
    "num_classes = 2        # Binary classification\n",
    "sequence_length = 5     # Sequence length\n",
    "batch_size = 16\n",
    "\n",
    "# -----------------------\n",
    "# 2. Dummy dataset\n",
    "#    X: [batch, seq, features]\n",
    "#    y: [batch]\n",
    "# -----------------------\n",
    "X = np.random.randn(batch_size, sequence_length, input_size).astype(np.float32)\n",
    "y = np.random.randint(0, num_classes, size=(batch_size,))\n",
    "\n",
    "# -----------------------\n",
    "# 3. Build simple RNN model\n",
    "# -----------------------\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(\n",
    "        units=hidden_size,\n",
    "        activation=\"tanh\",\n",
    "        return_sequences=False,   # Only final hidden state\n",
    "        input_shape=(sequence_length, input_size)\n",
    "    ),\n",
    "    tf.keras.layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 4. Train the model\n",
    "# -----------------------\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=5,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 5. Evaluate\n",
    "# -----------------------\n",
    "logits = model.predict(X)\n",
    "print(\"Logits from model:\\n\", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0cd85",
   "metadata": {
    "editable": true
   },
   "source": [
    "This recurrent neural network uses the TensorFlow/Keras SimpleRNN, which is the counterpart to PyTorch’s nn.RNN.\n",
    "In this code we have used\n",
    "1. sequence$\\_$length is the number of time steps in each input sequence fed into a recurrent neural network. It represents how many time points we provide at once. It is the number of ordered observations in each sample of our dataset.\n",
    "\n",
    "2. return_sequences=False makes it output only the last hidden state, which is fed to the classifier. Also, we have\n",
    "\n",
    "3. from_logits=True matches the PyTorch CrossEntropyLoss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9fc8e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Similar example using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59508241",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# -----------------------\n",
    "# 1. Hyperparameters\n",
    "# -----------------------\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "num_classes = 2\n",
    "sequence_length = 5\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "\n",
    "# -----------------------\n",
    "# 2. Dummy dataset\n",
    "# -----------------------\n",
    "X = torch.randn(batch_size, sequence_length, input_size)\n",
    "y = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# -----------------------\n",
    "# 3. Simple RNN model\n",
    "# -----------------------\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            nonlinearity=\"tanh\"\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, h_n = self.rnn(x)   # out: [batch, seq, hidden]\n",
    "\n",
    "        # ---- FIX: take only the last time-step tensor ----\n",
    "        last_hidden = out[:, -1, :]  # [batch, hidden]\n",
    "\n",
    "        logits = self.fc(last_hidden)\n",
    "        return logits\n",
    "\n",
    "model = SimpleRNN(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Training step\n",
    "# -----------------------\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "logits = model(X)\n",
    "loss = criterion(logits, y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39a68f4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Backpropagation Through Time (BPTT) and Gradients\n",
    "\n",
    "**Backpropagation Through Time (BPTT).**\n",
    "\n",
    "1. Training an RNN involves computing gradients through time by unfolding the network: treat the unrolled RNN as a very deep feedforward net.\n",
    "\n",
    "2. We compute the loss $L = \\frac{1}{T}\\sum_{t=1}^T \\ell(y_t,\\hat y_t)$ and backpropagate from $t=T$ down to $t=1.$\n",
    "\n",
    "3. The computational graphs in the figures below shows how each hidden state depends on inputs and parameters across time ￼.\n",
    "\n",
    "4. BPTT applies the chain rule along this graph, accumulating gradients from each time step into the shared parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e4584",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Truncated BPTT and Gradient Clipping\n",
    "\n",
    "1. Truncated BPTT: Instead of backpropagating through all T steps, we may backpropagate through a fixed window of length $\\tau$. This approximates the full gradient and reduces computation.\n",
    "\n",
    "2. Concretely, one computes gradients up to $\\tau$ steps and treats gradients beyond as zero. This still allows learning short-term patterns efficiently.\n",
    "\n",
    "3. Gradient Clipping: Cap the gradient norm to a maximum value to prevent explosion. For example in PyTorch: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) ensures $\\|\\nabla\\|\\le 1$.\n",
    "\n",
    "4. These techniques help stabilize training, but the fundamental vanishing problem motivates using alternative RNN cells (LSTM/GRU) in practice (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b748ee5c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Limitations and Considerations\n",
    "1. Vanishing Gradients: Simple RNNs have fundamental difficulty learning long-term dependencies due to gradient decay ￼.\n",
    "\n",
    "2. Capacity: Without gates, RNNs may struggle with tasks requiring remembering far-back inputs. Training can be slow as it’s inherently sequential.\n",
    "\n",
    "3. Alternatives: In practice, gated RNNs (LSTM/GRU) or Transformers are often used for long-range dependencies. However, simple RNNs are still instructive and sometimes sufficient for short sequences ￼ ￼.\n",
    "\n",
    "4. Regularization: Weight decay or dropout (on inputs/states) can help generalization but must be applied carefully due to temporal correlations.\n",
    "\n",
    "5. Statefulness: For very long sequences, one can preserve hidden state across batches (stateful RNN) to avoid resetting memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179902b1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## PyTorch RNN Time Series Example\n",
    "\n",
    "We first implement a simple RNN in PyTorch to forecast a univariate\n",
    "time series (a sine wave). The steps are: (1) generate synthetic data\n",
    "and form input/output sequences; (2) define an nn.RNN model; (3) train\n",
    "the model with MSE loss and an optimizer; (4) evaluate on a held-out\n",
    "test set. For example, using a sine wave as in prior tutorials ￼, we\n",
    "create sliding windows of length seq_length. The code below shows each\n",
    "step. We use nn.RNN (the basic recurrent layer) followed by a linear\n",
    "output. The training loop (with MSELoss and Adam) updates the model to\n",
    "minimize prediction error ￼."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e8f539",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# 1. Data preparation: generate a sine wave and create input-output sequences\n",
    "time_steps = np.linspace(0, 100, 500)\n",
    "data = np.sin(time_steps)                   # shape (500,)\n",
    "seq_length = 20\n",
    "X, y = [], []\n",
    "for i in range(len(data) - seq_length):\n",
    "    X.append(data[i:i+seq_length])         # sequence of length seq_length\n",
    "    y.append(data[i+seq_length])           # next value to predict\n",
    "X = np.array(X)                            # shape (480, seq_length)\n",
    "y = np.array(y)                            # shape (480,)\n",
    "# Add feature dimension (1) for the RNN input\n",
    "X = X[..., None]                           # shape (480, seq_length, 1)\n",
    "y = y[..., None]                           # shape (480, 1)\n",
    "\n",
    "# Split into train/test sets (80/20 split)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train = torch.tensor(X[:train_size], dtype=torch.float32)\n",
    "y_train = torch.tensor(y[:train_size], dtype=torch.float32)\n",
    "X_test  = torch.tensor(X[train_size:],  dtype=torch.float32)\n",
    "y_test  = torch.tensor(y[train_size:],  dtype=torch.float32)\n",
    "\n",
    "# 2. Model definition: simple RNN followed by a linear layer\n",
    "class SimpleRNNModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=16, num_layers=1):\n",
    "        super(SimpleRNNModel, self).__init__()\n",
    "        # nn.RNN for sequential data (batch_first=True expects (batch, seq_len, features))\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)    # output layer for prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)                 # out: (batch, seq_len, hidden_size)\n",
    "        out = out[:, -1, :]                  # take output of last time step\n",
    "        return self.fc(out)                 # linear layer to 1D output\n",
    "\n",
    "model = SimpleRNNModel(input_size=1, hidden_size=16, num_layers=1)\n",
    "print(model)  # print model summary (structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd02d7",
   "metadata": {
    "editable": true
   },
   "source": [
    "Model Explanation: Here input$\\_$size=1 because each time step has one\n",
    "feature. The RNN hidden state has size 16, and batch$\\_$first=True means\n",
    "input tensors have shape (batch, seq$\\_$len, features). We take the last\n",
    "RNN output and feed it through a linear layer to predict the next\n",
    "value ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2aed1d7",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 3. Training loop: MSE loss and Adam optimizer\n",
    "criterion = nn.MSELoss()                  # mean squared error loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)               # forward pass\n",
    "    loss = criterion(output, y_train)     # compute training loss\n",
    "    loss.backward()                       # backpropagate\n",
    "    optimizer.step()                      # update weights\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033cc00",
   "metadata": {
    "editable": true
   },
   "source": [
    "Training Details: We train for 50 epochs, printing the training loss\n",
    "every 10 epochs. As training proceeds, the loss (MSE) typically\n",
    "decreases, indicating the RNN is learning the sine-wave pattern ￼."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d227666f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 4. Evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(X_test)\n",
    "    test_loss = criterion(pred, y_test)\n",
    "print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# (Optional) View a few actual vs. predicted values\n",
    "print(\"Actual:\", y_test[:5].flatten().numpy())\n",
    "print(\"Pred : \", pred[:5].flatten().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc57dfb",
   "metadata": {
    "editable": true
   },
   "source": [
    "Evaluation: We switch to eval mode and compute loss on the test\n",
    "set. The lower test loss indicates how well the model generalizes. The\n",
    "code prints a few sample predictions against actual values for\n",
    "qualitative assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20f8a9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Tensorflow (Keras) RNN Time Series Example\n",
    "\n",
    "Next, we use TensorFlow/Keras to do the same task. We build a\n",
    "tf.keras.Sequential model with a SimpleRNN layer (the most basic\n",
    "recurrent layer) ￼ followed by a Dense output. The workflow is\n",
    "similar: create the same synthetic sine data and split it into\n",
    "train/test sets; then define, train, and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9eaa5f5",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Data preparation: same sine wave data and sequences as above\n",
    "time_steps = np.linspace(0, 100, 500)\n",
    "data = np.sin(time_steps)                     # (500,)\n",
    "seq_length = 20\n",
    "X, y = [], []\n",
    "for i in range(len(data) - seq_length):\n",
    "    X.append(data[i:i+seq_length])\n",
    "    y.append(data[i+seq_length])\n",
    "X = np.array(X)                               # (480, seq_length)\n",
    "y = np.array(y)                               # (480,)\n",
    "# reshape for RNN: (samples, timesteps, features)\n",
    "X = X.reshape(-1, seq_length, 1)             # (480, 20, 1)\n",
    "y = y.reshape(-1, 1)                         # (480, 1)\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba58ed",
   "metadata": {
    "editable": true
   },
   "source": [
    "Data: We use the same sine-wave sequence and sliding-window split as\n",
    "in the PyTorch example ￼. The arrays are reshaped to (batch,\n",
    "timesteps, features) for Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b154b97b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2. Model definition: Keras SimpleRNN and Dense\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(16, input_shape=(seq_length, 1)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')   # MSE loss and Adam optimizer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a1913",
   "metadata": {
    "editable": true
   },
   "source": [
    "Explanation: Here SimpleRNN(16) creates 16 recurrent units. The model\n",
    "summary shows the shapes and number of parameters. (Keras handles the\n",
    "sequence dimension internally.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97234d22",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,    # use 20% of train data for validation\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86620467",
   "metadata": {
    "editable": true
   },
   "source": [
    "Training: We train for 50 epochs. The fit call also reports validation\n",
    "loss (using a 20$%$ split of the training data) to monitor\n",
    "generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f54caa9",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 4. Evaluation on test set\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# (Optional) Predictions\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Actual:\", y_test.flatten()[:5])\n",
    "print(\"Pred : \", predictions.flatten()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d91712",
   "metadata": {
    "editable": true
   },
   "source": [
    "Evaluation: After training, we call model.evaluate on the test set. A\n",
    "low test loss indicates good forecasting accuracy. We also predict and\n",
    "compare a few samples of actual vs. predicted values. This completes\n",
    "the simple RNN forecasting example in TensorFlow.\n",
    "\n",
    "Both examples use only basic RNN cells (no LSTM/GRU) and include data\n",
    "preparation, model definition, training loop, and evaluation. The\n",
    "PyTorch code uses nn.RNN as and the Keras\n",
    "code uses SimpleRNN layer. Each code block above is self-contained\n",
    "and can be run independently with standard libraries (NumPy, PyTorch\n",
    "or TensorFlow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b4087",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The mathematics of RNNs, the basic architecture\n",
    "\n",
    "See notebook at <https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week7/ipynb/rnnmath.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe33d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gating mechanism: Long Short Term Memory (LSTM)\n",
    "\n",
    "Besides a simple recurrent neural network layer, as discussed above, there are two other\n",
    "commonly used types of recurrent neural network layers: Long Short\n",
    "Term Memory (LSTM) and Gated Recurrent Unit (GRU).  For a short\n",
    "introduction to these layers see <https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b>\n",
    "and <https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b>.\n",
    "\n",
    "LSTM uses a memory cell for \n",
    "modeling long-range dependencies and avoid vanishing gradient\n",
    " problems.\n",
    "Capable of modeling longer term dependencies by having\n",
    "memory cells and gates that controls the information flow along\n",
    "with the memory cells.\n",
    "\n",
    "1. Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).\n",
    "\n",
    "2. They designed a memory cell using logistic and linear units with multiplicative interactions.\n",
    "\n",
    "3. Information gets into the cell whenever its “write” gate is on.\n",
    "\n",
    "4. The information stays in the cell so long as its **keep** gate is on.\n",
    "\n",
    "5. Information can be read from the cell by turning on its **read** gate. \n",
    "\n",
    "The LSTM were first introduced to overcome the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2970f564",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Implementing a memory cell in a neural network\n",
    "\n",
    "To preserve information for a long time in\n",
    "the activities of an RNN, we use a circuit\n",
    "that implements an analog memory cell.\n",
    "\n",
    "1. A linear unit that has a self-link with a weight of 1 will maintain its state.\n",
    "\n",
    "2. Information is stored in the cell by activating its write gate.\n",
    "\n",
    "3. Information is retrieved by activating the read gate.\n",
    "\n",
    "4. We can backpropagate through this circuit because logistics are have nice derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981b14f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## LSTM details\n",
    "\n",
    "The LSTM is a unit cell that is made of three gates:\n",
    "1. the input gate,\n",
    "\n",
    "2. the forget gate,\n",
    "\n",
    "3. and the output gate.\n",
    "\n",
    "It also introduces a cell state $c$, which can be thought of as the\n",
    "long-term memory, and a hidden state $h$ which can be thought of as\n",
    "the short-term memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf8b63",
   "metadata": {
    "editable": true
   },
   "source": [
    "## LSTM Cell and Gates\n",
    "1. Each LSTM cell contains a memory cell $C_t$ and three gates (forget $f_t$, input $i_t$, output $o_t$) that control information flow.\n",
    "\n",
    "2. **Forget gate** ($f_t$): chooses which information to erase from the previous cell state $C_{t-1}$ \n",
    "\n",
    "3. **Input gate** ($i_t$): decides which new information $\\tilde{C}_t$ to add to the cell state.\n",
    "\n",
    "4. **Output gate** ($o_t$): controls which parts of the cell state become the output $h_t$.\n",
    "\n",
    "5. The cell state update: $C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9024745",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Core LSTM Equations\n",
    "**The gate computations and state updates are given by:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff71241",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    f_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f), \\\\\n",
    "    i_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i), \\\\\n",
    "    \\tilde{C}_t &= \\tanh(W_C [h_{t-1}, x_t] + b_C), \\\\\n",
    "    C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t, \\\\\n",
    "    o_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o), \\\\\n",
    "    h_t &= o_t \\odot \\tanh(C_t).\n",
    "  \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f71fe",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. $\\sigma$ is the sigmoid function, $\\odot$ is elementwise product [oai_citation:4‡jaketae.github.io](https://jaketae.github.io/study/dissecting-lstm/#:~:text=%5C%5B%5Cbegin,align).\n",
    "\n",
    "2. These equations define how LSTM retains/updates memory and produces outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09020f5b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gate Intuition and Dynamics\n",
    "1. Forget gate $f_t$ acts as a soft “erase” signal: $f_t \\approx 0$ forgets, $f_t \\approx 1$ retains previous memory.\n",
    "\n",
    "2. Input gate $i_t$ scales how much new candidate memory $\\tilde{C}_t$ is written.\n",
    "\n",
    "3. Output gate $o_t$ determines how much of the cell's memory flows into the hidden state $h_t$.\n",
    "\n",
    "4. By controlling these gates, LSTM effectively keeps long-term information when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3323a3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basic layout (All figures from Raschka *et al.,*)\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/LSTM1.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/LSTM1.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f3ad9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## LSTM details\n",
    "\n",
    "The first stage is called the forget gate, where we combine the input\n",
    "at (say, time $t$), and the hidden cell state input at $t-1$, passing\n",
    "it through the Sigmoid activation function and then performing an\n",
    "element-wise multiplication, denoted by $\\odot$.\n",
    "\n",
    "Mathematically we have (see also figure below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d0241",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{f}^{(t)} = \\sigma(W_{fx}\\mathbf{x}^{(t)} + W_{fh}\\mathbf{h}^{(t-1)} + \\mathbf{b}_f)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096d646",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the $W$s are the weights to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a31f2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Comparing with a standard  RNN\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/LSTM2.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/LSTM2.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54734e2e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## LSTM details I\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/LSTM3.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/LSTM3.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d5067c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## LSTM details II\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/LSTM4.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/LSTM4.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc7943",
   "metadata": {
    "editable": true
   },
   "source": [
    "## LSTM details III\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/LSTM5.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/LSTM5.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18353f46",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Forget gate\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/LSTM6.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/LSTM6.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d7aaf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The forget gate\n",
    "\n",
    "The naming forget gate stems from the fact that  the Sigmoid activation function's\n",
    "outputs are very close to $0$ if the argument for the function is very\n",
    "negative, and $1$ if the argument is very positive. Hence we can\n",
    "control the amount of information we want to take from the long-term\n",
    "memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e8298",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{f}^{(t)} = \\sigma(W_{fx}\\mathbf{x}^{(t)} + W_{fh}\\mathbf{h}^{(t-1)} + \\mathbf{b}_f)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9e0e4",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the $W$s are the weights to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a723aa",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basic layout\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/LSTM7.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/LSTM7.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ab8ec",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Input gate\n",
    "\n",
    "The next stage is the input gate, which consists of both a Sigmoid\n",
    "function ($\\sigma_i$), which decide what percentage of the input will\n",
    "be stored in the long-term memory, and the $\\tanh_i$ function, which\n",
    "decide what is the full memory that can be stored in the long term\n",
    "memory. When these results are calculated and multiplied together, it\n",
    "is added to the cell state or stored in the long-term memory, denoted\n",
    "as $\\oplus$. \n",
    "\n",
    "We have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5e8fc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{i}^{(t)} = \\sigma_g(W_{ix}\\mathbf{x}^{(t)} + W_{ih}\\mathbf{h}^{(t-1)} + \\mathbf{b}_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8b081",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d7f6e0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{g}^{(t)} = \\tanh(W_{gx}\\mathbf{x}^{(t)} + W_{gh}\\mathbf{h}^{(t-1)} + \\mathbf{b}_g),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c711110",
   "metadata": {
    "editable": true
   },
   "source": [
    "again the $W$s are the weights to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74acb1e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Short summary\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/LSTM8.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/LSTM8.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39858044",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Forget and input\n",
    "\n",
    "The forget gate and the input gate together also update the cell state with the following equation,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93491a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{c}^{(t)} = \\mathbf{f}^{(t)} \\otimes \\mathbf{c}^{(t-1)} + \\mathbf{i}^{(t)} \\otimes \\mathbf{g}^{(t)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95846753",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $f^{(t)}$ and $i^{(t)}$ are the outputs of the forget gate and the input gate, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a524a3af",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basic layout\n",
    "\n",
    "<!-- dom:FIGURE: [figslides/LSTM9.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figslides/LSTM9.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81907e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Output gate\n",
    "\n",
    "The final stage of the LSTM is the output gate, and its purpose is to\n",
    "update the short-term memory.  To achieve this, we take the newly\n",
    "generated long-term memory and process it through a hyperbolic tangent\n",
    "($\\tanh$) function creating a potential new short-term memory. We then\n",
    "multiply this potential memory by the output of the Sigmoid function\n",
    "($\\sigma_o$). This multiplication generates the final output as well\n",
    "as the input for the next hidden cell ($h^{\\langle t \\rangle}$) within\n",
    "the LSTM cell.\n",
    "\n",
    "We have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd90cd",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{o}^{(t)} &= \\sigma_g(W_o\\mathbf{x}^{(t)} + U_o\\mathbf{h}^{(t-1)} + \\mathbf{b}_o), \\\\\n",
    "\\mathbf{h}^{(t)} &= \\mathbf{o}^{(t)} \\otimes \\sigma_h(\\mathbf{c}^{(t)}). \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942453b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\mathbf{W_o,U_o}$ are the weights of the output gate and $\\mathbf{b_o}$ is the bias of the output gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36589b5b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## LSTM Implementation (Code Example)\n",
    "1. Using high-level libraries (Keras, PyTorch) simplifies LSTM usage.\n",
    "\n",
    "2. define and train a Keras LSTM on a univariate time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ce76398",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# X_train shape: (samples, timesteps, 1)\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(None, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5f977",
   "metadata": {
    "editable": true
   },
   "source": [
    "The model learns to map sequences to outputs; input sequences can be constructed via sliding windows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314f760",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Example: Modeling Dynamical Systems\n",
    "1. LSTMs can learn complex time evolution of physical systems (e.g. Lorenz attractor, fluid dynamics) from data.\n",
    "\n",
    "2. Serve as data-driven surrogates for ODE/PDE solvers (trained on RK4-generated time series).\n",
    "\n",
    "3. For example, an LSTM surrogate accurately forecast 36h lake hydrodynamics (velocity, temperature) with $<6\\%$ error.\n",
    "\n",
    "4. Such models dramatically speed up predictions compared to full numerical simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8bbc42",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Example: Biological Sequences\n",
    "1. Biological sequences (DNA/RNA/proteins) are effectively categorical time series.\n",
    "\n",
    "2. LSTMs capture sequence motifs and long-range dependencies (akin to language models).\n",
    "\n",
    "3. Widely used in genomics and proteomics (e.g., protein function, gene expression).\n",
    "\n",
    "4. They naturally handle variable-length input by processing one element at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bbc9bd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Training Tips and Variants\n",
    "1. Preprocess time series (normalize features, windowing); handle variable lengths (padding/truncation).\n",
    "\n",
    "2. Experiment with network depth, hidden units, and regularization (dropout) to avoid overfitting.\n",
    "\n",
    "3. Consider bidirectional LSTM or stacking multiple LSTM layers for complex patterns.\n",
    "\n",
    "4. GRU is a simpler gated RNN that combines forget/input gates into one update gate.\n",
    "\n",
    "5. Monitor gradients during training; use gradient clipping to stabilize learning if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaceca9f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## LSTM Summary\n",
    "1. LSTMs extend RNNs with gated cells to remember long-term context, addressing RNN gradient issues.\n",
    "\n",
    "2. Core update: $C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$, output $h_t = o_t \\odot \\tanh(C_t)$.\n",
    "\n",
    "3. Implementation is straightforward in libraries like Keras/PyTorch with few lines of code.\n",
    "\n",
    "4. Applications span science and engineering: forecasting dynamical systems, analyzing DNA/proteins, etc.\n",
    "\n",
    "5. For more details, see Goodfellow et al. (2016) Deep Learning, chapter 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2d01b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Summary of LSTM\n",
    "\n",
    "LSTMs provide a basic approach for modeling long-range dependencies in sequences.\n",
    "If you wish to read more, see **An Empirical Exploration of Recurrent Network Architectures**, authored\n",
    "by Rafal Jozefowicz *et al.,*  Proceedings of ICML, 2342-2350, 2015).\n",
    "\n",
    "An important recent development are the so-called **gated recurrent units (GRU)**, see for example the article\n",
    "by Junyoung Chung *et al.,*, at URL:\"https://arxiv.org/abs/1412.3555.\n",
    "This article is an excellent read if you are interested in learning\n",
    "more about these modern RNN architectures\n",
    "\n",
    "The GRUs have a simpler\n",
    "architecture than LSTMs. This leads to computationally more efficient methods, while their\n",
    "performance in some tasks, such as polyphonic music modeling, is comparable to LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16089d7f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## LSTM implementation using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "579af591",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Key points:\n",
    "1. The input images (28x28 pixels) are treated as sequences of 28 timesteps with 28 features each\n",
    "2. The LSTM layer processes this sequential data\n",
    "3. A final dense layer with softmax activation handles the classification\n",
    "4. Typical accuracy ranges between 95-98% (lower than CNNs but reasonable for demonstration)\n",
    "\n",
    "Note: LSTMs are not typically used for image classification (CNNs are more efficient), but this demonstrates how to adapt them for such tasks. Training might take longer compared to CNN architectures.\n",
    "\n",
    "To improve performance, you could:\n",
    "1. Add more LSTM layers\n",
    "2. Use Bidirectional LSTMs\n",
    "3. Increase the number of units\n",
    "4. Add dropout for regularization\n",
    "5. Use learning rate scheduling\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape data for LSTM (samples, timesteps, features)\n",
    "# MNIST images are 28x28, so we treat each image as 28 timesteps of 28 features\n",
    "x_train = x_train.reshape((-1, 28, 28))\n",
    "x_test = x_test.reshape((-1, 28, 28))\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(28, 28)))  # 128 LSTM units\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                   batch_size=64,\n",
    "                   epochs=10,\n",
    "                   validation_split=0.2)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668049c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## And the corresponding one with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6792d769",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Key components:\n",
    "1. **Data Handling**: Uses PyTorch DataLoader with MNIST dataset\n",
    "2. **LSTM Architecture**:\n",
    "  - Input sequence of 28 timesteps (image rows)\n",
    "  - 128 hidden units in LSTM layer\n",
    "  - Fully connected layer for classification\n",
    "3. **Training**:\n",
    "  - Cross-entropy loss\n",
    "  - Adam optimizer\n",
    "  - Automatic GPU utilization if available\n",
    "\n",
    "This implementation typically achieves **97-98% accuracy** after 10 epochs. The main differences from the TensorFlow/Keras version:\n",
    "- Explicit device management (CPU/GPU)\n",
    "- Manual training loop\n",
    "- Different data loading pipeline\n",
    "- More explicit tensor reshaping\n",
    "\n",
    "To improve performance, you could:\n",
    "1. Add dropout regularization\n",
    "2. Use bidirectional LSTM\n",
    "3. Implement learning rate scheduling\n",
    "4. Add batch normalization\n",
    "5. Increase model capacity (more layers/units)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28     # Number of features (pixels per row)\n",
    "hidden_size = 128   # LSTM hidden state size\n",
    "num_classes = 10    # Digits 0-9\n",
    "num_epochs = 10     # Training iterations\n",
    "batch_size = 64     # Batch size\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data',\n",
    "                              train=True,\n",
    "                              transform=transform,\n",
    "                              download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data',\n",
    "                             train=False,\n",
    "                             transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "# LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "   def __init__(self, input_size, hidden_size, num_classes):\n",
    "       super(LSTMModel, self).__init__()\n",
    "       self.hidden_size = hidden_size\n",
    "       self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "       self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "   def forward(self, x):\n",
    "       # Reshape input to (batch_size, sequence_length, input_size)\n",
    "       x = x.reshape(-1, 28, 28)\n",
    "\n",
    "       # Forward propagate LSTM\n",
    "       out, _ = self.lstm(x)  # out: (batch_size, seq_length, hidden_size)\n",
    "\n",
    "       # Decode the hidden state of the last time step\n",
    "       out = out[:, -1, :]\n",
    "       out = self.fc(out)\n",
    "       return out\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMModel(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "   model.train()\n",
    "   for i, (images, labels) in enumerate(train_loader):\n",
    "       images = images.to(device)\n",
    "       labels = labels.to(device)\n",
    "\n",
    "       # Forward pass\n",
    "       outputs = model(images)\n",
    "       loss = criterion(outputs, labels)\n",
    "\n",
    "       # Backward and optimize\n",
    "       optimizer.zero_grad()\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "\n",
    "       if (i+1) % 100 == 0:\n",
    "           print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "\n",
    "   # Test the model\n",
    "   model.eval()\n",
    "   with torch.no_grad():\n",
    "       correct = 0\n",
    "       total = 0\n",
    "       for images, labels in test_loader:\n",
    "           images = images.to(device)\n",
    "           labels = labels.to(device)\n",
    "           outputs = model(images)\n",
    "           _, predicted = torch.max(outputs.data, 1)\n",
    "           total += labels.size(0)\n",
    "           correct += (predicted == labels).sum().item()\n",
    "\n",
    "       print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8918c8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Dynamical ordinary differential equation\n",
    "\n",
    "Let us illustrate how we could train an RNN using data from the\n",
    "solution of a well-known differential equation, namely Newton's\n",
    "equation for oscillatory motion for an object being forced into\n",
    "harmonic oscillations by an applied external force.\n",
    "\n",
    "We will start with the basic algorithm for solving this type of\n",
    "equations using the Runge-Kutta-4 approach. The first code example is\n",
    "a standalone differential equation solver. It yields positions and\n",
    "velocities as function of time, starting with an initial time $t_0$\n",
    "and ending with a final time.\n",
    "\n",
    "The data the program produces will in turn be used to train an RNN for\n",
    "a selected number of training data. With a trained RNN, we will then\n",
    "use the network to make predictions for data not included in the\n",
    "training. That is, we will train a model which should be able to\n",
    "reproduce velocities and positions not included in training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03be6b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Runge-Kutta-4 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e86c0cf",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "FIGURE_ID = \"Results/FigureFiles\"\n",
    "DATA_ID = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')\n",
    "\n",
    "\n",
    "def SpringForce(v,x,t):\n",
    "#   note here that we have divided by mass and we return the acceleration\n",
    "    return  -2*gamma*v-x+Ftilde*cos(t*Omegatilde)\n",
    "\n",
    "\n",
    "def RK4(v,x,t,n,Force):\n",
    "    for i in range(n-1):\n",
    "# Setting up k1\n",
    "        k1x = DeltaT*v[i]\n",
    "        k1v = DeltaT*Force(v[i],x[i],t[i])\n",
    "# Setting up k2\n",
    "        vv = v[i]+k1v*0.5\n",
    "        xx = x[i]+k1x*0.5\n",
    "        k2x = DeltaT*vv\n",
    "        k2v = DeltaT*Force(vv,xx,t[i]+DeltaT*0.5)\n",
    "# Setting up k3\n",
    "        vv = v[i]+k2v*0.5\n",
    "        xx = x[i]+k2x*0.5\n",
    "        k3x = DeltaT*vv\n",
    "        k3v = DeltaT*Force(vv,xx,t[i]+DeltaT*0.5)\n",
    "# Setting up k4\n",
    "        vv = v[i]+k3v\n",
    "        xx = x[i]+k3x\n",
    "        k4x = DeltaT*vv\n",
    "        k4v = DeltaT*Force(vv,xx,t[i]+DeltaT)\n",
    "# Final result\n",
    "        x[i+1] = x[i]+(k1x+2*k2x+2*k3x+k4x)/6.\n",
    "        v[i+1] = v[i]+(k1v+2*k2v+2*k3v+k4v)/6.\n",
    "        t[i+1] = t[i] + DeltaT\n",
    "\n",
    "\n",
    "# Main part begins here\n",
    "\n",
    "DeltaT = 0.001\n",
    "#set up arrays \n",
    "tfinal = 20 # in dimensionless time\n",
    "n = ceil(tfinal/DeltaT)\n",
    "# set up arrays for t, v, and x\n",
    "t = np.zeros(n)\n",
    "v = np.zeros(n)\n",
    "x = np.zeros(n)\n",
    "# Initial conditions (can change to more than one dim)\n",
    "x0 =  1.0 \n",
    "v0 = 0.0\n",
    "x[0] = x0\n",
    "v[0] = v0\n",
    "gamma = 0.2\n",
    "Omegatilde = 0.5\n",
    "Ftilde = 1.0\n",
    "# Start integrating using Euler's method\n",
    "# Note that we define the force function as a SpringForce\n",
    "RK4(v,x,t,n,SpringForce)\n",
    "\n",
    "# Plot position as function of time    \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel('x[m]')\n",
    "ax.set_xlabel('t[s]')\n",
    "ax.plot(t, x)\n",
    "fig.tight_layout()\n",
    "save_fig(\"ForcedBlockRK4\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afff211",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using the above data to train an RNN\n",
    "\n",
    "In the code here we have reworked the previous example in order to\n",
    "generate data that can be handled by recurrent neural networks in\n",
    "order to train our model. The first code is written using Tensorflow/keras while the second example uses PyTorch.\n",
    "In both cases we use the Runge Kutta to fourth order as a way to generate the data. We have implemented a simple RNN only.\n",
    "We leave it as an exercise (possible path in project 3) to implement LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfb3bf24",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train_rnn_from_rk4.py\n",
    "\n",
    "import runpy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# ---------- Load RK4-generated data from your script ----------\n",
    "# This runs rungekutta.py and collects its globals. It must populate 't' and 'x' arrays.\n",
    "g = runpy.run_path('rungekutta.py')\n",
    "\n",
    "if not all(k in g for k in ('t','x','v')):\n",
    "    raise RuntimeError(\"rungekutta.py did not expose required variables 't', 'x', 'v' in its globals.\")\n",
    "\n",
    "t = np.array(g['t']).ravel()\n",
    "x = np.array(g['x']).ravel()\n",
    "v = np.array(g['v']).ravel()\n",
    "\n",
    "print(\"Loaded shapes:\", t.shape, x.shape, v.shape)\n",
    "\n",
    "# Simple plot of the original trajectory\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(t, x)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x')\n",
    "plt.title('True trajectory from RK4')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Prepare datasets ----------\n",
    "def make_dataset(series, input_len):\n",
    "    X, y = [], []\n",
    "    N = len(series)\n",
    "    for i in range(N - input_len):\n",
    "        X.append(series[i:i+input_len])\n",
    "        y.append(series[i+input_len])\n",
    "    X = np.array(X).reshape(-1, input_len, 1)  # (samples, timesteps, 1)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "# normalize using global mean/std\n",
    "mean_x, std_x = x.mean(), x.std()\n",
    "x_norm = (x - mean_x) / std_x\n",
    "\n",
    "print(f\"Normalization: mean={mean_x:.6f}, std={std_x:.6f}\")\n",
    "\n",
    "# Model A: input_len = 1 (x_t -> x_{t+1})\n",
    "input_len_A = 1\n",
    "X_A, y_A = make_dataset(x_norm, input_len_A)\n",
    "\n",
    "# Model B: input_len = 10 (used for autoregressive generation)\n",
    "input_len_B = 10\n",
    "X_B, y_B = make_dataset(x_norm, input_len_B)\n",
    "\n",
    "# train/test split\n",
    "test_size = 0.2\n",
    "random_seed = 42\n",
    "Xa_train, Xa_test, ya_train, ya_test = train_test_split(X_A, y_A, test_size=test_size, random_state=random_seed)\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(X_B, y_B, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "print(\"Model A shapes:\", Xa_train.shape, ya_train.shape, \"Model B shapes:\", Xb_train.shape, yb_train.shape)\n",
    "\n",
    "# ---------- Build models ----------\n",
    "def build_simple_rnn(input_len, hidden_size=32):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(input_len,1)),\n",
    "        tf.keras.layers.SimpleRNN(hidden_size, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss='mse',\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "model_A = build_simple_rnn(input_len_A, hidden_size=32)\n",
    "model_B = build_simple_rnn(input_len_B, hidden_size=64)\n",
    "\n",
    "print(\"Model A summary:\")\n",
    "model_A.summary()\n",
    "print(\"\\nModel B summary:\")\n",
    "model_B.summary()\n",
    "\n",
    "# ---------- Train ----------\n",
    "epochs_A = 30\n",
    "epochs_B = 40\n",
    "\n",
    "hist_A = model_A.fit(Xa_train, ya_train, validation_data=(Xa_test, ya_test),\n",
    "                     epochs=epochs_A, batch_size=32, verbose=1)\n",
    "\n",
    "hist_B = model_B.fit(Xb_train, yb_train, validation_data=(Xb_test, yb_test),\n",
    "                     epochs=epochs_B, batch_size=32, verbose=1)\n",
    "\n",
    "# ---------- Plot training curves ----------\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist_A.history['loss'], label='train')\n",
    "plt.plot(hist_A.history['val_loss'], label='val')\n",
    "plt.title('Model A loss')\n",
    "plt.xlabel('epoch'); plt.ylabel('mse'); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist_B.history['loss'], label='train')\n",
    "plt.plot(hist_B.history['val_loss'], label='val')\n",
    "plt.title('Model B loss')\n",
    "plt.xlabel('epoch'); plt.ylabel('mse'); plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Evaluate one-step predictions ----------\n",
    "preds_A = model_A.predict(Xa_test)\n",
    "preds_A_un = preds_A.flatten() * std_x + mean_x\n",
    "ya_test_un = ya_test.flatten() * std_x + mean_x\n",
    "\n",
    "print(\"Model A one-step MSE (unnormalized):\", np.mean((preds_A_un - ya_test_un)**2))\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "nplot = min(100, len(ya_test_un))\n",
    "plt.plot(ya_test_un[:nplot], label='true next x')\n",
    "plt.plot(preds_A_un[:nplot], label='predicted next x (Model A)')\n",
    "plt.title(\"Model A: one-step predictions (segment)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Autoregressive generation using Model B ----------\n",
    "# Start from the first input_len_B true values, then generate the remainder autoregressively\n",
    "initial_window = x_norm[:input_len_B].reshape(1,input_len_B,1)\n",
    "gen_steps = len(x_norm) - input_len_B\n",
    "generated = []\n",
    "current_window = initial_window.copy()\n",
    "\n",
    "for i in range(gen_steps):\n",
    "    pred_norm = model_B.predict(current_window, verbose=0)  # shape (1,1)\n",
    "    generated.append(pred_norm.flatten()[0])\n",
    "    # roll the window and append prediction\n",
    "    current_window = np.concatenate([current_window[:,1:,:], pred_norm.reshape(1,1,1)], axis=1)\n",
    "\n",
    "generated_un = np.array(generated) * std_x + mean_x\n",
    "true_remainder = x[input_len_B:]\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(true_remainder, label='true remainder')\n",
    "plt.plot(generated_un, label='generated (Model B)')\n",
    "plt.title('Model B autoregressive generation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Save models ----------\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "path_A = os.path.join('saved_models','model_A_rnn.h5')\n",
    "path_B = os.path.join('saved_models','model_B_rnn.h5')\n",
    "model_A.save(path_A)\n",
    "model_B.save(path_B)\n",
    "print(\"Saved models to:\", path_A, path_B)\n",
    "\n",
    "# ---------- Final numeric summaries ----------\n",
    "preds_B = model_B.predict(Xb_test)\n",
    "preds_B_un = preds_B.flatten() * std_x + mean_x\n",
    "yb_test_un = yb_test.flatten() * std_x + mean_x\n",
    "mse_A = np.mean((preds_A_un - ya_test_un)**2)\n",
    "mse_B = np.mean((preds_B_un - yb_test_un)**2)\n",
    "print(f\"One-step MSE (Model A): {mse_A:.6e}\")\n",
    "print(f\"One-step MSE (Model B): {mse_B:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21972e5b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Similar code using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63a36dc3",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import runpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Load your RK4 integrator and generate the dataset\n",
    "# -------------------------------------------------------\n",
    "data = runpy.run_path(\"rungekutta.py\")\n",
    "\n",
    "t = np.array(data[\"t\"])\n",
    "x = np.array(data[\"x\"])\n",
    "\n",
    "x = x.reshape(-1, 1)          # shape: (T, 1)\n",
    "T = len(x)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Build supervised learning dataset\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# ---------- Task 1: one-step predictor x_t → x_{t+1} ----------\n",
    "X1 = x[:-1]\n",
    "Y1 = x[1:]\n",
    "\n",
    "X1_torch = torch.tensor(X1, dtype=torch.float32)\n",
    "Y1_torch = torch.tensor(Y1, dtype=torch.float32)\n",
    "\n",
    "# ---------- Task 2: sequence predictor ----------\n",
    "seq_len = 20        # length of input window\n",
    "pred_len = 20       # number of future steps to predict\n",
    "\n",
    "X2 = []\n",
    "Y2 = []\n",
    "\n",
    "for i in range(T - seq_len - pred_len):\n",
    "    X2.append(x[i : i + seq_len])\n",
    "    Y2.append(x[i + seq_len : i + seq_len + pred_len])\n",
    "\n",
    "X2 = np.array(X2)     # (N, seq_len, 1)\n",
    "Y2 = np.array(Y2)     # (N, pred_len, 1)\n",
    "\n",
    "X2_torch = torch.tensor(X2, dtype=torch.float32)\n",
    "Y2_torch = torch.tensor(Y2, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Define RNN models\n",
    "# -------------------------------------------------------\n",
    "\n",
    "class RNNOneStep(nn.Module):\n",
    "    \"\"\"Model 1: x_t → x_{t+1}\"\"\"\n",
    "    def __init__(self, hidden=32):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(1, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x.unsqueeze(1))   # shape (batch, 1, hidden)\n",
    "        out = out[:, -1, :]                 # last time step\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "class RNNSequence(nn.Module):\n",
    "    \"\"\"Model 2: Predict multiple future steps\"\"\"\n",
    "    def __init__(self, hidden=64):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(1, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)           # out: (batch, seq_len, hidden)\n",
    "        out = self.fc(out)             # (batch, seq_len, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Train Model 1 (single-step predictor)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "model1 = RNNOneStep()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model1(X1_torch)\n",
    "    loss = criterion(pred, Y1_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"One-step Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Train Model 2 (sequence predictor)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "model2 = RNNSequence()\n",
    "optimizer = optim.Adam(model4.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model2(X2_torch)\n",
    "    loss = criterion(pred, Y2_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Sequence Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. Evaluate: multi-step prediction\n",
    "# -------------------------------------------------------\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_input = X2_torch[10:11]      # shape (1, seq_len, 1)\n",
    "    predicted_seq = model4(sample_input).numpy().squeeze()\n",
    "    true_seq = Y2[10].squeeze()\n",
    "\n",
    "plt.plot(true_seq, label=\"True\")\n",
    "plt.plot(predicted_seq, label=\"Predicted\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.title(\"Sequence prediction (20 steps ahead)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab438a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Autoencoders: Overarching view\n",
    "\n",
    "Autoencoders are artificial neural networks capable of learning\n",
    "efficient representations of the input data (these representations are called codings)  without\n",
    "any supervision (i.e., the training set is unlabeled). These codings\n",
    "typically have a much lower dimensionality than the input data, making\n",
    "autoencoders useful for dimensionality reduction. \n",
    "\n",
    "Autoencoders learn to encode the\n",
    "input data into a lower-dimensional representation, and then decode it\n",
    "back to the original data. The goal of autoencoders is to minimize the\n",
    "reconstruction error, which measures how well the output matches the\n",
    "input. Autoencoders can be seen as a way of learning the latent\n",
    "features or hidden structure of the data, and they can be used for\n",
    "data compression, denoising, anomaly detection, and generative\n",
    "modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9176a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Powerful detectors\n",
    "\n",
    "More importantly, autoencoders act as powerful feature detectors, and\n",
    "they can be used for unsupervised pretraining of deep neural networks.\n",
    "\n",
    "Lastly, they are capable of randomly generating new data that looks\n",
    "very similar to the training data; this is called a generative\n",
    "model. For example, you could train an autoencoder on pictures of\n",
    "faces, and it would then be able to generate new faces.  Surprisingly,\n",
    "autoencoders work by simply learning to copy their inputs to their\n",
    "outputs. This may sound like a trivial task, but we will see that\n",
    "constraining the network in various ways can make it rather\n",
    "difficult. For example, you can limit the size of the internal\n",
    "representation, or you can add noise to the inputs and train the\n",
    "network to recover the original inputs. These constraints prevent the\n",
    "autoencoder from trivially copying the inputs directly to the outputs,\n",
    "which forces it to learn efficient ways of representing the data. In\n",
    "short, the codings are byproducts of the autoencoder’s attempt to\n",
    "learn the identity function under some constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f687fa2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## First introduction of AEs\n",
    "\n",
    "Autoencoders were first introduced by Rumelhart, Hinton, and Williams\n",
    "in 1986 with the goal of learning to reconstruct the input\n",
    "observations with the lowest error possible.\n",
    "\n",
    "Why would one want to learn to reconstruct the input observations? If\n",
    "you have problems imagining what that means, think of having a dataset\n",
    "made of images. An autoencoder would be an algorithm that can give as\n",
    "output an image that is as similar as possible to the input one. You\n",
    "may be confused, as there is no apparent reason of doing so. To better\n",
    "understand why autoencoders are useful we need a more informative\n",
    "(although not yet unambiguous) definition.\n",
    "\n",
    "An autoencoder is a type of algorithm with the primary purpose of learning an \"informative\" representation of the data that can be used for different applications ([see Bank, D., Koenigstein, N., and Giryes, R., Autoencoders](https://arxiv.org/abs/2003.05991)) by learning to reconstruct a set of input observations well enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788845d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Autoencoder structure\n",
    "\n",
    "Autoencoders are neural networks where the outputs are its own\n",
    "inputs. They are split into an **encoder part**\n",
    "which maps the input $\\boldsymbol{x}$ via a function $f(\\boldsymbol{x},\\boldsymbol{W})$ (this\n",
    "is the encoder part) to a **so-called code part** (or intermediate part)\n",
    "with the result $\\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb37fc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{h} = f(\\boldsymbol{x},\\boldsymbol{W})),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc7550",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\boldsymbol{W}$ are the weights to be determined.  The **decoder** parts maps, via its own parameters (weights given by the matrix $\\boldsymbol{V}$ and its own biases) to \n",
    "the final ouput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41716c7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{x}} = g(\\boldsymbol{h},\\boldsymbol{V})).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c828363",
   "metadata": {
    "editable": true
   },
   "source": [
    "The goal is to minimize the construction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a529ad2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Schematic image of an Autoencoder\n",
    "\n",
    "<!-- dom:FIGURE: [figures/ae1.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/ae1.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422016b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on the structure\n",
    "\n",
    "In most typical architectures, the encoder and the decoder are neural networks\n",
    "since they can be easily trained with existing software libraries such as TensorFlow or PyTorch with back propagation.\n",
    "\n",
    "In general, the encoder can be written as a function $g$ that will depend on some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35478d0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{h}_{i} = g(\\mathbf{x}_{i}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e5f513",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\mathbf{h}_{i}\\in\\mathbb{R}^{q}$  (the latent feature representation) is the output of the encoder block where we evaluate\n",
    "it using the input $\\mathbf{x}_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7973ec85",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Decoder part\n",
    "\n",
    "Note that we have $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{q}$\n",
    "The decoder and the output of the network $\\tilde{\\mathbf{x}}_{i}$ can be written then as a second generic function\n",
    "of the latent features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32529266",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\mathbf{x}}_{i} = f\\left(\\mathbf{h}_{i}\\right) = f\\left(g\\left(\\mathbf{x}_{i}\\right)\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc6996",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\tilde{\\mathbf{x}}_{i}\\mathbf{\\in }\\mathbb{R}^{n}$.\n",
    "\n",
    "Training an autoencoder simply means finding the functions $g(\\cdot)$ and $f(\\cdot)$\n",
    "that satisfy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5037582",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\textrm{arg}\\min_{f,g}<\\left[\\Delta (\\mathbf{x}_{i}, f(g\\left(\\mathbf{x}_{i}\\right))\\right]>.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8774936",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Typical AEs\n",
    "\n",
    "The standard setup is done via a standard feed forward neural network (FFNN), or what is called a Feed Forward Autoencoder.\n",
    "\n",
    "A typical FFNN architecture has a given  number of layers and is symmetrical with respect to the middle layer.\n",
    "\n",
    "Typically, the first layer has a number of neurons $n_{1} = n$ which equals the size of the input observation $\\mathbf{x}_{\\mathbf{i}}$.\n",
    "\n",
    "As we move toward the center of the network, the number of neurons in each layer drops in some measure.\n",
    "The middle layer usually has the smallest number of neurons.\n",
    "The fact that the number of neurons in this layer is smaller than the size of the input, is often called the **bottleneck**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31156b72",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Feed Forward Autoencoder\n",
    "\n",
    "<!-- dom:FIGURE: [figures/ae2.png, width=700 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/ae2.png\" width=\"700\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e15ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mirroring\n",
    "\n",
    "In almost all practical applications,\n",
    "the layers after the middle one are a mirrored version of the layers before the middle one.\n",
    "For example, an autoencoder with three layers could have the following numbers of neurons:\n",
    "\n",
    "$n_{1} = 10$, $n_{2} = 5$ and then $n_{3} = n_{1} = 10$ where the input dimension is equal to ten.\n",
    "\n",
    "All the layers up to and including the middle one, make what is called the encoder, and all the layers from and including\n",
    "the middle one (up to the output) make what is called the decoder.\n",
    "\n",
    "If the FFNN training is successful, the result will\n",
    "be a good approximation of the input $\\tilde{\\mathbf{x}}_{i}\\approx\\mathbf{x}_{i}$.\n",
    "\n",
    "What is essential to notice is that the decoder can reconstruct the\n",
    "input by using only a much smaller number of features than the input\n",
    "observations initially have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a31487f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Output of middle layer\n",
    "\n",
    "The output of the middle layer\n",
    "$\\mathbf{h}_{\\mathbf{i}}$ are also called a **learned representation** of the input observation $\\mathbf{x}_{i}$.\n",
    "\n",
    "The encoder can reduce the number of dimensions of the input\n",
    "observation and create a learned representation\n",
    "$\\mathbf{h}_{\\mathbf{i}}\\mathbf{) }$ of the input that has a smaller\n",
    "dimension $q<n$.\n",
    "\n",
    "This learned representation is enough for the decoder to reconstruct\n",
    "the input accurately (if the autoencoder training was successful as\n",
    "intended)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b2079",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Activation Function of the Output Layer\n",
    "\n",
    "In autoencoders based on neural networks, the output layer's\n",
    "activation function plays a particularly important role.  The most\n",
    "used functions are ReLU and Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b507fd3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## ReLU\n",
    "\n",
    "The  ReLU activation function can assume all values in the range $\\left[0,\\infty\\right]$. As a remainder, its formula is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a02f89",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\textrm{ReLU}\\left(x\\right) = \\max\\left(0,x\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229f9c1e",
   "metadata": {
    "editable": true
   },
   "source": [
    "This choice is good when the input observations \\(\\mathbf{x}_{i}\\) assume a wide range of positive values.\n",
    "If the input $\\mathbf{x}_{i}$ can assume negative values, the ReLU is, of course, a terrible choice, and the identity function is a much better choice. It is then common to replace to the ReLU with the so-called **Leaky ReLu** or just modified ReLU.\n",
    "\n",
    "The ReLU activation function for the output layer is well suited for cases when the input observations \\(\\mathbf{x}_{i}\\) assume a wide range of positive real values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed2ca2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Sigmoid\n",
    "\n",
    "The sigmoid function $\\sigma$ can assume all values in the range $[0,1]$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3602be",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sigma\\left(x\\right) =\\frac{1}{1+e^{-x}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8aa26",
   "metadata": {
    "editable": true
   },
   "source": [
    "This activation function can only be used if the input observations\n",
    "$\\mathbf{x}_{i}$ are all in the range $[0,1]$  or if you have\n",
    "normalized them to be in that range. Consider as an example the MNIST\n",
    "dataset. Each value of the input observation $\\mathbf{x}_{i}$ (one\n",
    "image) is the gray values of the pixels that can assume any value from\n",
    "0 to 255. Normalizing the data by dividing the pixel values by 255\n",
    "would make each observation (each image) have only pixel values\n",
    "between 0 and 1. In this case, the sigmoid would be a good choice for\n",
    "the output layer's activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e390293",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Cost/Loss Function\n",
    "\n",
    "If an autoencoder is trying to solve a regression problem, the most\n",
    "common choice as a loss function is the Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32cd4c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "L_{\\textrm{MSE}} = \\textrm{MSE} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left\\vert\\vert\\mathbf{x}_{i}-\\tilde{\\mathbf{x}}_{i}\\right\\vert\\vert^{2}_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe6c14",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Binary Cross-Entropy\n",
    "\n",
    "If the activation function of the output layer of the AE is a sigmoid\n",
    "function, thus limiting neuron outputs to be between 0 and 1, and the\n",
    "input features are normalized to be between 0 and 1 we can use as loss\n",
    "function the binary cross-entropy. This cots/loss function is\n",
    "typically used in classification problems, but it works well for\n",
    "autoencoders. The formula for it is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51275ff",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "L_{\\textrm{CE}} = -\\frac{1}{n}\\sum_{i = 1}^{n}\\sum_{j = 1}^{p}[x_{j,i} \\log\\tilde{x}_{j,i}+\\left(1-x_{j,i}\\right)\\log (1-\\tilde{x}_{j,i})].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a8438",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reconstruction Error\n",
    "\n",
    "The reconstruction error (RE) is a metric that gives you an indication of how good (or bad) the autoencoder was able to reconstruct\n",
    "the input observation $\\mathbf{x}_{i}$. The most typical RE used is the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de7825",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\textrm{RE}\\equiv \\textrm{MSE} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left\\vert\\vert\\mathbf{x}_{i}-\\tilde{\\mathbf{x}}_{i}\\right\\vert\\vert^{2}_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c34b48",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Implementation using TensorFlow\n",
    "\n",
    "The code here has the following structure\n",
    "1. Data Loading: The MNIST dataset is loaded and normalized to a range of $[0, 1]$. Each image is reshaped into a flat vector.\n",
    "\n",
    "2. Model Definition: An autoencoder architecture is defined with an encoder that compresses the input and a decoder that reconstructs it back to its original form.\n",
    "\n",
    "3. Training: The model is trained using binary crossentropy as the loss function over several epochs.\n",
    "\n",
    "4. Visualization: After training completes, it visualizes original images alongside their reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66aafb5e",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Autoencoder Implementation in TensorFlow/Keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1] range and reshape them to (num_samples, 28*28)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), -1))\n",
    "x_test = x_test.reshape((len(x_test), -1))\n",
    "\n",
    "# Define the Autoencoder Model\n",
    "input_dim = x_train.shape[1]\n",
    "encoding_dim = 64  # Dimension of the encoding layer\n",
    "\n",
    "# Encoder\n",
    "input_img = layers.Input(shape=(input_dim,))\n",
    "encoded = layers.Dense(256, activation='relu')(input_img)\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = layers.Dense(256, activation='relu')(encoded)\n",
    "decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)  # Use sigmoid since we normalized input between 0 and 1.\n",
    "\n",
    "# Autoencoder Model\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=10,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "# Visualize some results after training\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 8  # Number of digits to display\n",
    "plt.figure(figsize=(9,4))\n",
    "for i in range(n):\n",
    "    # Display original images on top row \n",
    "    ax = plt.subplot(2,n,i+1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Display reconstructed images on bottom row \n",
    "    ax = plt.subplot(2,n,i+n+1)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefe509",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Implementation using PyTorch\n",
    "\n",
    "The code here as the same structure as the previous one which uses TensorFlow.\n",
    "1. Data Loading: The MNIST dataset is loaded with normalization applied.\n",
    "\n",
    "2. Model Definition: An *Autoencoder* class defines both encoder and decoder networks.\n",
    "\n",
    "3. Training part: The network is trained over several epochs using Mean Squared Error (MSE) as the loss function.\n",
    "\n",
    "4. Visualization: After training completes, it visualizes original images alongside their reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a33a90f8",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 28 * 28),\n",
    "            nn.Tanh()   # Use Tanh since we normalized input between -1 and 1.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the image tensor into vectors.\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded.view(-1, 1, 28, 28)   # Reshape back to original image dimensions.\n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        img, _ = data\n",
    "        \n",
    "        # Forward pass \n",
    "        output = model(img)\n",
    "        \n",
    "        # Compute loss \n",
    "        loss = criterion(output, img)\n",
    "        \n",
    "        # Backward pass and optimization \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Visualize some results after training\n",
    "with torch.no_grad():\n",
    "    sample_data = next(iter(train_loader))[0]\n",
    "    reconstructed_data = model(sample_data)\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "for i in range(8):\n",
    "    ax = plt.subplot(2,8,i+1)\n",
    "    plt.imshow(sample_data[i][0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax = plt.subplot(2,8,i+9)\n",
    "    plt.imshow(reconstructed_data[i][0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dec691",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Dimensionality reduction and links with Principal component analysis\n",
    "\n",
    "The hope is that the training of the autoencoder can unravel some\n",
    "useful properties of the function $f$. They are often trained with\n",
    "only single-layer neural networks (although deep networks can improve\n",
    "the training) and are essentially given by feed forward neural\n",
    "networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867c31d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear functions\n",
    "\n",
    "If the function $f$ and $g$ are given by a linear dependence on the\n",
    "weight matrices $\\boldsymbol{W}$ and $\\boldsymbol{V}$, we can show that for a\n",
    "regression case, by miminizing the mean squared error between $\\boldsymbol{x}$\n",
    "and $\\tilde{\\boldsymbol{x}}$, the autoencoder learns the same subspace as the\n",
    "standard principal component analysis (PCA).\n",
    "\n",
    "In order to see this, we define then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62de128",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{h} = f(\\boldsymbol{x},\\boldsymbol{W}))=\\boldsymbol{W}\\boldsymbol{x},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfc07b",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467bc7d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{x}} = g(\\boldsymbol{h},\\boldsymbol{V}))=\\boldsymbol{V}\\boldsymbol{h}=\\boldsymbol{V}\\boldsymbol{W}\\boldsymbol{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c6acf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## AE mean-squared error\n",
    "\n",
    "With the above linear dependence we can in turn define our\n",
    "optimization problem in terms of the optimization of the mean-squared\n",
    "error, that is we wish to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543a7fe",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\min_{\\boldsymbol{W},\\boldsymbol{V}\\in {\\mathbb{R}}}\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(x_i-\\tilde{x}_i\\right)^2=\\frac{1}{n}\\vert\\vert \\boldsymbol{x}-\\boldsymbol{V}\\boldsymbol{W}\\boldsymbol{x}\\vert\\vert_2^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284e94a",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have used the definition of  a norm-2 vector, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11d160",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_2 = \\sqrt{\\sum_i x_i^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655ed1a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "This is equivalent to our functions learning the same subspace as\n",
    "the PCA method. This means that we can interpret AEs as a\n",
    "dimensionality reduction method.  To see this, we need to remind\n",
    "ourselves about the PCA method. This will be the topic of the last lecture, on Monday November 24. We will use this lecture (second lecture) to summarize the course as well. Stay tuned."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}