{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f26310a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week36.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Week 36: Linear Regression and Gradient descent -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc2d76",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Week 36: Linear Regression and Gradient descent\n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo, Norway\n",
    "\n",
    "Date: **September 1-5, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7420c76",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plans for week 36\n",
    "\n",
    "**Material for the lecture on Monday September 1:**\n",
    "1. Linear Regression, ordinary least squares (OLS), Ridge and Lasso and mathematical analysis\n",
    "\n",
    "2. Derivation of Gradient descent and discussion of implementations for\n",
    "<!-- * [Video of lecture](https://youtu.be/oHjqjUB36KE) -->\n",
    "<!-- * [Whiteboard notes](https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2024/NotesSeptember2.pdf) -->\n",
    "\n",
    "**Material for the lab sessions on Tuesday and Wednesday (see at the end of these slides):**\n",
    "1. Technicalities concerning Ridge and Lasso linear regression.\n",
    "\n",
    "2. Presentation and discussion of the first project\n",
    "<!-- * [Video of lab session](https://youtu.be/ZrIdZdZtHe0) -->\n",
    "\n",
    "**Reading suggestion:**\n",
    "1. Goodfellow et al, Deep Learning, introduction to gradient descent, see chapter 4.3 at <https://www.deeplearningbook.org/contents/numerical.html>\n",
    "\n",
    "2. Rashcka et al, pages 37-44 and pages 278-283 with focus on linear regression.\n",
    "\n",
    "3. Video on gradient descent at <https://www.youtube.com/watch?v=sDv4f4s2SB8>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f92875",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Material for lecture Monday September 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b4769",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematical Interpretation of Ordinary Least Squares\n",
    "\n",
    "What is presented here is a mathematical analysis of various regression algorithms (ordinary least  squares, Ridge and Lasso Regression). The analysis is based on an important algorithm in linear algebra, the so-called Singular Value Decomposition (SVD). \n",
    "\n",
    "We have shown that in ordinary least squares the optimal parameters $\\theta$ are given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a85fa",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d4cc3",
   "metadata": {
    "editable": true
   },
   "source": [
    "The **hat** over $\\boldsymbol{\\theta}$ means we have the optimal parameters after minimization of the cost function.\n",
    "\n",
    "This means that our best model is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca01dcd",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}=\\boldsymbol{X}\\hat{\\boldsymbol{\\theta}} = \\boldsymbol{X}\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764f0b7",
   "metadata": {
    "editable": true
   },
   "source": [
    "We now define a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e7402",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{A}=\\boldsymbol{X}\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3742c0a5",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53940e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}=\\boldsymbol{X}\\hat{\\boldsymbol{\\theta}} = \\boldsymbol{A}\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925aaae0",
   "metadata": {
    "editable": true
   },
   "source": [
    "The matrix $\\boldsymbol{A}$ has the important property that $\\boldsymbol{A}^2=\\boldsymbol{A}$. This is the definition of a projection matrix.\n",
    "We can then interpret our optimal model $\\tilde{\\boldsymbol{y}}$ as being represented  by an orthogonal  projection of $\\boldsymbol{y}$ onto a space defined by the column vectors of $\\boldsymbol{X}$.  In our case here the matrix $\\boldsymbol{A}$ is a square matrix. If it is a general rectangular matrix we have an oblique projection matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f3796",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Residual Error\n",
    "\n",
    "We have defined the residual error as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40f4a3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\epsilon}=\\boldsymbol{y}-\\tilde{\\boldsymbol{y}}=\\left[\\boldsymbol{I}-\\boldsymbol{X}\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\right]\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff168d",
   "metadata": {
    "editable": true
   },
   "source": [
    "The residual errors are then the projections of $\\boldsymbol{y}$ onto the orthogonal component of the space defined by the column vectors of $\\boldsymbol{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c9b8e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple case\n",
    "\n",
    "If the matrix $\\boldsymbol{X}$ is an orthogonal (or unitary in case of complex values) matrix, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b643b8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}=\\boldsymbol{X}\\boldsymbol{X}^T = \\boldsymbol{I}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7469316",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this case the matrix $\\boldsymbol{A}$ becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731f75b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{A}=\\boldsymbol{X}\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T)=\\boldsymbol{I},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457dbc6",
   "metadata": {
    "editable": true
   },
   "source": [
    "and we have the obvious case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a8400",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\epsilon}=\\boldsymbol{y}-\\tilde{\\boldsymbol{y}}=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54905391",
   "metadata": {
    "editable": true
   },
   "source": [
    "This serves also as a useful test of our codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a87de7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The singular value decomposition\n",
    "\n",
    "The examples we have looked at so far are cases where we normally can\n",
    "invert the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$. Using a polynomial expansion where we fit of various functions leads to\n",
    "row vectors of the design matrix which are essentially orthogonal due\n",
    "to the polynomial character of our model. Obtaining the inverse of the\n",
    "design matrix is then often done via a so-called LU, QR or Cholesky\n",
    "decomposition.\n",
    "\n",
    "As we will also see in the first project, \n",
    "this may\n",
    "however not the be case in general and a standard matrix inversion\n",
    "algorithm based on say LU, QR or Cholesky decomposition may lead to singularities. We will see examples of this below and in other examples.\n",
    "\n",
    "There is however a way to circumvent this problem and also\n",
    "gain some insights about the ordinary least squares approach, and\n",
    "later shrinkage methods like Ridge and Lasso regressions.\n",
    "\n",
    "This is given by the **Singular Value Decomposition** (SVD) algorithm,\n",
    "perhaps the most powerful linear algebra algorithm.  The SVD provides\n",
    "a numerically stable matrix decomposition that is used in a large\n",
    "swath oc applications and the decomposition is always stable\n",
    "numerically.\n",
    "\n",
    "In machine learning it plays a central role in dealing with for\n",
    "example design matrices that may be near singular or singular.\n",
    "Furthermore, as we will see here, the singular values can be related\n",
    "to the covariance matrix (and thereby the correlation matrix) and in\n",
    "turn the variance of a given quantity. It plays also an important role\n",
    "in the principal component analysis where high-dimensional data can be\n",
    "reduced to the statistically relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61069f87",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear Regression Problems\n",
    "\n",
    "One of the typical problems we encounter with linear regression, in particular \n",
    "when the matrix $\\boldsymbol{X}$ (our so-called design matrix) is high-dimensional, \n",
    "are problems with near singular or singular matrices. The column vectors of $\\boldsymbol{X}$ \n",
    "may be linearly dependent, normally referred to as super-collinearity.  \n",
    "This means that the matrix may be rank deficient and it is basically impossible to \n",
    "to model the data using linear regression. As an example, consider the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa26f6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{X} & =  \\left[\n",
    "\\begin{array}{rrr}\n",
    "1 & -1 & 2\n",
    "\\\\\n",
    "1 & 0 & 1\n",
    "\\\\\n",
    "1 & 2  & -1\n",
    "\\\\\n",
    "1 & 1  & 0\n",
    "\\end{array} \\right]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d233acc",
   "metadata": {
    "editable": true
   },
   "source": [
    "The columns of $\\boldsymbol{X}$ are linearly dependent. We see this easily since the \n",
    "the first column is the row-wise sum of the other two columns. The rank (more correct,\n",
    "the column rank) of a matrix is the dimension of the space spanned by the\n",
    "column vectors. Hence, the rank of $\\mathbf{X}$ is equal to the number\n",
    "of linearly independent columns. In this particular case the matrix has rank 2.\n",
    "\n",
    "Super-collinearity of an $(n \\times p)$-dimensional design matrix $\\mathbf{X}$ implies\n",
    "that the inverse of the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$ (the matrix we need to invert to solve the linear regression equations) is non-invertible. If we have a square matrix that does not have an inverse, we say this matrix singular. The example here demonstrates this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a427601",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{X} & =  \\left[\n",
    "\\begin{array}{rr}\n",
    "1 & -1\n",
    "\\\\\n",
    "1 & -1\n",
    "\\end{array} \\right].\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1f1c7",
   "metadata": {
    "editable": true
   },
   "source": [
    "We see easily that  $\\mbox{det}(\\boldsymbol{X}) = x_{11} x_{22} - x_{12} x_{21} = 1 \\times (-1) - 1 \\times (-1) = 0$. Hence, $\\mathbf{X}$ is singular and its inverse is undefined.\n",
    "This is equivalent to saying that the matrix $\\boldsymbol{X}$ has at least an eigenvalue which is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab6e3a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Fixing the singularity\n",
    "\n",
    "If our design matrix $\\boldsymbol{X}$ which enters the linear regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f936e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\theta}  =  (\\boldsymbol{X}^{T} \\boldsymbol{X})^{-1} \\boldsymbol{X}^{T} \\boldsymbol{y},\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9be8d6",
   "metadata": {
    "editable": true
   },
   "source": [
    "has linearly dependent column vectors, we will not be able to compute the inverse\n",
    "of $\\boldsymbol{X}^T\\boldsymbol{X}$ and we cannot find the parameters (estimators) $\\theta_i$. \n",
    "The estimators are only well-defined if $(\\boldsymbol{X}^{T}\\boldsymbol{X})^{-1}$ exists. \n",
    "This is more likely to happen when the matrix $\\boldsymbol{X}$ is high-dimensional. In this case it is likely to encounter a situation where \n",
    "the regression parameters $\\theta_i$ cannot be estimated.\n",
    "\n",
    "A cheap  *ad hoc* approach is  simply to add a small diagonal component to the matrix to invert, that is we change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e826ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^{T} \\boldsymbol{X} \\rightarrow \\boldsymbol{X}^{T} \\boldsymbol{X}+\\lambda \\boldsymbol{I},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e34b6",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\boldsymbol{I}$ is the identity matrix.  When we discuss **Ridge** regression this is actually what we end up evaluating. The parameter $\\lambda$ is called a hyperparameter. More about this later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1823039",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Ridge and LASSO Regression\n",
    "\n",
    "Let us remind ourselves about the expression for the standard Mean Squared Error (MSE) which we used to define our cost function and the equations for the ordinary least squares (OLS) method, that is \n",
    "our optimization problem is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acabb24",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in {\\mathbb{R}}^{p}}}\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d97682d",
   "metadata": {
    "editable": true
   },
   "source": [
    "or we can state it as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3933ec",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f1b59",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have used the definition of  a norm-2 vector, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d252",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_2 = \\sqrt{\\sum_i x_i^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b8c4c",
   "metadata": {
    "editable": true
   },
   "source": [
    "By minimizing the above equation with respect to the parameters\n",
    "$\\boldsymbol{\\theta}$ we could then obtain an analytical expression for the\n",
    "parameters $\\boldsymbol{\\theta}$.  We can add a regularization parameter $\\lambda$ by\n",
    "defining a new cost function to be optimized, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f0a60",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f918db",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to the Ridge regression minimization problem where we\n",
    "require that $\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_2^2\\le t$, where $t$ is\n",
    "a finite number larger than zero. By defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4270811a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\theta})=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abcebd1",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have a new optimization equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b33bd8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62fd9b",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to Lasso regression. Lasso stands for least absolute shrinkage and selection operator. \n",
    "\n",
    "Here we have defined the norm-1 as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400332c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_1 = \\sum_i \\vert x_i\\vert.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8245891",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Deriving the  Ridge Regression Equations\n",
    "\n",
    "Using the matrix-vector expression for Ridge regression and dropping the parameter $1/n$ in front of the standard means squared error equation, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ad5eb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\theta})=\\left\\{(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})\\right\\}+\\lambda\\boldsymbol{\\theta}^T\\boldsymbol{\\theta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae3de1",
   "metadata": {
    "editable": true
   },
   "source": [
    "and \n",
    "taking the derivatives with respect to $\\boldsymbol{\\theta}$ we obtain then\n",
    "a slightly modified matrix inversion problem which for finite values\n",
    "of $\\lambda$ does not suffer from singularity problems. We obtain\n",
    "the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8dee1b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}_{\\mathrm{Ridge}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff9c8d",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\boldsymbol{I}$ being a $p\\times p$ identity matrix with the constraint that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba1fdf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sum_{i=0}^{p-1} \\theta_i^2 \\leq t,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51009a97",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $t$ a finite positive number. \n",
    "\n",
    "If we keep the $1/n$ factor, the equation for the optimal $\\theta$ changes to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7637a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}_{\\mathrm{Ridge}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}+n\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad03dda",
   "metadata": {
    "editable": true
   },
   "source": [
    "In many textbooks the $1/n$ term is often omitted. Note that a library like **Scikit-Learn** does not include the $1/n$ factor in the setup of the cost function.\n",
    "\n",
    "When we compare this with the ordinary least squares result we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31b1595",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}_{\\mathrm{OLS}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457072d",
   "metadata": {
    "editable": true
   },
   "source": [
    "which can lead to singular matrices. However, with the SVD, we can always compute the inverse of the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$.\n",
    "\n",
    "We see that Ridge regression is nothing but the standard OLS with a\n",
    "modified diagonal term added to $\\boldsymbol{X}^T\\boldsymbol{X}$. The consequences, in\n",
    "particular for our discussion of the bias-variance tradeoff are rather\n",
    "interesting. We will see that for specific values of $\\lambda$, we may\n",
    "even reduce the variance of the optimal parameters $\\boldsymbol{\\theta}$. These topics and other related ones, will be discussed after the more linear algebra oriented analysis here.\n",
    "\n",
    "When we have discussed the singular value decomposition of the design\n",
    "matrix $\\boldsymbol{X}$, we will in turn perform a more rigorous mathematical\n",
    "discussion of Ridge regression.\n",
    "\n",
    "The code here is a simple demonstration of how to implement Ridge regression with our own code and compare this with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1968205f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "\n",
    "# A seed just to ensure that the random numbers are the same for every run.\n",
    "# Useful for eventual debugging.\n",
    "np.random.seed(3155)\n",
    "\n",
    "n = 100\n",
    "x = np.random.rand(n)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)\n",
    "\n",
    "Maxpolydegree = 20\n",
    "X = np.zeros((n,Maxpolydegree))\n",
    "#We include explicitely the intercept column\n",
    "for degree in range(Maxpolydegree):\n",
    "    X[:,degree] = x**degree\n",
    "# We split the data in test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "p = Maxpolydegree\n",
    "I = np.eye(p,p)\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 6\n",
    "MSEOwnRidgePredict = np.zeros(nlambdas)\n",
    "MSERidgePredict = np.zeros(nlambdas)\n",
    "lambdas = np.logspace(-4, 2, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "    OwnRidgeTheta = np.linalg.pinv(X_train.T @ X_train+lmb*I) @ X_train.T @ y_train\n",
    "    # Note: we include the intercept column and no scaling\n",
    "    RegRidge = linear_model.Ridge(lmb,fit_intercept=False)\n",
    "    RegRidge.fit(X_train,y_train)\n",
    "    # and then make the prediction\n",
    "    ytildeOwnRidge = X_train @ OwnRidgeTheta\n",
    "    ypredictOwnRidge = X_test @ OwnRidgeTheta\n",
    "    ytildeRidge = RegRidge.predict(X_train)\n",
    "    ypredictRidge = RegRidge.predict(X_test)\n",
    "    MSEOwnRidgePredict[i] = MSE(y_test,ypredictOwnRidge)\n",
    "    MSERidgePredict[i] = MSE(y_test,ypredictRidge)\n",
    "    print(\"Theta values for own Ridge implementation\")\n",
    "    print(OwnRidgeTheta)\n",
    "    print(\"Theta values for Scikit-Learn Ridge implementation\")\n",
    "    print(RegRidge.coef_)\n",
    "    print(\"MSE values for own Ridge implementation\")\n",
    "    print(MSEOwnRidgePredict[i])\n",
    "    print(\"MSE values for Scikit-Learn Ridge implementation\")\n",
    "    print(MSERidgePredict[i])\n",
    "\n",
    "# Now plot the results\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), MSEOwnRidgePredict, 'r', label = 'MSE own Ridge Test')\n",
    "plt.plot(np.log10(lambdas), MSERidgePredict, 'g', label = 'MSE Ridge Test')\n",
    "\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d94ffe",
   "metadata": {
    "editable": true
   },
   "source": [
    "The results here agree when we force **Scikit-Learn**'s Ridge function to include the first column in our design matrix.\n",
    "We see that the results agree very well. Here we have thus explicitely included the intercept column in the design matrix.\n",
    "What happens if we do not include the intercept in our fit? We will discuss this in more detail next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f3e6d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Basic math of the SVD\n",
    "\n",
    "From standard linear algebra we know that a square matrix $\\boldsymbol{X}$ can be diagonalized if and only if it is \n",
    "a so-called [normal matrix](https://en.wikipedia.org/wiki/Normal_matrix), that is if $\\boldsymbol{X}\\in {\\mathbb{R}}^{n\\times n}$\n",
    "we have $\\boldsymbol{X}\\boldsymbol{X}^T=\\boldsymbol{X}^T\\boldsymbol{X}$ or if $\\boldsymbol{X}\\in {\\mathbb{C}}^{n\\times n}$ we have $\\boldsymbol{X}\\boldsymbol{X}^{\\dagger}=\\boldsymbol{X}^{\\dagger}\\boldsymbol{X}$.\n",
    "The matrix has then a set of eigenpairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91463cc2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "(\\lambda_1,\\boldsymbol{u}_1),\\dots, (\\lambda_n,\\boldsymbol{u}_n),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa4ad6",
   "metadata": {
    "editable": true
   },
   "source": [
    "and the eigenvalues are given by the diagonal matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c94f7d5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\Sigma}=\\mathrm{Diag}(\\lambda_1, \\dots,\\lambda_n).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274125d",
   "metadata": {
    "editable": true
   },
   "source": [
    "The matrix $\\boldsymbol{X}$ can be written in terms of an orthogonal/unitary transformation $\\boldsymbol{U}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964ee70",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2bdbb7",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\boldsymbol{U}\\boldsymbol{U}^T=\\boldsymbol{I}$ or $\\boldsymbol{U}\\boldsymbol{U}^{\\dagger}=\\boldsymbol{I}$.\n",
    "\n",
    "Not all square matrices are diagonalizable. A matrix like the one discussed above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802269b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X} = \\begin{bmatrix} \n",
    "1&  -1 \\\\\n",
    "1& -1\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50600f8",
   "metadata": {
    "editable": true
   },
   "source": [
    "is not diagonalizable, it is a so-called [defective matrix](https://en.wikipedia.org/wiki/Defective_matrix). It is easy to see that the condition\n",
    "$\\boldsymbol{X}\\boldsymbol{X}^T=\\boldsymbol{X}^T\\boldsymbol{X}$ is not fulfilled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a85d1ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The SVD, a Fantastic Algorithm\n",
    "\n",
    "However, and this is the strength of the SVD algorithm, any general\n",
    "matrix $\\boldsymbol{X}$ can be decomposed in terms of a diagonal matrix and\n",
    "two orthogonal/unitary matrices.  The [Singular Value Decompostion\n",
    "(SVD) theorem](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "states that a general $m\\times n$ matrix $\\boldsymbol{X}$ can be written in\n",
    "terms of a diagonal matrix $\\boldsymbol{\\Sigma}$ of dimensionality $m\\times n$\n",
    "and two orthognal matrices $\\boldsymbol{U}$ and $\\boldsymbol{V}$, where the first has\n",
    "dimensionality $m \\times m$ and the last dimensionality $n\\times n$.\n",
    "We have then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1eebec",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112113df",
   "metadata": {
    "editable": true
   },
   "source": [
    "As an example, the above defective matrix can be decomposed as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4773d0b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}  1&  1 \\\\ 1& -1\\\\ \\end{bmatrix} \\begin{bmatrix}  2&  0 \\\\ 0& 0\\\\ \\end{bmatrix}    \\frac{1}{\\sqrt{2}}\\begin{bmatrix}  1&  -1 \\\\ 1& 1\\\\ \\end{bmatrix}=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec45f0",
   "metadata": {
    "editable": true
   },
   "source": [
    "with eigenvalues $\\sigma_1=2$ and $\\sigma_2=0$. \n",
    "The SVD exits always! \n",
    "\n",
    "The SVD\n",
    "decomposition (singular values) gives eigenvalues \n",
    "$\\sigma_i\\geq\\sigma_{i+1}$ for all $i$ and for dimensions larger than $i=p$, the\n",
    "eigenvalues (singular values) are zero.\n",
    "\n",
    "In the general case, where our design matrix $\\boldsymbol{X}$ has dimension\n",
    "$n\\times p$, the matrix is thus decomposed into an $n\\times n$\n",
    "orthogonal matrix $\\boldsymbol{U}$, a $p\\times p$ orthogonal matrix $\\boldsymbol{V}$\n",
    "and a diagonal matrix $\\boldsymbol{\\Sigma}$ with $r=\\mathrm{min}(n,p)$\n",
    "singular values $\\sigma_i\\geq 0$ on the main diagonal and zeros filling\n",
    "the rest of the matrix.  There are at most $p$ singular values\n",
    "assuming that $n > p$. In our regression examples for the nuclear\n",
    "masses and the equation of state this is indeed the case, while for\n",
    "the Ising model we have $p > n$. These are often cases that lead to\n",
    "near singular or singular matrices.\n",
    "\n",
    "The columns of $\\boldsymbol{U}$ are called the left singular vectors while the columns of $\\boldsymbol{V}$ are the right singular vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c2b8f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Economy-size SVD\n",
    "\n",
    "If we assume that $n > p$, then our matrix $\\boldsymbol{U}$ has dimension $n\n",
    "\\times n$. The last $n-p$ columns of $\\boldsymbol{U}$ become however\n",
    "irrelevant in our calculations since they are multiplied with the\n",
    "zeros in $\\boldsymbol{\\Sigma}$.\n",
    "\n",
    "The economy-size decomposition removes extra rows or columns of zeros\n",
    "from the diagonal matrix of singular values, $\\boldsymbol{\\Sigma}$, along with the columns\n",
    "in either $\\boldsymbol{U}$ or $\\boldsymbol{V}$ that multiply those zeros in the expression. \n",
    "Removing these zeros and columns can improve execution time\n",
    "and reduce storage requirements without compromising the accuracy of\n",
    "the decomposition.\n",
    "\n",
    "If $n > p$, we keep only the first $p$ columns of $\\boldsymbol{U}$ and $\\boldsymbol{\\Sigma}$ has dimension $p\\times p$. \n",
    "If $p > n$, then only the first $n$ columns of $\\boldsymbol{V}$ are computed and $\\boldsymbol{\\Sigma}$ has dimension $n\\times n$.\n",
    "The $n=p$ case is obvious, we retain the full SVD. \n",
    "In general the economy-size SVD leads to less FLOPS and still conserving the desired accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deaea3a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Codes for the SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dcfad0e",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# SVD inversion\n",
    "def SVD(A):\n",
    "    ''' Takes as input a numpy matrix A and returns inv(A) based on singular value decomposition (SVD).\n",
    "    SVD is numerically more stable than the inversion algorithms provided by\n",
    "    numpy and scipy.linalg at the cost of being slower.\n",
    "    '''\n",
    "    U, S, VT = np.linalg.svd(A,full_matrices=True)\n",
    "    print('test U')\n",
    "    print( (np.transpose(U) @ U - U @np.transpose(U)))\n",
    "    print('test VT')\n",
    "    print( (np.transpose(VT) @ VT - VT @np.transpose(VT)))\n",
    "    print(U)\n",
    "    print(S)\n",
    "    print(VT)\n",
    "\n",
    "    D = np.zeros((len(U),len(VT)))\n",
    "    for i in range(0,len(VT)):\n",
    "        D[i,i]=S[i]\n",
    "    return U @ D @ VT\n",
    "\n",
    "\n",
    "X = np.array([ [1.0,-1.0], [1.0,-1.0]])\n",
    "#X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "print(X)\n",
    "C = SVD(X)\n",
    "# Print the difference between the original matrix and the SVD one\n",
    "print(C-X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9365fd",
   "metadata": {
    "editable": true
   },
   "source": [
    "The matrix $\\boldsymbol{X}$ has columns that are linearly dependent. The first\n",
    "column is the row-wise sum of the other two columns. The rank of a\n",
    "matrix (the column rank) is the dimension of space spanned by the\n",
    "column vectors. The rank of the matrix is the number of linearly\n",
    "independent columns, in this case just $2$. We see this from the\n",
    "singular values when running the above code. Running the standard\n",
    "inversion algorithm for matrix inversion with $\\boldsymbol{X}^T\\boldsymbol{X}$ results\n",
    "in the program terminating due to a singular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025df917",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Note about SVD Calculations\n",
    "\n",
    "The $U$, $S$, and $V$ matrices returned from the **svd()** function\n",
    "cannot be multiplied directly.\n",
    "\n",
    "As you can see from the code, the $S$ vector must be converted into a\n",
    "diagonal matrix. This may cause a problem as the size of the matrices\n",
    "do not fit the rules of matrix multiplication, where the number of\n",
    "columns in a matrix must match the number of rows in the subsequent\n",
    "matrix.\n",
    "\n",
    "If you wish to include the zero singular values, you will need to\n",
    "resize the matrices and set up a diagonal matrix as done in the above\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042fd24e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematics of the SVD and implications\n",
    "\n",
    "Let us take a closer look at the mathematics of the SVD and the various implications for machine learning studies.\n",
    "\n",
    "Our starting point is our design matrix $\\boldsymbol{X}$ of dimension $n\\times p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9723b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\begin{bmatrix}\n",
    "x_{0,0} & x_{0,1} & x_{0,2}& \\dots & \\dots x_{0,p-1}\\\\\n",
    "x_{1,0} & x_{1,1} & x_{1,2}& \\dots & \\dots x_{1,p-1}\\\\\n",
    "x_{2,0} & x_{2,1} & x_{2,2}& \\dots & \\dots x_{2,p-1}\\\\\n",
    "\\dots & \\dots & \\dots & \\dots \\dots & \\dots \\\\\n",
    "x_{n-2,0} & x_{n-2,1} & x_{n-2,2}& \\dots & \\dots x_{n-2,p-1}\\\\\n",
    "x_{n-1,0} & x_{n-1,1} & x_{n-1,2}& \\dots & \\dots x_{n-1,p-1}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfaab0",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can SVD decompose our matrix as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f34a49",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee4a1b",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\boldsymbol{U}$ is an orthogonal matrix of dimension $n\\times n$, meaning that $\\boldsymbol{U}\\boldsymbol{U}^T=\\boldsymbol{U}^T\\boldsymbol{U}=\\boldsymbol{I}_n$. Here $\\boldsymbol{I}_n$ is the unit matrix of dimension $n \\times n$.\n",
    "\n",
    "Similarly, $\\boldsymbol{V}$ is an orthogonal matrix of dimension $p\\times p$, meaning that $\\boldsymbol{V}\\boldsymbol{V}^T=\\boldsymbol{V}^T\\boldsymbol{V}=\\boldsymbol{I}_p$. Here $\\boldsymbol{I}_n$ is the unit matrix of dimension $p \\times p$.\n",
    "\n",
    "Finally $\\boldsymbol{\\Sigma}$ contains the singular values $\\sigma_i$. This matrix has dimension $n\\times p$ and the singular values $\\sigma_i$ are all positive. The non-zero values are ordered in descending order, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02751419",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sigma_0 > \\sigma_1 > \\sigma_2 > \\dots > \\sigma_{p-1} > 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a962c",
   "metadata": {
    "editable": true
   },
   "source": [
    "All values beyond $p-1$ are all zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202783d2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Example Matrix\n",
    "\n",
    "As an example, consider the following $3\\times 2$ example for the matrix $\\boldsymbol{\\Sigma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad44ea8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\Sigma}=\n",
    "\\begin{bmatrix}\n",
    "2& 0 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976bb686",
   "metadata": {
    "editable": true
   },
   "source": [
    "The singular values are $\\sigma_0=2$ and $\\sigma_1=1$. It is common to rewrite the matrix $\\boldsymbol{\\Sigma}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e560034",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\Sigma}=\n",
    "\\begin{bmatrix}\n",
    "\\boldsymbol{\\tilde{\\Sigma}}\\\\\n",
    "\\boldsymbol{0}\\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bffb03b",
   "metadata": {
    "editable": true
   },
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b4bfe",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\tilde{\\Sigma}}=\n",
    "\\begin{bmatrix}\n",
    "2& 0 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e905d0",
   "metadata": {
    "editable": true
   },
   "source": [
    "contains only the singular values.   Note also (and we will use this below) that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c1c3b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}=\n",
    "\\begin{bmatrix}\n",
    "4& 0 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df833c67",
   "metadata": {
    "editable": true
   },
   "source": [
    "which is a $2\\times 2 $ matrix while"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad72f69",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}^T=\n",
    "\\begin{bmatrix}\n",
    "4& 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 0\\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700a204",
   "metadata": {
    "editable": true
   },
   "source": [
    "is a $3\\times 3 $ matrix. The last row and column of this last matrix\n",
    "contain only zeros. This will have important consequences for our SVD\n",
    "decomposition of the design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00136519",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Setting up the Matrix to be inverted\n",
    "\n",
    "The matrix that may cause problems for us is $\\boldsymbol{X}^T\\boldsymbol{X}$. Using the SVD we can rewrite this matrix as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489139ae",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}=\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc6730",
   "metadata": {
    "editable": true
   },
   "source": [
    "and using the orthogonality of the matrix $\\boldsymbol{U}$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc9558",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}=\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}\\boldsymbol{V}^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27936a",
   "metadata": {
    "editable": true
   },
   "source": [
    "We define $\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}=\\tilde{\\boldsymbol{\\Sigma}}^2$ which is  a diagonal matrix containing only the singular values squared. It has dimensionality $p \\times p$.\n",
    "\n",
    "We can now insert the result for the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$ into our equation for ordinary least squares where"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9232949",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{y}_{\\mathrm{OLS}}=\\boldsymbol{X}\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aad43a",
   "metadata": {
    "editable": true
   },
   "source": [
    "and using our SVD decomposition of $\\boldsymbol{X}$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a3213",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{y}_{\\mathrm{OLS}}=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\\left(\\boldsymbol{V}\\tilde{\\boldsymbol{\\Sigma}}^{2}(\\boldsymbol{V}^T\\right)^{-1}\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977efd9c",
   "metadata": {
    "editable": true
   },
   "source": [
    "which gives us, using the orthogonality of the matrix $\\boldsymbol{V}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce7fb8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{y}_{\\mathrm{OLS}}=\\sum_{i=0}^{p-1}\\boldsymbol{u}_i\\boldsymbol{u}^T_i\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2775fb",
   "metadata": {
    "editable": true
   },
   "source": [
    "which is not the same as $\\tilde{y}_{\\mathrm{OLS}}=\\boldsymbol{U}\\boldsymbol{U}^T\\boldsymbol{y}$, which due to the orthogonality of $\\boldsymbol{U}$ would have given us that the model equals the output.\n",
    "\n",
    "It means that the ordinary least square model (with the optimal\n",
    "parameters) $\\boldsymbol{\\tilde{y}}$, corresponds to an orthogonal\n",
    "transformation of the output (or target) vector $\\boldsymbol{y}$ by the\n",
    "vectors of the matrix $\\boldsymbol{U}$. **Note that the summation ends at**\n",
    "$p-1$, that is $\\boldsymbol{\\tilde{y}}\\ne \\boldsymbol{y}$. We can thus not use the\n",
    "orthogonality relation for the matrix $\\boldsymbol{U}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d9099",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Further properties (important for our analyses later)\n",
    "\n",
    "Let us study again $\\boldsymbol{X}^T\\boldsymbol{X}$ in terms of our SVD,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abb202",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}=\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T=\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}\\boldsymbol{V}^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7d81c",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we now multiply from the right with $\\boldsymbol{V}$ (using the orthogonality of $\\boldsymbol{V}$) we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602dd4b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)\\boldsymbol{V}=\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10090c9c",
   "metadata": {
    "editable": true
   },
   "source": [
    "This means the vectors $\\boldsymbol{v}_i$ of the orthogonal matrix $\\boldsymbol{V}$ are the eigenvectors of the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$\n",
    "with eigenvalues given by the singular values squared, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d7ab3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)\\boldsymbol{v}_i=\\boldsymbol{v}_i\\sigma_i^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfe2b49",
   "metadata": {
    "editable": true
   },
   "source": [
    "Similarly, if we use the SVD decomposition for the matrix $\\boldsymbol{X}\\boldsymbol{X}^T$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02423018",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}\\boldsymbol{X}^T=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32097116",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we now multiply from the right with $\\boldsymbol{U}$ (using the orthogonality of $\\boldsymbol{U}$) we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94297730",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\boldsymbol{X}\\boldsymbol{X}^T\\right)\\boldsymbol{U}=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b73af09",
   "metadata": {
    "editable": true
   },
   "source": [
    "This means the vectors $\\boldsymbol{u}_i$ of the orthogonal matrix $\\boldsymbol{U}$ are the eigenvectors of the matrix $\\boldsymbol{X}\\boldsymbol{X}^T$\n",
    "with eigenvalues given by the singular values squared, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d425656a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\boldsymbol{X}\\boldsymbol{X}^T\\right)\\boldsymbol{u}_i=\\boldsymbol{u}_i\\sigma_i^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a90f7",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Important note**: we have defined our design matrix $\\boldsymbol{X}$ to be an\n",
    "$n\\times p$ matrix. In most supervised learning cases we have that $n\n",
    "\\ge p$, and quite often we have $n >> p$. For linear algebra based methods like ordinary least squares or Ridge regression, this leads to a matrix $\\boldsymbol{X}^T\\boldsymbol{X}$ which is small and thereby easier to handle from a computational point of view (in terms of number of floating point operations).\n",
    "\n",
    "In our lectures, the number of columns will\n",
    "always refer to the number of features in our data set, while the\n",
    "number of rows represents the number of data inputs. Note that in\n",
    "other texts you may find the opposite notation. This has consequences\n",
    "for the definition of for example the covariance matrix and its relation to the SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9471d3d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back to Ridge and LASSO Regression\n",
    "\n",
    "Let us remind ourselves about the expression for the standard Mean Squared Error (MSE) which we used to define our cost function and the equations for the ordinary least squares (OLS) method, that is \n",
    "our optimization problem is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91ce22",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in {\\mathbb{R}}^{p}}}\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6b5c3",
   "metadata": {
    "editable": true
   },
   "source": [
    "or we can state it as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855a130",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c481ca",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have used the definition of  a norm-2 vector, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d065c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_2 = \\sqrt{\\sum_i x_i^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaad40a",
   "metadata": {
    "editable": true
   },
   "source": [
    "By minimizing the above equation with respect to the parameters\n",
    "$\\boldsymbol{\\theta}$ we could then obtain an analytical expression for the\n",
    "parameters $\\boldsymbol{\\theta}$.  We can add a regularization parameter $\\lambda$ by\n",
    "defining a new cost function to be optimized, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ad907",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826fa98a",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to the Ridge regression minimization problem where we\n",
    "require that $\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_2^2\\le t$, where $t$ is\n",
    "a finite number larger than zero. By defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c2e34",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\theta})=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0158273",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have a new optimization equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb60bc5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b25f35",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to Lasso regression. Lasso stands for least absolute shrinkage and selection operator. \n",
    "\n",
    "Here we have defined the norm-1 as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db32c09",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_1 = \\sum_i \\vert x_i\\vert.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4551db",
   "metadata": {
    "editable": true
   },
   "source": [
    "Ridge regression, as discussed above,  is nothing but the standard OLS with a\n",
    "modified diagonal term added to $\\boldsymbol{X}^T\\boldsymbol{X}$. The consequences, in\n",
    "particular for our discussion of the bias-variance tradeoff are rather\n",
    "interesting. We will see that for specific values of $\\lambda$, we may\n",
    "even reduce the variance of the optimal parameters $\\boldsymbol{\\theta}$. These topics and other related ones, will be discussed after the more linear algebra oriented analysis here.\n",
    "\n",
    "Using our insights about the SVD of the design matrix $\\boldsymbol{X}$ \n",
    "We have already analyzed the OLS solutions in terms of the eigenvectors (the columns) of the right singular value matrix $\\boldsymbol{U}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7205dc0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}_{\\mathrm{OLS}}=\\boldsymbol{X}\\boldsymbol{\\theta}  =\\boldsymbol{U}\\boldsymbol{U}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66221da9",
   "metadata": {
    "editable": true
   },
   "source": [
    "For Ridge regression this becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade2ffe",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}_{\\mathrm{Ridge}}=\\boldsymbol{X}\\boldsymbol{\\theta}_{\\mathrm{Ridge}} = \\boldsymbol{U\\Sigma V^T}\\left(\\boldsymbol{V}\\boldsymbol{\\Sigma}^2\\boldsymbol{V}^T+\\lambda\\boldsymbol{I} \\right)^{-1}(\\boldsymbol{U\\Sigma V^T})^T\\boldsymbol{y}=\\sum_{j=0}^{p-1}\\boldsymbol{u}_j\\boldsymbol{u}_j^T\\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda}\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e634643",
   "metadata": {
    "editable": true
   },
   "source": [
    "with the vectors $\\boldsymbol{u}_j$ being the columns of $\\boldsymbol{U}$ from the SVD of the matrix $\\boldsymbol{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89fa91",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Interpreting the Ridge results\n",
    "\n",
    "Since $\\lambda \\geq 0$, it means that compared to OLS, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be7934",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda} \\leq 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab8dff",
   "metadata": {
    "editable": true
   },
   "source": [
    "Ridge regression finds the coordinates of $\\boldsymbol{y}$ with respect to the\n",
    "orthonormal basis $\\boldsymbol{U}$, it then shrinks the coordinates by\n",
    "$\\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda}$. Recall that the SVD has\n",
    "eigenvalues ordered in a descending way, that is $\\sigma_i \\geq\n",
    "\\sigma_{i+1}$.\n",
    "\n",
    "For small eigenvalues $\\sigma_i$ it means that their contributions become less important, a fact which can be used to reduce the number of degrees of freedom. More about this when we have covered the material on a statistical interpretation of various linear regression methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fb2f9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More interpretations\n",
    "\n",
    "For the sake of simplicity, let us assume that the design matrix is orthonormal, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5061ce",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}=(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} =\\boldsymbol{I}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cded9",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this case the standard OLS results in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0d841",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta}^{\\mathrm{OLS}} = \\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eda9de",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3197e1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta}^{\\mathrm{Ridge}} = \\left(\\boldsymbol{I}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}=\\left(1+\\lambda\\right)^{-1}\\boldsymbol{\\theta}^{\\mathrm{OLS}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94dab2",
   "metadata": {
    "editable": true
   },
   "source": [
    "that is the Ridge estimator scales the OLS estimator by the inverse of a factor $1+\\lambda$, and\n",
    "the Ridge estimator converges to zero when the hyperparameter goes to\n",
    "infinity.\n",
    "\n",
    "We will come back to more interpretions after we have gone through some of the statistical analysis part. \n",
    "\n",
    "For more discussions of Ridge and Lasso regression, [Wessel van Wieringen's](https://arxiv.org/abs/1509.09169) article is highly recommended.\n",
    "Similarly, [Mehta et al's article](https://arxiv.org/abs/1803.08823) is also recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be31967",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Deriving the  Lasso Regression Equations\n",
    "\n",
    "Using the matrix-vector expression for Lasso regression, we have the following **cost** function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f368f8d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\theta})=\\frac{1}{n}\\left\\{(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})\\right\\}+\\lambda\\vert\\vert\\boldsymbol{\\theta}\\vert\\vert_1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6570585e",
   "metadata": {
    "editable": true
   },
   "source": [
    "Taking the derivative with respect to $\\boldsymbol{\\theta}$ and recalling that the derivative of the absolute value is (we drop the boldfaced vector symbol for simplicty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e63fec",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{d \\vert \\theta\\vert}{d \\theta}=\\mathrm{sgn}(\\theta)=\\left\\{\\begin{array}{cc} 1 & \\theta > 0 \\\\-1 & \\theta < 0, \\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd00b10",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have that the derivative of the cost function is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e3f71",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{X},\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}=-\\frac{2}{n}\\boldsymbol{X}^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})+\\lambda sgn(\\boldsymbol{\\theta})=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e10d67",
   "metadata": {
    "editable": true
   },
   "source": [
    "and reordering we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c52b5c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}+\\frac{n}{2}\\lambda sgn(\\boldsymbol{\\theta})=\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de75936d",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can redefine $\\lambda$ to absorb the constant $n/2$ and we rewrite the last equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb05c6e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}+\\lambda sgn(\\boldsymbol{\\theta})=\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a601145",
   "metadata": {
    "editable": true
   },
   "source": [
    "This equation does not lead to a nice analytical equation as in either Ridge regression or ordinary least squares. This equation can however be solved by using standard convex optimization algorithms.We will discuss how to code the above methods using gradient descent methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c6b5a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Optimization and gradient descent, the central part of any Machine Learning algortithm\n",
    "\n",
    "Almost every problem in machine learning and data science starts with\n",
    "a dataset $X$, a model $g(\\theta)$, which is a function of the\n",
    "parameters $\\theta$ and a cost function $C(X, g(\\theta))$ that allows\n",
    "us to judge how well the model $g(\\theta)$ explains the observations\n",
    "$X$. The model is fit by finding the values of $\\theta$ that minimize\n",
    "the cost function. Ideally we would be able to solve for $\\theta$\n",
    "analytically, however this is not possible in general and we must use\n",
    "some approximative/numerical method to compute the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ab287",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reminder on Newton-Raphson's method\n",
    "\n",
    "Let us quickly remind ourselves how we derive the above method.\n",
    "\n",
    "Perhaps the most celebrated of all one-dimensional root-finding\n",
    "routines is Newton's method, also called the Newton-Raphson\n",
    "method. This method  requires the evaluation of both the\n",
    "function $f$ and its derivative $f'$ at arbitrary points. \n",
    "If you can only calculate the derivative\n",
    "numerically and/or your function is not of the smooth type, we\n",
    "normally discourage the use of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a734b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The equations\n",
    "\n",
    "The Newton-Raphson formula consists geometrically of extending the\n",
    "tangent line at a current point until it crosses zero, then setting\n",
    "the next guess to the abscissa of that zero-crossing.  The mathematics\n",
    "behind this method is rather simple. Employing a Taylor expansion for\n",
    "$x$ sufficiently close to the solution $s$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b539735",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:taylornr\"></div>\n",
    "\n",
    "$$\n",
    "f(s)=0=f(x)+(s-x)f'(x)+\\frac{(s-x)^2}{2}f''(x) +\\dots.\n",
    "    \\label{eq:taylornr} \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a7db18",
   "metadata": {
    "editable": true
   },
   "source": [
    "For small enough values of the function and for well-behaved\n",
    "functions, the terms beyond linear are unimportant, hence we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352ae00",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f(x)+(s-x)f'(x)\\approx 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f738de3",
   "metadata": {
    "editable": true
   },
   "source": [
    "yielding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686752fc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "s\\approx x-\\frac{f(x)}{f'(x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab7f1e9",
   "metadata": {
    "editable": true
   },
   "source": [
    "Having in mind an iterative procedure, it is natural to start iterating with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ca81c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7908f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple geometric interpretation\n",
    "\n",
    "The above is Newton-Raphson's method. It has a simple geometric\n",
    "interpretation, namely $x_{n+1}$ is the point where the tangent from\n",
    "$(x_n,f(x_n))$ crosses the $x$-axis.  Close to the solution,\n",
    "Newton-Raphson converges fast to the desired result. However, if we\n",
    "are far from a root, where the higher-order terms in the series are\n",
    "important, the Newton-Raphson formula can give grossly inaccurate\n",
    "results. For instance, the initial guess for the root might be so far\n",
    "from the true root as to let the search interval include a local\n",
    "maximum or minimum of the function.  If an iteration places a trial\n",
    "guess near such a local extremum, so that the first derivative nearly\n",
    "vanishes, then Newton-Raphson may fail totally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774053a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Extending to more than one variable\n",
    "\n",
    "Newton's method can be generalized to systems of several non-linear equations\n",
    "and variables. Consider the case with two equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4ec75",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{cc} f_1(x_1,x_2) &=0\\\\\n",
    "                     f_2(x_1,x_2) &=0,\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605ba57",
   "metadata": {
    "editable": true
   },
   "source": [
    "which we Taylor expand to obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384fa39",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{cc} 0=f_1(x_1+h_1,x_2+h_2)=&f_1(x_1,x_2)+h_1\n",
    "                     \\partial f_1/\\partial x_1+h_2\n",
    "                     \\partial f_1/\\partial x_2+\\dots\\\\\n",
    "                     0=f_2(x_1+h_1,x_2+h_2)=&f_2(x_1,x_2)+h_1\n",
    "                     \\partial f_2/\\partial x_1+h_2\n",
    "                     \\partial f_2/\\partial x_2+\\dots\n",
    "                       \\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5464e0f",
   "metadata": {
    "editable": true
   },
   "source": [
    "Defining the Jacobian matrix ${\\bf \\boldsymbol{J}}$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3622a2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\bf \\boldsymbol{J}}=\\left( \\begin{array}{cc}\n",
    "                         \\partial f_1/\\partial x_1  & \\partial f_1/\\partial x_2 \\\\\n",
    "                          \\partial f_2/\\partial x_1     &\\partial f_2/\\partial x_2\n",
    "             \\end{array} \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359b337",
   "metadata": {
    "editable": true
   },
   "source": [
    "we can rephrase Newton's method as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba027c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\begin{array}{c} x_1^{n+1} \\\\ x_2^{n+1} \\end{array} \\right)=\n",
    "\\left(\\begin{array}{c} x_1^{n} \\\\ x_2^{n} \\end{array} \\right)+\n",
    "\\left(\\begin{array}{c} h_1^{n} \\\\ h_2^{n} \\end{array} \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1465c5",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62444328",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\begin{array}{c} h_1^{n} \\\\ h_2^{n} \\end{array} \\right)=\n",
    "   -{\\bf \\boldsymbol{J}}^{-1}\n",
    "   \\left(\\begin{array}{c} f_1(x_1^{n},x_2^{n}) \\\\ f_2(x_1^{n},x_2^{n}) \\end{array} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f2190",
   "metadata": {
    "editable": true
   },
   "source": [
    "We need thus to compute the inverse of the Jacobian matrix and it\n",
    "is to understand that difficulties  may\n",
    "arise in case ${\\bf \\boldsymbol{J}}$ is nearly singular.\n",
    "\n",
    "It is rather straightforward to extend the above scheme to systems of\n",
    "more than two non-linear equations. In our case, the Jacobian matrix is given by the Hessian that represents the second derivative of cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b22394",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Steepest descent\n",
    "\n",
    "The basic idea of gradient descent is\n",
    "that a function $F(\\mathbf{x})$, \n",
    "$\\mathbf{x} \\equiv (x_1,\\cdots,x_n)$, decreases fastest if one goes from $\\bf {x}$ in the\n",
    "direction of the negative gradient $-\\nabla F(\\mathbf{x})$.\n",
    "\n",
    "It can be shown that if"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0ea9c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\gamma_k \\nabla F(\\mathbf{x}_k),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c07d7c7",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\gamma_k > 0$.\n",
    "\n",
    "For $\\gamma_k$ small enough, then $F(\\mathbf{x}_{k+1}) \\leq\n",
    "F(\\mathbf{x}_k)$. This means that for a sufficiently small $\\gamma_k$\n",
    "we are always moving towards smaller function values, i.e a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8998c7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on Steepest descent\n",
    "\n",
    "The previous observation is the basis of the method of steepest\n",
    "descent, which is also referred to as just gradient descent (GD). One\n",
    "starts with an initial guess $\\mathbf{x}_0$ for a minimum of $F$ and\n",
    "computes new approximations according to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b19ba",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\gamma_k \\nabla F(\\mathbf{x}_k), \\ \\ k \\geq 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab496e9",
   "metadata": {
    "editable": true
   },
   "source": [
    "The parameter $\\gamma_k$ is often referred to as the step length or\n",
    "the learning rate within the context of Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8ce68",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The ideal\n",
    "\n",
    "Ideally the sequence $\\{\\mathbf{x}_k \\}_{k=0}$ converges to a global\n",
    "minimum of the function $F$. In general we do not know if we are in a\n",
    "global or local minimum. In the special case when $F$ is a convex\n",
    "function, all local minima are also global minima, so in this case\n",
    "gradient descent can converge to the global solution. The advantage of\n",
    "this scheme is that it is conceptually simple and straightforward to\n",
    "implement. However the method in this form has some severe\n",
    "limitations:\n",
    "\n",
    "In machine learing we are often faced with non-convex high dimensional\n",
    "cost functions with many local minima. Since GD is deterministic we\n",
    "will get stuck in a local minimum, if the method converges, unless we\n",
    "have a very good intial guess. This also implies that the scheme is\n",
    "sensitive to the chosen initial condition.\n",
    "\n",
    "Note that the gradient is a function of $\\mathbf{x} =\n",
    "(x_1,\\cdots,x_n)$ which makes it expensive to compute numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe39cb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The sensitiveness of the gradient descent\n",
    "\n",
    "The gradient descent method \n",
    "is sensitive to the choice of learning rate $\\gamma_k$. This is due\n",
    "to the fact that we are only guaranteed that $F(\\mathbf{x}_{k+1}) \\leq\n",
    "F(\\mathbf{x}_k)$ for sufficiently small $\\gamma_k$. The problem is to\n",
    "determine an optimal learning rate. If the learning rate is chosen too\n",
    "small the method will take a long time to converge and if it is too\n",
    "large we can experience erratic behavior.\n",
    "\n",
    "Many of these shortcomings can be alleviated by introducing\n",
    "randomness. One such method is that of Stochastic Gradient Descent\n",
    "(SGD), to be discussed next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea0c64a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Convex functions\n",
    "\n",
    "Ideally we want our cost/loss function to be convex(concave).\n",
    "\n",
    "First we give the definition of a convex set: A set $C$ in\n",
    "$\\mathbb{R}^n$ is said to be convex if, for all $x$ and $y$ in $C$ and\n",
    "all $t \\in (0,1)$ , the point $(1  t)x + ty$ also belongs to\n",
    "C. Geometrically this means that every point on the line segment\n",
    "connecting $x$ and $y$ is in $C$ as discussed below.\n",
    "\n",
    "The convex subsets of $\\mathbb{R}$ are the intervals of\n",
    "$\\mathbb{R}$. Examples of convex sets of $\\mathbb{R}^2$ are the\n",
    "regular polygons (triangles, rectangles, pentagons, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603bdcc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Convex function\n",
    "\n",
    "**Convex function**: Let $X \\subset \\mathbb{R}^n$ be a convex set. Assume that the function $f: X \\rightarrow \\mathbb{R}$ is continuous, then $f$ is said to be convex if $$f(tx_1 + (1-t)x_2) \\leq tf(x_1) + (1-t)f(x_2) $$ for all $x_1, x_2 \\in X$ and for all $t \\in [0,1]$. If $\\leq$ is replaced with a strict inequaltiy in the definition, we demand $x_1 \\neq x_2$ and $t\\in(0,1)$ then $f$ is said to be strictly convex. For a single variable function, convexity means that if you draw a straight line connecting $f(x_1)$ and $f(x_2)$, the value of the function on the interval $[x_1,x_2]$ is always below the line as illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f6dbe",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Conditions on convex functions\n",
    "\n",
    "In the following we state first and second-order conditions which\n",
    "ensures convexity of a function $f$. We write $D_f$ to denote the\n",
    "domain of $f$, i.e the subset of $R^n$ where $f$ is defined. For more\n",
    "details and proofs we refer to: [S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press](http://stanford.edu/boyd/cvxbook/, 2004).\n",
    "\n",
    "**First order condition.**\n",
    "\n",
    "Suppose $f$ is differentiable (i.e $\\nabla f(x)$ is well defined for\n",
    "all $x$ in the domain of $f$). Then $f$ is convex if and only if $D_f$\n",
    "is a convex set and $$f(y) \\geq f(x) + \\nabla f(x)^T (y-x) $$ holds\n",
    "for all $x,y \\in D_f$. This condition means that for a convex function\n",
    "the first order Taylor expansion (right hand side above) at any point\n",
    "a global under estimator of the function. To convince yourself you can\n",
    "make a drawing of $f(x) = x^2+1$ and draw the tangent line to $f(x)$ and\n",
    "note that it is always below the graph.\n",
    "\n",
    "**Second order condition.**\n",
    "\n",
    "Assume that $f$ is twice\n",
    "differentiable, i.e the Hessian matrix exists at each point in\n",
    "$D_f$. Then $f$ is convex if and only if $D_f$ is a convex set and its\n",
    "Hessian is positive semi-definite for all $x\\in D_f$. For a\n",
    "single-variable function this reduces to $f''(x) \\geq 0$. Geometrically this means that $f$ has nonnegative curvature\n",
    "everywhere.\n",
    "\n",
    "This condition is particularly useful since it gives us an procedure for determining if the function under consideration is convex, apart from using the definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a67e20",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on convex functions\n",
    "\n",
    "The next result is of great importance to us and the reason why we are\n",
    "going on about convex functions. In machine learning we frequently\n",
    "have to minimize a loss/cost function in order to find the best\n",
    "parameters for the model we are considering. \n",
    "\n",
    "Ideally we want the\n",
    "global minimum (for high-dimensional models it is hard to know\n",
    "if we have local or global minimum). However, if the cost/loss function\n",
    "is convex the following result provides invaluable information:\n",
    "\n",
    "**Any minimum is global for convex functions.**\n",
    "\n",
    "Consider the problem of finding $x \\in \\mathbb{R}^n$ such that $f(x)$\n",
    "is minimal, where $f$ is convex and differentiable. Then, any point\n",
    "$x^*$ that satisfies $\\nabla f(x^*) = 0$ is a global minimum.\n",
    "\n",
    "This result means that if we know that the cost/loss function is convex and we are able to find a minimum, we are guaranteed that it is a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa47103",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Some simple problems\n",
    "\n",
    "1. Show that $f(x)=x^2$ is convex for $x \\in \\mathbb{R}$ using the definition of convexity. Hint: If you re-write the definition, $f$ is convex if the following holds for all $x,y \\in D_f$ and any $\\lambda \\in [0,1]$ $\\lambda f(x)+(1-\\lambda)f(y)-f(\\lambda x + (1-\\lambda) y ) \\geq 0$.\n",
    "\n",
    "2. Using the second order condition show that the following functions are convex on the specified domain.\n",
    "\n",
    " * $f(x) = e^x$ is convex for $x \\in \\mathbb{R}$.\n",
    "\n",
    " * $g(x) = -\\ln(x)$ is convex for $x \\in (0,\\infty)$.\n",
    "\n",
    "3. Let $f(x) = x^2$ and $g(x) = e^x$. Show that $f(g(x))$ and $g(f(x))$ is convex for $x \\in \\mathbb{R}$. Also show that if $f(x)$ is any convex function than $h(x) = e^{f(x)}$ is convex.\n",
    "\n",
    "4. A norm is any function that satisfy the following properties\n",
    "\n",
    " * $f(\\alpha x) = |\\alpha| f(x)$ for all $\\alpha \\in \\mathbb{R}$.\n",
    "\n",
    " * $f(x+y) \\leq f(x) + f(y)$\n",
    "\n",
    " * $f(x) \\leq 0$ for all $x \\in \\mathbb{R}^n$ with equality if and only if $x = 0$\n",
    "\n",
    "Using the definition of convexity, try to show that a function satisfying the properties above is convex (the third condition is not needed to show this)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1085ea4b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Revisiting Ordinary Least Squares\n",
    "\n",
    "We will use linear regression as a case study for the gradient descent\n",
    "methods. Linear regression is a great test case for the gradient\n",
    "descent methods discussed in the lectures since it has several\n",
    "desirable properties such as:\n",
    "\n",
    "1. An analytical solution (recall homework sets for week 35).\n",
    "\n",
    "2. The gradient can be computed analytically.\n",
    "\n",
    "3. The cost function is convex which guarantees that gradient descent converges for small enough learning rates\n",
    "\n",
    "We revisit an example similar to what we had in the first homework set. We had a function  of the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6444430a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = 2*np.random.rand(m,1)\n",
    "y = 4+3*x+np.random.randn(m,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d03d1",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $x_i \\in [0,1] $ is chosen randomly using a uniform distribution. Additionally we have a stochastic noise chosen according to a normal distribution $\\cal {N}(0,1)$. \n",
    "The linear regression model is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6079d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "h_\\theta(x) = \\boldsymbol{y} = \\theta_0 + \\theta_1 x,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce537b82",
   "metadata": {
    "editable": true
   },
   "source": [
    "such that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9a455",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{y}_i = \\theta_0 + \\theta_1 x_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3b01b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradient descent example\n",
    "\n",
    "Let $\\mathbf{y} = (y_1,\\cdots,y_n)^T$, $\\mathbf{\\boldsymbol{y}} = (\\boldsymbol{y}_1,\\cdots,\\boldsymbol{y}_n)^T$ and $\\theta = (\\theta_0, \\theta_1)^T$\n",
    "\n",
    "It is convenient to write $\\mathbf{\\boldsymbol{y}} = X\\theta$ where $X \\in \\mathbb{R}^{100 \\times 2} $ is the design matrix given by (we keep the intercept here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4cb92",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "X \\equiv \\begin{bmatrix}\n",
    "1 & x_1  \\\\\n",
    "\\vdots & \\vdots  \\\\\n",
    "1 & x_{100} &  \\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f351fff7",
   "metadata": {
    "editable": true
   },
   "source": [
    "The cost/loss/risk function is given by ("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293bcabf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\theta) = \\frac{1}{n}||X\\theta-\\mathbf{y}||_{2}^{2} = \\frac{1}{n}\\sum_{i=1}^{100}\\left[ (\\theta_0 + \\theta_1 x_i)^2 - 2 y_i (\\theta_0 + \\theta_1 x_i) + y_i^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664bd53e",
   "metadata": {
    "editable": true
   },
   "source": [
    "and we want to find $\\theta$ such that $C(\\theta)$ is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d03b9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The derivative of the cost/loss function\n",
    "\n",
    "Computing $\\partial C(\\theta) / \\partial \\theta_0$ and $\\partial C(\\theta) / \\partial \\theta_1$ we can show  that the gradient can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51408adf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\theta} C(\\theta) = \\frac{2}{n}\\begin{bmatrix} \\sum_{i=1}^{100} \\left(\\theta_0+\\theta_1x_i-y_i\\right) \\\\\n",
    "\\sum_{i=1}^{100}\\left( x_i (\\theta_0+\\theta_1x_i)-y_ix_i\\right) \\\\\n",
    "\\end{bmatrix} = \\frac{2}{n}X^T(X\\theta - \\mathbf{y}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4e8dd",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $X$ is the design matrix defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79fa8f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Hessian matrix\n",
    "The Hessian matrix of $C(\\theta)$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b134ed",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{H} \\equiv \\begin{bmatrix}\n",
    "\\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0^2} & \\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0 \\partial \\theta_1}  \\\\\n",
    "\\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0 \\partial \\theta_1} & \\frac{\\partial^2 C(\\theta)}{\\partial \\theta_1^2} &  \\\\\n",
    "\\end{bmatrix} = \\frac{2}{n}X^T X.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b5bfa",
   "metadata": {
    "editable": true
   },
   "source": [
    "This result implies that $C(\\theta)$ is a convex function since the matrix $X^T X$ always is positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fce0e1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple program\n",
    "\n",
    "We can now write a program that minimizes $C(\\theta)$ using the gradient descent method with a constant learning rate $\\gamma$ according to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfabff8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\gamma \\nabla_\\theta C(\\theta_k), \\ k=0,1,\\cdots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47704d64",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can use the expression we computed for the gradient and let use a\n",
    "$\\theta_0$ be chosen randomly and let $\\gamma = 0.001$. Stop iterating\n",
    "when $||\\nabla_\\theta C(\\theta_k) || \\leq \\epsilon = 10^{-8}$. **Note that the code below does not include the latter stop criterion**.\n",
    "\n",
    "And finally we can compare our solution for $\\theta$ with the analytic result given by \n",
    "$\\theta= (X^TX)^{-1} X^T \\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8f118",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradient Descent Example\n",
    "\n",
    "Here our simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad7c5f2",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Importing various packages\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "\n",
    "# the number of datapoints\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* X.T @ X\n",
    "# Get the eigenvalues\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta_linreg = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(theta_linreg)\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradient = (2.0/n)*X.T @ (X @ theta-y)\n",
    "    theta -= eta*gradient\n",
    "\n",
    "print(theta)\n",
    "xnew = np.array([[0],[2]])\n",
    "xbnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = xbnew.dot(theta)\n",
    "ypredict2 = xbnew.dot(theta_linreg)\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Gradient descent example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ef49d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradient descent and Ridge\n",
    "\n",
    "We have also discussed Ridge regression where the loss function contains a regularized term given by the $L_2$ norm of $\\theta$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04b3a5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C_{\\text{ridge}}(\\theta) = \\frac{1}{n}||X\\theta -\\mathbf{y}||^2 + \\lambda ||\\theta||^2, \\ \\lambda \\geq 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befc6eb",
   "metadata": {
    "editable": true
   },
   "source": [
    "In order to minimize $C_{\\text{ridge}}(\\theta)$ using GD we adjust the gradient as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4df04",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_\\theta C_{\\text{ridge}}(\\theta)  = \\frac{2}{n}\\begin{bmatrix} \\sum_{i=1}^{100} \\left(\\theta_0+\\theta_1x_i-y_i\\right) \\\\\n",
    "\\sum_{i=1}^{100}\\left( x_i (\\theta_0+\\theta_1x_i)-y_ix_i\\right) \\\\\n",
    "\\end{bmatrix} + 2\\lambda\\begin{bmatrix} \\theta_0 \\\\ \\theta_1\\end{bmatrix} = 2 (\\frac{1}{n}X^T(X\\theta - \\mathbf{y})+\\lambda \\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b49efe3",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can easily extend our program to minimize $C_{\\text{ridge}}(\\theta)$ using gradient descent and compare with the analytical solution given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62c69b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{\\text{ridge}} = \\left(X^T X + n\\lambda I_{2 \\times 2} \\right)^{-1} X^T \\mathbf{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee2714",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Hessian matrix for Ridge Regression\n",
    "The Hessian matrix of Ridge Regression for our simple example  is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a4532",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{H} \\equiv \\begin{bmatrix}\n",
    "\\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0^2} & \\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0 \\partial \\theta_1}  \\\\\n",
    "\\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0 \\partial \\theta_1} & \\frac{\\partial^2 C(\\theta)}{\\partial \\theta_1^2} &  \\\\\n",
    "\\end{bmatrix} = \\frac{2}{n}X^T X+2\\lambda\\boldsymbol{I}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e5654",
   "metadata": {
    "editable": true
   },
   "source": [
    "This implies that the Hessian matrix  is positive definite, hence the stationary point is a\n",
    "minimum.\n",
    "Note that the Ridge cost function is convex being  a sum of two convex\n",
    "functions. Therefore, the stationary point is a global\n",
    "minimum of this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d420e0ec",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Program example for gradient descent with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200aa258",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "\n",
    "# the number of datapoints\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "\n",
    "#Ridge parameter lambda\n",
    "lmbda  = 0.001\n",
    "Id = n*lmbda* np.eye(XT_X.shape[0])\n",
    "\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X+2*lmbda* np.eye(XT_X.shape[0])\n",
    "# Get the eigenvalues\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "\n",
    "theta_linreg = np.linalg.inv(XT_X+Id) @ X.T @ y\n",
    "print(theta_linreg)\n",
    "# Start plain gradient descent\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 100\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = 2.0/n*X.T @ (X @ (theta)-y)+2*lmbda*theta\n",
    "    theta -= eta*gradients\n",
    "\n",
    "print(theta)\n",
    "ypredict = X @ theta\n",
    "ypredict2 = X @ theta_linreg\n",
    "plt.plot(x, ypredict, \"r-\")\n",
    "plt.plot(x, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Gradient descent example for Ridge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bb84c3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using gradient descent methods, limitations\n",
    "\n",
    "* **Gradient descent (GD) finds local minima of our function**. Since the GD algorithm is deterministic, if it converges, it will converge to a local minimum of our cost/loss/risk function. Because in ML we are often dealing with extremely rugged landscapes with many local minima, this can lead to poor performance.\n",
    "\n",
    "* **GD is sensitive to initial conditions**. One consequence of the local nature of GD is that initial conditions matter. Depending on where one starts, one will end up at a different local minima. Therefore, it is very important to think about how one initializes the training process. This is true for GD as well as more complicated variants of GD.\n",
    "\n",
    "* **Gradients are computationally expensive to calculate for large datasets**. In many cases in statistics and ML, the cost/loss/risk function is a sum of terms, with one term for each data point. For example, in linear regression, $E \\propto \\sum_{i=1}^n (y_i - \\mathbf{w}^T\\cdot\\mathbf{x}_i)^2$; for logistic regression, the square error is replaced by the cross entropy. To calculate the gradient we have to sum over *all* $n$ data points. Doing this at every GD step becomes extremely computationally expensive. An ingenious solution to this, is to calculate the gradients using small subsets of the data called \"mini batches\". This has the added benefit of introducing stochasticity into our algorithm.\n",
    "\n",
    "* **GD is very sensitive to choices of learning rates**. GD is extremely sensitive to the choice of learning rates. If the learning rate is very small, the training process take an extremely long time. For larger learning rates, GD can diverge and give poor results. Furthermore, depending on what the local landscape looks like, we have to modify the learning rates to ensure convergence. Ideally, we would *adaptively* choose the learning rates to match the landscape.\n",
    "\n",
    "* **GD treats all directions in parameter space uniformly.** Another major drawback of GD is that unlike Newton's method, the learning rate for GD is the same in all directions in parameter space. For this reason, the maximum learning rate is set by the behavior of the steepest direction and this can significantly slow down training. Ideally, we would like to take large steps in flat directions and small steps in steep directions. Since we are exploring rugged landscapes where curvatures change, this requires us to keep track of not only the gradient but second derivatives. The ideal scenario would be to calculate the Hessian but this proves to be too computationally expensive. \n",
    "\n",
    "* GD can take exponential time to escape saddle points, even with random initialization. As we mentioned, GD is extremely sensitive to initial condition since it determines the particular local minimum GD would eventually reach. However, even with a good initialization scheme, through the introduction of randomness, GD can still take exponential time to escape saddle points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bea27c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Material for lab sessions  sessions Tuesday and Wednesday\n",
    "\n",
    "The material here contains a summary of the lecture on Monday and discussion of SVD, Ridge and Lasso regression with examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f578797",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear Regression and  the SVD\n",
    "\n",
    "We used the SVD to analyse the matrix to invert in ordinary lineat regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e09668",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}=\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{U}^T\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T=\\boldsymbol{V}\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma}\\boldsymbol{V}^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99fd28",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since the matrices here have dimension $p\\times p$, with $p$ corresponding to the singular values, we defined last week the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944106f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\Sigma}^T\\boldsymbol{\\Sigma} = \\begin{bmatrix} \\tilde{\\boldsymbol{\\Sigma}} & \\boldsymbol{0}\\\\ \\end{bmatrix}\\begin{bmatrix} \\tilde{\\boldsymbol{\\Sigma}} \\\\ \\boldsymbol{0}\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3108c",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the tilde-matrix $\\tilde{\\boldsymbol{\\Sigma}}$ is a matrix of dimension $p\\times p$ containing only the singular values $\\sigma_i$, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf6f44",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{\\Sigma}}=\\begin{bmatrix} \\sigma_0 & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "                                    0 & \\sigma_1 & 0 & \\dots & 0 & 0 \\\\\n",
    "\t\t\t\t    0 & 0 & \\sigma_2 & \\dots & 0 & 0 \\\\\n",
    "\t\t\t\t    0 & 0 & 0 & \\dots & \\sigma_{p-2} & 0 \\\\\n",
    "\t\t\t\t    0 & 0 & 0 & \\dots & 0 & \\sigma_{p-1} \\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3a9f2",
   "metadata": {
    "editable": true
   },
   "source": [
    "meaning we can write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcc57e6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}=\\boldsymbol{V}\\tilde{\\boldsymbol{\\Sigma}}^2\\boldsymbol{V}^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27385198",
   "metadata": {
    "editable": true
   },
   "source": [
    "Multiplying from the right with $\\boldsymbol{V}$ (using the orthogonality of $\\boldsymbol{V}$) we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec3a57",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)\\boldsymbol{V}=\\boldsymbol{V}\\tilde{\\boldsymbol{\\Sigma}}^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3980c65",
   "metadata": {
    "editable": true
   },
   "source": [
    "## What does it mean?\n",
    "\n",
    "This means the vectors $\\boldsymbol{v}_i$ of the orthogonal matrix $\\boldsymbol{V}$\n",
    "are the eigenvectors of the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$ with eigenvalues\n",
    "given by the singular values squared, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e3b57",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)\\boldsymbol{v}_i=\\boldsymbol{v}_i\\sigma_i^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6ba0f",
   "metadata": {
    "editable": true
   },
   "source": [
    "In other words, each non-zero singular value of $\\boldsymbol{X}$ is a positive\n",
    "square root of an eigenvalue of $\\boldsymbol{X}^T\\boldsymbol{X}$.  It means also that\n",
    "the columns of $\\boldsymbol{V}$ are the eigenvectors of\n",
    "$\\boldsymbol{X}^T\\boldsymbol{X}$. Since we have ordered the singular values of\n",
    "$\\boldsymbol{X}$ in a descending order, it means that the column vectors\n",
    "$\\boldsymbol{v}_i$ are hierarchically ordered by how much correlation they\n",
    "encode from the columns of $\\boldsymbol{X}$. \n",
    "\n",
    "Note that these are also the eigenvectors and eigenvalues of the\n",
    "Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea004e89",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Ridge and LASSO Regression\n",
    "\n",
    "Let us remind ourselves about the expression for the standard Mean Squared Error (MSE) which we used to define our cost function and the equations for the ordinary least squares (OLS) method, that is \n",
    "our optimization problem is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6ca82",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in {\\mathbb{R}}^{p}}}\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ddb9a",
   "metadata": {
    "editable": true
   },
   "source": [
    "or we can state it as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1341a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a133b",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have used the definition of  a norm-2 vector, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c03f9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_2 = \\sqrt{\\sum_i x_i^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23d4f2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## From OLS to Ridge and Lasso\n",
    "\n",
    "By minimizing the above equation with respect to the parameters\n",
    "$\\boldsymbol{\\theta}$ we could then obtain an analytical expression for the\n",
    "parameters $\\boldsymbol{\\theta}$.  We can add a regularization parameter $\\lambda$ by\n",
    "defining a new cost function to be optimized, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f80a3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23b37f",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to the Ridge regression minimization problem where we\n",
    "require that $\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_2^2\\le t$, where $t$ is\n",
    "a finite number larger than zero. We do not include such a constraints in the discussions here.\n",
    "\n",
    "By defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b19e9d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\theta})=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c47b3",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have a new optimization equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c9b0bc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\theta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0133c8b9",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to Lasso regression. Lasso stands for least absolute shrinkage and selection operator. \n",
    "\n",
    "Here we have defined the norm-1 as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e8c08",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_1 = \\sum_i \\vert x_i\\vert.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf1ed6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Deriving the  Ridge Regression Equations\n",
    "\n",
    "Using the matrix-vector expression for Ridge regression and dropping the parameter $1/n$ in front of the standard means squared error equation, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e83da",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\theta})=\\left\\{(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})\\right\\}+\\lambda\\boldsymbol{\\theta}^T\\boldsymbol{\\theta},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3bad9",
   "metadata": {
    "editable": true
   },
   "source": [
    "and \n",
    "taking the derivatives with respect to $\\boldsymbol{\\theta}$ we obtain then\n",
    "a slightly modified matrix inversion problem which for finite values\n",
    "of $\\lambda$ does not suffer from singularity problems. We obtain\n",
    "the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f851f7b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}_{\\mathrm{Ridge}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff385b9b",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\boldsymbol{I}$ being a $p\\times p$ identity matrix with the constraint that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996e96e0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sum_{i=0}^{p-1} \\theta_i^2 \\leq t,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028b535",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $t$ a finite positive number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a7e86",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Note on Scikit-Learn\n",
    "\n",
    "Note well that a library like **Scikit-Learn** does not include the $1/n$ factor in the expression for the mean-squared error. If you include it, the optimal parameter $\\theta$ becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939185a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}_{\\mathrm{Ridge}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}+n\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056b54e",
   "metadata": {
    "editable": true
   },
   "source": [
    "In our codes where we compare our own codes with **Scikit-Learn**, we do thus not include the $1/n$ factor in the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5ab02",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Comparison with OLS\n",
    "When we compare this with the ordinary least squares result we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773f651",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}_{\\mathrm{OLS}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6238f",
   "metadata": {
    "editable": true
   },
   "source": [
    "which can lead to singular matrices. However, with the SVD, we can always compute the inverse of the matrix $\\boldsymbol{X}^T\\boldsymbol{X}$.\n",
    "\n",
    "We see that Ridge regression is nothing but the standard OLS with a\n",
    "modified diagonal term added to $\\boldsymbol{X}^T\\boldsymbol{X}$. The consequences, in\n",
    "particular for our discussion of the bias-variance tradeoff are rather\n",
    "interesting. We will see that for specific values of $\\lambda$, we may\n",
    "even reduce the variance of the optimal parameters $\\boldsymbol{\\theta}$. These topics and other related ones, will be discussed after the more linear algebra oriented analysis here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fbae84",
   "metadata": {
    "editable": true
   },
   "source": [
    "## SVD analysis\n",
    "\n",
    "Using our insights about the SVD of the design matrix $\\boldsymbol{X}$ \n",
    "We have already analyzed the OLS solutions in terms of the eigenvectors (the columns) of the right singular value matrix $\\boldsymbol{U}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41f73a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}_{\\mathrm{OLS}}=\\boldsymbol{X}\\boldsymbol{\\theta}  =\\boldsymbol{U}\\boldsymbol{U}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0548c40",
   "metadata": {
    "editable": true
   },
   "source": [
    "For Ridge regression this becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8afc2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}_{\\mathrm{Ridge}}=\\boldsymbol{X}\\boldsymbol{\\theta}_{\\mathrm{Ridge}} = \\boldsymbol{U\\Sigma V^T}\\left(\\boldsymbol{V}\\boldsymbol{\\Sigma}^2\\boldsymbol{V}^T+\\lambda\\boldsymbol{I} \\right)^{-1}(\\boldsymbol{U\\Sigma V^T})^T\\boldsymbol{y}=\\sum_{j=0}^{p-1}\\boldsymbol{u}_j\\boldsymbol{u}_j^T\\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda}\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48c74d",
   "metadata": {
    "editable": true
   },
   "source": [
    "with the vectors $\\boldsymbol{u}_j$ being the columns of $\\boldsymbol{U}$ from the SVD of the matrix $\\boldsymbol{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbe639",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Interpreting the Ridge results\n",
    "\n",
    "Since $\\lambda \\geq 0$, it means that compared to OLS, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900f79a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda} \\leq 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0eca1",
   "metadata": {
    "editable": true
   },
   "source": [
    "Ridge regression finds the coordinates of $\\boldsymbol{y}$ with respect to the\n",
    "orthonormal basis $\\boldsymbol{U}$, it then shrinks the coordinates by\n",
    "$\\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda}$. Recall that the SVD has\n",
    "eigenvalues ordered in a descending way, that is $\\sigma_i \\geq\n",
    "\\sigma_{i+1}$.\n",
    "\n",
    "For small eigenvalues $\\sigma_i$ it means that their contributions become less important, a fact which can be used to reduce the number of degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d31204",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More interpretations\n",
    "\n",
    "For the sake of simplicity, let us assume that the design matrix is orthonormal, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1270f73f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}=(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} =\\boldsymbol{I}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279dcbe",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this case the standard OLS results in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab346223",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta}^{\\mathrm{OLS}} = \\boldsymbol{X}^T\\boldsymbol{y}=\\sum_{i=0}^{n-1}\\boldsymbol{u}_i\\boldsymbol{u}_i^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c314e4",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da873ac",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta}^{\\mathrm{Ridge}} = \\left(\\boldsymbol{I}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}=\\left(1+\\lambda\\right)^{-1}\\boldsymbol{\\theta}^{\\mathrm{OLS}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a38ddd3",
   "metadata": {
    "editable": true
   },
   "source": [
    "that is the Ridge estimator scales the OLS estimator by the inverse of a factor $1+\\lambda$, and\n",
    "the Ridge estimator converges to zero when the hyperparameter goes to\n",
    "infinity.\n",
    "\n",
    "We will come back to more interpreations after we have gone through some of the statistical analysis part. \n",
    "\n",
    "For more discussions of Ridge and Lasso regression, [Wessel van Wieringen's](https://arxiv.org/abs/1509.09169) article is highly recommended.\n",
    "Similarly, [Mehta et al's article](https://arxiv.org/abs/1803.08823) is also recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c06ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Deriving the  Lasso Regression Equations\n",
    "\n",
    "Using the matrix-vector expression for Lasso regression, we have the following **cost** function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ef5a1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\theta})=\\frac{1}{n}\\left\\{(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})\\right\\}+\\lambda\\vert\\vert\\boldsymbol{\\theta}\\vert\\vert_1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99f764",
   "metadata": {
    "editable": true
   },
   "source": [
    "Taking the derivative with respect to $\\boldsymbol{\\theta}$ and recalling that the derivative of the absolute value is (we drop the boldfaced vector symbol for simplicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13fc69",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{d \\vert \\theta\\vert}{d \\theta}=\\mathrm{sgn}(\\theta)=\\left\\{\\begin{array}{cc} 1 & \\theta > 0 \\\\-1 & \\theta < 0, \\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d61677",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have that the derivative of the cost function is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b6519",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{X},\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}=-\\frac{2}{n}\\boldsymbol{X}^T(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta})+\\lambda sgn(\\boldsymbol{\\theta})=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e977149",
   "metadata": {
    "editable": true
   },
   "source": [
    "and reordering we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef4c85",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}+\\lambda sgn(\\boldsymbol{\\theta})=\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49241ef3",
   "metadata": {
    "editable": true
   },
   "source": [
    "This equation does not lead to a nice analytical equation as in Ridge regression or ordinary least squares. We have absorbed the factor $2/n$ in a redefinition of the parameter $\\lambda$. We will solve this type of problems using libraries like **scikit-learn** and using our own gradient descent code in project 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d639a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple example to illustrate Ordinary Least Squares, Ridge and Lasso Regression\n",
    "\n",
    "Let us assume that our design matrix is given by unit (identity) matrix, that is a square diagonal matrix with ones only along the\n",
    "diagonal. In this case we have an equal number of rows and columns $n=p$.\n",
    "\n",
    "Our model approximation is just $\\tilde{\\boldsymbol{y}}=\\boldsymbol{\\theta}$ and the mean squared error and thereby the cost function for ordinary least sqquares (OLS) is then (we drop the term $1/n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c494942",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\theta})=\\sum_{i=0}^{p-1}(y_i-\\theta_i)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82836338",
   "metadata": {
    "editable": true
   },
   "source": [
    "and minimizing we have that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c40c0b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\theta}_i^{\\mathrm{OLS}} = y_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702262e3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Ridge Regression\n",
    "\n",
    "For Ridge regression our cost function is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a0e11",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\theta})=\\sum_{i=0}^{p-1}(y_i-\\theta_i)^2+\\lambda\\sum_{i=0}^{p-1}\\theta_i^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb91962a",
   "metadata": {
    "editable": true
   },
   "source": [
    "and minimizing we have that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ed7e7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\theta}_i^{\\mathrm{Ridge}} = \\frac{y_i}{1+\\lambda}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c22ae1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Lasso Regression\n",
    "\n",
    "For Lasso regression our cost function is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53383f33",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\theta})=\\sum_{i=0}^{p-1}(y_i-\\theta_i)^2+\\lambda\\sum_{i=0}^{p-1}\\vert\\theta_i\\vert=\\sum_{i=0}^{p-1}(y_i-\\theta_i)^2+\\lambda\\sum_{i=0}^{p-1}\\sqrt{\\theta_i^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd57e86",
   "metadata": {
    "editable": true
   },
   "source": [
    "and minimizing we have that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d21d0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-2\\sum_{i=0}^{p-1}(y_i-\\theta_i)+\\lambda \\sum_{i=0}^{p-1}\\frac{(\\theta_i)}{\\vert\\theta_i\\vert}=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d93b9f",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa67eee",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}_i^{\\mathrm{Lasso}} = \\left\\{\\begin{array}{ccc}y_i-\\frac{\\lambda}{2} &\\mathrm{if} & y_i> \\frac{\\lambda}{2}\\\\\n",
    "                                                          y_i+\\frac{\\lambda}{2} &\\mathrm{if} & y_i< -\\frac{\\lambda}{2}\\\\\n",
    "\t\t\t\t\t\t\t  0 &\\mathrm{if} & \\vert y_i\\vert\\le  \\frac{\\lambda}{2}\\end{array}\\right.\\\\.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552c054",
   "metadata": {
    "editable": true
   },
   "source": [
    "Plotting these results shows clearly that Lasso regression suppresses (sets to zero) values of $\\theta_i$ for specific values of $\\lambda$. Ridge regression reduces on the other hand the values of $\\theta_i$ as function of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781fad91",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Yet another Example\n",
    "\n",
    "Let us assume we have a data set with outputs/targets given by the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a2d7f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{y}=\\begin{bmatrix}4 \\\\ 2 \\\\3\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8932631e",
   "metadata": {
    "editable": true
   },
   "source": [
    "and our inputs as a $3\\times 2$ design matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875fd35c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\begin{bmatrix}2 & 0\\\\ 0 & 1 \\\\ 0 & 0\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ac90e",
   "metadata": {
    "editable": true
   },
   "source": [
    "meaning that we have two features and two unknown parameters $\\theta_0$ and $\\theta_1$ to be determined either by ordinary least squares, Ridge or Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab8556",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The OLS case\n",
    "\n",
    "For ordinary least squares (OLS) we know that the optimal solution is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddfe96",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}^{\\mathrm{OLS}}=\\left( \\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d299afd4",
   "metadata": {
    "editable": true
   },
   "source": [
    "Inserting the above values we obtain that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ccb67c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}^{\\mathrm{OLS}}=\\begin{bmatrix}2 \\\\ 2\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aae5a8",
   "metadata": {
    "editable": true
   },
   "source": [
    "The code which implements this simpler case is presented after the discussion of Ridge and Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c056e312",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Ridge case\n",
    "\n",
    "For Ridge regression we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1aebee",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}^{\\mathrm{Ridge}}=\\left( \\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c13c4",
   "metadata": {
    "editable": true
   },
   "source": [
    "Inserting the above values we obtain that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02861f1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}^{\\mathrm{Ridge}}=\\begin{bmatrix}\\frac{8}{4+\\lambda} \\\\ \\frac{2}{1+\\lambda}\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f958c",
   "metadata": {
    "editable": true
   },
   "source": [
    "There is normally a constraint on the value of $\\vert\\vert \\boldsymbol{\\theta}\\vert\\vert_2$ via the parameter $\\lambda$.\n",
    "Let us for simplicity assume that $\\theta_0^2+\\theta_1^2=1$ as constraint. This will allow us to find an expression for the optimal values of $\\theta$ and $\\lambda$.\n",
    "\n",
    "To see this, let us write the cost function for Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47c40d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Writing the Cost Function\n",
    "\n",
    "We define the MSE without the $1/n$ factor and have then, using that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893e293",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}\\boldsymbol{\\theta}=\\begin{bmatrix} 2\\theta_0 \\\\ \\theta_1 \\\\0 \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc59387",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\theta})=(4-2\\theta_0)^2+(2-\\theta_1)^2+\\lambda(\\theta_0^2+\\theta_1^2),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9834c1a9",
   "metadata": {
    "editable": true
   },
   "source": [
    "and taking the derivative with respect to $\\theta_0$ we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23231256",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_0=\\frac{8}{4+\\lambda},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a7b7c",
   "metadata": {
    "editable": true
   },
   "source": [
    "and for $\\theta_1$ we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb51472",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_1=\\frac{2}{1+\\lambda},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eebcc5b",
   "metadata": {
    "editable": true
   },
   "source": [
    "Using the constraint for $\\theta_0^2+\\theta_1^2=1$ we can constrain $\\lambda$ by solving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e88346",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\frac{8}{4+\\lambda}\\right)^2+\\left(\\frac{2}{1+\\lambda}\\right)^2=1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283d4efc",
   "metadata": {
    "editable": true
   },
   "source": [
    "which gives $\\lambda=4.571$ and $\\theta_0=0.933$ and $\\theta_1=0.359$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc385baf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Lasso case\n",
    "\n",
    "For Lasso we need now, keeping a  constraint on $\\vert\\theta_0\\vert+\\vert\\theta_1\\vert=1$,  to take the derivative of the absolute values of $\\theta_0$\n",
    "and $\\theta_1$. This gives us the following derivatives of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10cc50e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\theta})=(4-2\\theta_0)^2+(2-\\theta_1)^2+\\lambda(\\vert\\theta_0\\vert+\\vert\\theta_1\\vert),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08faccc9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{\\theta})}{\\partial \\theta_0}=-4(4-2\\theta_0)+\\lambda\\mathrm{sgn}(\\theta_0)=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27e27b",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ccf28",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C(\\boldsymbol{\\theta})}{\\partial \\theta_1}=-2(2-\\theta_1)+\\lambda\\mathrm{sgn}(\\theta_1)=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62c569",
   "metadata": {
    "editable": true
   },
   "source": [
    "We have now four cases to solve besides the trivial cases $\\theta_0$ and/or $\\theta_1$ are zero, namely\n",
    "1. $\\theta_0 > 0$ and $\\theta_1 > 0$,\n",
    "\n",
    "2. $\\theta_0 > 0$ and $\\theta_1 < 0$,\n",
    "\n",
    "3. $\\theta_0 < 0$ and $\\theta_1 > 0$,\n",
    "\n",
    "4. $\\theta_0 < 0$ and $\\theta_1 < 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ca67f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The first Case\n",
    "\n",
    "If we consider the first case, we have then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c49ff",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-4(4-2\\theta_0)+\\lambda=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a9392",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f266e2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-2(2-\\theta_1)+\\lambda=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c520b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "which yields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30040418",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_0=\\frac{16+\\lambda}{8},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8df188",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190ff77",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_1=\\frac{4+\\lambda}{2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca82691",
   "metadata": {
    "editable": true
   },
   "source": [
    "Using the constraint on $\\theta_0$ and $\\theta_1$ we can then find the optimal value of $\\lambda$ for the different cases. We leave this as an exercise to you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac38601",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple code for solving the above problem\n",
    "\n",
    "Here we set up the OLS, Ridge and Lasso functionality in order to study the above example. Note that here we have opted for a set of values of $\\lambda$, meaning that we need to perform a search in order to find the optimal values.\n",
    "\n",
    "First we study and compare the OLS and Ridge results.  The next code compares all three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca4918a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "\n",
    "# A seed just to ensure that the random numbers are the same for every run.\n",
    "# Useful for eventual debugging.\n",
    "\n",
    "X = np.array( [ [ 2, 0], [0, 1], [0,0]])\n",
    "y = np.array( [4, 2, 3])\n",
    "\n",
    "\n",
    "# matrix inversion to find beta\n",
    "OLSbeta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(OLSbeta)\n",
    "# and then make the prediction\n",
    "ytildeOLS = X @ OLSbeta\n",
    "print(\"Training MSE for OLS\")\n",
    "print(MSE(y,ytildeOLS))\n",
    "ypredictOLS = X @ OLSbeta\n",
    "\n",
    "# Repeat now for Ridge regression and various values of the regularization parameter\n",
    "I = np.eye(2,2)\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 100\n",
    "MSEPredict = np.zeros(nlambdas)\n",
    "lambdas = np.logspace(-4, 4, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "    Ridgebeta = np.linalg.inv(X.T @ X+lmb*I) @ X.T @ y\n",
    "#    print(Ridgebeta)\n",
    "    # and then make the prediction\n",
    "    ypredictRidge = X @ Ridgebeta\n",
    "    MSEPredict[i] = MSE(y,ypredictRidge)\n",
    "#    print(MSEPredict[i])\n",
    "    # Now plot the results\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), MSEPredict, 'r--', label = 'MSE Ridge Train')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278eae3",
   "metadata": {
    "editable": true
   },
   "source": [
    "We see here that we reach a plateau. What is actually happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b59ba1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## With Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6ba6a2",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "\n",
    "# A seed just to ensure that the random numbers are the same for every run.\n",
    "# Useful for eventual debugging.\n",
    "\n",
    "X = np.array( [ [ 2, 0], [0, 1], [0,0]])\n",
    "y = np.array( [4, 2, 3])\n",
    "\n",
    "\n",
    "# matrix inversion to find beta\n",
    "OLSbeta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(OLSbeta)\n",
    "# and then make the prediction\n",
    "ytildeOLS = X @ OLSbeta\n",
    "print(\"Training MSE for OLS\")\n",
    "print(MSE(y,ytildeOLS))\n",
    "ypredictOLS = X @ OLSbeta\n",
    "\n",
    "# Repeat now for Ridge regression and various values of the regularization parameter\n",
    "I = np.eye(2,2)\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 100\n",
    "MSERidgePredict = np.zeros(nlambdas)\n",
    "MSELassoPredict = np.zeros(nlambdas)\n",
    "lambdas = np.logspace(-4, 4, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "    Ridgebeta = np.linalg.inv(X.T @ X+lmb*I) @ X.T @ y\n",
    "    print(Ridgebeta)\n",
    "    # and then make the prediction\n",
    "    ypredictRidge = X @ Ridgebeta\n",
    "    MSERidgePredict[i] = MSE(y,ypredictRidge)\n",
    "    RegLasso = linear_model.Lasso(lmb,fit_intercept=False)\n",
    "    RegLasso.fit(X,y)\n",
    "    ypredictLasso = RegLasso.predict(X)\n",
    "    print(RegLasso.coef_)\n",
    "    MSELassoPredict[i] = MSE(y,ypredictLasso)\n",
    "# Now plot the results\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), MSERidgePredict, 'r--', label = 'MSE Ridge Train')\n",
    "plt.plot(np.log10(lambdas), MSELassoPredict, 'r--', label = 'MSE Lasso Train')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f503db8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Another Example, now with a polynomial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3410ffb2",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "\n",
    "# A seed just to ensure that the random numbers are the same for every run.\n",
    "# Useful for eventual debugging.\n",
    "np.random.seed(3155)\n",
    "\n",
    "x = np.random.rand(100)\n",
    "y = 2.0+5*x*x+0.1*np.random.randn(100)\n",
    "\n",
    "# number of features p (here degree of polynomial\n",
    "p = 3\n",
    "#  The design matrix now as function of a given polynomial\n",
    "X = np.zeros((len(x),p))\n",
    "X[:,0] = 1.0\n",
    "X[:,1] = x\n",
    "X[:,2] = x*x\n",
    "# We split the data in test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# matrix inversion to find beta\n",
    "OLSbeta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "print(OLSbeta)\n",
    "# and then make the prediction\n",
    "ytildeOLS = X_train @ OLSbeta\n",
    "print(\"Training MSE for OLS\")\n",
    "print(MSE(y_train,ytildeOLS))\n",
    "ypredictOLS = X_test @ OLSbeta\n",
    "print(\"Test MSE OLS\")\n",
    "print(MSE(y_test,ypredictOLS))\n",
    "\n",
    "# Repeat now for Lasso and Ridge regression and various values of the regularization parameter\n",
    "I = np.eye(p,p)\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 100\n",
    "MSEPredict = np.zeros(nlambdas)\n",
    "MSETrain = np.zeros(nlambdas)\n",
    "MSELassoPredict = np.zeros(nlambdas)\n",
    "MSELassoTrain = np.zeros(nlambdas)\n",
    "lambdas = np.logspace(-4, 4, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "    Ridgebeta = np.linalg.inv(X_train.T @ X_train+lmb*I) @ X_train.T @ y_train\n",
    "    # include lasso using Scikit-Learn\n",
    "    RegLasso = linear_model.Lasso(lmb,fit_intercept=False)\n",
    "    RegLasso.fit(X_train,y_train)\n",
    "    # and then make the prediction\n",
    "    ytildeRidge = X_train @ Ridgebeta\n",
    "    ypredictRidge = X_test @ Ridgebeta\n",
    "    ytildeLasso = RegLasso.predict(X_train)\n",
    "    ypredictLasso = RegLasso.predict(X_test)\n",
    "    MSEPredict[i] = MSE(y_test,ypredictRidge)\n",
    "    MSETrain[i] = MSE(y_train,ytildeRidge)\n",
    "    MSELassoPredict[i] = MSE(y_test,ypredictLasso)\n",
    "    MSELassoTrain[i] = MSE(y_train,ytildeLasso)\n",
    "\n",
    "# Now plot the results\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), MSETrain, label = 'MSE Ridge train')\n",
    "plt.plot(np.log10(lambdas), MSEPredict, 'r--', label = 'MSE Ridge Test')\n",
    "plt.plot(np.log10(lambdas), MSELassoTrain, label = 'MSE Lasso train')\n",
    "plt.plot(np.log10(lambdas), MSELassoPredict, 'r--', label = 'MSE Lasso Test')\n",
    "\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}