{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d679faf",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week40.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Week 40: Gradient descent methods (continued) and start Neural networks -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d86ca93",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Week 40: Gradient descent methods (continued) and start Neural networks\n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo, Norway\n",
    "\n",
    "Date: **September 29-October 3, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a8d43",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Lecture Monday September 30, 2024\n",
    "1. Stochastic Gradient descent with examples and automatic differentiation\n",
    "\n",
    "2. If we get time, we start with the basics of Neural Networks, setting up the basic steps, from the simple perceptron model to the multi-layer perceptron model\n",
    "\n",
    "3. [Video of lecture](https://youtu.be/jdJoOrCIdII)\n",
    "\n",
    "4. Whiteboard notes at <https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2024/NotesSeptember30.pdf>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ef548",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Suggested readings and videos\n",
    "**Readings and Videos:**\n",
    "\n",
    "1. The lecture notes for week 40 (these notes)\n",
    "\n",
    "2. For a good discussion on gradient methods, we would like to recommend Goodfellow et al section 4.3-4.5 and sections 8.3-8.6. We will come back to the latter chapter in our discussion of Neural networks as well.\n",
    "\n",
    "3. For neural networks we recommend Goodfellow et al chapter 6 and Raschka et al chapter 2 (contains also material about gradient descent) and chapter 11 (we will use this next week)\n",
    "\n",
    "4. Video on gradient descent at <https://www.youtube.com/watch?v=sDv4f4s2SB8>\n",
    "\n",
    "5. Video on stochastic gradient descent at <https://www.youtube.com/watch?v=vMh0zPT0tLI>\n",
    "\n",
    "6. Neural Networks demystified at <https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU&ab_channel=WelchLabs>\n",
    "\n",
    "7. Building Neural Networks from scratch at URL:https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3&ab_channel=sentdex\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccf861",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Lab sessions Tuesday and Wednesday\n",
    "**Material for the active learning sessions on Tuesday and Wednesday.**\n",
    "\n",
    "  * Work on project 1 and discussions on how to structure your report\n",
    "\n",
    "  * No weekly exercises for week 40, project work only\n",
    "\n",
    "  * Video on how to write scientific reports recorded during one of the lab sessions at <https://youtu.be/tVW1ZDmZnwM>\n",
    "\n",
    "  * A general guideline can be found at <https://github.com/CompPhysics/MachineLearning/blob/master/doc/Projects/EvaluationGrading/EvaluationForm.md>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d567b76",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "[Automatic differentiation (AD)](https://en.wikipedia.org/wiki/Automatic_differentiation), \n",
    "also called algorithmic\n",
    "differentiation or computational differentiation,is a set of\n",
    "techniques to numerically evaluate the derivative of a function\n",
    "specified by a computer program. AD exploits the fact that every\n",
    "computer program, no matter how complicated, executes a sequence of\n",
    "elementary arithmetic operations (addition, subtraction,\n",
    "multiplication, division, etc.) and elementary functions (exp, log,\n",
    "sin, cos, etc.). By applying the chain rule repeatedly to these\n",
    "operations, derivatives of arbitrary order can be computed\n",
    "automatically, accurately to working precision, and using at most a\n",
    "small constant factor more arithmetic operations than the original\n",
    "program.\n",
    "\n",
    "Automatic differentiation is neither:\n",
    "\n",
    "* Symbolic differentiation, nor\n",
    "\n",
    "* Numerical differentiation (the method of finite differences).\n",
    "\n",
    "Symbolic differentiation can lead to inefficient code and faces the\n",
    "difficulty of converting a computer program into a single expression,\n",
    "while numerical differentiation can introduce round-off errors in the\n",
    "discretization process and cancellation\n",
    "\n",
    "Python has tools for so-called **automatic differentiation**.\n",
    "Consider the following example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29059bbb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f(x) = \\sin\\left(2\\pi x + x^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000637ed",
   "metadata": {
    "editable": true
   },
   "source": [
    "which has the following derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73499fad",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f'(x) = \\cos\\left(2\\pi x + x^2\\right)\\left(2\\pi + 2x\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c54b7e",
   "metadata": {
    "editable": true
   },
   "source": [
    "Using **autograd** we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45af1608",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import autograd.numpy as np\n",
    "\n",
    "# To do elementwise differentiation:\n",
    "from autograd import elementwise_grad as egrad \n",
    "\n",
    "# To plot:\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(2*np.pi*x + x**2)\n",
    "\n",
    "def f_grad_analytic(x):\n",
    "    return np.cos(2*np.pi*x + x**2)*(2*np.pi + 2*x)\n",
    "\n",
    "# Do the comparison:\n",
    "x = np.linspace(0,1,1000)\n",
    "\n",
    "f_grad = egrad(f)\n",
    "\n",
    "computed = f_grad(x)\n",
    "analytic = f_grad_analytic(x)\n",
    "\n",
    "plt.title('Derivative computed from Autograd compared with the analytical derivative')\n",
    "plt.plot(x,computed,label='autograd')\n",
    "plt.plot(x,analytic,label='analytic')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"The max absolute difference is: %g\"%(np.max(np.abs(computed - analytic))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa94bf0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using autograd\n",
    "\n",
    "Here we\n",
    "experiment with what kind of functions Autograd is capable\n",
    "of finding the gradient of. The following Python functions are just\n",
    "meant to illustrate what Autograd can do, but please feel free to\n",
    "experiment with other, possibly more complicated, functions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3182e1",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def f1(x):\n",
    "    return x**3 + 1\n",
    "\n",
    "f1_grad = grad(f1)\n",
    "\n",
    "# Remember to send in float as argument to the computed gradient from Autograd!\n",
    "a = 1.0\n",
    "\n",
    "# See the evaluated gradient at a using autograd:\n",
    "print(\"The gradient of f1 evaluated at a = %g using autograd is: %g\"%(a,f1_grad(a)))\n",
    "\n",
    "# Compare with the analytical derivative, that is f1'(x) = 3*x**2 \n",
    "grad_analytical = 3*a**2\n",
    "print(\"The gradient of f1 evaluated at a = %g by finding the analytic expression is: %g\"%(a,grad_analytical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b52d711",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Autograd with more complicated functions\n",
    "\n",
    "To differentiate with respect to two (or more) arguments of a Python\n",
    "function, Autograd need to know at which variable the function if\n",
    "being differentiated with respect to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f046ce",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f2(x1,x2):\n",
    "    return 3*x1**3 + x2*(x1 - 5) + 1\n",
    "\n",
    "# By sending the argument 0, Autograd will compute the derivative w.r.t the first variable, in this case x1\n",
    "f2_grad_x1 = grad(f2,0)\n",
    "\n",
    "# ... and differentiate w.r.t x2 by sending 1 as an additional arugment to grad\n",
    "f2_grad_x2 = grad(f2,1)\n",
    "\n",
    "x1 = 1.0\n",
    "x2 = 3.0 \n",
    "\n",
    "print(\"Evaluating at x1 = %g, x2 = %g\"%(x1,x2))\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Compare with the analytical derivatives:\n",
    "\n",
    "# Derivative of f2 w.r.t x1 is: 9*x1**2 + x2:\n",
    "f2_grad_x1_analytical = 9*x1**2 + x2\n",
    "\n",
    "# Derivative of f2 w.r.t x2 is: x1 - 5:\n",
    "f2_grad_x2_analytical = x1 - 5\n",
    "\n",
    "# See the evaluated derivations:\n",
    "print(\"The derivative of f2 w.r.t x1: %g\"%( f2_grad_x1(x1,x2) ))\n",
    "print(\"The analytical derivative of f2 w.r.t x1: %g\"%( f2_grad_x1(x1,x2) ))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"The derivative of f2 w.r.t x2: %g\"%( f2_grad_x2(x1,x2) ))\n",
    "print(\"The analytical derivative of f2 w.r.t x2: %g\"%( f2_grad_x2(x1,x2) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8014a36",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that the grad function will not produce the true gradient of the function. The true gradient of a function with two or more variables will produce a vector, where each element is the function differentiated w.r.t a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12725415",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More complicated functions using the elements of their arguments directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "628f6795",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f3(x): # Assumes x is an array of length 5 or higher\n",
    "    return 2*x[0] + 3*x[1] + 5*x[2] + 7*x[3] + 11*x[4]**2\n",
    "\n",
    "f3_grad = grad(f3)\n",
    "\n",
    "x = np.linspace(0,4,5)\n",
    "\n",
    "# Print the computed gradient:\n",
    "print(\"The computed gradient of f3 is: \", f3_grad(x))\n",
    "\n",
    "# The analytical gradient is: (2, 3, 5, 7, 22*x[4])\n",
    "f3_grad_analytical = np.array([2, 3, 5, 7, 22*x[4]])\n",
    "\n",
    "# Print the analytical gradient:\n",
    "print(\"The analytical gradient of f3 is: \", f3_grad_analytical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce6ad6",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that in this case, when sending an array as input argument, the\n",
    "output from Autograd is another array. This is the true gradient of\n",
    "the function, as opposed to the function in the previous example. By\n",
    "using arrays to represent the variables, the output from Autograd\n",
    "might be easier to work with, as the output is closer to what one\n",
    "could expect form a gradient-evaluting function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb6cad9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Functions using mathematical functions from Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d192367",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f4(x):\n",
    "    return np.sqrt(1+x**2) + np.exp(x) + np.sin(2*np.pi*x)\n",
    "\n",
    "f4_grad = grad(f4)\n",
    "\n",
    "x = 2.7\n",
    "\n",
    "# Print the computed derivative:\n",
    "print(\"The computed derivative of f4 at x = %g is: %g\"%(x,f4_grad(x)))\n",
    "\n",
    "# The analytical derivative is: x/sqrt(1 + x**2) + exp(x) + cos(2*pi*x)*2*pi\n",
    "f4_grad_analytical = x/np.sqrt(1 + x**2) + np.exp(x) + np.cos(2*np.pi*x)*2*np.pi\n",
    "\n",
    "# Print the analytical gradient:\n",
    "print(\"The analytical gradient of f4 at x = %g is: %g\"%(x,f4_grad_analytical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec62ef",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec7a34e",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f5(x):\n",
    "    if x >= 0:\n",
    "        return x**2\n",
    "    else:\n",
    "        return -3*x + 1\n",
    "\n",
    "f5_grad = grad(f5)\n",
    "\n",
    "x = 2.7\n",
    "\n",
    "# Print the computed derivative:\n",
    "print(\"The computed derivative of f5 at x = %g is: %g\"%(x,f5_grad(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18fb67b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## And  with loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54456259",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f6_for(x):\n",
    "    val = 0\n",
    "    for i in range(10):\n",
    "        val = val + x**i\n",
    "    return val\n",
    "\n",
    "def f6_while(x):\n",
    "    val = 0\n",
    "    i = 0\n",
    "    while i < 10:\n",
    "        val = val + x**i\n",
    "        i = i + 1\n",
    "    return val\n",
    "\n",
    "f6_for_grad = grad(f6_for)\n",
    "f6_while_grad = grad(f6_while)\n",
    "\n",
    "x = 0.5\n",
    "\n",
    "# Print the computed derivaties of f6_for and f6_while\n",
    "print(\"The computed derivative of f6_for at x = %g is: %g\"%(x,f6_for_grad(x)))\n",
    "print(\"The computed derivative of f6_while at x = %g is: %g\"%(x,f6_while_grad(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cb94658",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "# Both of the functions are implementation of the sum: sum(x**i) for i = 0, ..., 9\n",
    "# The analytical derivative is: sum(i*x**(i-1)) \n",
    "f6_grad_analytical = 0\n",
    "for i in range(10):\n",
    "    f6_grad_analytical += i*x**(i-1)\n",
    "\n",
    "print(\"The analytical derivative of f6 at x = %g is: %g\"%(x,f6_grad_analytical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e990b8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba57a27f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def f7(n): # Assume that n is an integer\n",
    "    if n == 1 or n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n*f7(n-1)\n",
    "\n",
    "f7_grad = grad(f7)\n",
    "\n",
    "n = 2.0\n",
    "\n",
    "print(\"The computed derivative of f7 at n = %d is: %g\"%(n,f7_grad(n)))\n",
    "\n",
    "# The function f7 is an implementation of the factorial of n.\n",
    "# By using the product rule, one can find that the derivative is:\n",
    "\n",
    "f7_grad_analytical = 0\n",
    "for i in range(int(n)-1):\n",
    "    tmp = 1\n",
    "    for k in range(int(n)-1):\n",
    "        if k != i:\n",
    "            tmp *= (n - k)\n",
    "    f7_grad_analytical += tmp\n",
    "\n",
    "print(\"The analytical derivative of f7 at n = %d is: %g\"%(n,f7_grad_analytical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed8fa4",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that if n is equal to zero or one, Autograd will give an error message. This message appears when the output is independent on input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56614917",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using Autograd with OLS\n",
    "\n",
    "We conclude the part on optmization by showing how we can make codes\n",
    "for linear regression and logistic regression using **autograd**. The\n",
    "first example shows results with ordinary leats squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6f4267",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients for OLS\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(beta):\n",
    "    return (1.0/n)*np.sum((y-X @ beta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "# define the gradient\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = training_gradient(theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317708a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Same code but now with momentum gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b91eabff",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients for OLS\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(beta):\n",
    "    return (1.0/n)*np.sum((y-X @ beta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x#+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 30\n",
    "\n",
    "# define the gradient\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = training_gradient(theta)\n",
    "    theta -= eta*gradients\n",
    "    print(iter,gradients[0],gradients[1])\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "# Now improve with momentum gradient descent\n",
    "change = 0.0\n",
    "delta_momentum = 0.3\n",
    "for iter in range(Niterations):\n",
    "    # calculate gradient\n",
    "    gradients = training_gradient(theta)\n",
    "    # calculate update\n",
    "    new_change = eta*gradients+delta_momentum*change\n",
    "    # take a step\n",
    "    theta -= new_change\n",
    "    # save the change\n",
    "    change = new_change\n",
    "    print(iter,gradients[0],gradients[1])\n",
    "print(\"theta from own gd wth momentum\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4da26",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Including Stochastic Gradient Descent with Autograd\n",
    "In this code we include the stochastic gradient descent approach discussed above. Note here that we specify which argument we are taking the derivative with respect to when using **autograd**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec7a759d",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using SGD\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = (1.0/n)*training_gradient(y, X, theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "# Can you figure out a better way of setting up the contributions to each batch?\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        theta = theta - eta*gradients\n",
    "print(\"theta from own sdg\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605be519",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Same code but now with momentum gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fc8795f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using SGD\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 100\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = (1.0/n)*training_gradient(y, X, theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "change = 0.0\n",
    "delta_momentum = 0.3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+delta_momentum*change\n",
    "        # take a step\n",
    "        theta -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "print(\"theta from own sdg with momentum\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f6a0c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Similar (second order function now) problem but now with AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3bcc017",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using AdaGrad and Stochastic Gradient descent\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 1000\n",
    "x = np.random.rand(n,1)\n",
    "y = 2.0+3*x +4*x*x\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "\n",
    "# Value for learning rate\n",
    "eta = 0.01\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*eta/(delta+np.sqrt(Giter))\n",
    "        theta -= update\n",
    "print(\"theta from own AdaGrad\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3536497d",
   "metadata": {
    "editable": true
   },
   "source": [
    "Running this code we note an almost perfect agreement with the results from matrix inversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e701a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RMSprop for adaptive learning rate with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57e7949a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using RMSprop  and Stochastic Gradient descent\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 1000\n",
    "x = np.random.rand(n,1)\n",
    "y = 2.0+3*x +4*x*x# +np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "\n",
    "# Value for learning rate\n",
    "eta = 0.01\n",
    "# Value for parameter rho\n",
    "rho = 0.99\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "\t# Accumulated gradient\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        Giter = (rho*Giter+(1-rho)*gradients*gradients)\n",
    "\t# Taking the diagonal only and inverting\n",
    "        update = gradients*eta/(delta+np.sqrt(Giter))\n",
    "\t# Hadamard product\n",
    "        theta -= update\n",
    "print(\"theta from own RMSprop\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910fed9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## And finally [ADAM](https://arxiv.org/pdf/1412.6980.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4765630f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using RMSprop  and Stochastic Gradient descent\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 1000\n",
    "x = np.random.rand(n,1)\n",
    "y = 2.0+3*x +4*x*x# +np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "\n",
    "# Value for learning rate\n",
    "eta = 0.01\n",
    "# Value for parameters beta1 and beta2, see https://arxiv.org/abs/1412.6980\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-7\n",
    "iter = 0\n",
    "for epoch in range(n_epochs):\n",
    "    first_moment = 0.0\n",
    "    second_moment = 0.0\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        # Computing moments first\n",
    "        first_moment = beta1*first_moment + (1-beta1)*gradients\n",
    "        second_moment = beta2*second_moment+(1-beta2)*gradients*gradients\n",
    "        first_term = first_moment/(1.0-beta1**iter)\n",
    "        second_term = second_moment/(1.0-beta2**iter)\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        update = eta*first_term/(np.sqrt(second_term)+delta)\n",
    "        theta -= update\n",
    "print(\"theta from own ADAM\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28731b27",
   "metadata": {
    "editable": true
   },
   "source": [
    "## And Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "814be546",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (np.tanh(x / 2.) + 1)\n",
    "\n",
    "def logistic_predictions(weights, inputs):\n",
    "    # Outputs probability of a label being true according to logistic model.\n",
    "    return sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "def training_loss(weights):\n",
    "    # Training loss is the negative log-likelihood of the training labels.\n",
    "    preds = logistic_predictions(weights, inputs)\n",
    "    label_probabilities = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -np.sum(np.log(label_probabilities))\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])\n",
    "\n",
    "# Define a function that returns gradients of training loss using Autograd.\n",
    "training_gradient_fun = grad(training_loss)\n",
    "\n",
    "# Optimize weights using gradient descent.\n",
    "weights = np.array([0.0, 0.0, 0.0])\n",
    "print(\"Initial loss:\", training_loss(weights))\n",
    "for i in range(100):\n",
    "    weights -= training_gradient_fun(weights) * 0.01\n",
    "\n",
    "print(\"Trained loss:\", training_loss(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab7076",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Introducing [JAX](https://jax.readthedocs.io/en/latest/)\n",
    "\n",
    "Presently, instead of using **autograd**, we recommend using [JAX](https://jax.readthedocs.io/en/latest/)\n",
    "\n",
    "**JAX** is Autograd and [XLA (Accelerated Linear Algebra))](https://www.tensorflow.org/xla),\n",
    "brought together for high-performance numerical computing and machine learning research.\n",
    "It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44fe55",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Getting started with Jax, note the way we import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96d73091",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax import grad as jax_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a09d4c",
   "metadata": {
    "editable": true
   },
   "source": [
    "### A warm-up example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4914cdc2",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    return x**2\n",
    "\n",
    "def analytical_gradient(x):\n",
    "    return 2*x\n",
    "\n",
    "def gradient_descent(starting_point, learning_rate, num_iterations, solver=\"analytical\"):\n",
    "    x = starting_point\n",
    "    trajectory_x = [x]\n",
    "    trajectory_y = [function(x)]\n",
    "\n",
    "    if solver == \"analytical\":\n",
    "        grad = analytical_gradient    \n",
    "    elif solver == \"jax\":\n",
    "        grad = jax_grad(function)\n",
    "        x = jnp.float64(x)\n",
    "        learning_rate = jnp.float64(learning_rate)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        \n",
    "        x = x - learning_rate * grad(x)\n",
    "        trajectory_x.append(x)\n",
    "        trajectory_y.append(function(x))\n",
    "\n",
    "    return trajectory_x, trajectory_y\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, function(x), label=\"f(x)\")\n",
    "\n",
    "descent_x, descent_y = gradient_descent(5, 0.1, 10, solver=\"analytical\")\n",
    "jax_descend_x, jax_descend_y = gradient_descent(5, 0.1, 10, solver=\"jax\")\n",
    "\n",
    "plt.plot(descent_x, descent_y, label=\"Gradient descent\", marker=\"o\")\n",
    "plt.plot(jax_descend_x, jax_descend_y, label=\"JAX\", marker=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d209f7a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### A more advanced example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77a1efbb",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "backend = np\n",
    "\n",
    "def function(x):\n",
    "    return x*backend.sin(x**2 + 1)\n",
    "\n",
    "def analytical_gradient(x):\n",
    "    return backend.sin(x**2 + 1) + 2*x**2*backend.cos(x**2 + 1)\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, function(x), label=\"f(x)\")\n",
    "\n",
    "descent_x, descent_y = gradient_descent(1, 0.01, 300, solver=\"analytical\")\n",
    "\n",
    "# Change the backend to JAX\n",
    "backend = jnp\n",
    "jax_descend_x, jax_descend_y = gradient_descent(1, 0.01, 300, solver=\"jax\")\n",
    "\n",
    "plt.scatter(descent_x, descent_y, label=\"Gradient descent\", marker=\"v\", s=10, color=\"red\") \n",
    "plt.scatter(jax_descend_x, jax_descend_y, label=\"JAX\", marker=\"x\", s=5, color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb9838b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Introduction to Neural networks\n",
    "\n",
    "Artificial neural networks are computational systems that can learn to\n",
    "perform tasks by considering examples, generally without being\n",
    "programmed with any task-specific rules. It is supposed to mimic a\n",
    "biological system, wherein neurons interact by sending signals in the\n",
    "form of mathematical functions between layers. All layers can contain\n",
    "an arbitrary number of neurons, and each connection is represented by\n",
    "a weight variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1332c8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Artificial neurons\n",
    "\n",
    "The field of artificial neural networks has a long history of\n",
    "development, and is closely connected with the advancement of computer\n",
    "science and computers in general. A model of artificial neurons was\n",
    "first developed by McCulloch and Pitts in 1943 to study signal\n",
    "processing in the brain and has later been refined by others. The\n",
    "general idea is to mimic neural networks in the human brain, which is\n",
    "composed of billions of neurons that communicate with each other by\n",
    "sending electrical signals.  Each neuron accumulates its incoming\n",
    "signals, which must exceed an activation threshold to yield an\n",
    "output. If the threshold is not overcome, the neuron remains inactive,\n",
    "i.e. has zero output.\n",
    "\n",
    "This behaviour has inspired a simple mathematical model for an artificial neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4d54b",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"artificialNeuron\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y = f\\left(\\sum_{i=1}^n w_ix_i\\right) = f(u)\n",
    "\\label{artificialNeuron} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9994e",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, the output $y$ of the neuron is the value of its activation function, which have as input\n",
    "a weighted sum of signals $x_i, \\dots ,x_n$ received by $n$ other neurons.\n",
    "\n",
    "Conceptually, it is helpful to divide neural networks into four\n",
    "categories:\n",
    "1. general purpose neural networks for supervised learning,\n",
    "\n",
    "2. neural networks designed specifically for image processing, the most prominent example of this class being Convolutional Neural Networks (CNNs),\n",
    "\n",
    "3. neural networks for sequential data such as Recurrent Neural Networks (RNNs), and\n",
    "\n",
    "4. neural networks for unsupervised learning such as Deep Boltzmann Machines.\n",
    "\n",
    "In natural science, DNNs and CNNs have already found numerous\n",
    "applications. In statistical physics, they have been applied to detect\n",
    "phase transitions in 2D Ising and Potts models, lattice gauge\n",
    "theories, and different phases of polymers, or solving the\n",
    "Navier-Stokes equation in weather forecasting.  Deep learning has also\n",
    "found interesting applications in quantum physics. Various quantum\n",
    "phase transitions can be detected and studied using DNNs and CNNs,\n",
    "topological phases, and even non-equilibrium many-body\n",
    "localization. Representing quantum states as DNNs quantum state\n",
    "tomography are among some of the impressive achievements to reveal the\n",
    "potential of DNNs to facilitate the study of quantum systems.\n",
    "\n",
    "In quantum information theory, it has been shown that one can perform\n",
    "gate decompositions with the help of neural. \n",
    "\n",
    "The applications are not limited to the natural sciences. There is a\n",
    "plethora of applications in essentially all disciplines, from the\n",
    "humanities to life science and medicine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513bdfb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Neural network types\n",
    "\n",
    "An artificial neural network (ANN), is a computational model that\n",
    "consists of layers of connected neurons, or nodes or units.  We will\n",
    "refer to these interchangeably as units or nodes, and sometimes as\n",
    "neurons.\n",
    "\n",
    "It is supposed to mimic a biological nervous system by letting each\n",
    "neuron interact with other neurons by sending signals in the form of\n",
    "mathematical functions between layers.  A wide variety of different\n",
    "ANNs have been developed, but most of them consist of an input layer,\n",
    "an output layer and eventual layers in-between, called *hidden\n",
    "layers*. All layers can contain an arbitrary number of nodes, and each\n",
    "connection between two nodes is associated with a weight variable.\n",
    "\n",
    "Neural networks (also called neural nets) are neural-inspired\n",
    "nonlinear models for supervised learning.  As we will see, neural nets\n",
    "can be viewed as natural, more powerful extensions of supervised\n",
    "learning methods such as linear and logistic regression and soft-max\n",
    "methods we discussed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80a3c0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Feed-forward neural networks\n",
    "\n",
    "The feed-forward neural network (FFNN) was the first and simplest type\n",
    "of ANNs that were devised. In this network, the information moves in\n",
    "only one direction: forward through the layers.\n",
    "\n",
    "Nodes are represented by circles, while the arrows display the\n",
    "connections between the nodes, including the direction of information\n",
    "flow. Additionally, each arrow corresponds to a weight variable\n",
    "(figure to come).  We observe that each node in a layer is connected\n",
    "to *all* nodes in the subsequent layer, making this a so-called\n",
    "*fully-connected* FFNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de15fe45",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "A different variant of FFNNs are *convolutional neural networks*\n",
    "(CNNs), which have a connectivity pattern inspired by the animal\n",
    "visual cortex. Individual neurons in the visual cortex only respond to\n",
    "stimuli from small sub-regions of the visual field, called a receptive\n",
    "field. This makes the neurons well-suited to exploit the strong\n",
    "spatially local correlation present in natural images. The response of\n",
    "each neuron can be approximated mathematically as a convolution\n",
    "operation.  (figure to come)\n",
    "\n",
    "Convolutional neural networks emulate the behaviour of neurons in the\n",
    "visual cortex by enforcing a *local* connectivity pattern between\n",
    "nodes of adjacent layers: Each node in a convolutional layer is\n",
    "connected only to a subset of the nodes in the previous layer, in\n",
    "contrast to the fully-connected FFNN.  Often, CNNs consist of several\n",
    "convolutional layers that learn local features of the input, with a\n",
    "fully-connected layer at the end, which gathers all the local data and\n",
    "produces the outputs. They have wide applications in image and video\n",
    "recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c028ac51",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "So far we have only mentioned ANNs where information flows in one\n",
    "direction: forward. *Recurrent neural networks* on the other hand,\n",
    "have connections between nodes that form directed *cycles*. This\n",
    "creates a form of internal memory which are able to capture\n",
    "information on what has been calculated before; the output is\n",
    "dependent on the previous computations. Recurrent NNs make use of\n",
    "sequential information by performing the same task for every element\n",
    "in a sequence, where each element depends on previous elements. An\n",
    "example of such information is sentences, making recurrent NNs\n",
    "especially well-suited for handwriting and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476ba1f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Other types of networks\n",
    "\n",
    "There are many other kinds of ANNs that have been developed. One type\n",
    "that is specifically designed for interpolation in multidimensional\n",
    "space is the radial basis function (RBF) network. RBFs are typically\n",
    "made up of three layers: an input layer, a hidden layer with\n",
    "non-linear radial symmetric activation functions and a linear output\n",
    "layer (''linear'' here means that each node in the output layer has a\n",
    "linear activation function). The layers are normally fully-connected\n",
    "and there are no cycles, thus RBFs can be viewed as a type of\n",
    "fully-connected FFNN. They are however usually treated as a separate\n",
    "type of NN due the unusual activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7b958f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Multilayer perceptrons\n",
    "\n",
    "One uses often so-called fully-connected feed-forward neural networks\n",
    "with three or more layers (an input layer, one or more hidden layers\n",
    "and an output layer) consisting of neurons that have non-linear\n",
    "activation functions.\n",
    "\n",
    "Such networks are often called *multilayer perceptrons* (MLPs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b829eda",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Why multilayer perceptrons?\n",
    "\n",
    "According to the *Universal approximation theorem*, a feed-forward\n",
    "neural network with just a single hidden layer containing a finite\n",
    "number of neurons can approximate a continuous multidimensional\n",
    "function to arbitrary accuracy, assuming the activation function for\n",
    "the hidden layer is a **non-constant, bounded and\n",
    "monotonically-increasing continuous function**.\n",
    "\n",
    "Note that the requirements on the activation function only applies to\n",
    "the hidden layer, the output nodes are always assumed to be linear, so\n",
    "as to not restrict the range of output values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e38937",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Illustration of a single perceptron model and a multi-perceptron model\n",
    "\n",
    "<!-- dom:FIGURE: [figures/nns.png, width=600 frac=0.8]  In a) we show a single perceptron model while in b) we dispay a network with two  hidden layers, an input layer and an output layer. -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/nns.png\" width=\"600\"><p style=\"font-size: 0.9em\"><i>Figure 1: In a) we show a single perceptron model while in b) we dispay a network with two  hidden layers, an input layer and an output layer.</i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793fa0f6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Examples of XOR, OR and AND gates\n",
    "\n",
    "Let us first try to fit various gates using standard linear\n",
    "regression. The gates we are thinking of are the classical XOR, OR and\n",
    "AND gates, well-known elements in computer science. The tables here\n",
    "show how we can set up the inputs $x_1$ and $x_2$ in order to yield a\n",
    "specific target $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31c32fb1",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple code that tests XOR, OR and AND gates with linear regression\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "# Design matrix\n",
    "X = np.array([ [1, 0, 0], [1, 0, 1], [1, 1, 0],[1, 1, 1]],dtype=np.float64)\n",
    "print(f\"The X.TX  matrix:{X.T @ X}\")\n",
    "Xinv = np.linalg.pinv(X.T @ X)\n",
    "print(f\"The invers of X.TX  matrix:{Xinv}\")\n",
    "\n",
    "# The XOR gate \n",
    "yXOR = np.array( [ 0, 1 ,1, 0])\n",
    "ThetaXOR  = Xinv @ X.T @ yXOR\n",
    "print(f\"The values of theta for the XOR gate:{ThetaXOR}\")\n",
    "print(f\"The linear regression prediction  for the XOR gate:{X @ ThetaXOR}\")\n",
    "\n",
    "\n",
    "# The OR gate \n",
    "yOR = np.array( [ 0, 1 ,1, 1])\n",
    "ThetaOR  = Xinv @ X.T @ yOR\n",
    "print(f\"The values of theta for the OR gate:{ThetaOR}\")\n",
    "print(f\"The linear regression prediction  for the OR gate:{X @ ThetaOR}\")\n",
    "\n",
    "\n",
    "# The OR gate \n",
    "yAND = np.array( [ 0, 0 ,0, 1])\n",
    "ThetaAND  = Xinv @ X.T @ yAND\n",
    "print(f\"The values of theta for the AND gate:{ThetaAND}\")\n",
    "print(f\"The linear regression prediction  for the AND gate:{X @ ThetaAND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652d853",
   "metadata": {
    "editable": true
   },
   "source": [
    "What is happening here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f95bfa",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Does Logistic Regression do a better Job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ed55d22",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple code that tests XOR and OR gates with linear regression\n",
    "and logistic regression\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Design matrix\n",
    "X = np.array([ [1, 0, 0], [1, 0, 1], [1, 1, 0],[1, 1, 1]],dtype=np.float64)\n",
    "print(f\"The X.TX  matrix:{X.T @ X}\")\n",
    "Xinv = np.linalg.pinv(X.T @ X)\n",
    "print(f\"The invers of X.TX  matrix:{Xinv}\")\n",
    "\n",
    "# The XOR gate \n",
    "yXOR = np.array( [ 0, 1 ,1, 0])\n",
    "ThetaXOR  = Xinv @ X.T @ yXOR\n",
    "print(f\"The values of theta for the XOR gate:{ThetaXOR}\")\n",
    "print(f\"The linear regression prediction  for the XOR gate:{X @ ThetaXOR}\")\n",
    "\n",
    "\n",
    "# The OR gate \n",
    "yOR = np.array( [ 0, 1 ,1, 1])\n",
    "ThetaOR  = Xinv @ X.T @ yOR\n",
    "print(f\"The values of theta for the OR gate:{ThetaOR}\")\n",
    "print(f\"The linear regression prediction  for the OR gate:{X @ ThetaOR}\")\n",
    "\n",
    "\n",
    "# The OR gate \n",
    "yAND = np.array( [ 0, 0 ,0, 1])\n",
    "ThetaAND  = Xinv @ X.T @ yAND\n",
    "print(f\"The values of theta for the AND gate:{ThetaAND}\")\n",
    "print(f\"The linear regression prediction  for the AND gate:{X @ ThetaAND}\")\n",
    "\n",
    "# Now we change to logistic regression\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, yOR)\n",
    "print(\"Test set accuracy with Logistic Regression for OR gate: {:.2f}\".format(logreg.score(X,yOR)))\n",
    "\n",
    "logreg.fit(X, yXOR)\n",
    "print(\"Test set accuracy with Logistic Regression for XOR gate: {:.2f}\".format(logreg.score(X,yXOR)))\n",
    "\n",
    "\n",
    "logreg.fit(X, yAND)\n",
    "print(\"Test set accuracy with Logistic Regression for AND gate: {:.2f}\".format(logreg.score(X,yAND)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593bafc0",
   "metadata": {
    "editable": true
   },
   "source": [
    "Not exactly impressive, but somewhat better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772ee66",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Adding Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3131b91b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# and now neural networks with Scikit-Learn and the XOR\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, yXOR = make_classification(n_samples=100, random_state=1)\n",
    "FFNN = MLPClassifier(random_state=1, max_iter=300).fit(X, yXOR)\n",
    "FFNN.predict_proba(X)\n",
    "print(f\"Test set accuracy with Feed Forward Neural Network  for XOR gate:{FFNN.score(X, yXOR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3072a05",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematical model\n",
    "\n",
    "The output $y$ is produced via the activation function $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501b704",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "y = f\\left(\\sum_{i=1}^n w_ix_i + b_i\\right) = f(z),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9284d8",
   "metadata": {
    "editable": true
   },
   "source": [
    "This function receives $x_i$ as inputs.\n",
    "Here the activation $z=(\\sum_{i=1}^n w_ix_i+b_i)$. \n",
    "In an FFNN of such neurons, the *inputs* $x_i$ are the *outputs* of\n",
    "the neurons in the preceding layer. Furthermore, an MLP is\n",
    "fully-connected, which means that each neuron receives a weighted sum\n",
    "of the outputs of *all* neurons in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0a94c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematical model\n",
    "\n",
    "First, for each node $i$ in the first hidden layer, we calculate a weighted sum $z_i^1$ of the input coordinates $x_j$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b7427",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} z_i^1 = \\sum_{j=1}^{M} w_{ij}^1 x_j + b_i^1\n",
    "\\label{_auto1} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db3236",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here $b_i$ is the so-called bias which is normally needed in\n",
    "case of zero activation weights or inputs. How to fix the biases and\n",
    "the weights will be discussed below.  The value of $z_i^1$ is the\n",
    "argument to the activation function $f_i$ of each node $i$, The\n",
    "variable $M$ stands for all possible inputs to a given node $i$ in the\n",
    "first layer.  We define  the output $y_i^1$ of all neurons in layer 1 as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90159e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"outputLayer1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y_i^1 = f(z_i^1) = f\\left(\\sum_{j=1}^M w_{ij}^1 x_j  + b_i^1\\right)\n",
    "\\label{outputLayer1} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8a63c",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we assume that all nodes in the same layer have identical\n",
    "activation functions, hence the notation $f$. In general, we could assume in the more general case that different layers have different activation functions.\n",
    "In this case we would identify these functions with a superscript $l$ for the $l$-th layer,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99259806",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"generalLayer\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y_i^l = f^l(u_i^l) = f^l\\left(\\sum_{j=1}^{N_{l-1}} w_{ij}^l y_j^{l-1} + b_i^l\\right)\n",
    "\\label{generalLayer} \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3efcd2",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $N_l$ is the number of nodes in layer $l$. When the output of\n",
    "all the nodes in the first hidden layer are computed, the values of\n",
    "the subsequent layer can be calculated and so forth until the output\n",
    "is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1752b7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematical model\n",
    "\n",
    "The output of neuron $i$ in layer 2 is thus,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38f49c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y_i^2 = f^2\\left(\\sum_{j=1}^N w_{ij}^2 y_j^1 + b_i^2\\right) \n",
    "\\label{_auto2} \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17273cad",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"outputLayer2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    " = f^2\\left[\\sum_{j=1}^N w_{ij}^2f^1\\left(\\sum_{k=1}^M w_{jk}^1 x_k + b_j^1\\right) + b_i^2\\right]\n",
    "\\label{outputLayer2} \\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f1932",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have substituted $y_k^1$ with the inputs $x_k$. Finally, the ANN output reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e276cb",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y_i^3 = f^3\\left(\\sum_{j=1}^N w_{ij}^3 y_j^2 + b_i^3\\right) \n",
    "\\label{_auto3} \\tag{7}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bedcd",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    " = f_3\\left[\\sum_{j} w_{ij}^3 f^2\\left(\\sum_{k} w_{jk}^2 f^1\\left(\\sum_{m} w_{km}^1 x_m + b_k^1\\right) + b_j^2\\right)\n",
    "  + b_1^3\\right]\n",
    "\\label{_auto4} \\tag{8}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0c334",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematical model\n",
    "\n",
    "We can generalize this expression to an MLP with $l$ hidden\n",
    "layers. The complete functional form is,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23d2ac",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"completeNN\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y^{l+1}_i = f^{l+1}\\left[\\!\\sum_{j=1}^{N_l} w_{ij}^3 f^l\\left(\\sum_{k=1}^{N_{l-1}}w_{jk}^{l-1}\\left(\\dots f^1\\left(\\sum_{n=1}^{N_0} w_{mn}^1 x_n+ b_m^1\\right)\\dots\\right)+b_k^2\\right)+b_1^3\\right] \n",
    "\\label{completeNN} \\tag{9}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68361f19",
   "metadata": {
    "editable": true
   },
   "source": [
    "which illustrates a basic property of MLPs: The only independent\n",
    "variables are the input values $x_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a0407",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Mathematical model\n",
    "\n",
    "This confirms that an MLP, despite its quite convoluted mathematical\n",
    "form, is nothing more than an analytic function, specifically a\n",
    "mapping of real-valued vectors $\\hat{x} \\in \\mathbb{R}^n \\rightarrow\n",
    "\\hat{y} \\in \\mathbb{R}^m$.\n",
    "\n",
    "Furthermore, the flexibility and universality of an MLP can be\n",
    "illustrated by realizing that the expression is essentially a nested\n",
    "sum of scaled activation functions of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4cb0f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " f(x) = c_1 f(c_2 x + c_3) + c_4\n",
    "\\label{_auto5} \\tag{10}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f185a",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the parameters $c_i$ are weights and biases. By adjusting these\n",
    "parameters, the activation functions can be shifted up and down or\n",
    "left and right, change slope or be rescaled which is the key to the\n",
    "flexibility of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17ac89",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Matrix-vector notation\n",
    "\n",
    "We can introduce a more convenient notation for the activations in an A NN. \n",
    "\n",
    "Additionally, we can represent the biases and activations\n",
    "as layer-wise column vectors $\\hat{b}_l$ and $\\hat{y}_l$, so that the $i$-th element of each vector \n",
    "is the bias $b_i^l$ and activation $y_i^l$ of node $i$ in layer $l$ respectively. \n",
    "\n",
    "We have that $\\mathrm{W}_l$ is an $N_{l-1} \\times N_l$ matrix, while $\\hat{b}_l$ and $\\hat{y}_l$ are $N_l \\times 1$ column vectors. \n",
    "With this notation, the sum becomes a matrix-vector multiplication, and we can write\n",
    "the equation for the activations of hidden layer 2 (assuming three nodes for simplicity) as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa341fe",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto6\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\hat{y}_2 = f_2(\\mathrm{W}_2 \\hat{y}_{1} + \\hat{b}_{2}) = \n",
    " f_2\\left(\\left[\\begin{array}{ccc}\n",
    "    w^2_{11} &w^2_{12} &w^2_{13} \\\\\n",
    "    w^2_{21} &w^2_{22} &w^2_{23} \\\\\n",
    "    w^2_{31} &w^2_{32} &w^2_{33} \\\\\n",
    "    \\end{array} \\right] \\cdot\n",
    "    \\left[\\begin{array}{c}\n",
    "           y^1_1 \\\\\n",
    "           y^1_2 \\\\\n",
    "           y^1_3 \\\\\n",
    "          \\end{array}\\right] + \n",
    "    \\left[\\begin{array}{c}\n",
    "           b^2_1 \\\\\n",
    "           b^2_2 \\\\\n",
    "           b^2_3 \\\\\n",
    "          \\end{array}\\right]\\right).\n",
    "\\label{_auto6} \\tag{11}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15b63b",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Matrix-vector notation  and activation\n",
    "\n",
    "The activation of node $i$ in layer 2 is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4055a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto7\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " y^2_i = f_2\\Bigr(w^2_{i1}y^1_1 + w^2_{i2}y^1_2 + w^2_{i3}y^1_3 + b^2_i\\Bigr) = \n",
    " f_2\\left(\\sum_{j=1}^3 w^2_{ij} y_j^1 + b^2_i\\right).\n",
    "\\label{_auto7} \\tag{12}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4e6e6",
   "metadata": {
    "editable": true
   },
   "source": [
    "This is not just a convenient and compact notation, but also a useful\n",
    "and intuitive way to think about MLPs: The output is calculated by a\n",
    "series of matrix-vector multiplications and vector additions that are\n",
    "used as input to the activation functions. For each operation\n",
    "$\\mathrm{W}_l \\hat{y}_{l-1}$ we move forward one layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae2c264",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Activation functions\n",
    "\n",
    "A property that characterizes a neural network, other than its\n",
    "connectivity, is the choice of activation function(s).  As described\n",
    "in, the following restrictions are imposed on an activation function\n",
    "for a FFNN to fulfill the universal approximation theorem\n",
    "\n",
    "  * Non-constant\n",
    "\n",
    "  * Bounded\n",
    "\n",
    "  * Monotonically-increasing\n",
    "\n",
    "  * Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f06e3",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Activation functions, Logistic and Hyperbolic ones\n",
    "\n",
    "The second requirement excludes all linear functions. Furthermore, in\n",
    "a MLP with only linear activation functions, each layer simply\n",
    "performs a linear transformation of its inputs.\n",
    "\n",
    "Regardless of the number of layers, the output of the NN will be\n",
    "nothing but a linear function of the inputs. Thus we need to introduce\n",
    "some kind of non-linearity to the NN to be able to fit non-linear\n",
    "functions Typical examples are the logistic *Sigmoid*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6872f0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120447fb",
   "metadata": {
    "editable": true
   },
   "source": [
    "and the *hyperbolic tangent* function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809db622",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f(x) = \\tanh(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38a685",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Relevance\n",
    "\n",
    "The *sigmoid* function are more biologically plausible because the\n",
    "output of inactive neurons are zero. Such activation function are\n",
    "called *one-sided*. However, it has been shown that the hyperbolic\n",
    "tangent performs better than the sigmoid for training MLPs.  has\n",
    "become the most popular for *deep neural networks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11e0216d",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"The sigmoid function (or the logistic curve) is a \n",
    "function that takes any real number, z, and outputs a number (0,1).\n",
    "It is useful in neural networks for assigning weights on a relative scale.\n",
    "The value z is the weighted sum of parameters involved in the learning algorithm.\"\"\"\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import math as mt\n",
    "\n",
    "z = numpy.arange(-5, 5, .1)\n",
    "sigma_fn = numpy.vectorize(lambda z: 1/(1+numpy.exp(-z)))\n",
    "sigma = sigma_fn(z)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(z, sigma)\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "ax.set_xlim([-5,5])\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('z')\n",
    "ax.set_title('sigmoid function')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Step Function\"\"\"\n",
    "z = numpy.arange(-5, 5, .02)\n",
    "step_fn = numpy.vectorize(lambda z: 1.0 if z >= 0.0 else 0.0)\n",
    "step = step_fn(z)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(z, step)\n",
    "ax.set_ylim([-0.5, 1.5])\n",
    "ax.set_xlim([-5,5])\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('z')\n",
    "ax.set_title('step function')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Sine Function\"\"\"\n",
    "z = numpy.arange(-2*mt.pi, 2*mt.pi, 0.1)\n",
    "t = numpy.sin(z)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(z, t)\n",
    "ax.set_ylim([-1.0, 1.0])\n",
    "ax.set_xlim([-2*mt.pi,2*mt.pi])\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('z')\n",
    "ax.set_title('sine function')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Plots a graph of the squashing function used by a rectified linear\n",
    "unit\"\"\"\n",
    "z = numpy.arange(-2, 2, .1)\n",
    "zero = numpy.zeros(len(z))\n",
    "y = numpy.max([zero, z], axis=0)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(z, y)\n",
    "ax.set_ylim([-2.0, 2.0])\n",
    "ax.set_xlim([-2.0, 2.0])\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('z')\n",
    "ax.set_title('Rectified linear unit')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
