<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week40.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week40-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 40: Gradient descent methods (continued) and start Neural networks">
<title>Week 40: Gradient descent methods (continued) and start Neural networks</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week40.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week40-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Lecture Monday September 29, 2025',
               2,
               None,
               'lecture-monday-september-29-2025'),
              ('Suggested readings and videos',
               2,
               None,
               'suggested-readings-and-videos'),
              ('Lab sessions Tuesday and Wednesday',
               2,
               None,
               'lab-sessions-tuesday-and-wednesday'),
              ('Logistic Regression, from last week',
               2,
               None,
               'logistic-regression-from-last-week'),
              ('Classification problems', 2, None, 'classification-problems'),
              ('Optimization and Deep learning',
               2,
               None,
               'optimization-and-deep-learning'),
              ('Basics', 2, None, 'basics'),
              ('Two parameters', 2, None, 'two-parameters'),
              ('Maximum likelihood', 2, None, 'maximum-likelihood'),
              ('The cost function rewritten',
               2,
               None,
               'the-cost-function-rewritten'),
              ('Minimizing the cross entropy',
               2,
               None,
               'minimizing-the-cross-entropy'),
              ('A more compact expression',
               2,
               None,
               'a-more-compact-expression'),
              ('Extending to more predictors',
               2,
               None,
               'extending-to-more-predictors'),
              ('Including more classes', 2, None, 'including-more-classes'),
              ('More classes', 2, None, 'more-classes'),
              ('Optimization, the central part of any Machine Learning '
               'algortithm',
               2,
               None,
               'optimization-the-central-part-of-any-machine-learning-algortithm'),
              ('Revisiting our Logistic Regression case',
               2,
               None,
               'revisiting-our-logistic-regression-case'),
              ('The equations to solve', 2, None, 'the-equations-to-solve'),
              ("Solving using Newton-Raphson's method",
               2,
               None,
               'solving-using-newton-raphson-s-method'),
              ('Example code for Logistic Regression',
               2,
               None,
               'example-code-for-logistic-regression'),
              ('Synthetic data generation',
               3,
               None,
               'synthetic-data-generation'),
              ('Using _Scikit-learn_', 2, None, 'using-scikit-learn'),
              ('Using the correlation matrix',
               2,
               None,
               'using-the-correlation-matrix'),
              ('Discussing the correlation data',
               2,
               None,
               'discussing-the-correlation-data'),
              ('Other measures in classification studies',
               2,
               None,
               'other-measures-in-classification-studies'),
              ('Introduction to Neural networks',
               2,
               None,
               'introduction-to-neural-networks'),
              ('Artificial neurons', 2, None, 'artificial-neurons'),
              ('Neural network types', 2, None, 'neural-network-types'),
              ('Feed-forward neural networks',
               2,
               None,
               'feed-forward-neural-networks'),
              ('Convolutional Neural Network',
               2,
               None,
               'convolutional-neural-network'),
              ('Recurrent neural networks',
               2,
               None,
               'recurrent-neural-networks'),
              ('Other types of networks', 2, None, 'other-types-of-networks'),
              ('Multilayer perceptrons', 2, None, 'multilayer-perceptrons'),
              ('Why multilayer perceptrons?',
               2,
               None,
               'why-multilayer-perceptrons'),
              ('Illustration of a single perceptron model and a '
               'multi-perceptron model',
               2,
               None,
               'illustration-of-a-single-perceptron-model-and-a-multi-perceptron-model'),
              ('Examples of XOR, OR and AND gates',
               2,
               None,
               'examples-of-xor-or-and-and-gates'),
              ('Does Logistic Regression do a better Job?',
               2,
               None,
               'does-logistic-regression-do-a-better-job'),
              ('Adding Neural Networks', 2, None, 'adding-neural-networks'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Matrix-vector notation', 3, None, 'matrix-vector-notation'),
              ('Matrix-vector notation  and activation',
               3,
               None,
               'matrix-vector-notation-and-activation'),
              ('Activation functions', 3, None, 'activation-functions'),
              ('Activation functions, Logistic and Hyperbolic ones',
               3,
               None,
               'activation-functions-logistic-and-hyperbolic-ones'),
              ('Relevance', 3, None, 'relevance')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week40-bs.html">Week 40: Gradient descent methods (continued) and start Neural networks</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week40-bs001.html#lecture-monday-september-29-2025" style="font-size: 80%;"><b>Lecture Monday September 29, 2025</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs002.html#suggested-readings-and-videos" style="font-size: 80%;"><b>Suggested readings and videos</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs003.html#lab-sessions-tuesday-and-wednesday" style="font-size: 80%;"><b>Lab sessions Tuesday and Wednesday</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs004.html#logistic-regression-from-last-week" style="font-size: 80%;"><b>Logistic Regression, from last week</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs005.html#classification-problems" style="font-size: 80%;"><b>Classification problems</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs006.html#optimization-and-deep-learning" style="font-size: 80%;"><b>Optimization and Deep learning</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs007.html#basics" style="font-size: 80%;"><b>Basics</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs008.html#two-parameters" style="font-size: 80%;"><b>Two parameters</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs009.html#maximum-likelihood" style="font-size: 80%;"><b>Maximum likelihood</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs010.html#the-cost-function-rewritten" style="font-size: 80%;"><b>The cost function rewritten</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs011.html#minimizing-the-cross-entropy" style="font-size: 80%;"><b>Minimizing the cross entropy</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs012.html#a-more-compact-expression" style="font-size: 80%;"><b>A more compact expression</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs013.html#extending-to-more-predictors" style="font-size: 80%;"><b>Extending to more predictors</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs014.html#including-more-classes" style="font-size: 80%;"><b>Including more classes</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs015.html#more-classes" style="font-size: 80%;"><b>More classes</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs016.html#optimization-the-central-part-of-any-machine-learning-algortithm" style="font-size: 80%;"><b>Optimization, the central part of any Machine Learning algortithm</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs017.html#revisiting-our-logistic-regression-case" style="font-size: 80%;"><b>Revisiting our Logistic Regression case</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs018.html#the-equations-to-solve" style="font-size: 80%;"><b>The equations to solve</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs019.html#solving-using-newton-raphson-s-method" style="font-size: 80%;"><b>Solving using Newton-Raphson's method</b></a></li>
     <!-- navigation toc: --> <li><a href="#example-code-for-logistic-regression" style="font-size: 80%;"><b>Example code for Logistic Regression</b></a></li>
     <!-- navigation toc: --> <li><a href="#synthetic-data-generation" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Synthetic data generation</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs021.html#using-scikit-learn" style="font-size: 80%;"><b>Using <b>Scikit-learn</b></b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs022.html#using-the-correlation-matrix" style="font-size: 80%;"><b>Using the correlation matrix</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs023.html#discussing-the-correlation-data" style="font-size: 80%;"><b>Discussing the correlation data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs024.html#other-measures-in-classification-studies" style="font-size: 80%;"><b>Other measures in classification studies</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs025.html#introduction-to-neural-networks" style="font-size: 80%;"><b>Introduction to Neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs026.html#artificial-neurons" style="font-size: 80%;"><b>Artificial neurons</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs027.html#neural-network-types" style="font-size: 80%;"><b>Neural network types</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs028.html#feed-forward-neural-networks" style="font-size: 80%;"><b>Feed-forward neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs029.html#convolutional-neural-network" style="font-size: 80%;"><b>Convolutional Neural Network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs030.html#recurrent-neural-networks" style="font-size: 80%;"><b>Recurrent neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs031.html#other-types-of-networks" style="font-size: 80%;"><b>Other types of networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs032.html#multilayer-perceptrons" style="font-size: 80%;"><b>Multilayer perceptrons</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs033.html#why-multilayer-perceptrons" style="font-size: 80%;"><b>Why multilayer perceptrons?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs034.html#illustration-of-a-single-perceptron-model-and-a-multi-perceptron-model" style="font-size: 80%;"><b>Illustration of a single perceptron model and a multi-perceptron model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs035.html#examples-of-xor-or-and-and-gates" style="font-size: 80%;"><b>Examples of XOR, OR and AND gates</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs036.html#does-logistic-regression-do-a-better-job" style="font-size: 80%;"><b>Does Logistic Regression do a better Job?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs037.html#adding-neural-networks" style="font-size: 80%;"><b>Adding Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs042.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs042.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs042.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs042.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs042.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs043.html#matrix-vector-notation" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs044.html#matrix-vector-notation-and-activation" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation  and activation</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs045.html#activation-functions" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs046.html#activation-functions-logistic-and-hyperbolic-ones" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions, Logistic and Hyperbolic ones</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs047.html#relevance" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Relevance</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0020"></a>
<!-- !split -->
<h2 id="example-code-for-logistic-regression" class="anchor">Example code for Logistic Regression </h2>

<p>Here we make a class for Logistic regression. The code uses a simple data set and includes both a binary case and a multiclass case.</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>

<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">LogisticRegression</span>:
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Logistic Regression for binary and multiclass classification.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, lr<span style="color: #666666">=0.01</span>, epochs<span style="color: #666666">=1000</span>, fit_intercept<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, verbose<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>):
        <span style="color: #008000">self</span><span style="color: #666666">.</span>lr <span style="color: #666666">=</span> lr                  <span style="color: #408080; font-style: italic"># Learning rate for gradient descent</span>
        <span style="color: #008000">self</span><span style="color: #666666">.</span>epochs <span style="color: #666666">=</span> epochs          <span style="color: #408080; font-style: italic"># Number of iterations</span>
        <span style="color: #008000">self</span><span style="color: #666666">.</span>fit_intercept <span style="color: #666666">=</span> fit_intercept  <span style="color: #408080; font-style: italic"># Whether to add intercept (bias)</span>
        <span style="color: #008000">self</span><span style="color: #666666">.</span>verbose <span style="color: #666666">=</span> verbose        <span style="color: #408080; font-style: italic"># Print loss during training if True</span>
        <span style="color: #008000">self</span><span style="color: #666666">.</span>weights <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">None</span>
        <span style="color: #008000">self</span><span style="color: #666666">.</span>multi_class <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>      <span style="color: #408080; font-style: italic"># Will be determined at fit time</span>

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">_add_intercept</span>(<span style="color: #008000">self</span>, X):
        <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;Add intercept term (column of ones) to feature matrix.&quot;&quot;&quot;</span>
        intercept <span style="color: #666666">=</span> np<span style="color: #666666">.</span>ones((X<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>], <span style="color: #666666">1</span>))
        <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>concatenate((intercept, X), axis<span style="color: #666666">=1</span>)

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">_sigmoid</span>(<span style="color: #008000">self</span>, z):
        <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;Sigmoid function for binary logistic.&quot;&quot;&quot;</span>
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">1</span> <span style="color: #666666">/</span> (<span style="color: #666666">1</span> <span style="color: #666666">+</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>z))

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">_softmax</span>(<span style="color: #008000">self</span>, Z):
        <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;Softmax function for multiclass logistic.&quot;&quot;&quot;</span>
        exp_Z <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(Z <span style="color: #666666">-</span> np<span style="color: #666666">.</span>max(Z, axis<span style="color: #666666">=1</span>, keepdims<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>))
        <span style="color: #008000; font-weight: bold">return</span> exp_Z <span style="color: #666666">/</span> np<span style="color: #666666">.</span>sum(exp_Z, axis<span style="color: #666666">=1</span>, keepdims<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">fit</span>(<span style="color: #008000">self</span>, X, y):
        <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Train the logistic regression model using gradient descent.</span>
<span style="color: #BA2121; font-style: italic">        Supports binary (sigmoid) and multiclass (softmax) based on y.</span>
<span style="color: #BA2121; font-style: italic">        &quot;&quot;&quot;</span>
        X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(X)
        y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(y)
        n_samples, n_features <span style="color: #666666">=</span> X<span style="color: #666666">.</span>shape

        <span style="color: #408080; font-style: italic"># Add intercept if needed</span>
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>fit_intercept:
            X <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>_add_intercept(X)
            n_features <span style="color: #666666">+=</span> <span style="color: #666666">1</span>

        <span style="color: #408080; font-style: italic"># Determine classes and mode (binary vs multiclass)</span>
        unique_classes <span style="color: #666666">=</span> np<span style="color: #666666">.</span>unique(y)
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">len</span>(unique_classes) <span style="color: #666666">&gt;</span> <span style="color: #666666">2</span>:
            <span style="color: #008000">self</span><span style="color: #666666">.</span>multi_class <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">True</span>
        <span style="color: #008000; font-weight: bold">else</span>:
            <span style="color: #008000">self</span><span style="color: #666666">.</span>multi_class <span style="color: #666666">=</span> <span style="color: #008000; font-weight: bold">False</span>

        <span style="color: #408080; font-style: italic"># ----- Multiclass case -----</span>
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>multi_class:
            n_classes <span style="color: #666666">=</span> <span style="color: #008000">len</span>(unique_classes)
            <span style="color: #408080; font-style: italic"># Map original labels to 0...n_classes-1</span>
            class_to_index <span style="color: #666666">=</span> {c: idx <span style="color: #008000; font-weight: bold">for</span> idx, c <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(unique_classes)}
            y_indices <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([class_to_index[c] <span style="color: #008000; font-weight: bold">for</span> c <span style="color: #AA22FF; font-weight: bold">in</span> y])
            <span style="color: #408080; font-style: italic"># Initialize weight matrix (features x classes)</span>
            <span style="color: #008000">self</span><span style="color: #666666">.</span>weights <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n_features, n_classes))

            <span style="color: #408080; font-style: italic"># One-hot encode y</span>
            Y_onehot <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((n_samples, n_classes))
            Y_onehot[np<span style="color: #666666">.</span>arange(n_samples), y_indices] <span style="color: #666666">=</span> <span style="color: #666666">1</span>

            <span style="color: #408080; font-style: italic"># Gradient descent</span>
            <span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">self</span><span style="color: #666666">.</span>epochs):
                scores <span style="color: #666666">=</span> X<span style="color: #666666">.</span>dot(<span style="color: #008000">self</span><span style="color: #666666">.</span>weights)          <span style="color: #408080; font-style: italic"># Linear scores (n_samples x n_classes)</span>
                probs <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>_softmax(scores)        <span style="color: #408080; font-style: italic"># Probabilities (n_samples x n_classes)</span>
                <span style="color: #408080; font-style: italic"># Compute gradient (features x classes)</span>
                gradient <span style="color: #666666">=</span> (<span style="color: #666666">1</span> <span style="color: #666666">/</span> n_samples) <span style="color: #666666">*</span> X<span style="color: #666666">.</span>T<span style="color: #666666">.</span>dot(probs <span style="color: #666666">-</span> Y_onehot)
                <span style="color: #408080; font-style: italic"># Update weights</span>
                <span style="color: #008000">self</span><span style="color: #666666">.</span>weights <span style="color: #666666">-=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>lr <span style="color: #666666">*</span> gradient

                <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>verbose <span style="color: #AA22FF; font-weight: bold">and</span> epoch <span style="color: #666666">%</span> <span style="color: #666666">100</span> <span style="color: #666666">==</span> <span style="color: #666666">0</span>:
                    <span style="color: #408080; font-style: italic"># Compute current loss (categorical cross-entropy)</span>
                    loss <span style="color: #666666">=</span> <span style="color: #666666">-</span>np<span style="color: #666666">.</span>sum(Y_onehot <span style="color: #666666">*</span> np<span style="color: #666666">.</span>log(probs <span style="color: #666666">+</span> <span style="color: #666666">1e-15</span>)) <span style="color: #666666">/</span> n_samples
                    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;[Epoch </span><span style="color: #BB6688; font-weight: bold">{</span>epoch<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">] Multiclass loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)

        <span style="color: #408080; font-style: italic"># ----- Binary case -----</span>
        <span style="color: #008000; font-weight: bold">else</span>:
            <span style="color: #408080; font-style: italic"># Convert y to 0/1 if not already</span>
            <span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> np<span style="color: #666666">.</span>array_equal(unique_classes, [<span style="color: #666666">0</span>, <span style="color: #666666">1</span>]):
                <span style="color: #408080; font-style: italic"># Map the two classes to 0 and 1</span>
                class0, class1 <span style="color: #666666">=</span> unique_classes
                y_binary <span style="color: #666666">=</span> np<span style="color: #666666">.</span>where(y <span style="color: #666666">==</span> class1, <span style="color: #666666">1</span>, <span style="color: #666666">0</span>)
            <span style="color: #008000; font-weight: bold">else</span>:
                y_binary <span style="color: #666666">=</span> y<span style="color: #666666">.</span>copy()<span style="color: #666666">.</span>astype(<span style="color: #008000">int</span>)

            <span style="color: #408080; font-style: italic"># Initialize weights vector (features,)</span>
            <span style="color: #008000">self</span><span style="color: #666666">.</span>weights <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n_features)

            <span style="color: #408080; font-style: italic"># Gradient descent</span>
            <span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">self</span><span style="color: #666666">.</span>epochs):
                linear_model <span style="color: #666666">=</span> X<span style="color: #666666">.</span>dot(<span style="color: #008000">self</span><span style="color: #666666">.</span>weights)     <span style="color: #408080; font-style: italic"># (n_samples,)</span>
                probs <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>_sigmoid(linear_model)   <span style="color: #408080; font-style: italic"># (n_samples,)</span>
                <span style="color: #408080; font-style: italic"># Gradient for binary cross-entropy</span>
                gradient <span style="color: #666666">=</span> (<span style="color: #666666">1</span> <span style="color: #666666">/</span> n_samples) <span style="color: #666666">*</span> X<span style="color: #666666">.</span>T<span style="color: #666666">.</span>dot(probs <span style="color: #666666">-</span> y_binary)
                <span style="color: #008000">self</span><span style="color: #666666">.</span>weights <span style="color: #666666">-=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>lr <span style="color: #666666">*</span> gradient

                <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>verbose <span style="color: #AA22FF; font-weight: bold">and</span> epoch <span style="color: #666666">%</span> <span style="color: #666666">100</span> <span style="color: #666666">==</span> <span style="color: #666666">0</span>:
                    <span style="color: #408080; font-style: italic"># Compute binary cross-entropy loss</span>
                    loss <span style="color: #666666">=</span> <span style="color: #666666">-</span>np<span style="color: #666666">.</span>mean(
                        y_binary <span style="color: #666666">*</span> np<span style="color: #666666">.</span>log(probs <span style="color: #666666">+</span> <span style="color: #666666">1e-15</span>) <span style="color: #666666">+</span> 
                        (<span style="color: #666666">1</span> <span style="color: #666666">-</span> y_binary) <span style="color: #666666">*</span> np<span style="color: #666666">.</span>log(<span style="color: #666666">1</span> <span style="color: #666666">-</span> probs <span style="color: #666666">+</span> <span style="color: #666666">1e-15</span>)
                    )
                    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;[Epoch </span><span style="color: #BB6688; font-weight: bold">{</span>epoch<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">] Binary loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">predict_prob</span>(<span style="color: #008000">self</span>, X):
        <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Compute probability estimates. Returns a 1D array for binary or</span>
<span style="color: #BA2121; font-style: italic">        a 2D array (n_samples x n_classes) for multiclass.</span>
<span style="color: #BA2121; font-style: italic">        &quot;&quot;&quot;</span>
        X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(X)
        <span style="color: #408080; font-style: italic"># Add intercept if the model used it</span>
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>fit_intercept:
            X <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>_add_intercept(X)
        scores <span style="color: #666666">=</span> X<span style="color: #666666">.</span>dot(<span style="color: #008000">self</span><span style="color: #666666">.</span>weights)
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>multi_class:
            <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>_softmax(scores)
        <span style="color: #008000; font-weight: bold">else</span>:
            <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>_sigmoid(scores)

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">predict</span>(<span style="color: #008000">self</span>, X):
        <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">        Predict class labels for samples in X.</span>
<span style="color: #BA2121; font-style: italic">        Returns integer class labels (0,1 for binary, or 0...C-1 for multiclass).</span>
<span style="color: #BA2121; font-style: italic">        &quot;&quot;&quot;</span>
        probs <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>predict_prob(X)
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>multi_class:
            <span style="color: #408080; font-style: italic"># Choose class with highest probability</span>
            <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>argmax(probs, axis<span style="color: #666666">=1</span>)
        <span style="color: #008000; font-weight: bold">else</span>:
            <span style="color: #408080; font-style: italic"># Threshold at 0.5 for binary</span>
            <span style="color: #008000; font-weight: bold">return</span> (probs <span style="color: #666666">&gt;=</span> <span style="color: #666666">0.5</span>)<span style="color: #666666">.</span>astype(<span style="color: #008000">int</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>The class implements the sigmoid and softmax internally. During fit(),
we check the number of classes: if more than 2, we set
self.multi_class=True and perform multinomial logistic regression. We
one-hot encode the target vector and update a weight matrix with
softmax probabilities. Otherwise, we do standard binary logistic
regression, converting labels to 0/1 if needed and updating a weight
vector. In both cases we use batch gradient descent on the
cross-entropy loss (we add a small epsilon 1e-15 to logs for numerical
stability). Progress (loss) can be printed if verbose=True.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># Evaluation Metrics</span>
<span style="color: #408080; font-style: italic">#We define helper functions for accuracy and cross-entropy loss. Accuracy is the fraction of correct predictions . For loss, we compute the appropriate cross-entropy:</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">accuracy_score</span>(y_true, y_pred):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;Accuracy = (# correct predictions) / (total samples).&quot;&quot;&quot;</span>
    y_true <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(y_true)
    y_pred <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(y_pred)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>mean(y_true <span style="color: #666666">==</span> y_pred)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">binary_cross_entropy</span>(y_true, y_prob):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Binary cross-entropy loss.</span>
<span style="color: #BA2121; font-style: italic">    y_true: true binary labels (0 or 1), y_prob: predicted probabilities for class 1.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    y_true <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(y_true)
    y_prob <span style="color: #666666">=</span> np<span style="color: #666666">.</span>clip(np<span style="color: #666666">.</span>array(y_prob), <span style="color: #666666">1e-15</span>, <span style="color: #666666">1-1e-15</span>)
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">-</span>np<span style="color: #666666">.</span>mean(y_true <span style="color: #666666">*</span> np<span style="color: #666666">.</span>log(y_prob) <span style="color: #666666">+</span> (<span style="color: #666666">1</span> <span style="color: #666666">-</span> y_true) <span style="color: #666666">*</span> np<span style="color: #666666">.</span>log(<span style="color: #666666">1</span> <span style="color: #666666">-</span> y_prob))

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">categorical_cross_entropy</span>(y_true, y_prob):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Categorical cross-entropy loss for multiclass.</span>
<span style="color: #BA2121; font-style: italic">    y_true: true labels (0...C-1), y_prob: array of predicted probabilities (n_samples x C).</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    y_true <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(y_true, dtype<span style="color: #666666">=</span><span style="color: #008000">int</span>)
    y_prob <span style="color: #666666">=</span> np<span style="color: #666666">.</span>clip(np<span style="color: #666666">.</span>array(y_prob), <span style="color: #666666">1e-15</span>, <span style="color: #666666">1-1e-15</span>)
    <span style="color: #408080; font-style: italic"># One-hot encode true labels</span>
    n_samples, n_classes <span style="color: #666666">=</span> y_prob<span style="color: #666666">.</span>shape
    one_hot <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros_like(y_prob)
    one_hot[np<span style="color: #666666">.</span>arange(n_samples), y_true] <span style="color: #666666">=</span> <span style="color: #666666">1</span>
    <span style="color: #408080; font-style: italic"># Compute cross-entropy</span>
    loss_vec <span style="color: #666666">=</span> <span style="color: #666666">-</span>np<span style="color: #666666">.</span>sum(one_hot <span style="color: #666666">*</span> np<span style="color: #666666">.</span>log(y_prob), axis<span style="color: #666666">=1</span>)
    <span style="color: #008000; font-weight: bold">return</span> np<span style="color: #666666">.</span>mean(loss_vec)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h3 id="synthetic-data-generation" class="anchor">Synthetic data generation </h3>

<p>Binary classification data: Create two Gaussian clusters in 2D. For example, class 0 around mean [-2,-2] and class 1 around [2,2].
Multiclass data: Create several Gaussian clusters (one per class) spread out in feature space.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">generate_binary_data</span>(n_samples<span style="color: #666666">=100</span>, n_features<span style="color: #666666">=2</span>, random_state<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Generate synthetic binary classification data.</span>
<span style="color: #BA2121; font-style: italic">    Returns (X, y) where X is (n_samples x n_features), y in {0,1}.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    rng <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>RandomState(random_state)
    <span style="color: #408080; font-style: italic"># Half samples for class 0, half for class 1</span>
    n0 <span style="color: #666666">=</span> n_samples <span style="color: #666666">//</span> <span style="color: #666666">2</span>
    n1 <span style="color: #666666">=</span> n_samples <span style="color: #666666">-</span> n0
    <span style="color: #408080; font-style: italic"># Class 0 around mean -2, class 1 around +2</span>
    mean0 <span style="color: #666666">=</span> <span style="color: #666666">-2</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>ones(n_features)
    mean1 <span style="color: #666666">=</span>  <span style="color: #666666">2</span> <span style="color: #666666">*</span> np<span style="color: #666666">.</span>ones(n_features)
    X0 <span style="color: #666666">=</span> rng<span style="color: #666666">.</span>randn(n0, n_features) <span style="color: #666666">+</span> mean0
    X1 <span style="color: #666666">=</span> rng<span style="color: #666666">.</span>randn(n1, n_features) <span style="color: #666666">+</span> mean1
    X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>vstack((X0, X1))
    y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([<span style="color: #666666">0</span>]<span style="color: #666666">*</span>n0 <span style="color: #666666">+</span> [<span style="color: #666666">1</span>]<span style="color: #666666">*</span>n1)
    <span style="color: #008000; font-weight: bold">return</span> X, y

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">generate_multiclass_data</span>(n_samples<span style="color: #666666">=150</span>, n_features<span style="color: #666666">=2</span>, n_classes<span style="color: #666666">=3</span>, random_state<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Generate synthetic multiclass data with n_classes Gaussian clusters.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    rng <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>RandomState(random_state)
    X <span style="color: #666666">=</span> []
    y <span style="color: #666666">=</span> []
    samples_per_class <span style="color: #666666">=</span> n_samples <span style="color: #666666">//</span> n_classes
    <span style="color: #008000; font-weight: bold">for</span> <span style="color: #008000">cls</span> <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n_classes):
        <span style="color: #408080; font-style: italic"># Random cluster center for each class</span>
        center <span style="color: #666666">=</span> rng<span style="color: #666666">.</span>uniform(<span style="color: #666666">-5</span>, <span style="color: #666666">5</span>, size<span style="color: #666666">=</span>n_features)
        Xi <span style="color: #666666">=</span> rng<span style="color: #666666">.</span>randn(samples_per_class, n_features) <span style="color: #666666">+</span> center
        yi <span style="color: #666666">=</span> [<span style="color: #008000">cls</span>] <span style="color: #666666">*</span> samples_per_class
        X<span style="color: #666666">.</span>append(Xi)
        y<span style="color: #666666">.</span>extend(yi)
    X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>vstack(X)
    y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(y)
    <span style="color: #008000; font-weight: bold">return</span> X, y


<span style="color: #408080; font-style: italic"># Generate and test on binary data</span>
X_bin, y_bin <span style="color: #666666">=</span> generate_binary_data(n_samples<span style="color: #666666">=200</span>, n_features<span style="color: #666666">=2</span>, random_state<span style="color: #666666">=42</span>)
model_bin <span style="color: #666666">=</span> LogisticRegression(lr<span style="color: #666666">=0.1</span>, epochs<span style="color: #666666">=1000</span>)
model_bin<span style="color: #666666">.</span>fit(X_bin, y_bin)
y_prob_bin <span style="color: #666666">=</span> model_bin<span style="color: #666666">.</span>predict_prob(X_bin)      <span style="color: #408080; font-style: italic"># probabilities for class 1</span>
y_pred_bin <span style="color: #666666">=</span> model_bin<span style="color: #666666">.</span>predict(X_bin)           <span style="color: #408080; font-style: italic"># predicted classes 0 or 1</span>

acc_bin <span style="color: #666666">=</span> accuracy_score(y_bin, y_pred_bin)
loss_bin <span style="color: #666666">=</span> binary_cross_entropy(y_bin, y_prob_bin)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Binary Classification - Accuracy: </span><span style="color: #BB6688; font-weight: bold">{</span>acc_bin<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.2f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">, Cross-Entropy Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss_bin<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.2f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
<span style="color: #408080; font-style: italic">#For multiclass:</span>
<span style="color: #408080; font-style: italic"># Generate and test on multiclass data</span>
X_multi, y_multi <span style="color: #666666">=</span> generate_multiclass_data(n_samples<span style="color: #666666">=300</span>, n_features<span style="color: #666666">=2</span>, n_classes<span style="color: #666666">=3</span>, random_state<span style="color: #666666">=1</span>)
model_multi <span style="color: #666666">=</span> LogisticRegression(lr<span style="color: #666666">=0.1</span>, epochs<span style="color: #666666">=1000</span>)
model_multi<span style="color: #666666">.</span>fit(X_multi, y_multi)
y_prob_multi <span style="color: #666666">=</span> model_multi<span style="color: #666666">.</span>predict_prob(X_multi)     <span style="color: #408080; font-style: italic"># (n_samples x 3) probabilities</span>
y_pred_multi <span style="color: #666666">=</span> model_multi<span style="color: #666666">.</span>predict(X_multi)          <span style="color: #408080; font-style: italic"># predicted labels 0,1,2</span>

acc_multi <span style="color: #666666">=</span> accuracy_score(y_multi, y_pred_multi)
loss_multi <span style="color: #666666">=</span> categorical_cross_entropy(y_multi, y_prob_multi)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Multiclass Classification - Accuracy: </span><span style="color: #BB6688; font-weight: bold">{</span>acc_multi<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.2f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">, Cross-Entropy Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss_multi<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.2f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)

<span style="color: #408080; font-style: italic"># CSV Export</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">csv</span>

<span style="color: #408080; font-style: italic"># Export binary results</span>
<span style="color: #008000; font-weight: bold">with</span> <span style="color: #008000">open</span>(<span style="color: #BA2121">&#39;binary_results.csv&#39;</span>, mode<span style="color: #666666">=</span><span style="color: #BA2121">&#39;w&#39;</span>, newline<span style="color: #666666">=</span><span style="color: #BA2121">&#39;&#39;</span>) <span style="color: #008000; font-weight: bold">as</span> f:
    writer <span style="color: #666666">=</span> csv<span style="color: #666666">.</span>writer(f)
    writer<span style="color: #666666">.</span>writerow([<span style="color: #BA2121">&quot;TrueLabel&quot;</span>, <span style="color: #BA2121">&quot;PredictedLabel&quot;</span>])
    <span style="color: #008000; font-weight: bold">for</span> true, pred <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">zip</span>(y_bin, y_pred_bin):
        writer<span style="color: #666666">.</span>writerow([true, pred])

<span style="color: #408080; font-style: italic"># Export multiclass results</span>
<span style="color: #008000; font-weight: bold">with</span> <span style="color: #008000">open</span>(<span style="color: #BA2121">&#39;multiclass_results.csv&#39;</span>, mode<span style="color: #666666">=</span><span style="color: #BA2121">&#39;w&#39;</span>, newline<span style="color: #666666">=</span><span style="color: #BA2121">&#39;&#39;</span>) <span style="color: #008000; font-weight: bold">as</span> f:
    writer <span style="color: #666666">=</span> csv<span style="color: #666666">.</span>writer(f)
    writer<span style="color: #666666">.</span>writerow([<span style="color: #BA2121">&quot;TrueLabel&quot;</span>, <span style="color: #BA2121">&quot;PredictedLabel&quot;</span>])
    <span style="color: #008000; font-weight: bold">for</span> true, pred <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">zip</span>(y_multi, y_pred_multi):
        writer<span style="color: #666666">.</span>writerow([true, pred])
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week40-bs019.html">&laquo;</a></li>
  <li><a href="._week40-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week40-bs012.html">13</a></li>
  <li><a href="._week40-bs013.html">14</a></li>
  <li><a href="._week40-bs014.html">15</a></li>
  <li><a href="._week40-bs015.html">16</a></li>
  <li><a href="._week40-bs016.html">17</a></li>
  <li><a href="._week40-bs017.html">18</a></li>
  <li><a href="._week40-bs018.html">19</a></li>
  <li><a href="._week40-bs019.html">20</a></li>
  <li class="active"><a href="._week40-bs020.html">21</a></li>
  <li><a href="._week40-bs021.html">22</a></li>
  <li><a href="._week40-bs022.html">23</a></li>
  <li><a href="._week40-bs023.html">24</a></li>
  <li><a href="._week40-bs024.html">25</a></li>
  <li><a href="._week40-bs025.html">26</a></li>
  <li><a href="._week40-bs026.html">27</a></li>
  <li><a href="._week40-bs027.html">28</a></li>
  <li><a href="._week40-bs028.html">29</a></li>
  <li><a href="._week40-bs029.html">30</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week40-bs047.html">48</a></li>
  <li><a href="._week40-bs021.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>

