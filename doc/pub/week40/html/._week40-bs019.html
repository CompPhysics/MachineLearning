<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week40.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week40-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 40: Gradient descent methods (continued) and start Neural networks">
<title>Week 40: Gradient descent methods (continued) and start Neural networks</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week40.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week40-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Lecture Monday September 30, 2024',
               2,
               None,
               'lecture-monday-september-30-2024'),
              ('Suggested readings and videos',
               2,
               None,
               'suggested-readings-and-videos'),
              ('Lab sessions Tuesday and Wednesday',
               2,
               None,
               'lab-sessions-tuesday-and-wednesday'),
              ('Automatic differentiation',
               2,
               None,
               'automatic-differentiation'),
              ('Using autograd', 2, None, 'using-autograd'),
              ('Autograd with more complicated functions',
               2,
               None,
               'autograd-with-more-complicated-functions'),
              ('More complicated functions using the elements of their '
               'arguments directly',
               2,
               None,
               'more-complicated-functions-using-the-elements-of-their-arguments-directly'),
              ('Functions using mathematical functions from Numpy',
               2,
               None,
               'functions-using-mathematical-functions-from-numpy'),
              ('More autograd', 2, None, 'more-autograd'),
              ('And  with loops', 2, None, 'and-with-loops'),
              ('Using recursion', 2, None, 'using-recursion'),
              ('Using Autograd with OLS', 2, None, 'using-autograd-with-ols'),
              ('Same code but now with momentum gradient descent',
               2,
               None,
               'same-code-but-now-with-momentum-gradient-descent'),
              ('Including Stochastic Gradient Descent with Autograd',
               2,
               None,
               'including-stochastic-gradient-descent-with-autograd'),
              ('Same code but now with momentum gradient descent',
               2,
               None,
               'same-code-but-now-with-momentum-gradient-descent'),
              ('Similar (second order function now) problem but now with '
               'AdaGrad',
               2,
               None,
               'similar-second-order-function-now-problem-but-now-with-adagrad'),
              ('RMSprop for adaptive learning rate with Stochastic Gradient '
               'Descent',
               2,
               None,
               'rmsprop-for-adaptive-learning-rate-with-stochastic-gradient-descent'),
              ('And finally "ADAM":"https://arxiv.org/pdf/1412.6980.pdf"',
               2,
               None,
               'and-finally-adam-https-arxiv-org-pdf-1412-6980-pdf'),
              ('And Logistic Regression', 2, None, 'and-logistic-regression'),
              ('Introducing "JAX":"https://jax.readthedocs.io/en/latest/"',
               2,
               None,
               'introducing-jax-https-jax-readthedocs-io-en-latest'),
              ('Getting started with Jax, note the way we import numpy',
               3,
               None,
               'getting-started-with-jax-note-the-way-we-import-numpy'),
              ('A warm-up example', 3, None, 'a-warm-up-example'),
              ('A more advanced example', 3, None, 'a-more-advanced-example'),
              ('Introduction to Neural networks',
               2,
               None,
               'introduction-to-neural-networks'),
              ('Artificial neurons', 2, None, 'artificial-neurons'),
              ('Neural network types', 2, None, 'neural-network-types'),
              ('Feed-forward neural networks',
               2,
               None,
               'feed-forward-neural-networks'),
              ('Convolutional Neural Network',
               2,
               None,
               'convolutional-neural-network'),
              ('Recurrent neural networks',
               2,
               None,
               'recurrent-neural-networks'),
              ('Other types of networks', 2, None, 'other-types-of-networks'),
              ('Multilayer perceptrons', 2, None, 'multilayer-perceptrons'),
              ('Why multilayer perceptrons?',
               2,
               None,
               'why-multilayer-perceptrons'),
              ('Illustration of a single perceptron model and a '
               'multi-perceptron model',
               2,
               None,
               'illustration-of-a-single-perceptron-model-and-a-multi-perceptron-model'),
              ('Examples of XOR, OR and AND gates',
               2,
               None,
               'examples-of-xor-or-and-and-gates'),
              ('Does Logistic Regression do a better Job?',
               2,
               None,
               'does-logistic-regression-do-a-better-job'),
              ('Adding Neural Networks', 2, None, 'adding-neural-networks'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Mathematical model', 2, None, 'mathematical-model'),
              ('Matrix-vector notation', 3, None, 'matrix-vector-notation'),
              ('Matrix-vector notation  and activation',
               3,
               None,
               'matrix-vector-notation-and-activation'),
              ('Activation functions', 3, None, 'activation-functions'),
              ('Activation functions, Logistic and Hyperbolic ones',
               3,
               None,
               'activation-functions-logistic-and-hyperbolic-ones'),
              ('Relevance', 3, None, 'relevance')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week40-bs.html">Week 40: Gradient descent methods (continued) and start Neural networks</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week40-bs001.html#lecture-monday-september-30-2024" style="font-size: 80%;"><b>Lecture Monday September 30, 2024</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs002.html#suggested-readings-and-videos" style="font-size: 80%;"><b>Suggested readings and videos</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs003.html#lab-sessions-tuesday-and-wednesday" style="font-size: 80%;"><b>Lab sessions Tuesday and Wednesday</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs004.html#automatic-differentiation" style="font-size: 80%;"><b>Automatic differentiation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs005.html#using-autograd" style="font-size: 80%;"><b>Using autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs006.html#autograd-with-more-complicated-functions" style="font-size: 80%;"><b>Autograd with more complicated functions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs007.html#more-complicated-functions-using-the-elements-of-their-arguments-directly" style="font-size: 80%;"><b>More complicated functions using the elements of their arguments directly</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs008.html#functions-using-mathematical-functions-from-numpy" style="font-size: 80%;"><b>Functions using mathematical functions from Numpy</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs009.html#more-autograd" style="font-size: 80%;"><b>More autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs010.html#and-with-loops" style="font-size: 80%;"><b>And  with loops</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs011.html#using-recursion" style="font-size: 80%;"><b>Using recursion</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs012.html#using-autograd-with-ols" style="font-size: 80%;"><b>Using Autograd with OLS</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs015.html#same-code-but-now-with-momentum-gradient-descent" style="font-size: 80%;"><b>Same code but now with momentum gradient descent</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs014.html#including-stochastic-gradient-descent-with-autograd" style="font-size: 80%;"><b>Including Stochastic Gradient Descent with Autograd</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs015.html#same-code-but-now-with-momentum-gradient-descent" style="font-size: 80%;"><b>Same code but now with momentum gradient descent</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs016.html#similar-second-order-function-now-problem-but-now-with-adagrad" style="font-size: 80%;"><b>Similar (second order function now) problem but now with AdaGrad</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs017.html#rmsprop-for-adaptive-learning-rate-with-stochastic-gradient-descent" style="font-size: 80%;"><b>RMSprop for adaptive learning rate with Stochastic Gradient Descent</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs018.html#and-finally-adam-https-arxiv-org-pdf-1412-6980-pdf" style="font-size: 80%;"><b>And finally "ADAM":"https://arxiv.org/pdf/1412.6980.pdf"</b></a></li>
     <!-- navigation toc: --> <li><a href="#and-logistic-regression" style="font-size: 80%;"><b>And Logistic Regression</b></a></li>
     <!-- navigation toc: --> <li><a href="#introducing-jax-https-jax-readthedocs-io-en-latest" style="font-size: 80%;"><b>Introducing "JAX":"https://jax.readthedocs.io/en/latest/"</b></a></li>
     <!-- navigation toc: --> <li><a href="#getting-started-with-jax-note-the-way-we-import-numpy" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Getting started with Jax, note the way we import numpy</a></li>
     <!-- navigation toc: --> <li><a href="#a-warm-up-example" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;A warm-up example</a></li>
     <!-- navigation toc: --> <li><a href="#a-more-advanced-example" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;A more advanced example</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs020.html#introduction-to-neural-networks" style="font-size: 80%;"><b>Introduction to Neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs021.html#artificial-neurons" style="font-size: 80%;"><b>Artificial neurons</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs022.html#neural-network-types" style="font-size: 80%;"><b>Neural network types</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs023.html#feed-forward-neural-networks" style="font-size: 80%;"><b>Feed-forward neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs024.html#convolutional-neural-network" style="font-size: 80%;"><b>Convolutional Neural Network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs025.html#recurrent-neural-networks" style="font-size: 80%;"><b>Recurrent neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs026.html#other-types-of-networks" style="font-size: 80%;"><b>Other types of networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs027.html#multilayer-perceptrons" style="font-size: 80%;"><b>Multilayer perceptrons</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs028.html#why-multilayer-perceptrons" style="font-size: 80%;"><b>Why multilayer perceptrons?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs029.html#illustration-of-a-single-perceptron-model-and-a-multi-perceptron-model" style="font-size: 80%;"><b>Illustration of a single perceptron model and a multi-perceptron model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs030.html#examples-of-xor-or-and-and-gates" style="font-size: 80%;"><b>Examples of XOR, OR and AND gates</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs031.html#does-logistic-regression-do-a-better-job" style="font-size: 80%;"><b>Does Logistic Regression do a better Job?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs032.html#adding-neural-networks" style="font-size: 80%;"><b>Adding Neural Networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs037.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs037.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs037.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs037.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs037.html#mathematical-model" style="font-size: 80%;"><b>Mathematical model</b></a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs038.html#matrix-vector-notation" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs039.html#matrix-vector-notation-and-activation" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Matrix-vector notation  and activation</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs040.html#activation-functions" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs041.html#activation-functions-logistic-and-hyperbolic-ones" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions, Logistic and Hyperbolic ones</a></li>
     <!-- navigation toc: --> <li><a href="._week40-bs042.html#relevance" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Relevance</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0019"></a>
<!-- !split -->
<h2 id="and-logistic-regression" class="anchor">And Logistic Regression </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">autograd.numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">autograd</span> <span style="color: #008000; font-weight: bold">import</span> grad

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">sigmoid</span>(x):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">0.5</span> <span style="color: #666666">*</span> (np<span style="color: #666666">.</span>tanh(x <span style="color: #666666">/</span> <span style="color: #666666">2.</span>) <span style="color: #666666">+</span> <span style="color: #666666">1</span>)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">logistic_predictions</span>(weights, inputs):
    <span style="color: #408080; font-style: italic"># Outputs probability of a label being true according to logistic model.</span>
    <span style="color: #008000; font-weight: bold">return</span> sigmoid(np<span style="color: #666666">.</span>dot(inputs, weights))

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">training_loss</span>(weights):
    <span style="color: #408080; font-style: italic"># Training loss is the negative log-likelihood of the training labels.</span>
    preds <span style="color: #666666">=</span> logistic_predictions(weights, inputs)
    label_probabilities <span style="color: #666666">=</span> preds <span style="color: #666666">*</span> targets <span style="color: #666666">+</span> (<span style="color: #666666">1</span> <span style="color: #666666">-</span> preds) <span style="color: #666666">*</span> (<span style="color: #666666">1</span> <span style="color: #666666">-</span> targets)
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">-</span>np<span style="color: #666666">.</span>sum(np<span style="color: #666666">.</span>log(label_probabilities))

<span style="color: #408080; font-style: italic"># Build a toy dataset.</span>
inputs <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[<span style="color: #666666">0.52</span>, <span style="color: #666666">1.12</span>,  <span style="color: #666666">0.77</span>],
                   [<span style="color: #666666">0.88</span>, <span style="color: #666666">-1.08</span>, <span style="color: #666666">0.15</span>],
                   [<span style="color: #666666">0.52</span>, <span style="color: #666666">0.06</span>, <span style="color: #666666">-1.30</span>],
                   [<span style="color: #666666">0.74</span>, <span style="color: #666666">-2.49</span>, <span style="color: #666666">1.39</span>]])
targets <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([<span style="color: #008000; font-weight: bold">True</span>, <span style="color: #008000; font-weight: bold">True</span>, <span style="color: #008000; font-weight: bold">False</span>, <span style="color: #008000; font-weight: bold">True</span>])

<span style="color: #408080; font-style: italic"># Define a function that returns gradients of training loss using Autograd.</span>
training_gradient_fun <span style="color: #666666">=</span> grad(training_loss)

<span style="color: #408080; font-style: italic"># Optimize weights using gradient descent.</span>
weights <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([<span style="color: #666666">0.0</span>, <span style="color: #666666">0.0</span>, <span style="color: #666666">0.0</span>])
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Initial loss:&quot;</span>, training_loss(weights))
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">100</span>):
    weights <span style="color: #666666">-=</span> training_gradient_fun(weights) <span style="color: #666666">*</span> <span style="color: #666666">0.01</span>

<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Trained loss:&quot;</span>, training_loss(weights))
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="introducing-jax-https-jax-readthedocs-io-en-latest" class="anchor">Introducing <a href="https://jax.readthedocs.io/en/latest/" target="_self">JAX</a> </h2>

<p>Presently, instead of using <b>autograd</b>, we recommend using <a href="https://jax.readthedocs.io/en/latest/" target="_self">JAX</a></p>

<p><b>JAX</b> is Autograd and <a href="https://www.tensorflow.org/xla" target="_self">XLA (Accelerated Linear Algebra))</a>,
brought together for high-performance numerical computing and machine learning research.
It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more.
</p>
<h3 id="getting-started-with-jax-note-the-way-we-import-numpy" class="anchor">Getting started with Jax, note the way we import numpy </h3>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">jax</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">jax.numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">jnp</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>

<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">jax</span> <span style="color: #008000; font-weight: bold">import</span> grad <span style="color: #008000; font-weight: bold">as</span> jax_grad
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h3 id="a-warm-up-example" class="anchor">A warm-up example </h3>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">function</span>(x):
    <span style="color: #008000; font-weight: bold">return</span> x<span style="color: #666666">**2</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">analytical_gradient</span>(x):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">2*</span>x

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">gradient_descent</span>(starting_point, learning_rate, num_iterations, solver<span style="color: #666666">=</span><span style="color: #BA2121">&quot;analytical&quot;</span>):
    x <span style="color: #666666">=</span> starting_point
    trajectory_x <span style="color: #666666">=</span> [x]
    trajectory_y <span style="color: #666666">=</span> [function(x)]

    <span style="color: #008000; font-weight: bold">if</span> solver <span style="color: #666666">==</span> <span style="color: #BA2121">&quot;analytical&quot;</span>:
        grad <span style="color: #666666">=</span> analytical_gradient    
    <span style="color: #008000; font-weight: bold">elif</span> solver <span style="color: #666666">==</span> <span style="color: #BA2121">&quot;jax&quot;</span>:
        grad <span style="color: #666666">=</span> jax_grad(function)
        x <span style="color: #666666">=</span> jnp<span style="color: #666666">.</span>float64(x)
        learning_rate <span style="color: #666666">=</span> jnp<span style="color: #666666">.</span>float64(learning_rate)

    <span style="color: #008000; font-weight: bold">for</span> _ <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(num_iterations):
        
        x <span style="color: #666666">=</span> x <span style="color: #666666">-</span> learning_rate <span style="color: #666666">*</span> grad(x)
        trajectory_x<span style="color: #666666">.</span>append(x)
        trajectory_y<span style="color: #666666">.</span>append(function(x))

    <span style="color: #008000; font-weight: bold">return</span> trajectory_x, trajectory_y

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-5</span>, <span style="color: #666666">5</span>, <span style="color: #666666">100</span>)
plt<span style="color: #666666">.</span>plot(x, function(x), label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;f(x)&quot;</span>)

descent_x, descent_y <span style="color: #666666">=</span> gradient_descent(<span style="color: #666666">5</span>, <span style="color: #666666">0.1</span>, <span style="color: #666666">10</span>, solver<span style="color: #666666">=</span><span style="color: #BA2121">&quot;analytical&quot;</span>)
jax_descend_x, jax_descend_y <span style="color: #666666">=</span> gradient_descent(<span style="color: #666666">5</span>, <span style="color: #666666">0.1</span>, <span style="color: #666666">10</span>, solver<span style="color: #666666">=</span><span style="color: #BA2121">&quot;jax&quot;</span>)

plt<span style="color: #666666">.</span>plot(descent_x, descent_y, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Gradient descent&quot;</span>, marker<span style="color: #666666">=</span><span style="color: #BA2121">&quot;o&quot;</span>)
plt<span style="color: #666666">.</span>plot(jax_descend_x, jax_descend_y, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;JAX&quot;</span>, marker<span style="color: #666666">=</span><span style="color: #BA2121">&quot;x&quot;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h3 id="a-more-advanced-example" class="anchor">A more advanced example </h3>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">backend <span style="color: #666666">=</span> np

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">function</span>(x):
    <span style="color: #008000; font-weight: bold">return</span> x<span style="color: #666666">*</span>backend<span style="color: #666666">.</span>sin(x<span style="color: #666666">**2</span> <span style="color: #666666">+</span> <span style="color: #666666">1</span>)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">analytical_gradient</span>(x):
    <span style="color: #008000; font-weight: bold">return</span> backend<span style="color: #666666">.</span>sin(x<span style="color: #666666">**2</span> <span style="color: #666666">+</span> <span style="color: #666666">1</span>) <span style="color: #666666">+</span> <span style="color: #666666">2*</span>x<span style="color: #666666">**2*</span>backend<span style="color: #666666">.</span>cos(x<span style="color: #666666">**2</span> <span style="color: #666666">+</span> <span style="color: #666666">1</span>)


x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-5</span>, <span style="color: #666666">5</span>, <span style="color: #666666">100</span>)
plt<span style="color: #666666">.</span>plot(x, function(x), label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;f(x)&quot;</span>)

descent_x, descent_y <span style="color: #666666">=</span> gradient_descent(<span style="color: #666666">1</span>, <span style="color: #666666">0.01</span>, <span style="color: #666666">300</span>, solver<span style="color: #666666">=</span><span style="color: #BA2121">&quot;analytical&quot;</span>)

<span style="color: #408080; font-style: italic"># Change the backend to JAX</span>
backend <span style="color: #666666">=</span> jnp
jax_descend_x, jax_descend_y <span style="color: #666666">=</span> gradient_descent(<span style="color: #666666">1</span>, <span style="color: #666666">0.01</span>, <span style="color: #666666">300</span>, solver<span style="color: #666666">=</span><span style="color: #BA2121">&quot;jax&quot;</span>)

plt<span style="color: #666666">.</span>scatter(descent_x, descent_y, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Gradient descent&quot;</span>, marker<span style="color: #666666">=</span><span style="color: #BA2121">&quot;v&quot;</span>, s<span style="color: #666666">=10</span>, color<span style="color: #666666">=</span><span style="color: #BA2121">&quot;red&quot;</span>) 
plt<span style="color: #666666">.</span>scatter(jax_descend_x, jax_descend_y, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;JAX&quot;</span>, marker<span style="color: #666666">=</span><span style="color: #BA2121">&quot;x&quot;</span>, s<span style="color: #666666">=5</span>, color<span style="color: #666666">=</span><span style="color: #BA2121">&quot;black&quot;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week40-bs018.html">&laquo;</a></li>
  <li><a href="._week40-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week40-bs011.html">12</a></li>
  <li><a href="._week40-bs012.html">13</a></li>
  <li><a href="._week40-bs013.html">14</a></li>
  <li><a href="._week40-bs014.html">15</a></li>
  <li><a href="._week40-bs015.html">16</a></li>
  <li><a href="._week40-bs016.html">17</a></li>
  <li><a href="._week40-bs017.html">18</a></li>
  <li><a href="._week40-bs018.html">19</a></li>
  <li class="active"><a href="._week40-bs019.html">20</a></li>
  <li><a href="._week40-bs020.html">21</a></li>
  <li><a href="._week40-bs021.html">22</a></li>
  <li><a href="._week40-bs022.html">23</a></li>
  <li><a href="._week40-bs023.html">24</a></li>
  <li><a href="._week40-bs024.html">25</a></li>
  <li><a href="._week40-bs025.html">26</a></li>
  <li><a href="._week40-bs026.html">27</a></li>
  <li><a href="._week40-bs027.html">28</a></li>
  <li><a href="._week40-bs028.html">29</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week40-bs042.html">43</a></li>
  <li><a href="._week40-bs020.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>

