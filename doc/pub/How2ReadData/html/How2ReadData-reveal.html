\
<!DOCTYPE html>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Data Analysis and Machine Learning: Getting started, our first data and Machine Learning encounters">

<title>Data Analysis and Machine Learning: Getting started, our first data and Machine Learning encounters</title>







<!-- reveal.js: http://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<style type="text/css">
    hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
    hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    .reveal .alert-text-small   { font-size: 80%;  }
    .reveal .alert-text-large   { font-size: 130%; }
    .reveal .alert-text-normal  { font-size: 90%;  }
    .reveal .alert {
             padding:8px 35px 8px 14px; margin-bottom:18px;
             text-shadow:0 1px 0 rgba(255,255,255,0.5);
             border:5px solid #bababa;
             -webkit-border-radius: 14px; -moz-border-radius: 14px;
             border-radius:14px;
             background-position: 10px 10px;
             background-repeat: no-repeat;
             background-size: 38px;
             padding-left: 30px; /* 55px; if icon */
     }
     .reveal .alert-block {padding-top:14px; padding-bottom:14px}
     .reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
     /*.reveal .alert li {margin-top: 1em}*/
     .reveal .alert-block p+p {margin-top:5px}
     /*.reveal .alert-notice { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
     .reveal .alert-summary  { background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
     .reveal .alert-warning { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
     .reveal .alert-question {background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */

</style>



<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>

<body>
<div class="reveal">

<!-- Any section element inside the <div class="slides"> container
     is displayed as a slide -->

<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    



<section>
<!-- ------------------- main content ---------------------- -->



<center><h1 style="text-align: center;">Data Analysis and Machine Learning: Getting started, our first data and Machine Learning encounters</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>&nbsp;<br>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>&nbsp;<br>
<center><h4>May 29, 2018</h4></center> <!-- date -->
<br>

<h2 id="___sec0">Introduction </h2>

<p>
Our emphasis throughout this series of lectures  
is on understanding the mathematical aspects of
different algorithms used in the fields of data analysis and machine learning.

<p>
However, where possible we will emphasize the
importance of using available software. We start thus with a hands-on
and top-down approach to machine learning. The aim is thus to start with
relevant data or data we have produced 
and use these to introduce statistical data analysis
concepts and machine learning algorithms before we delve into the
algorithms themselves. The examples we will use in the beginning, start with simple
polynomials with random noise added. We will use the Python
software package <a href="http://scikit-learn.org/stable/" target="_blank">Scikit-learn</a> and
introduce various machine learning algorithms to make fits of
the data and predictions. We move thereafter to more interesting
cases such as the simulation of financial transactions or disease
models. These are examples where we can easily set up the data and
then use machine learning algorithms included in for example
<b>scikit-learn</b>.

<p>
These examples will serve us the purpose of getting
started. Furthermore, they allow us to catch more than two birds with
a stone. They will allow us to bring in some programming specific
topics and tools as well as showing the power of various Python (and
R) packages for machine learning and statistical data analysis. In the
lectures on linear algebra we cover in more detail various programming
features of languages like Python and C++ (and other), we will also
look into more specific linear functions which are relevant for the
various algorithms we will discuss. Here, we will mainly focus on two
specific Python packages for Machine Learning, scikit-learn and
tensorflow (see below for links etc).  Moreover, the examples we
introduce will serve as inputs to many of our discussions later, as
well as allowing you to set up models and produce your own data and
get started with programming.

<h2 id="___sec1">Software and needed installations </h2>

<p>
We will make extensive use of Python as programming language and its
myriad of available libraries.  You will find
IPython/Jupyter notebooks invaluable in your work.  You can run <b>R</b>
codes in the Jupyter/IPython notebooks, with the immediate benefit of
visualizing your data. You can also use compiled languages like C++,
Rust, Fortran etc if you prefer. The focus in these lectures will be
on Python, but we will provide many code examples for those of you who
prefer R or compiled languages. You can integrate C++ codes and R in for example
a Jupyter notebook.

<p>
If you have Python installed (we recommend Python3) and you feel
pretty familiar with installing different packages, we recommend that
you install the following Python packages via <b>pip</b> as 

<ol>
<p><li> pip install numpy scipy matplotlib ipython scikit-learn mglearn sympy pandas pillow</li> 
</ol>
<p>

For Python3, replace <b>pip</b> with <b>pip3</b>.

<p>
For OSX users we recommend, after having installed Xcode, to
install <b>brew</b>. Brew allows for a seamless installation of additional
software via for example 

<ol>
<p><li> brew install python3</li>
</ol>
<p>

For Linux users, with its variety of distributions like for example the widely popular Ubuntu distribution,
you can use <b>pip</b> as well and simply install Python as 

<ol>
<p><li> sudo apt-get install python3  (or python for pyhton2.7)</li>
</ol>
<p>

etc etc.

<h2 id="___sec2">Python installers </h2>

<p>
If you don't want to perform these operations separately and venture
into the hassle of exploring how to set up dependencies and paths, we
recommend two widely used distrubutions which set up all relevant
dependencies for Python, namely 

<ul>
<p><li> <a href="https://docs.anaconda.com/" target="_blank">Anaconda</a>,</li> 
</ul>
<p>

which is an open source
distribution of the Python and R programming languages for large-scale
data processing, predictive analytics, and scientific computing, that
aims to simplify package management and deployment. Package versions
are managed by the package management system <b>conda</b>. 

<ul>
<p><li> <a href="https://www.enthought.com/product/canopy/" target="_blank">Enthought canopy</a></li> 
</ul>
<p>

is a Python
distribution for scientific and analytic computing distribution and
analysis environment, available for free and under a commercial
license.

<h2 id="___sec3">Installing R, C++, cython or Julia </h2>

<p>
You will also find it convenient to utilize R. Although we will mainly
use Python during lectures and in various projects and exercises, we
provide a full R set of codes for the same examples. Those of you
already familiar with R should feel free to continue using R, keeping
however an eye on the parallel Python set ups. Similarly, if you are a
Python afecionado, feel free to explore R as well.  Jupyter/Ipython
notebook allows you to run <b>R</b> codes interactively in your
browser. The software library <b>R</b> is tuned to statistically analysis
and allows for an easy usage of the tools we will discuss in these
texts.

<p>
To install <b>R</b> with Jupyter notebook 
<a href="https://mpacer.org/maths/r-kernel-for-ipython-notebook" target="_blank">follow the link here</a>

<h2 id="___sec4">Installing R, C++, cython, Numba etc </h2>

<p>
For the C++ aficionados, Jupyter/IPython notebook allows you also to
install C++ and run codes written in this language interactively in
the browser. Since we will emphasize writing many of the algorithms
yourself, you can thus opt for either Python or C++ (or Fortran or other compiled languages) as programming
languages.

<p>
To add more entropy, <b>cython</b> can also be used when running your
notebooks. It means that Python with the Jupyter/IPython notebook
setup allows you to integrate widely popular softwares and tools for
scientific computing. Similarly, the 
<a href="https://numba.pydata.org/" target="_blank">Numba Python package</a> delivers increased performance
capabilities with minimal rewrites of your codes.  With its
versatility, including symbolic operations, Python offers a unique
computational environment. Your Jupyter/IPython notebook can easily be
converted into a nicely rendered <b>PDF</b> file or a Latex file for
further processing. For example, convert to latex as

<p>

<!-- code=text typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>pycod jupyter nbconvert filename.ipynb --to latex 
</pre></div>
<p>
And to add more versatility, the Python package <a href="http://www.sympy.org/en/index.html" target="_blank">SymPy</a> is a Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS)  and is entirely written in Python.

<p>
Finally, if you wish to use the light mark-up language 
<a href="https://github.com/hplgit/doconce" target="_blank">doconce</a> you can convert a standard ascii text file into various HTML 
formats, ipython notebooks, latex files, pdf files etc with minimal edits.

<h2 id="___sec5">Simple linear regression model using <b>scikit-learn</b> </h2>

<p>
We start with perhaps our simplest possible example, using <b>scikit-learn</b> to perform linear regression analysis on a data set produced by us. 
What follows is a simple Python code where we have defined  function \( y \) in terms of the variable \( x \). Both are defined as vectors of dimension \( 1\times 100 \). The entries to the vector \( \hat{x} \)  are given by random numbers generated with a uniform distribution with entries \( x_i \in [0,1] \) (more about probability distribution functions later). These values are then used to define a function \( y(x) \) (tabulated again as a vector) with a linear dependence on \( x \) plus a random noise added via the normal distribution.

<p>
The Numpy functions are imported used the <b>import numpy as np</b>
statement and the random number generator for the uniform distribution
is called using the function <b>np.random.rand()</b>, where we specificy
that we want \( 100 \) random variables.  Using Numpy we define
automatically an array with the specified number of elements, \( 100 \) in
our case.  With the Numpy function <b>randn()</b> we can compute random
numbers with the normal distribution (mean value \( \mu \) equal to zero and
variance \( \sigma^2 \) set to one) and produce the values of \( y \) assuming a linear
dependence as function of \( x \)

<p>&nbsp;<br>
$$
y = 2x+N(0,1),
$$
<p>&nbsp;<br>

<p>
where \( N(0,1) \) represents random numbers generated by the normal
distribution.  From <b>scikit-learn</b> we import then the
<b>LinearRegression</b> functionality and make a prediction \( \tilde{y} =
\alpha + \beta x \) using the function <b>fit(x,y)</b>. We call the set of
data \( (\hat{x},\hat{y}) \) for our training data. The Python package
<b>scikit-learn</b> has also a functionality which extracts the above
fitting parameters \( \alpha \) and \( \beta \) (see below). Later we will
distinguish between training data and test data.

<p>
For plotting we use the Python package
<a href="https://matplotlib.org/" target="_blank">matplotlib</a> which produces publication
quality figures. Feel free to explore the extensive
<a href="https://matplotlib.org/gallery/index.html" target="_blank">gallery</a> of examples. In
this example we plot our original values of \( x \) and \( y \) as well as the
prediction <b>ypredict</b> (\( \tilde{y} \)), which attempts at fitting our
data with a straight line.

<p>
The Python code follows here.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22"># Importing various packages</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearRegression

x = np.random.rand(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)
y = <span style="color: #B452CD">2</span>*x+np.random.randn(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)
linreg = LinearRegression()
linreg.fit(x,y)
xnew = np.array([[<span style="color: #B452CD">0</span>],[<span style="color: #B452CD">1</span>]])
ypredict = linreg.predict(xnew)

plt.plot(xnew, ypredict, <span style="color: #CD5555">&quot;r-&quot;</span>)
plt.plot(x, y ,<span style="color: #CD5555">&#39;ro&#39;</span>)
plt.axis([<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1.0</span>,<span style="color: #B452CD">0</span>, <span style="color: #B452CD">5.0</span>])
plt.xlabel(<span style="color: #CD5555">r&#39;$x$&#39;</span>)
plt.ylabel(<span style="color: #CD5555">r&#39;$y$&#39;</span>)
plt.title(<span style="color: #CD5555">r&#39;Simple Linear Regression&#39;</span>)
plt.show()
</pre></div>
<p>
This example serves several aims. It allows us to demonstrate several
aspects of data analysis and later machine learning algorithms. The
immediate visualization shows that our linear fit is not
impressive. It goes through the data points, but there are many
outliers which are not reproduced by our linear regression.  We could
now play around with this small program and change for example the
factor in front of \( x \) and the normal distribution.  Try to change the
function \( y \) to

<p>&nbsp;<br>
$$
y = 10x+0.01 \times N(0,1),
$$
<p>&nbsp;<br>

<p>
where \( x \) is defined as before.  Does the fit look better? Indeed, by
reducing the role of the normal distribution we see immediately that
our linear prediction seemingly reproduces better the training
set. However, this testing 'by the eye' is obviouly not satisfactory in the
long run. Here we have only defined the training data and our model, and 
have not discussed a more rigorous approach to the <b>cost</b> function.

<p>
We need more rigorous criteria in defining whether we have succeeded or
not in modeling our training data.  You will be surprised to see that
many scientists seldomly venture beyond this 'by the eye' approach. A
standard approach for the <em>cost</em> function is the so-called \( \chi^2 \)
function

<p>&nbsp;<br>
$$ \chi^2 = \frac{1}{n}
\sum_{i=0}^{n-1}\frac{(y_i-\tilde{y}_i)^2}{\sigma_i^2}, 
$$
<p>&nbsp;<br>

<p>
where \( \sigma_i^2 \) is the variance (to be defined later) of the entry
\( y_i \).  We may not know the explicit value of \( \sigma_i^2 \), it serves
however the aim of scaling the equations and make the cost function
dimensionless.

<p>
Minimizing the cost function is a central aspect of
our discussions to come. Finding its minima as function of the model
parameters (\( \alpha \) and \( \beta \) in our case) will be a recurring
theme in these series of lectures. Essentially all machine learning
algorithms we will discuss center around the minimization of the
chosen cost function. This depends in turn on our specific
model for describing the data, a typical situation in supervised
learning. Automatizing the search for the minima of the cost function is a
central ingredient in all algorithms. Typical methods which are
employed are various variants of <b>gradient</b> methods. These will be
discussed in more detail later. Again, you'll be surprised to hear that
many practitioners minimize the above function ''by the eye', popularly dubbed as 
'chi by the eye'. That is, change a parameter and see (visually and numerically) that 
the  \( \chi^2 \) function becomes smaller.

<p>
There are many ways to define the cost function. A simpler approach is to look at the relative difference between the training data and the predicted data, that is we define 
the relative error as

<p>&nbsp;<br>
$$
\epsilon_{\mathrm{relative}}= \frac{\vert \hat{y} -\hat{\tilde{y}}\vert}{\vert \hat{y}\vert}.
$$
<p>&nbsp;<br>

We can modify easily the above Python code and plot the relative instead
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearRegression

x = np.random.rand(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)
y = <span style="color: #B452CD">5</span>*x+<span style="color: #B452CD">0.01</span>*np.random.randn(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)
linreg = LinearRegression()
linreg.fit(x,y)
ypredict = linreg.predict(x)

plt.plot(x, np.abs(ypredict-y)/<span style="color: #658b00">abs</span>(y), <span style="color: #CD5555">&quot;ro&quot;</span>)
plt.axis([<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1.0</span>,<span style="color: #B452CD">0.0</span>, <span style="color: #B452CD">0.5</span>])
plt.xlabel(<span style="color: #CD5555">r&#39;$x$&#39;</span>)
plt.ylabel(<span style="color: #CD5555">r&#39;$\epsilon_{\mathrm{relative}}$&#39;</span>)
plt.title(<span style="color: #CD5555">r&#39;Relative error&#39;</span>)
plt.show()
</pre></div>
<p>
Depending on the parameter in front of the normal distribution, we may
have a small or larger relative error. Try to play around with
different training data sets and study (graphically) the value of the
relative error.

<p>
As mentioned above, <b>scikit-learn</b> has an impressive functionality.
We can for example extract the values of \( \alpha \) and \( \beta \) and
their error estimates, or the variance and standard deviation and many
other properties from the statistical data analysis.

<p>
Here we show an
example of the functionality of scikit-learn.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span> 
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span> 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearRegression 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> mean_squared_error, r2_score, mean_squared_log_error, mean_absolute_error

x = np.random.rand(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)
y = <span style="color: #B452CD">2.0</span>+ <span style="color: #B452CD">5</span>*x+<span style="color: #B452CD">0.5</span>np.random.randn(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)
linreg = LinearRegression()
linreg.fit(x,y)
ypredict = linreg.predict(x)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;The intercept alpha: \n&#39;</span>, linreg.intercept_)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Coefficient beta : \n&#39;</span>, linreg.coef_)
<span style="color: #228B22"># The mean squared error                               </span>
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Mean squared error: %.2f&quot;</span> % mean_squared_error(y, ypredict))
<span style="color: #228B22"># Explained variance score: 1 is perfect prediction                                 </span>
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Variance score: %.2f&#39;</span> % r2_score(y, ypredict))
<span style="color: #228B22"># Mean squared log error                                                        </span>
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Mean squared log error: %.2f&#39;</span> % mean_squared_log_error(y, ypredict) )
<span style="color: #228B22"># Mean absolute error                                                           </span>
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Mean absolute error: %.2f&#39;</span> % mean_absolute_error(y, ypredict))
plt.plot(x, ypredict, <span style="color: #CD5555">&quot;r-&quot;</span>)
plt.plot(x, y ,<span style="color: #CD5555">&#39;ro&#39;</span>)
plt.axis([<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>,<span style="color: #B452CD">1.5</span>, <span style="color: #B452CD">7.0</span>])
plt.xlabel(<span style="color: #CD5555">r&#39;$x$&#39;</span>)
plt.ylabel(<span style="color: #CD5555">r&#39;$y$&#39;</span>)
plt.title(<span style="color: #CD5555">r&#39;Linear Regression fit &#39;</span>)
plt.show()
</pre></div>
<p>
The function <b>coef</b> gives us the parameter \( \beta \) of our fit while <b>intercept</b> yields 
\( \alpha \). Depending on the constant in front of the normal distribution, we get values near or far from \( alpha =2 \) and \( \beta =5 \). Try to play around with different parameters in front of the normal distribution. The function <b>meansquarederror</b> gives us the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss defined as
<p>&nbsp;<br>
$$ MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
$$
<p>&nbsp;<br>

<p>
The smaller the value, the better the fit. Ideally we would like to
have an MSE equal zero.  The attentive reader has probably recognized
this function as being similar to the \( \chi^2 \) function defined above.

<p>
The <b>r2score</b> function computes \( R^2 \), the coefficient of
determination. It provides a measure of how well future samples are
likely to be predicted by the model. Best possible score is 1.0 and it
can be negative (because the model can be arbitrarily worse). A
constant model that always predicts the expected value of \( \hat{y} \),
disregarding the input features, would get a \( R^2 \) score of \( 0.0 \).

<p>
If \( \tilde{\hat{y}}_i \) is the predicted value of the \( i-th \) sample and \( y_i \) is the corresponding true value, then the score \( R^2 \) is defined as
<p>&nbsp;<br>
$$
R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
$$
<p>&nbsp;<br>

where we have defined the mean value  of \( \hat{y} \) as
<p>&nbsp;<br>
$$
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
$$
<p>&nbsp;<br>

Another quantity will meet again in our discussions of regression analysis is 
 mean absolute error (MAE), a risk metric corresponding to the expected value of the absolute error loss or what we call the \( l1 \)-norm loss. In our discussion above we presented the relative error.
The MAE is defined as follows
<p>&nbsp;<br>
$$
\text{MAE}(\hat{y}, \hat{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n-1} \left| y_i - \tilde{y}_i \right|.
$$
<p>&nbsp;<br>

Finally we present the 
squared logarithmic (quadratic) error
<p>&nbsp;<br>
$$
\text{MSLE}(\hat{y}, \hat{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n - 1} (\log_e (1 + y_i) - \log_e (1 + \tilde{y}_i) )^2,
$$
<p>&nbsp;<br>

<p>
where \( \log_e (x) \) stands for the natural logarithm of \( x \). This error
estimate is best to use when targets having exponential growth, such
as population counts, average sales of a commodity over a span of
years etc.

<p>
We will discuss in more
detail these and other functions in the various lectures.  We conclude this part with another example. Instead of 
a linear \( x \)-dependence we study now a cubic polynomial and use the polynomial regression analysis tools of scikit-learn.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">random</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> Ridge
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.pipeline</span> <span style="color: #8B008B; font-weight: bold">import</span> make_pipeline
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearRegression

x=np.linspace(<span style="color: #B452CD">0.02</span>,<span style="color: #B452CD">0.98</span>,<span style="color: #B452CD">200</span>)
noise = np.asarray(random.sample((<span style="color: #658b00">range</span>(<span style="color: #B452CD">200</span>)),<span style="color: #B452CD">200</span>))
y=x**<span style="color: #B452CD">3</span>*noise
yn=x**<span style="color: #B452CD">3</span>*<span style="color: #B452CD">100</span>
poly3 = PolynomialFeatures(degree=<span style="color: #B452CD">3</span>)
X = poly3.fit_transform(x[:,np.newaxis])
clf3 = LinearRegression()
clf3.fit(X,y)

Xplot=poly3.fit_transform(x[:,np.newaxis])
poly3_plot=plt.plot(x, clf3.predict(Xplot), label=<span style="color: #CD5555">&#39;Cubic Fit&#39;</span>)
plt.plot(x,yn, color=<span style="color: #CD5555">&#39;red&#39;</span>, label=<span style="color: #CD5555">&quot;True Cubic&quot;</span>)
plt.scatter(x, y, label=<span style="color: #CD5555">&#39;Data&#39;</span>, color=<span style="color: #CD5555">&#39;orange&#39;</span>, s=<span style="color: #B452CD">15</span>)
plt.legend()
plt.show()

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">error</span>(a):
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> y:
        err=(y-yn)/yn
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">abs</span>(np.sum(err))/<span style="color: #658b00">len</span>(err)

<span style="color: #8B008B; font-weight: bold">print</span> (error(y))
</pre></div>
<p>
Similarly, using <b>R</b>, we can perform similar studies. The following <b>R</b> code illustrates this.

<h2 id="___sec6">Non-Linear Least squares in R  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<!-- code=r (!bc r) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">set.seed</span>(<span style="color: #B452CD">1485</span>)
len = <span style="color: #B452CD">24</span>
x = runif(len)
y = x^<span style="color: #B452CD">3</span>+rnorm(len, <span style="color: #B452CD">0</span>,<span style="color: #B452CD">0.06</span>)
ds = <span style="color: #00688B; font-weight: bold">data.frame</span>(x = x, y = y)
str(ds)
plot( y ~ x, main =<span style="color: #CD5555">&quot;Known cubic with noise&quot;</span>)
s  = <span style="color: #8B008B; font-weight: bold">seq</span>(<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>,length =<span style="color: #B452CD">100</span>)
lines(s, s^<span style="color: #B452CD">3</span>, lty =<span style="color: #B452CD">2</span>, col =<span style="color: #CD5555">&quot;green&quot;</span>)
m = nls(y ~ <span style="color: #8B008B; font-weight: bold">I</span>(x^power), data = ds, start = <span style="color: #00688B; font-weight: bold">list</span>(power=<span style="color: #B452CD">1</span>), trace = <span style="color: #658b00">T</span>)
<span style="color: #8B008B; font-weight: bold">class</span>(m)
<span style="color: #8B008B; font-weight: bold">summary</span>(m)
power = <span style="color: #8B008B; font-weight: bold">round</span>(<span style="color: #8B008B; font-weight: bold">summary</span>(m)$coefficients[<span style="color: #B452CD">1</span>], <span style="color: #B452CD">3</span>)
power.se = <span style="color: #8B008B; font-weight: bold">round</span>(<span style="color: #8B008B; font-weight: bold">summary</span>(m)$coefficients[<span style="color: #B452CD">2</span>], <span style="color: #B452CD">3</span>)
plot(y ~ x, main = <span style="color: #CD5555">&quot;Fitted power model&quot;</span>, sub = <span style="color: #CD5555">&quot;Blue: fit; green: known&quot;</span>)
s = <span style="color: #8B008B; font-weight: bold">seq</span>(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, length = <span style="color: #B452CD">100</span>)
lines(s, s^<span style="color: #B452CD">3</span>, lty = <span style="color: #B452CD">2</span>, col = <span style="color: #CD5555">&quot;green&quot;</span>)
lines(s, predict(m, <span style="color: #00688B; font-weight: bold">list</span>(x = s)), lty = <span style="color: #B452CD">1</span>, col = <span style="color: #CD5555">&quot;blue&quot;</span>)
text(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">0.5</span>, <span style="color: #8B008B; font-weight: bold">paste</span>(<span style="color: #CD5555">&quot;y =x^ (&quot;</span>, power, <span style="color: #CD5555">&quot; +/- &quot;</span>, power.se, <span style="color: #CD5555">&quot;)&quot;</span>, sep = <span style="color: #CD5555">&quot;&quot;</span>), pos = <span style="color: #B452CD">4</span>)
</pre></div>

</div>

<p>
In our lectures on regression analysis (and other ones as well), we will discuss in more details various <b>R</b> functionalities.

<p>
Another useful Python package is
<a href="https://pandas.pydata.org/" target="_blank">pandas</a>, which is an open source library
providing high-performance, easy-to-use data structures and data
analysis tools for Python. The following simple example shows how we can, in an easy way make tables of our data. Here we define a data set which includes names, city of residence and age, and displays the data in an easy to read way. We will see repeated use of <b>pandas</b>, in particular in connection with classification of data.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">IPython.display</span> <span style="color: #8B008B; font-weight: bold">import</span> display
data = {<span style="color: #CD5555">&#39;Name&#39;</span>: [<span style="color: #CD5555">&quot;John&quot;</span>, <span style="color: #CD5555">&quot;Anna&quot;</span>, <span style="color: #CD5555">&quot;Peter&quot;</span>, <span style="color: #CD5555">&quot;Linda&quot;</span>], <span style="color: #CD5555">&#39;Location&#39;</span>: [<span style="color: #CD5555">&quot;Nairobi&quot;</span>, <span style="color: #CD5555">&quot;Napoli&quot;</span>, <span style="color: #CD5555">&quot;London&quot;</span>, <span style="color: #CD5555">&quot;Buenos Aires&quot;</span>], <span style="color: #CD5555">&#39;Age&#39;</span>:[<span style="color: #B452CD">51</span>, <span style="color: #B452CD">21</span>, <span style="color: #B452CD">34</span>, <span style="color: #B452CD">45</span>]}
data_pandas = pd.DataFrame(data)
display(data_pandas)
</pre></div>

<h2 id="___sec7">Examples </h2>

<p>
We present here several examples, with pertinent Python codes that we
will us to illustrate various machine learning methods and ways to
analyze, from simple to complex, various data sets. Many of these
examples allow us to generate the data we want to analyze, following
much of the same philosophy we discussed above when
fitting various polynomials.

<p>
We start with a simple exponential growth model that is meant to mimick an ecoli lab experiment.
We can easily model this system and then produce the data used to train various machine learning algorithms.
Another model from the life sciences is the so-called predator-prey model from ecology. Thereafter we present 
a simple model for financial transactions before moving to a random walk model and ending with 
the simulation of velocities of a non-interacting atom or molecule confined to move in  a one-dimensional region.

<h3 id="___sec8">Ecoli lab experiment </h3>

<p>
A typical pattern seen in population models is that the population grows faster and faster. <a href="http://www.zo.utexas.edu/courses/Thoc/PopGrowth.html" target="_blank">Why? Is there an underlying (general) mechanism</a>?
Here we will construct a model for cell growth based on a simple difference equation for the growth. We make the following assumptions
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<ol>
<p><li> Cells divide after \( T \) seconds on average (one generation)</li>
<p><li> \( 2N \) celles divide into twice as many new cells \( \Delta N \) in a time
   interval \( \Delta t \) as \( N \) cells would: \( \Delta N \propto N \)</li>
<p><li> \( N \) cells result in twice as many new individuals \( \Delta N \) in
   time \( 2\Delta t \) as in time \( \Delta t \): \( \Delta N \propto\Delta t \)</li>
<p><li> Same proportionality wrt death</li> 
<p><li> Proposed model: \( \Delta N = b\Delta t N - d\Delta tN \) for some unknown
   constants \( b \) (births) and \( d \) (deaths)</li>
<p><li> Describe evolution in discrete time: \( t_n=n\Delta t \)</li>
<p><li> Program-friendly notation: \( N \) at \( t_n \) is \( N^n \)</li>
<p><li> Math model: \( N^{n+1} = N^n + r\Delta t\, N \) (with \( \ r=b-d \))</li>
<p><li> Program model: <code>N[n+1] = N[n] + r*dt*N[n]</code></li>
</ol>
</div>

<p>
The difference equation can be programmed in a simple was, and in order to get started we
set \( r=1.5 \), \( N^0=1 \), \( \Delta t=0.5 \). The program reads

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>

t = np.linspace(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">10</span>, <span style="color: #B452CD">21</span>)  <span style="color: #228B22"># 20 intervals in [0, 10]</span>
dt = t[<span style="color: #B452CD">1</span>] - t[<span style="color: #B452CD">0</span>]
N = np.zeros(t.size)

N[<span style="color: #B452CD">0</span>] = <span style="color: #B452CD">1</span>
r = <span style="color: #B452CD">0.5</span>

<span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">0</span>, N.size-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>):
    N[n+<span style="color: #B452CD">1</span>] = N[n] + r*dt*N[n]
    <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&#39;N[%d]=%.1f&#39;</span> % (n+<span style="color: #B452CD">1</span>, N[n+<span style="color: #B452CD">1</span>])
</pre></div>
<p>
and it generates the following output
<p>

<!-- code=text typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>N[1]=1.2
N[2]=1.6
N[3]=2.0
N[4]=2.4
N[5]=3.1
N[6]=3.8
N[7]=4.8
N[8]=6.0
N[9]=7.5
N[10]=9.3
N[11]=11.6
N[12]=14.6
N[13]=18.2
N[14]=22.7
N[15]=28.4
N[16]=35.5
N[17]=44.4
N[18]=55.5
N[19]=69.4
N[20]=86.7
</pre></div>
<p>
This forms our data which later will define our training set. 
In this case we defined the value of the parameter \( r \). We could alternatively assume that we just received the 
above data file and where asked to use find \( r \). How can we estimate \( r \) from data?

<p>
We can use the difference equation with the experimental data
<p>&nbsp;<br>
$$ N^{n+1} = N^n + r\Delta t N^n$$
<p>&nbsp;<br>

Suppose now that  \( N^{n+1} \) and \( N^n \) are known from data. Then we could solve with respect to  \( r \) as follows
<p>&nbsp;<br>
$$ r = \frac{N^{n+1}-N^n}{N^n\Delta t} $$
<p>&nbsp;<br>

Suppose we set \( t_1=600 \), \( t_2=1200 \),
\( N^1=140 \) and \( N^2=250 \). We obtain then \( r=0.0013 \). The exact value is \( r = 0.000694 \)
The following code plot 
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>

<span style="color: #228B22"># Estimate r</span>
data = np.loadtxt(<span style="color: #CD5555">&#39;ecoli.csv&#39;</span>, delimiter=<span style="color: #CD5555">&#39;,&#39;</span>)
t_e = data[:,<span style="color: #B452CD">0</span>]
N_e = data[:,<span style="color: #B452CD">1</span>]
i = <span style="color: #B452CD">2</span>  <span style="color: #228B22"># Data point (i,i+1) used to estimate r</span>
r = (N_e[i+<span style="color: #B452CD">1</span>] - N_e[i])/(N_e[i]*(t_e[i+<span style="color: #B452CD">1</span>] - t_e[i]))
<span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&#39;Estimated r=%.5f&#39;</span> % r
<span style="color: #228B22"># Can experiment with r values and see if the model can</span>
<span style="color: #228B22"># match the data better</span>

T = <span style="color: #B452CD">1200</span>     <span style="color: #228B22"># cell can divide after T sec</span>
t_max = <span style="color: #B452CD">5</span>*T  <span style="color: #228B22"># 5 generations in experiment</span>
t = np.linspace(<span style="color: #B452CD">0</span>, t_max, <span style="color: #B452CD">1000</span>)
dt = t[<span style="color: #B452CD">1</span>] - t[<span style="color: #B452CD">0</span>]
N = np.zeros(t.size)

N[<span style="color: #B452CD">0</span>] = <span style="color: #B452CD">100</span>
<span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">0</span>, <span style="color: #658b00">len</span>(t)-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>):
    N[n+<span style="color: #B452CD">1</span>] = N[n] + r*dt*N[n]

<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
plt.plot(t, N, <span style="color: #CD5555">&#39;r-&#39;</span>, t_e, N_e, <span style="color: #CD5555">&#39;bo&#39;</span>)
plt.xlabel(<span style="color: #CD5555">&#39;time [s]&#39;</span>);  plt.ylabel(<span style="color: #CD5555">&#39;N&#39;</span>)
plt.legend([<span style="color: #CD5555">&#39;model&#39;</span>, <span style="color: #CD5555">&#39;experiment&#39;</span>], loc=<span style="color: #CD5555">&#39;upper left&#39;</span>)
plt.show()
</pre></div>
<p>
We can then change the parameter \( r \) in the program and play around to make a better fit. By now we know that this
'search bythe eye' approach is not the most optimal one.

<h3 id="___sec9">Predator-Prey model from ecology </h3>

<p>
The population dynamics of a simple predator-prey system is a
classical example shown in many biology textbooks when ecological
systems are discussed. The system contains all elements of the
scientific method:

<ul>
 <p><li> The set up of a specific hypothesis combined with</li>
 <p><li> the experimental methods needed (one can study existing data or perform experiments)</li>
 <p><li> analyzing and interpreting the data and performing further experiments if needed</li>
 <p><li> trying to extract general behaviors and extract eventual laws or patterns</li>
 <p><li> develop mathematical relations for the uncovered regularities/laws and test these by per forming new experiments</li>
</ul>
<p>

Lots of data about populations of hares and lynx collected from furs in Hudson Bay, Canada, are available. It is known that the populations oscillate. Why?
Here we start by

<ol>
<p><li> plotting the data</li>
<p><li> derive a simple model for the population dynamics</li>
<p><li> (fitting parameters in the model to the data)</li>
<p><li> using the model predict the evolution other predator-pray systems</li>
</ol>
<p>

Most mammalian predators rely on a variety of prey, which complicates mathematical modeling; however, a few predators have become highly specialized and seek almost exclusively a single prey species. An example of this simplified predator-prey interaction is seen in Canadian northern forests, where the populations of the lynx and the snowshoe hare are intertwined in a life and death struggle.

<p>
One reason that this particular system has been so extensively studied is that the Hudson Bay company kept careful records of all furs from the early 1800s into the 1900s. The records for the furs collected by the Hudson Bay company showed distinct oscillations (approximately 12 year periods), suggesting that these species caused almost periodic fluctuations of each other's populations. The table here shows data from 1900 to 1920.

<p>
<table border="1">
<thead>
<tr><th align="center">Year</th> <th align="center">Hares (x1000)</th> <th align="center">Lynx (x1000)</th> </tr>
</thead>
<tbody>
<tr><td align="left">   1900    </td> <td align="right">   30.0             </td> <td align="right">   4.0             </td> </tr>
<tr><td align="left">   1901    </td> <td align="right">   47.2             </td> <td align="right">   6.1             </td> </tr>
<tr><td align="left">   1902    </td> <td align="right">   70.2             </td> <td align="right">   9.8             </td> </tr>
<tr><td align="left">   1903    </td> <td align="right">   77.4             </td> <td align="right">   35.2            </td> </tr>
<tr><td align="left">   1904    </td> <td align="right">   36.3             </td> <td align="right">   59.4            </td> </tr>
<tr><td align="left">   1905    </td> <td align="right">   20.6             </td> <td align="right">   41.7            </td> </tr>
<tr><td align="left">   1906    </td> <td align="right">   18.1             </td> <td align="right">   19.0            </td> </tr>
<tr><td align="left">   1907    </td> <td align="right">   21.4             </td> <td align="right">   13.0            </td> </tr>
<tr><td align="left">   1908    </td> <td align="right">   22.0             </td> <td align="right">   8.3             </td> </tr>
<tr><td align="left">   1909    </td> <td align="right">   25.4             </td> <td align="right">   9.1             </td> </tr>
<tr><td align="left">   1910    </td> <td align="right">   27.1             </td> <td align="right">   7.4             </td> </tr>
<tr><td align="left">   1911    </td> <td align="right">   40.3             </td> <td align="right">   8.0             </td> </tr>
<tr><td align="left">   1912    </td> <td align="right">   57               </td> <td align="right">   12.3            </td> </tr>
<tr><td align="left">   1913    </td> <td align="right">   76.6             </td> <td align="right">   19.5            </td> </tr>
<tr><td align="left">   1914    </td> <td align="right">   52.3             </td> <td align="right">   45.7            </td> </tr>
<tr><td align="left">   1915    </td> <td align="right">   19.5             </td> <td align="right">   51.1            </td> </tr>
<tr><td align="left">   1916    </td> <td align="right">   11.2             </td> <td align="right">   29.7            </td> </tr>
<tr><td align="left">   1917    </td> <td align="right">   7.6              </td> <td align="right">   15.8            </td> </tr>
<tr><td align="left">   1918    </td> <td align="right">   14.6             </td> <td align="right">   9.7             </td> </tr>
<tr><td align="left">   1919    </td> <td align="right">   16.2             </td> <td align="right">   10.1            </td> </tr>
<tr><td align="left">   1920    </td> <td align="right">   24.7             </td> <td align="right">   8.6             </td> </tr>
</tbody>
</table>
<p>

<!-- code=python (!bc pypro) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span>  <span style="color: #008b45; text-decoration: underline">matplotlib</span> <span style="color: #8B008B; font-weight: bold">import</span> pyplot <span style="color: #8B008B; font-weight: bold">as</span> plt

<span style="color: #228B22"># Load in data file</span>
data = np.loadtxt(<span style="color: #CD5555">&#39;src/Hudson_Bay.csv&#39;</span>, delimiter=<span style="color: #CD5555">&#39;,&#39;</span>, skiprows=<span style="color: #B452CD">1</span>)
<span style="color: #228B22"># Make arrays containing x-axis and hares and lynx populations</span>
year = data[:,<span style="color: #B452CD">0</span>]
hares = data[:,<span style="color: #B452CD">1</span>]
lynx = data[:,<span style="color: #B452CD">2</span>]

plt.plot(year, hares ,<span style="color: #CD5555">&#39;b-+&#39;</span>, year, lynx, <span style="color: #CD5555">&#39;r-o&#39;</span>)
plt.axis([<span style="color: #B452CD">1900</span>,<span style="color: #B452CD">1920</span>,<span style="color: #B452CD">0</span>, <span style="color: #B452CD">100.0</span>])
plt.xlabel(<span style="color: #CD5555">r&#39;Year&#39;</span>)
plt.ylabel(<span style="color: #CD5555">r&#39;Numbers of hares and lynx &#39;</span>)
plt.legend((<span style="color: #CD5555">&#39;Hares&#39;</span>,<span style="color: #CD5555">&#39;Lynx&#39;</span>), loc=<span style="color: #CD5555">&#39;upper right&#39;</span>)
plt.title(<span style="color: #CD5555">r&#39;Population of hares and lynx from 1900-1920 (x1000)}&#39;</span>)
plt.savefig(<span style="color: #CD5555">&#39;Hudson_Bay_data.pdf&#39;</span>)
plt.savefig(<span style="color: #CD5555">&#39;Hudson_Bay_data.png&#39;</span>)
plt.show()
</pre></div>
<p>
<br /><br /><center><p><img src="fig/Hudson_Bay_data.png" align="bottom" width=700></p></center><br /><br />

<p>
We see from the plot that there are indeed fluctuations.
We would like to create a mathematical model that explains these
population fluctuations. Ecologists have predicted that in a simple
predator-prey system that a rise in prey population is followed (with
a lag) by a rise in the predator population. When the predator
population is sufficiently high, then the prey population begins
dropping. After the prey population falls, then the predator
population falls, which allows the prey population to recover and
complete one cycle of this interaction. Thus, we see that
qualitatively oscillations occur. Can a mathematical model predict
this? What causes cycles to slow or speed up? What affects the
amplitude of the oscillation or do you expect to see the oscillations
damp to a stable equilibrium? The models tend to ignore factors like
climate and other complicating factors. How significant are these?

<ul>
 <p><li> We see oscillations in the data</li>
 <p><li> What causes cycles to slow or speed up?</li>
 <p><li> What affects the amplitude of the oscillation or do you expect to see the oscillations damp to a stable equilibrium?</li>
 <p><li> With a model we can better <em>understand the data</em></li>
 <p><li> More important: Can we understand the ecology dynamics of predator-pray populations?</li>
</ul>
<p>

The classical way (in all books) is to present the Lotka-Volterra equations:

<p>&nbsp;<br>
$$
\begin{align*}
\frac{dH}{dt} &= H(a - b L)\\
\frac{dL}{dt} &= - L(d - c  H)
\end{align*}
$$
<p>&nbsp;<br>

<p>
Here,

<ul>
 <p><li> \( H \) is the number of preys</li>
 <p><li> \( L \) the number of predators</li>
 <p><li> \( a \), \( b \), \( d \), \( c \) are parameters</li>
</ul>
<p>

The population of hares evolves due to births and deaths exactly as a bacteria population:

<p>&nbsp;<br>
$$
\Delta H = a \Delta t H^n
$$
<p>&nbsp;<br>

However, hares have an additional loss in the population because
they are eaten by lynx.
All the hares and lynx can form
\( H\cdot L \) pairs in total. When such pairs meet during a time
interval \( \Delta t \), there is some
small probablity that the lynx will eat the hare.
So in fraction \( b\Delta t HL \), the lynx eat hares. This
loss of hares must be accounted for. Subtracted in the equation for hares:

<p>&nbsp;<br>
$$ \Delta H = a\Delta t H^n - b \Delta t H^nL^n$$
<p>&nbsp;<br>

<p>
We assume that the primary growth for the lynx population depends on sufficient food for raising lynx kittens, which implies an adequate source of nutrients from predation on hares. Thus, the growth of the lynx population does not only depend of how many lynx there are, but on how many hares they can eat.
In a time interval \( \Delta t HL \) hares and lynx can meet, and in a
fraction \( b\Delta t HL \) the lynx eats the hare. All of this does not
contribute to the growth of lynx, again just a fraction of
\( b\Delta t HL \) that we write as
\( d\Delta t HL \). In addition, lynx die just as in the population
dynamics with one isolated animal population, leading to a loss
\( -c\Delta t L \).
The accounting of lynx then looks like
<p>&nbsp;<br>
$$ \Delta L = d\Delta t H^nL^n - c\Delta t L^n$$
<p>&nbsp;<br>

<p>
By writing up the definition of \( \Delta H \) and \( \Delta L \), and putting
all assumed known terms \( H^n \) and \( L^n \) on the right-hand side, we have

<p>&nbsp;<br>
$$ H^{n+1} = H^n + a\Delta t H^n - b\Delta t H^n L^n $$
<p>&nbsp;<br>

<p>&nbsp;<br>
$$ L^{n+1} = L^n + d\Delta t H^nL^n - c\Delta t L^n $$
<p>&nbsp;<br>

<p>
Note:

<ul>
 <p><li> These equations are ready to be implemented!</li>
 <p><li> But to start, we need \( H^0 \) and \( L^0 \) (which we can get from the data)</li>
 <p><li> We also need values for \( a \), \( b \), \( d \), \( c \)</li>
 <p><li> As always, models tend to be general - as here, applicable
   to &quot;all&quot; predator-pray systems</li>
 <p><li> The critical issue is whether the <em>interaction</em> between hares and lynx
   is sufficiently well modeled by \( \hbox{const}HL \)</li>
 <p><li> The parameters \( a \), \( b \), \( d \), and \( c \) must be
   estimated from data</li>
</ul>
<p>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<!-- code=python (!bc pypro) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">solver</span>(m, H0, L0, dt, a, b, c, d, t0):
    <span style="color: #CD5555">&quot;&quot;&quot;Solve the difference equations for H and L over m years</span>
<span style="color: #CD5555">    with time step dt (measured in years.&quot;&quot;&quot;</span>

    num_intervals = <span style="color: #658b00">int</span>(m/<span style="color: #658b00">float</span>(dt))
    t = np.linspace(t0, t0 + m, num_intervals+<span style="color: #B452CD">1</span>)
    H = np.zeros(t.size)
    L = np.zeros(t.size)

    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Init:&#39;</span>, H0, L0, dt)
    H[<span style="color: #B452CD">0</span>] = H0
    L[<span style="color: #B452CD">0</span>] = L0

    <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">0</span>, <span style="color: #658b00">len</span>(t)-<span style="color: #B452CD">1</span>):
        H[n+<span style="color: #B452CD">1</span>] = H[n] + a*dt*H[n] - b*dt*H[n]*L[n]
        L[n+<span style="color: #B452CD">1</span>] = L[n] + d*dt*H[n]*L[n] - c*dt*L[n]
    <span style="color: #8B008B; font-weight: bold">return</span> H, L, t

<span style="color: #228B22"># Load in data file</span>
data = np.loadtxt(<span style="color: #CD5555">&#39;src/Hudson_Bay.csv&#39;</span>, delimiter=<span style="color: #CD5555">&#39;,&#39;</span>, skiprows=<span style="color: #B452CD">1</span>)
<span style="color: #228B22"># Make arrays containing x-axis and hares and lynx populations</span>
t_e = data[:,<span style="color: #B452CD">0</span>]
H_e = data[:,<span style="color: #B452CD">1</span>]
L_e = data[:,<span style="color: #B452CD">2</span>]

<span style="color: #228B22"># Simulate using the model</span>
H, L, t = solver(m=<span style="color: #B452CD">20</span>, H0=<span style="color: #B452CD">34.91</span>, L0=<span style="color: #B452CD">3.857</span>, dt=<span style="color: #B452CD">0.1</span>,
                 a=<span style="color: #B452CD">0.4807</span>, b=<span style="color: #B452CD">0.02482</span>, c=<span style="color: #B452CD">0.9272</span>, d=<span style="color: #B452CD">0.02756</span>,
                 t0=<span style="color: #B452CD">1900</span>)

<span style="color: #228B22"># Visualize simulations and data</span>
plt.plot(t_e, H_e, <span style="color: #CD5555">&#39;b-+&#39;</span>, t_e, L_e, <span style="color: #CD5555">&#39;r-o&#39;</span>, t, H, <span style="color: #CD5555">&#39;m--&#39;</span>, t, L, <span style="color: #CD5555">&#39;k--&#39;</span>)
plt.xlabel(<span style="color: #CD5555">&#39;Year&#39;</span>)
plt.ylabel(<span style="color: #CD5555">&#39;Numbers of hares and lynx&#39;</span>)
plt.axis([<span style="color: #B452CD">1900</span>, <span style="color: #B452CD">1920</span>, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">140</span>])
plt.title(<span style="color: #CD5555">r&#39;Population of hares and lynx 1900-1920 (x1000)&#39;</span>)
plt.legend((<span style="color: #CD5555">&#39;H_e&#39;</span>, <span style="color: #CD5555">&#39;L_e&#39;</span>, <span style="color: #CD5555">&#39;H&#39;</span>, <span style="color: #CD5555">&#39;L&#39;</span>), loc=<span style="color: #CD5555">&#39;upper left&#39;</span>)
plt.savefig(<span style="color: #CD5555">&#39;Hudson_Bay_sim.pdf&#39;</span>)
plt.savefig(<span style="color: #CD5555">&#39;Hudson_Bay_sim.png&#39;</span>)
plt.show()
</pre></div>

</div>

<p>
<br /><br /><center><p><img src="fig/Hudson_Bay_sim.png" align="bottom" width=700></p></center><br /><br />

<p>
We will later perform a least-square fitting. Then we can find optimal
values for the parameters \( a \), \( b \), \( d \), \( c \). In our calculations here
we set \( a=0.4807 \), \( b=0.02482 \), \( d=0.9272 \) and \( c=0.02756 \). These
parameters result in a slightly modified initial conditions, namely
\( H(0) = 34.91 \) and \( L(0)=3.857 \).

<p>
The following Python demonstrates how we can use linear regression to fit for example the population of lynx.
Similarly, we have also used a decision tree algorithm to fit the lynx population data. As expected, the linear regression is not exactly impressive 
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">IPython.display</span> <span style="color: #8B008B; font-weight: bold">import</span> display
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sklearn</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearRegression
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeRegressor


data = np.loadtxt(<span style="color: #CD5555">&#39;src/Hudson_Bay.csv&#39;</span>, delimiter=<span style="color: #CD5555">&#39;,&#39;</span>, skiprows=<span style="color: #B452CD">1</span>)
x = data[:,<span style="color: #B452CD">0</span>]
y = data[:,<span style="color: #B452CD">1</span>]
line = np.linspace(<span style="color: #B452CD">1900</span>,<span style="color: #B452CD">1920</span>,<span style="color: #B452CD">1000</span>,endpoint=<span style="color: #658b00">False</span>).reshape(-<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>)
reg = DecisionTreeRegressor(min_samples_split=<span style="color: #B452CD">3</span>).fit(x.reshape(-<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>),y.reshape(-<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>))
plt.plot(line, reg.predict(line), label=<span style="color: #CD5555">&quot;decision tree&quot;</span>)
regline = LinearRegression().fit(x.reshape(-<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>),y.reshape(-<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>))
plt.plot(line, regline.predict(line), label= <span style="color: #CD5555">&quot;Linear Regression&quot;</span>)
plt.plot(x, y, label= <span style="color: #CD5555">&quot;Linear Regression&quot;</span>)
plt.show()
</pre></div>
<p>
The similar code for linear regression in <b>R</b> reads
<p>

<!-- code=r (!bc r) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>HudsonBay = read.csv(<span style="color: #CD5555">&quot;src/Hudson_Bay.csv&quot;</span>,header=<span style="color: #658b00">T</span>)
fix(HudsonBay)
<span style="color: #8B008B; font-weight: bold">dim</span>(HudsonBay)
<span style="color: #8B008B; font-weight: bold">names</span>(HudsonBay)
plot(HudsonBay$Year, HudsonBay$Hares..x1000.)
<span style="color: #8B008B; font-weight: bold">attach</span>(HudsonBay)
plot(Year, Hares..x1000.)
plot(Year, Hares..x1000., col=<span style="color: #CD5555">&quot;red&quot;</span>, varwidth=<span style="color: #658b00">T</span>, xlab=<span style="color: #CD5555">&quot;Years&quot;</span>, ylab=<span style="color: #CD5555">&quot;Haresx 1000&quot;</span>)
<span style="color: #8B008B; font-weight: bold">summary</span>(HudsonBay)
<span style="color: #8B008B; font-weight: bold">summary</span>(Hares..x1000.)
<span style="color: #8B008B; font-weight: bold">library</span>(MASS)
<span style="color: #8B008B; font-weight: bold">library</span>(ISLR)
scatter.smooth(x=Year, y = Hares..x1000.)
linearMod = lm(Hares..x1000. ~ Year)
<span style="color: #8B008B; font-weight: bold">print</span>(linearMod)
<span style="color: #8B008B; font-weight: bold">summary</span>(linearMod)
plot(linearMod)
confint(linearMod)
predict(linearMod,<span style="color: #00688B; font-weight: bold">data.frame</span>(Year=<span style="color: #00688B; font-weight: bold">c</span>(<span style="color: #B452CD">1910</span>,<span style="color: #B452CD">1914</span>,<span style="color: #B452CD">1920</span>)),interval=<span style="color: #CD5555">&quot;confidence&quot;</span>)
</pre></div>

<h3 id="___sec10">Simulating financial transactions </h3>

<p>
The aim here is to simulate financial transactions among financial agents
using Monte Carlo methods. The final goal is to extract a distribution of income  as function
of the income \( m \).   From Pareto's work (<a href="http://www.institutcoppet.org/2012/05/08/cours-deconomie-politique-1896-de-vilfredo-pareto" target="_blank">V.&nbsp;Pareto, 1897</a>) it is known from empirical studies
that the higher end of the distribution of money follows a distribution 
<p>&nbsp;<br>
$$
w_m\propto m^{-1-\alpha},
$$
<p>&nbsp;<br>

with \( \alpha\in [1,2] \). We will here follow the analysis made by <a href="http://www.sciencedirect.com/science/article/pii/S0378437104004327" target="_blank">Patriarca and collaborators</a>.

<p>
Here we will study numerically the relation between the micro-dynamic relations among financial 
agents and the  resulting macroscopic money distribution.

<p>
We assume we have \( N \) agents that exchange money in pairs \( (i,j) \). We assume also that all agents
start with the same amount of money \( m_0 > 0 \). At a given 'time step', we choose randomly a pair
of agents \( (i,j) \) and let a transaction take place. This means that agent \( i \)'s money \( m_i \) changes
to \( m_i' \) and similarly we have \( m_j\rightarrow m_j' \). 
Money is conserved during a transaction, meaning that
<p>&nbsp;<br>
$$
\begin{equation}
  m_i+m_j=m_i'+m_j'.
\tag{1}
\end{equation}
$$
<p>&nbsp;<br>

The change is done via a random reassignement (a random number) \( \epsilon \), meaning that

<p>&nbsp;<br>
$$
\begin{equation*}
m_i' = \epsilon(m_i+m_j),
\end{equation*}
$$
<p>&nbsp;<br>

leading to

<p>&nbsp;<br>
$$
\begin{equation*}
m_j'= (1-\epsilon)(m_i+m_j).
\end{equation*}
$$
<p>&nbsp;<br>

The number \( \epsilon \) is extracted from a uniform distribution.
In this simple model, no agents are left with a debt, that is \( m\ge 0 \).
Due to the conservation law above, one can show that the system relaxes toward an equilibrium
state given by a Gibbs distribution

<p>&nbsp;<br>
$$
\begin{equation*}
w_m=\beta \exp{(-\beta m)},
\end{equation*}
$$
<p>&nbsp;<br>

with

<p>&nbsp;<br>
$$
\begin{equation*}
\beta = \frac{1}{\langle m\rangle},
\end{equation*}
$$
<p>&nbsp;<br>

and \( \langle m\rangle=\sum_i m_i/N=m_0 \), the average money.
It means that after equilibrium has been reached that the majority of agents is left with a small
number of money, while the number of richest agents, those with \( m \) larger than a specific value \( m' \),
exponentially decreases with \( m' \).

<p>
We assume that we have \( N=500 \) agents.   In each simulation, we need a sufficiently large number of transactions, say \( 10^7 \). Our aim is find the final equilibrium distribution \( w_m \). In order to do that we would need
several runs of the above simulations, at least \( 10^3-10^4 \) runs (experiments).

<p>
Our task is to first set up an algorithm which simulates the above transactions with an initial
  amount \( m_0 \).
  The challenge here is to figure out a Monte Carlo  simulation  based on the
  above equations.
  You will in particular need to make an algorithm which sets up a histogram as function of \( m \).
  This histogram contains the number of times a value \( m \) is registered and represents
  \( w_m\Delta m \). You will need to set up a value for the interval \( \Delta m \)  (typically \( 0.01-0.05 \)).
  That means you need to account for the number of times you register an income in the interval
  \( m,m+\Delta m \). The number of times you register this income, represents the value that enters the histogram.
  You will also need to find a criterion for when the equilibrium situation has been reached.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22">#!/usr/bin/env python</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.mlab</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">mlab</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">random</span>

<span style="color: #228B22"># initialize the rng with a seed</span>
random.seed()
<span style="color: #228B22"># Hard coding of input parameters</span>
Agents  = <span style="color: #B452CD">500</span>
MCcounts = <span style="color: #B452CD">1000</span>
Transactions = <span style="color: #B452CD">100000</span>
startMoney = <span style="color: #B452CD">1.0</span>
Lambda = <span style="color: #B452CD">0.0</span>
FinancialAgents = startMoney*np.ones(Agents)
<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span> (<span style="color: #B452CD">1</span>, MCcounts, <span style="color: #B452CD">1</span>):
    <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span> (<span style="color: #B452CD">1</span>, Transactions, <span style="color: #B452CD">1</span>):
        agent_i = <span style="color: #658b00">int</span>(Agents*random.random())
        agent_j = <span style="color: #658b00">int</span>(Agents*random.random())
        epsilon = random.random()
        <span style="color: #8B008B; font-weight: bold">if</span> agent_i != agent_j:
           m1 = Lambda*FinancialAgents[agent_i] + (<span style="color: #B452CD">1</span>-Lambda)*epsilon*(FinancialAgents[agent_i] + FinancialAgents[agent_j])
           m2 = Lambda*FinancialAgents[agent_j] + (<span style="color: #B452CD">1</span>-Lambda)*(<span style="color: #B452CD">1</span>-epsilon)*(FinancialAgents[agent_i] + FinancialAgents[agent_j])
           FinancialAgents[agent_i] = m1
           FinancialAgents[agent_j] = m2

<span style="color: #228B22"># the histogram of the data</span>
n, bins, patches = plt.hist(FinancialAgents, <span style="color: #B452CD">50</span>, facecolor=<span style="color: #CD5555">&#39;green&#39;</span>)

plt.xlabel(<span style="color: #CD5555">&#39;$x$&#39;</span>)
plt.ylabel(<span style="color: #CD5555">&#39;Distribution of wealth&#39;</span>)
plt.title(<span style="color: #CD5555">r&#39;Money&#39;</span>)
plt.axis([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">10</span>, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">500</span>])
plt.grid(<span style="color: #658b00">True</span>)
plt.show()
</pre></div>
<p>
We can then change our model to allow for a saving criterion, meaning that the agents save
  a fraction \( \lambda \) of the money they have before the transaction is made. The final distribution will then no longer be given by Gibbs distribution. It could also include a taxation on financial transactions.

<p>
  The conservation law of Eq. <a href="#mjx-eqn-1">(1)</a> holds, but the money to be shared in a transaction between
  agent \( i \) and agent \( j \) is now \( (1-\lambda)(m_i+m_j) \). This means that we have

<p>&nbsp;<br>
$$
\begin{equation*}
  m_i' = \lambda m_i+\epsilon(1-\lambda)(m_i+m_j),
  \end{equation*}
$$
<p>&nbsp;<br>

  and

<p>&nbsp;<br>
$$
\begin{equation*}
  m_j' = \lambda m_j+(1-\epsilon)(1-\lambda)(m_i+m_j),
  \end{equation*}
$$
<p>&nbsp;<br>

  which can be written as

<p>&nbsp;<br>
$$
\begin{equation*}
  m_i'=m_i+\delta m
  \end{equation*}
$$
<p>&nbsp;<br>

  and

<p>&nbsp;<br>
$$
\begin{equation*}
  m_j'=m_j-\delta m,
  \end{equation*}
$$
<p>&nbsp;<br>

  with

<p>&nbsp;<br>
$$
\begin{equation*}
  \delta m=(1-\lambda)(\epsilon m_j-(1-\epsilon)m_i),
  \end{equation*}
$$
<p>&nbsp;<br>

  showing how money is conserved during a transaction.
  Select values of \( \lambda =0.25,0.5 \) and \( \lambda=0.9 \) and try to extract the corresponding
  equilibrium distributions and compare these with the Gibbs distribution. We will use this model to 
extract a parametrization of the above curves, see for example <a href="http://www.sciencedirect.com/science/article/pii/S0378437104004327" target="_blank">Patriarca and collaborators</a>.

<h3 id="___sec11">Particle in one dimension and velocity distribution </h3>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22"># Program to test the Metropolis algorithm with one particle at given temp in one dimension</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.mlab</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">mlab</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">random</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> sqrt, exp, log
<span style="color: #228B22"># initialize the rng with a seed</span>
random.seed()
<span style="color: #228B22"># Hard coding of input parameters</span>
MCcycles = <span style="color: #B452CD">100000</span>
Temperature = <span style="color: #B452CD">2.0</span>
beta = <span style="color: #B452CD">1.</span>/Temperature
InitialVelocity = -<span style="color: #B452CD">2.0</span>
CurrentVelocity = InitialVelocity
Energy = <span style="color: #B452CD">0.5</span>*InitialVelocity*InitialVelocity
VelocityRange = <span style="color: #B452CD">10</span>*sqrt(Temperature)
VelocityStep = <span style="color: #B452CD">2</span>*VelocityRange/<span style="color: #B452CD">10.</span>
AverageEnergy = Energy
AverageEnergy2 = Energy*Energy
VelocityValues = np.zeros(MCcycles)
<span style="color: #228B22"># The Monte Carlo sampling with Metropolis starts here</span>
<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span> (<span style="color: #B452CD">1</span>, MCcycles, <span style="color: #B452CD">1</span>):
    TrialVelocity = CurrentVelocity + (<span style="color: #B452CD">2.0</span>*random.random() - <span style="color: #B452CD">1.0</span>)*VelocityStep
    EnergyChange = <span style="color: #B452CD">0.5</span>*(TrialVelocity*TrialVelocity -CurrentVelocity*CurrentVelocity);
    <span style="color: #8B008B; font-weight: bold">if</span> random.random() &lt;= exp(-beta*EnergyChange):
        CurrentVelocity = TrialVelocity
        Energy += EnergyChange
        VelocityValues[i] = CurrentVelocity
    AverageEnergy += Energy
    AverageEnergy2 += Energy*Energy
<span style="color: #228B22">#Final averages</span>
AverageEnergy = AverageEnergy/MCcycles
AverageEnergy2 = AverageEnergy2/MCcycles
Variance = AverageEnergy2 - AverageEnergy*AverageEnergy
<span style="color: #8B008B; font-weight: bold">print</span>(AverageEnergy, Variance)
n, bins, patches = plt.hist(VelocityValues, <span style="color: #B452CD">400</span>, facecolor=<span style="color: #CD5555">&#39;green&#39;</span>)

plt.xlabel(<span style="color: #CD5555">&#39;$v$&#39;</span>)
plt.ylabel(<span style="color: #CD5555">&#39;Velocity distribution P(v)&#39;</span>)
plt.title(<span style="color: #CD5555">r&#39;Velocity histogram at $k_BT=2$&#39;</span>)
plt.axis([-<span style="color: #B452CD">5</span>, <span style="color: #B452CD">5</span>, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">600</span>])
plt.grid(<span style="color: #658b00">True</span>)
plt.show()
</pre></div>

<h3 id="___sec12">Random walk model </h3>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearRegression

steps=<span style="color: #B452CD">250</span>

distance=<span style="color: #B452CD">0</span>
x=<span style="color: #B452CD">0</span>
distance_list=[]
steps_list=[]
<span style="color: #8B008B; font-weight: bold">while</span> x&lt;steps:
    distance+=np.random.randint(-<span style="color: #B452CD">1</span>,<span style="color: #B452CD">2</span>)
    distance_list.append(distance)
    x+=<span style="color: #B452CD">1</span>
    steps_list.append(x)
plt.plot(steps_list,distance_list, color=<span style="color: #CD5555">&#39;green&#39;</span>, label=<span style="color: #CD5555">&quot;Random Walk Data&quot;</span>)

steps_list=np.asarray(steps_list)
distance_list=np.asarray(distance_list)

X=steps_list[:,np.newaxis]

<span style="color: #228B22">#Polynomial fits</span>

<span style="color: #228B22">#Degree 2</span>
poly_features=PolynomialFeatures(degree=<span style="color: #B452CD">2</span>, include_bias=<span style="color: #658b00">False</span>)
X_poly=poly_features.fit_transform(X)

lin_reg=LinearRegression()
poly_fit=lin_reg.fit(X_poly,distance_list)
b=lin_reg.coef_
c=lin_reg.intercept_
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;2nd degree coefficients:&quot;</span>)
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;zero power: &quot;</span>,c)
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;first power: &quot;</span>, b[<span style="color: #B452CD">0</span>])
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;second power: &quot;</span>,b[<span style="color: #B452CD">1</span>])

z = np.arange(<span style="color: #B452CD">0</span>, steps, .<span style="color: #B452CD">01</span>)
z_mod=b[<span style="color: #B452CD">1</span>]*z**<span style="color: #B452CD">2</span>+b[<span style="color: #B452CD">0</span>]*z+c

fit_mod=b[<span style="color: #B452CD">1</span>]*X**<span style="color: #B452CD">2</span>+b[<span style="color: #B452CD">0</span>]*X+c
plt.plot(z, z_mod, color=<span style="color: #CD5555">&#39;r&#39;</span>, label=<span style="color: #CD5555">&quot;2nd Degree Fit&quot;</span>)
plt.title(<span style="color: #CD5555">&quot;Polynomial Regression&quot;</span>)

plt.xlabel(<span style="color: #CD5555">&quot;Steps&quot;</span>)
plt.ylabel(<span style="color: #CD5555">&quot;Distance&quot;</span>)

<span style="color: #228B22">#Degree 10</span>
poly_features10=PolynomialFeatures(degree=<span style="color: #B452CD">10</span>, include_bias=<span style="color: #658b00">False</span>)
X_poly10=poly_features10.fit_transform(X)

poly_fit10=lin_reg.fit(X_poly10,distance_list)

y_plot=poly_fit10.predict(X_poly10)
plt.plot(X, y_plot, color=<span style="color: #CD5555">&#39;black&#39;</span>, label=<span style="color: #CD5555">&quot;10th Degree Fit&quot;</span>)

plt.legend()
plt.show()


<span style="color: #228B22">#Decision Tree Regression</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeRegressor
regr_1=DecisionTreeRegressor(max_depth=<span style="color: #B452CD">2</span>)
regr_2=DecisionTreeRegressor(max_depth=<span style="color: #B452CD">5</span>)
regr_3=DecisionTreeRegressor(max_depth=<span style="color: #B452CD">7</span>)
regr_1.fit(X, distance_list)
regr_2.fit(X, distance_list)
regr_3.fit(X, distance_list)

X_test = np.arange(<span style="color: #B452CD">0.0</span>, steps, <span style="color: #B452CD">0.01</span>)[:, np.newaxis]
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)
y_3=regr_3.predict(X_test)

<span style="color: #228B22"># Plot the results</span>
plt.figure()
plt.scatter(X, distance_list, s=<span style="color: #B452CD">2.5</span>, c=<span style="color: #CD5555">&quot;black&quot;</span>, label=<span style="color: #CD5555">&quot;data&quot;</span>)
plt.plot(X_test, y_1, color=<span style="color: #CD5555">&quot;red&quot;</span>,
         label=<span style="color: #CD5555">&quot;max_depth=2&quot;</span>, linewidth=<span style="color: #B452CD">2</span>)
plt.plot(X_test, y_2, color=<span style="color: #CD5555">&quot;green&quot;</span>, label=<span style="color: #CD5555">&quot;max_depth=5&quot;</span>, linewidth=<span style="color: #B452CD">2</span>)
plt.plot(X_test, y_3, color=<span style="color: #CD5555">&quot;m&quot;</span>, label=<span style="color: #CD5555">&quot;max_depth=7&quot;</span>, linewidth=<span style="color: #B452CD">2</span>)

plt.xlabel(<span style="color: #CD5555">&quot;Data&quot;</span>)
plt.ylabel(<span style="color: #CD5555">&quot;Darget&quot;</span>)
plt.title(<span style="color: #CD5555">&quot;Decision Tree Regression&quot;</span>)
plt.legend()
plt.show()
</pre></div>
<p>


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2018, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

    // Display navigation controls in the bottom right corner
    controls: true,

    // Display progress bar (below the horiz. slider)
    progress: true,

    // Display the page number of the current slide
    slideNumber: true,

    // Push each slide change to the browser history
    history: false,

    // Enable keyboard shortcuts for navigation
    keyboard: true,

    // Enable the slide overview mode
    overview: true,

    // Vertical centering of slides
    //center: true,
    center: false,

    // Enables touch navigation on devices with touch input
    touch: true,

    // Loop the presentation
    loop: false,

    // Change the presentation direction to be RTL
    rtl: false,

    // Turns fragments on and off globally
    fragments: true,

    // Flags if the presentation is running in an embedded mode,
    // i.e. contained within a limited portion of the screen
    embedded: false,

    // Number of milliseconds between automatically proceeding to the
    // next slide, disabled when set to 0, this value can be overwritten
    // by using a data-autoslide attribute on your slides
    autoSlide: 0,

    // Stop auto-sliding after user input
    autoSlideStoppable: true,

    // Enable slide navigation via mouse wheel
    mouseWheel: false,

    // Hides the address bar on mobile devices
    hideAddressBar: true,

    // Opens links in an iframe preview overlay
    previewLinks: false,

    // Transition style
    transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Transition speed
    transitionSpeed: 'default', // default/fast/slow

    // Transition style for full page slide backgrounds
    backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

    // Number of slides away from the current that are visible
    viewDistance: 3,

    // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

    // Parallax background size
    //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

    theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
    dependencies: [
        // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
        { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

        // Interpret Markdown in <section> elements
        { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

        // Syntax highlight for <code> elements
        { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

        // Zoom in and out with Alt+click
        { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

        // Speaker notes
        { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

        // Remote control your reveal.js presentation using a touch device
        //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

        // MathJax
        //{ src: 'reveal.js/plugin/math/math.js', async: true }
    ]
});

Reveal.initialize({

    // The "normal" size of the presentation, aspect ratio will be preserved
    // when the presentation is scaled to fit different resolutions. Can be
    // specified using percentage units.
    width: 1170,  // original: 960,
    height: 700,

    // Factor of the display size that should remain empty around the content
    margin: 0.1,

    // Bounds for smallest/largest possible scale to apply to content
    minScale: 0.2,
    maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
     end footer logo -->



</body>
</html>
