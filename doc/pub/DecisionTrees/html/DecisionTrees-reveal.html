<!DOCTYPE html>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Data Analysis and Machine Learning: From Decision Trees to Forests and all that">

<title>Data Analysis and Machine Learning: From Decision Trees to Forests and all that</title>







<!-- reveal.js: http://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<style type="text/css">
    hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
    hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    .reveal .alert-text-small   { font-size: 80%;  }
    .reveal .alert-text-large   { font-size: 130%; }
    .reveal .alert-text-normal  { font-size: 90%;  }
    .reveal .alert {
             padding:8px 35px 8px 14px; margin-bottom:18px;
             text-shadow:0 1px 0 rgba(255,255,255,0.5);
             border:5px solid #bababa;
             -webkit-border-radius: 14px; -moz-border-radius: 14px;
             border-radius:14px;
             background-position: 10px 10px;
             background-repeat: no-repeat;
             background-size: 38px;
             padding-left: 30px; /* 55px; if icon */
     }
     .reveal .alert-block {padding-top:14px; padding-bottom:14px}
     .reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
     /*.reveal .alert li {margin-top: 1em}*/
     .reveal .alert-block p+p {margin-top:5px}
     /*.reveal .alert-notice { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
     .reveal .alert-summary  { background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
     .reveal .alert-warning { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
     .reveal .alert-question {background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */

</style>



<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>

<body>
<div class="reveal">

<!-- Any section element inside the <div class="slides"> container
     is displayed as a slide -->

<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    



<section>
<!-- ------------------- main content ---------------------- -->



<center><h1 style="text-align: center;">Data Analysis and Machine Learning: From Decision Trees to Forests and all that</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>&nbsp;<br>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>&nbsp;<br>
<center><h4>Nov 24, 2019</h4></center> <!-- date -->
<br>
<p>

<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2019, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</section>


<section>
<h2 id="___sec0">Decision trees, overarching aims  </h2>

<p>
Decision trees are supervised learning algorithms used for both,
classification and regression tasks.

<p>
The main idea of decision trees
is to find those descriptive features which contain the most
<b>information</b> regarding the target feature and then split the dataset
along the values of these features such that the target feature values
for the resulting underlying datasets are as pure as possible.

<p>
The descriptive features which reproduce best the target/output features are normally  said
to be the most informative ones. The process of finding the <b>most
informative</b> feature is done until we accomplish a stopping criteria
where we then finally end up in so called <b>leaf nodes</b>.

<p>
A decision tree is typically divided into a <b>root node</b>, the <b>interior nodes</b>,
and the final <b>leaf nodes</b> or just <b>leaves</b>. These entities are then connected by so-called <b>branches</b>.

<p>
The leaf nodes
contain the predictions we will make for new query instances presented
to our trained model. This is possible since the model has 
learned the underlying structure of the training data and hence can,
given some assumptions, make predictions about the target feature value
(class) of unseen query instances.
</section>


<section>
<h2 id="___sec1">A typical Decision Tree with its pertinent Jargon, Classification Problem </h2>

<p>
<br /><br /><center><p><img src="DataFiles/cancer.png" align="bottom" width=600></p></center><br /><br />

<p>
This tree was produced using the Wisconsin cancer data (discussed here as well, see code examples below) using <b>Scikit-Learn</b>'s decision tree classifier. Here we have used the so-called <b>gini</b> index (see below) to split the various branches.
</section>


<section>
<h2 id="___sec2">General Features </h2>

<p>
The overarching approach to decision trees is a top-down approach.

<ul>
<p><li> A leaf provides the classification of a given instance.</li>
<p><li> A node specifies a test of some attribute of the instance.</li>
<p><li> A branch corresponds to a possible values of an attribute.</li>
<p><li> An instance is classified by starting at the root node of the tree, testing the attribute specified by this node, then moving down the tree branch corresponding to the value of the attribute in the given example.</li>
</ul>
<p>

This process is then repeated for the subtree rooted at the new
node.
</section>


<section>
<h2 id="___sec3">How do we set it up? </h2>

<p>
In simplified terms, the process of training a decision tree and
predicting the target features of query instances is as follows:

<ol>
<p><li> Present a dataset containing of a number of training instances characterized by a number of descriptive features and a target feature</li>
<p><li> Train the decision tree model by continuously splitting the target feature along the values of the descriptive features using a measure of information gain during the training process</li>
<p><li> Grow the tree until we accomplish a stopping criteria create leaf nodes which represent the <em>predictions</em> we want to make for new query instances</li>
<p><li> Show query instances to the tree and run down the tree until we arrive at leaf nodes</li>
</ol>
<p>

Then we are essentially done!
</section>


<section>
<h2 id="___sec4">Decision trees and Regression  </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearRegression

steps=<span style="color: #B452CD">250</span>

distance=<span style="color: #B452CD">0</span>
x=<span style="color: #B452CD">0</span>
distance_list=[]
steps_list=[]
<span style="color: #8B008B; font-weight: bold">while</span> x&lt;steps:
    distance+=np.random.randint(-<span style="color: #B452CD">1</span>,<span style="color: #B452CD">2</span>)
    distance_list.append(distance)
    x+=<span style="color: #B452CD">1</span>
    steps_list.append(x)
plt.plot(steps_list,distance_list, color=<span style="color: #CD5555">&#39;green&#39;</span>, label=<span style="color: #CD5555">&quot;Random Walk Data&quot;</span>)

steps_list=np.asarray(steps_list)
distance_list=np.asarray(distance_list)

X=steps_list[:,np.newaxis]

<span style="color: #228B22">#Polynomial fits</span>

<span style="color: #228B22">#Degree 2</span>
poly_features=PolynomialFeatures(degree=<span style="color: #B452CD">2</span>, include_bias=<span style="color: #658b00">False</span>)
X_poly=poly_features.fit_transform(X)

lin_reg=LinearRegression()
poly_fit=lin_reg.fit(X_poly,distance_list)
b=lin_reg.coef_
c=lin_reg.intercept_
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;2nd degree coefficients:&quot;</span>)
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;zero power: &quot;</span>,c)
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;first power: &quot;</span>, b[<span style="color: #B452CD">0</span>])
<span style="color: #8B008B; font-weight: bold">print</span> (<span style="color: #CD5555">&quot;second power: &quot;</span>,b[<span style="color: #B452CD">1</span>])

z = np.arange(<span style="color: #B452CD">0</span>, steps, .<span style="color: #B452CD">01</span>)
z_mod=b[<span style="color: #B452CD">1</span>]*z**<span style="color: #B452CD">2</span>+b[<span style="color: #B452CD">0</span>]*z+c

fit_mod=b[<span style="color: #B452CD">1</span>]*X**<span style="color: #B452CD">2</span>+b[<span style="color: #B452CD">0</span>]*X+c
plt.plot(z, z_mod, color=<span style="color: #CD5555">&#39;r&#39;</span>, label=<span style="color: #CD5555">&quot;2nd Degree Fit&quot;</span>)
plt.title(<span style="color: #CD5555">&quot;Polynomial Regression&quot;</span>)

plt.xlabel(<span style="color: #CD5555">&quot;Steps&quot;</span>)
plt.ylabel(<span style="color: #CD5555">&quot;Distance&quot;</span>)

<span style="color: #228B22">#Degree 10</span>
poly_features10=PolynomialFeatures(degree=<span style="color: #B452CD">10</span>, include_bias=<span style="color: #658b00">False</span>)
X_poly10=poly_features10.fit_transform(X)

poly_fit10=lin_reg.fit(X_poly10,distance_list)

y_plot=poly_fit10.predict(X_poly10)
plt.plot(X, y_plot, color=<span style="color: #CD5555">&#39;black&#39;</span>, label=<span style="color: #CD5555">&quot;10th Degree Fit&quot;</span>)

plt.legend()
plt.show()


<span style="color: #228B22">#Decision Tree Regression</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeRegressor
regr_1=DecisionTreeRegressor(max_depth=<span style="color: #B452CD">2</span>)
regr_2=DecisionTreeRegressor(max_depth=<span style="color: #B452CD">5</span>)
regr_3=DecisionTreeRegressor(max_depth=<span style="color: #B452CD">7</span>)
regr_1.fit(X, distance_list)
regr_2.fit(X, distance_list)
regr_3.fit(X, distance_list)

X_test = np.arange(<span style="color: #B452CD">0.0</span>, steps, <span style="color: #B452CD">0.01</span>)[:, np.newaxis]
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)
y_3=regr_3.predict(X_test)

<span style="color: #228B22"># Plot the results</span>
plt.figure()
plt.scatter(X, distance_list, s=<span style="color: #B452CD">2.5</span>, c=<span style="color: #CD5555">&quot;black&quot;</span>, label=<span style="color: #CD5555">&quot;data&quot;</span>)
plt.plot(X_test, y_1, color=<span style="color: #CD5555">&quot;red&quot;</span>,
         label=<span style="color: #CD5555">&quot;max_depth=2&quot;</span>, linewidth=<span style="color: #B452CD">2</span>)
plt.plot(X_test, y_2, color=<span style="color: #CD5555">&quot;green&quot;</span>, label=<span style="color: #CD5555">&quot;max_depth=5&quot;</span>, linewidth=<span style="color: #B452CD">2</span>)
plt.plot(X_test, y_3, color=<span style="color: #CD5555">&quot;m&quot;</span>, label=<span style="color: #CD5555">&quot;max_depth=7&quot;</span>, linewidth=<span style="color: #B452CD">2</span>)

plt.xlabel(<span style="color: #CD5555">&quot;Data&quot;</span>)
plt.ylabel(<span style="color: #CD5555">&quot;Darget&quot;</span>)
plt.title(<span style="color: #CD5555">&quot;Decision Tree Regression&quot;</span>)
plt.legend()
plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec5">Building a tree, regression </h2>

<p>
There are mainly two steps

<ol>
<p><li> We split the predictor space (the set of possible values \( x_1,x_2,\dots, x_p \)) into \( J \) distinct and non-non-overlapping regions, \( R_1,R_2,\dots,R_J \).</li>

<p><li> For every observation that falls into the region \( R_j \) , we make the same prediction, which is simply the mean of the response values for the training observations in \( R_j \).</li>
</ol>
<p>

How do we construct the regions \( R_1,\dots,R_J \)?  In theory, the
regions could have any shape. However, we choose to divide the
predictor space into high-dimensional rectangles, or boxes, for
simplicity and for ease of interpretation of the resulting predictive
model. The goal is to find boxes \( R_1,\dots,R_J \) that minimize the
MSE, given by

<p>&nbsp;<br>
$$
\sum_{j=1}^J\sum_{i\in R_j}(y_i-\overline{y}_{R_j})^2,
$$
<p>&nbsp;<br>

<p>
where \( \overline{y}_{R_j} \)  is the mean response for the training observations 
within box \( j \).
</section>


<section>
<h2 id="___sec6">A top-down approach, recursive binary splitting </h2>

<p>
Unfortunately, it is computationally infeasible to consider every
possible partition of the feature space into \( J \) boxes.  The common
strategy is to take a top-down approach

<p>
The approach is top-down because it begins at the top of the tree (all
observations belong to a single region) and then successively splits
the predictor space; each split is indicated via two new branches
further down on the tree. It is greedy because at each step of the
tree-building process, the best split is made at that particular step,
rather than looking ahead and picking a split that will lead to a
better tree in some future step.
</section>


<section>
<h2 id="___sec7">Making a tree </h2>

<p>
In order to implement the recursive binary splitting we start by selecting
the predictor \( x_j \) and a cutpoint \( s \) that splits the predictor space into two regions \( R_1 \) and \( R_2 \)
<p>&nbsp;<br>
$$
\left\{X\vert x_j < s\right\},
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
\left\{X\vert x_j \geq s\right\},
$$
<p>&nbsp;<br>

so that we obtain the lowest MSE, that is
<p>&nbsp;<br>
$$
\sum_{i:x_i\in R_j}(y_i-\overline{y}_{R_1})^2+\sum_{i:x_i\in R_2}(y_i-\overline{y}_{R_2})^2,
$$
<p>&nbsp;<br>

<p>
which we want to minimize by considering all predictors
\( x_1,x_2,\dots,x_p \).  We consider also all possible values of \( s \) for
each predictor. These values could be determined by randomly assigned
numbers or by starting at the midpoint and then proceed till we find
an optimal value.

<p>
For any \( j \) and \( s \), we define the pair of half-planes where
\( \overline{y}_{R_1} \) is the mean response for the training
observations in \( R_1(j,s) \), and \( \overline{y}_{R_2} \) is the mean
response for the training observations in \( R_2(j,s) \).

<p>
Finding the values of \( j \) and \( s \) that minimize the above equation can be
done quite quickly, especially when the number of features \( p \) is not
too large.

<p>
Next, we repeat the process, looking
for the best predictor and best cutpoint in order to split the data
further so as to minimize the MSE within each of the resulting
regions. However, this time, instead of splitting the entire predictor
space, we split one of the two previously identified regions. We now
have three regions. Again, we look to split one of these three regions
further, so as to minimize the MSE. The process continues until a
stopping criterion is reached; for instance, we may continue until no
region contains more than five observations.
</section>


<section>
<h2 id="___sec8">Pruning the tree </h2>

<p>
The above procedure is rather straightforward, but leads often to
overfitting and unnecessarily large and complicated trees. The basic
idea is to grow a large tree \( T_0 \) and then prune it back in order to
obtain a subtree. A smaller tree with fewer splits (fewer regions) can
lead to smaller variance and better interpretation at the cost of a
little more bias.

<p>
The so-called Cost complexity pruning algorithm gives us a
way to do just this. Rather than considering every possible subtree,
we consider a sequence of trees indexed by a nonnegative tuning
parameter \( \alpha \).
</section>


<section>
<h2 id="___sec9">Cost complexity pruning </h2>
For each value of \( \alpha \)  there corresponds a subtree \( T \in T_0 \) such that
<p>&nbsp;<br>
$$
\sum_{m=1}^{\overline{T}}\sum_{i:x_i\in R_m}(y_i-\overline{y}_{R_m})^2+\alpha\overline{T},
$$
<p>&nbsp;<br>

is as small as possible. Here \( \overline{T} \) is 
the number of terminal nodes of the tree \( T \) , \( R_m \) is the
rectangle (i.e. the subset of predictor space)  corresponding to the \( m \)-th terminal node.

<p>
The tuning parameter \( \alpha \) controls a trade-off between the subtree&#8217;s
com- plexity and its fit to the training data. When \( \alpha = 0 \), then the
subtree \( T \) will simply equal \( T_0 \), 
because then the above equation just measures the
training error. 
However, as \( \alpha \) increases, there is a price to pay for
having a tree with many terminal nodes. The above equation will
tend to be minimized for a smaller subtree.

<p>
It turns out that as we increase \( \alpha \) from zero
branches get pruned from the tree in a nested and predictable fashion,
so obtaining the whole sequence of subtrees as a function of \( \alpha \) is
easy. We can select a value of \( \alpha \) using a validation set or using
cross-validation. We then return to the full data set and obtain the
subtree corresponding to \( \alpha \).
</section>


<section>
<h2 id="___sec10">Schematic Regression Procedure </h2>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b>Building a Regression Tree.</b>
<ol>
<p><li> Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.</li>
<p><li> Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \( \alpha \).</li>
<p><li> Use for example \( K \)-fold cross-validation to choose \( \alpha \). Divide the training observations into \( K \) folds. For each \( k=1,2,\dots,K \) we:</li> 

<ul>

<p><li> repeat steps 1 and 2 on all but the \( k \)-th fold of the training data.</li>

<p><li> Then we valuate the mean squared prediction error on the data in the left-out \( k \)-th fold, as a function of \( \alpha \).</li>

<p><li> Finally  we average the results for each value of \( \alpha \), and pick \( \alpha \) to minimize the average error.</li>
</ul>
<p><li> Return the subtree from Step 2 that corresponds to the chosen value of \( \alpha \).</li> 
</ol>
</div>
</section>


<section>
<h2 id="___sec11">A Classification Tree </h2>

<p>
A classification tree is very similar to a regression tree, except
that it is used to predict a qualitative response rather than a
quantitative one. Recall that for a regression tree, the predicted
response for an observation is given by the mean response of the
training observations that belong to the same terminal node. In
contrast, for a classification tree, we predict that each observation
belongs to the most commonly occurring class of training observations
in the region to which it belongs. In interpreting the results of a
classification tree, we are often interested not only in the class
prediction corresponding to a particular terminal node region, but
also in the class proportions among the training observations that
fall into that region.
</section>


<section>
<h2 id="___sec12">Growing a classification tree </h2>

<p>
The task of growing a
classification tree is quite similar to the task of growing a
regression tree. Just as in the regression setting, we use recursive
binary splitting to grow a classification tree. However, in the
classification setting, the MSE cannot be used as a criterion for making
the binary splits.  A natural alternative to MSE is the <b>classification
error rate</b>. Since we plan to assign an observation in a given region
to the most commonly occurring error rate class of training
observations in that region, the classification error rate is simply
the fraction of the training observations in that region that do not
belong to the most common class.

<p>
When building a classification tree, either the Gini index or the
entropy are typically used to evaluate the quality of a particular
split, since these two approaches are more sensitive to node purity
than is the classification error rate.
</section>


<section>
<h2 id="___sec13">Classification tree, how to split nodes </h2>

<p>
If our targets are the outcome of a classification process that takes
for example \( k=1,2,\dots,K \) values, the only thing we need to think of
is to set up the splitting criteria for each node.

<p>
We define a PDF \( p_{mk} \) that represents the number of observations of
a class \( k \) in a region \( R_m \) with \( N_m \) observations. We represent
this likelihood function in terms of the proportion \( I(y_i=k) \) of
observations of this class in the region \( R_m \) as

<p>&nbsp;<br>
$$
p_{mk} = \frac{1}{N_m}\sum_{x_i\in R_m}I(y_i=k).
$$
<p>&nbsp;<br>

<p>
We let \( p_{mk} \) represent the majority class of observations in region
\( m \). The three most common ways of splitting a node are given by

<ul>
<p><li> Misclassification error</li> 
</ul>
<p>&nbsp;<br>
$$
p_{mk} = \frac{1}{N_m}\sum_{x_i\in R_m}I(y_i\ne k) = 1-p_{mk}.
$$
<p>&nbsp;<br>


<ul>
<p><li> Gini index \( g \)</li>
</ul>
<p>&nbsp;<br>
$$
g = \sum_{k=1}^K p_{mk}(1-p_{mk}).
$$
<p>&nbsp;<br>


<ul>
<p><li> Information entropy or just entropy \( s \)</li>
</ul>
<p>&nbsp;<br>
$$
s = -\sum_{k=1}^K p_{mk}\log{p_{mk}}.
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="___sec14">Visualizing the Tree, Classification </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">os</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> load_breast_cancer
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> confusion_matrix
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> export_graphviz

<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">IPython.display</span> <span style="color: #8B008B; font-weight: bold">import</span> Image 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">pydot</span> <span style="color: #8B008B; font-weight: bold">import</span> graph_from_dot_data
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>


cancer = load_breast_cancer()
X = pd.DataFrame(cancer.data, columns=cancer.feature_names)
<span style="color: #8B008B; font-weight: bold">print</span>(X)
y = pd.Categorical.from_codes(cancer.target, cancer.target_names)
y = pd.get_dummies(y)
<span style="color: #8B008B; font-weight: bold">print</span>(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span style="color: #B452CD">1</span>)
tree_clf = DecisionTreeClassifier(max_depth=<span style="color: #B452CD">5</span>)
tree_clf.fit(X_train, y_train)

export_graphviz(
    tree_clf,
    out_file=<span style="color: #CD5555">&quot;DataFiles/cancer.dot&quot;</span>,
    feature_names=cancer.feature_names,
    class_names=cancer.target_names,
    rounded=<span style="color: #658b00">True</span>,
    filled=<span style="color: #658b00">True</span>
)
cmd = <span style="color: #CD5555">&#39;dot -Tpng DataFiles/cancer.dot -o DataFiles/cancer.png&#39;</span>
os.system(cmd)
</pre></div>
</section>


<section>
<h2 id="___sec15">Visualizing the Tree, The Moons  </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22"># Common imports</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span>  train_test_split 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> make_moons
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> export_graphviz
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">pydot</span> <span style="color: #8B008B; font-weight: bold">import</span> graph_from_dot_data
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">os</span>

np.random.seed(<span style="color: #B452CD">42</span>)
X, y = make_moons(n_samples=<span style="color: #B452CD">100</span>, noise=<span style="color: #B452CD">0.25</span>, random_state=<span style="color: #B452CD">53</span>)
X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=<span style="color: #B452CD">0</span>)
tree_clf = DecisionTreeClassifier(max_depth=<span style="color: #B452CD">5</span>)
tree_clf.fit(X_train, y_train)

export_graphviz(
    tree_clf,
    out_file=<span style="color: #CD5555">&quot;DataFiles/moons.dot&quot;</span>,
    rounded=<span style="color: #658b00">True</span>,
    filled=<span style="color: #658b00">True</span>
)
cmd = <span style="color: #CD5555">&#39;dot -Tpng DataFiles/moons.dot -o DataFiles/moons.png&#39;</span>
os.system(cmd)
</pre></div>
</section>


<section>
<h2 id="___sec16">Algorithms for Setting up Decision Trees </h2>

<p>
Two algorithms stand out in the set up of decision trees:

<ol>
<p><li> The CART (Classification And Regression Tree) algorithm for both classification and regression</li>
<p><li> The ID3 algorithm based on the computation of the information gain for classification</li>
</ol>
<p>

We discuss both algorithms with applications here. The popular library
<b>Scikit-Learn</b> uses the CART algorithm. For classification problems
you can use either the <b>gini</b> index or the <b>entropy</b> to split a tree
in two branches.
</section>


<section>
<h2 id="___sec17">The CART algorithm for Classification </h2>

<p>
For classification, the CART algorithm splits the data set in two subsets using a single feature \( k \) and a threshold \( t_k \).
This could be for example a threshold set by a number below a certain circumference of a malign tumor.

<p>
How do we find these two quantities?
We search for the pair \( (k,t_k) \) that produces the purest subset using for example the <b>gini</b> factor \( G \).
The cost function it tries to minimize is then
<p>&nbsp;<br>
$$
C(k,t_k) = \frac{m_{\mathrm{left}}}{m}G_{\mathrm{left}}+ \frac{m_{\mathrm{right}}}{m}G_{\mathrm{right}},
$$
<p>&nbsp;<br>

where \( G_{\mathrm{left/right}} \) measures the impurity of the left/right subset  and \( m_{\mathrm{left/right}} \)
 is the number of instances in the left/right subset

<p>
Once it has successfully split the training set in two, it splits the subsets using the same logic, then the subsubsets
and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the
\( max\_depth \) hyperparameter), or if it cannot find a split that will reduce impurity. A few other
hyperparameters control additional stopping conditions such as the \( min\_samples\_split \),
\( min\_samples\_leaf \), \( min\_weight\_fraction\_leaf \), and \( max\_leaf\_nodes \).
</section>


<section>
<h2 id="___sec18">The CART algorithm for Regression </h2>

<p>
The CART algorithm for regression works is similar to the one for classification except that instead of trying to split the
training set in a way that minimizes say the <b>gini</b> or <b>entropy</b> impurity, it now tries to split the training set in a way that minimizes our well-known mean-squared error (MSE). The cost function is now
<p>&nbsp;<br>
$$
C(k,t_k) = \frac{m_{\mathrm{left}}}{m}\mathrm{MSE}_{\mathrm{left}}+ \frac{m_{\mathrm{right}}}{m}\mathrm{MSE}_{\mathrm{right}}.
$$
<p>&nbsp;<br>

Here the MSE for a specific node is defined as
<p>&nbsp;<br>
$$
\mathrm{MSE}_{\mathrm{node}}=\frac{1}{m_\mathrm{node}}\sum_{i\in \mathrm{node}}(\overline{y}_{\mathrm{node}}-y_i)^2,
$$
<p>&nbsp;<br>

with
<p>&nbsp;<br>
$$
\overline{y}_{\mathrm{node}}=\frac{1}{m_\mathrm{node}}\sum_{i\in \mathrm{node}}y_i,
$$
<p>&nbsp;<br>

the mean value of all observations in a specific node.

<p>
Without any regularization, the regression task for decision trees, 
just like for classification tasks, is  prone to overfitting.
</section>


<section>
<h2 id="___sec19">Computing the Gini index </h2>

<p>
The example we will look at is a classical one in many Machine
Learning applications. Based on various meteorological features, we
have several so-called attributes which decide whether we at the end
will do some outdoor activity like skiing, going for a bike ride etc
etc.  The table here contains the feautures <b>outlook</b>, <b>temperature</b>,
<b>humidity</b> and <b>wind</b>.  The target or output is whether we ride
(True=1) or whether we do something else that day (False=0). The
attributes for each feature are then sunny, overcast and rain for the
outlook, hot, cold and mild for temperature, high and normal for
humidity and weak and strong for wind.

<p>
The table here summarizes the various attributes and
<table border="1">
<thead>
<tr><th align="center">Day</th> <th align="center">Outlook </th> <th align="center">Temperature</th> <th align="center">Humidity</th> <th align="center"> Wind </th> <th align="center">Ride</th> </tr>
</thead>
<tbody>
<tr><td align="center">   1      </td> <td align="center">   Sunny       </td> <td align="center">   Hot            </td> <td align="center">   High        </td> <td align="center">   Weak      </td> <td align="center">   0       </td> </tr>
<tr><td align="center">   2      </td> <td align="center">   Sunny       </td> <td align="center">   Hot            </td> <td align="center">   High        </td> <td align="center">   Strong    </td> <td align="center">   1       </td> </tr>
<tr><td align="center">   3      </td> <td align="center">   Overcast    </td> <td align="center">   Hot            </td> <td align="center">   High        </td> <td align="center">   Weak      </td> <td align="center">   1       </td> </tr>
<tr><td align="center">   4      </td> <td align="center">   Rain        </td> <td align="center">   Mild           </td> <td align="center">   High        </td> <td align="center">   Weak      </td> <td align="center">   1       </td> </tr>
<tr><td align="center">   5      </td> <td align="center">   Rain        </td> <td align="center">   Cool           </td> <td align="center">   Normal      </td> <td align="center">   Weak      </td> <td align="center">   1       </td> </tr>
<tr><td align="center">   6      </td> <td align="center">   Rain        </td> <td align="center">   Cool           </td> <td align="center">   Normal      </td> <td align="center">   Strong    </td> <td align="center">   0       </td> </tr>
<tr><td align="center">   7      </td> <td align="center">   Overcast    </td> <td align="center">   Cool           </td> <td align="center">   Normal      </td> <td align="center">   Strong    </td> <td align="center">   1       </td> </tr>
<tr><td align="center">   8      </td> <td align="center">   Sunny       </td> <td align="center">   Mild           </td> <td align="center">   High        </td> <td align="center">   Weak      </td> <td align="center">   0       </td> </tr>
<tr><td align="center">   9      </td> <td align="center">   Sunny       </td> <td align="center">   Cool           </td> <td align="center">   Normal      </td> <td align="center">   Weak      </td> <td align="center">   1       </td> </tr>
<tr><td align="center">   10     </td> <td align="center">   Rain        </td> <td align="center">   Mild           </td> <td align="center">   Normal      </td> <td align="center">   Weak      </td> <td align="center">   1       </td> </tr>
<tr><td align="center">   11     </td> <td align="center">   Sunny       </td> <td align="center">   Mild           </td> <td align="center">   Normal      </td> <td align="center">   Strong    </td> <td align="center">   1       </td> </tr>
<tr><td align="center">   12     </td> <td align="center">   Overcast    </td> <td align="center">   Mild           </td> <td align="center">   High        </td> <td align="center">   Strong    </td> <td align="center">   1       </td> </tr>
<tr><td align="center">   13     </td> <td align="center">   Overcast    </td> <td align="center">   Hot            </td> <td align="center">   Normal      </td> <td align="center">   Weak      </td> <td align="center">   1       </td> </tr>
<tr><td align="center">   14     </td> <td align="center">   Rain        </td> <td align="center">   Mild           </td> <td align="center">   High        </td> <td align="center">   Strong    </td> <td align="center">   0       </td> </tr>
</tbody>
</table>
</section>


<section>
<h2 id="___sec20">Simple Python Code to read in Data and perform Classification </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22"># Common imports</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> export_graphviz
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler, OneHotEncoder
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.compose</span> <span style="color: #8B008B; font-weight: bold">import</span> ColumnTransformer
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">IPython.display</span> <span style="color: #8B008B; font-weight: bold">import</span> Image 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">pydot</span> <span style="color: #8B008B; font-weight: bold">import</span> graph_from_dot_data
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">os</span>

<span style="color: #228B22"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR = <span style="color: #CD5555">&quot;Results&quot;</span>
FIGURE_ID = <span style="color: #CD5555">&quot;Results/FigureFiles&quot;</span>
DATA_ID = <span style="color: #CD5555">&quot;DataFiles/&quot;</span>

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(FIGURE_ID):
    os.makedirs(FIGURE_ID)

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">image_path</span>(fig_id):
    <span style="color: #8B008B; font-weight: bold">return</span> os.path.join(FIGURE_ID, fig_id)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">data_path</span>(dat_id):
    <span style="color: #8B008B; font-weight: bold">return</span> os.path.join(DATA_ID, dat_id)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">save_fig</span>(fig_id):
    plt.savefig(image_path(fig_id) + <span style="color: #CD5555">&quot;.png&quot;</span>, format=<span style="color: #CD5555">&#39;png&#39;</span>)

infile = <span style="color: #658b00">open</span>(data_path(<span style="color: #CD5555">&quot;rideclass.csv&quot;</span>),<span style="color: #CD5555">&#39;r&#39;</span>)

<span style="color: #228B22"># Read the experimental data with Pandas</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">IPython.display</span> <span style="color: #8B008B; font-weight: bold">import</span> display
ridedata = pd.read_csv(infile,names = (<span style="color: #CD5555">&#39;Outlook&#39;</span>,<span style="color: #CD5555">&#39;Temperature&#39;</span>,<span style="color: #CD5555">&#39;Humidity&#39;</span>,<span style="color: #CD5555">&#39;Wind&#39;</span>,<span style="color: #CD5555">&#39;Ride&#39;</span>))
ridedata = pd.DataFrame(ridedata)

<span style="color: #228B22"># Features and targets</span>
X = ridedata.loc[:, ridedata.columns != <span style="color: #CD5555">&#39;Ride&#39;</span>].values
y = ridedata.loc[:, ridedata.columns == <span style="color: #CD5555">&#39;Ride&#39;</span>].values

<span style="color: #228B22"># Create the encoder.</span>
encoder = OneHotEncoder(handle_unknown=<span style="color: #CD5555">&quot;ignore&quot;</span>)
<span style="color: #228B22"># Assume for simplicity all features are categorical.</span>
encoder.fit(X)    
<span style="color: #228B22"># Apply the encoder.</span>
X = encoder.transform(X)
<span style="color: #8B008B; font-weight: bold">print</span>(X)
<span style="color: #228B22"># Then do a Classification tree</span>
tree_clf = DecisionTreeClassifier(max_depth=<span style="color: #B452CD">2</span>)
tree_clf.fit(X, y)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Train set accuracy with Decision Tree: {:.2f}&quot;</span>.format(tree_clf.score(X,y)))
<span style="color: #228B22">#transfer to a decision tree graph</span>
export_graphviz(
    tree_clf,
    out_file=<span style="color: #CD5555">&quot;DataFiles/ride.dot&quot;</span>,
    rounded=<span style="color: #658b00">True</span>,
    filled=<span style="color: #658b00">True</span>
)
cmd = <span style="color: #CD5555">&#39;dot -Tpng DataFiles/cancer.dot -o DataFiles/cancer.png&#39;</span>
os.system(cmd)
</pre></div>
</section>


<section>
<h2 id="___sec21">Computing the Gini Factor  </h2>

<p>
The above functions (gini, entropy and misclassification error) are
important components of the so-called CART algorithm. We will discuss
this algorithm below after we have discussed the information gain
algorithm ID3.

<p>
In the example here we have converted all our attributes into numerical values \( 0,1,2 \) etc.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22"># Split a dataset based on an attribute and an attribute value</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">test_split</span>(index, value, dataset):
	left, right = <span style="color: #658b00">list</span>(), <span style="color: #658b00">list</span>()
	<span style="color: #8B008B; font-weight: bold">for</span> row <span style="color: #8B008B">in</span> dataset:
		<span style="color: #8B008B; font-weight: bold">if</span> row[index] &lt; value:
			left.append(row)
		<span style="color: #8B008B; font-weight: bold">else</span>:
			right.append(row)
	<span style="color: #8B008B; font-weight: bold">return</span> left, right
 
<span style="color: #228B22"># Calculate the Gini index for a split dataset</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">gini_index</span>(groups, classes):
	<span style="color: #228B22"># count all samples at split point</span>
	n_instances = <span style="color: #658b00">float</span>(<span style="color: #658b00">sum</span>([<span style="color: #658b00">len</span>(group) <span style="color: #8B008B; font-weight: bold">for</span> group <span style="color: #8B008B">in</span> groups]))
	<span style="color: #228B22"># sum weighted Gini index for each group</span>
	gini = <span style="color: #B452CD">0.0</span>
	<span style="color: #8B008B; font-weight: bold">for</span> group <span style="color: #8B008B">in</span> groups:
		size = <span style="color: #658b00">float</span>(<span style="color: #658b00">len</span>(group))
		<span style="color: #228B22"># avoid divide by zero</span>
		<span style="color: #8B008B; font-weight: bold">if</span> size == <span style="color: #B452CD">0</span>:
			<span style="color: #8B008B; font-weight: bold">continue</span>
		score = <span style="color: #B452CD">0.0</span>
		<span style="color: #228B22"># score the group based on the score for each class</span>
		<span style="color: #8B008B; font-weight: bold">for</span> class_val <span style="color: #8B008B">in</span> classes:
			p = [row[-<span style="color: #B452CD">1</span>] <span style="color: #8B008B; font-weight: bold">for</span> row <span style="color: #8B008B">in</span> group].count(class_val) / size
			score += p * p
		<span style="color: #228B22"># weight the group score by its relative size</span>
		gini += (<span style="color: #B452CD">1.0</span> - score) * (size / n_instances)
	<span style="color: #8B008B; font-weight: bold">return</span> gini

<span style="color: #228B22"># Select the best split point for a dataset</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">get_split</span>(dataset):
	class_values = <span style="color: #658b00">list</span>(<span style="color: #658b00">set</span>(row[-<span style="color: #B452CD">1</span>] <span style="color: #8B008B; font-weight: bold">for</span> row <span style="color: #8B008B">in</span> dataset))
	b_index, b_value, b_score, b_groups = <span style="color: #B452CD">999</span>, <span style="color: #B452CD">999</span>, <span style="color: #B452CD">999</span>, <span style="color: #658b00">None</span>
	<span style="color: #8B008B; font-weight: bold">for</span> index <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #658b00">len</span>(dataset[<span style="color: #B452CD">0</span>])-<span style="color: #B452CD">1</span>):
		<span style="color: #8B008B; font-weight: bold">for</span> row <span style="color: #8B008B">in</span> dataset:
			groups = test_split(index, row[index], dataset)
			gini = gini_index(groups, class_values)
			<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;X%d &lt; %.3f Gini=%.3f&#39;</span> % ((index+<span style="color: #B452CD">1</span>), row[index], gini))
			<span style="color: #8B008B; font-weight: bold">if</span> gini &lt; b_score:
				b_index, b_value, b_score, b_groups = index, row[index], gini, groups
	<span style="color: #8B008B; font-weight: bold">return</span> {<span style="color: #CD5555">&#39;index&#39;</span>:b_index, <span style="color: #CD5555">&#39;value&#39;</span>:b_value, <span style="color: #CD5555">&#39;groups&#39;</span>:b_groups}
 
dataset = [[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>],
            [<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>],
            [<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>],
            [<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>],
            [<span style="color: #B452CD">2</span>,<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>],
            [<span style="color: #B452CD">2</span>,<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>],
            [<span style="color: #B452CD">1</span>,<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>],
            [<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>],
            [<span style="color: #B452CD">0</span>,<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>],
            [<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>],
            [<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>],
            [<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>],
            [<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>],
            [<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]]

split = get_split(dataset)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Split: [X%d &lt; %.3f]&#39;</span> % ((split[<span style="color: #CD5555">&#39;index&#39;</span>]+<span style="color: #B452CD">1</span>), split[<span style="color: #CD5555">&#39;value&#39;</span>]))
</pre></div>
</section>


<section>
<h2 id="___sec22">Entropy and the ID3 algorithm </h2>

<p>
ID3, learns decision trees by constructing
them topdown, beginning with the question <b>which attribute should be tested at the root of the tree</b>?

<ol>
<p><li> Each instance attribute is evaluated using a statistical test to determine how well it alone classifies the training examples.</li>
<p><li> The best attribute is selected and used as the test at the root node of the tree.</li>
<p><li> A descendant of the root node is then created for each possible value of this attribute.</li>
<p><li> Training examples are sorted to the appropriate descendant node.</li>
<p><li> The entire process is then repeated using the training examples associated with each descendant node to select the best attribute to test at that point in the tree.</li>
<p><li> This forms a greedy search for an acceptable decision tree, in which the algorithm never backtracks to reconsider earlier choices.</li> 
</ol>
<p>

The ID3 algorithm selects, which attribute to test at each node in the
tree.

<p>
We would like to select the attribute that is most useful for classifying
examples.

<p>
What is a good quantitative measure of the worth of an attribute?

<p>
Information gain measures how well a given attribute separates the
training examples according to their target classification.

<p>
The ID3 algorithm uses this information gain measure to select among the candidate
attributes at each step while growing the tree.
</section>


<section>
<h2 id="___sec23">Implementing the ID3 Algorithm </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">re</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">math</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">collections</span> <span style="color: #8B008B; font-weight: bold">import</span> deque

<span style="color: #228B22"># x is examples in training set</span>
<span style="color: #228B22"># y is set of targets</span>
<span style="color: #228B22"># label is target attributes</span>
<span style="color: #228B22"># Node is a class which has properties values, childs, and next</span>
<span style="color: #228B22"># root is top node in the decision tree</span>

<span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">Node</span>(<span style="color: #658b00">object</span>):
	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__init__</span>(<span style="color: #658b00">self</span>):
		<span style="color: #658b00">self</span>.value = <span style="color: #658b00">None</span>
		<span style="color: #658b00">self</span>.next = <span style="color: #658b00">None</span>
		<span style="color: #658b00">self</span>.childs = <span style="color: #658b00">None</span>

<span style="color: #228B22"># Simple class of Decision Tree</span>
<span style="color: #228B22"># Aimed for who want to learn Decision Tree, so it is not optimized</span>
<span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">DecisionTree</span>(<span style="color: #658b00">object</span>):
	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__init__</span>(<span style="color: #658b00">self</span>, sample, attributes, labels):
		<span style="color: #658b00">self</span>.sample = sample
		<span style="color: #658b00">self</span>.attributes = attributes
		<span style="color: #658b00">self</span>.labels = labels
		<span style="color: #658b00">self</span>.labelCodes = <span style="color: #658b00">None</span>
		<span style="color: #658b00">self</span>.labelCodesCount = <span style="color: #658b00">None</span>
		<span style="color: #658b00">self</span>.initLabelCodes()
		<span style="color: #228B22"># print(self.labelCodes)</span>
		<span style="color: #658b00">self</span>.root = <span style="color: #658b00">None</span>
		<span style="color: #658b00">self</span>.entropy = <span style="color: #658b00">self</span>.getEntropy([x <span style="color: #8B008B; font-weight: bold">for</span> x <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.labels))])

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">initLabelCodes</span>(<span style="color: #658b00">self</span>):
		<span style="color: #658b00">self</span>.labelCodes = []
		<span style="color: #658b00">self</span>.labelCodesCount = []
		<span style="color: #8B008B; font-weight: bold">for</span> l <span style="color: #8B008B">in</span> <span style="color: #658b00">self</span>.labels:
			<span style="color: #8B008B; font-weight: bold">if</span> l <span style="color: #8B008B">not</span> <span style="color: #8B008B">in</span> <span style="color: #658b00">self</span>.labelCodes:
				<span style="color: #658b00">self</span>.labelCodes.append(l)
				<span style="color: #658b00">self</span>.labelCodesCount.append(<span style="color: #B452CD">0</span>)
			<span style="color: #658b00">self</span>.labelCodesCount[<span style="color: #658b00">self</span>.labelCodes.index(l)] += <span style="color: #B452CD">1</span>

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">getLabelCodeId</span>(<span style="color: #658b00">self</span>, sampleId):
		<span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">self</span>.labelCodes.index(<span style="color: #658b00">self</span>.labels[sampleId])

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">getAttributeValues</span>(<span style="color: #658b00">self</span>, sampleIds, attributeId):
		vals = []
		<span style="color: #8B008B; font-weight: bold">for</span> sid <span style="color: #8B008B">in</span> sampleIds:
			val = <span style="color: #658b00">self</span>.sample[sid][attributeId]
			<span style="color: #8B008B; font-weight: bold">if</span> val <span style="color: #8B008B">not</span> <span style="color: #8B008B">in</span> vals:
				vals.append(val)
		<span style="color: #228B22"># print(vals)</span>
		<span style="color: #8B008B; font-weight: bold">return</span> vals

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">getEntropy</span>(<span style="color: #658b00">self</span>, sampleIds):
		entropy = <span style="color: #B452CD">0</span>
		labelCount = [<span style="color: #B452CD">0</span>] * <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.labelCodes)
		<span style="color: #8B008B; font-weight: bold">for</span> sid <span style="color: #8B008B">in</span> sampleIds:
			labelCount[<span style="color: #658b00">self</span>.getLabelCodeId(sid)] += <span style="color: #B452CD">1</span>
		<span style="color: #228B22"># print(&quot;-ge&quot;, labelCount)</span>
		<span style="color: #8B008B; font-weight: bold">for</span> lv <span style="color: #8B008B">in</span> labelCount:
			<span style="color: #228B22"># print(lv)</span>
			<span style="color: #8B008B; font-weight: bold">if</span> lv != <span style="color: #B452CD">0</span>:
				entropy += -lv/<span style="color: #658b00">len</span>(sampleIds) * math.log(lv/<span style="color: #658b00">len</span>(sampleIds), <span style="color: #B452CD">2</span>)
			<span style="color: #8B008B; font-weight: bold">else</span>:
				entropy += <span style="color: #B452CD">0</span>
		<span style="color: #8B008B; font-weight: bold">return</span> entropy

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">getDominantLabel</span>(<span style="color: #658b00">self</span>, sampleIds):
		labelCodesCount = [<span style="color: #B452CD">0</span>] * <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.labelCodes)
		<span style="color: #8B008B; font-weight: bold">for</span> sid <span style="color: #8B008B">in</span> sampleIds:
			labelCodesCount[<span style="color: #658b00">self</span>.labelCodes.index(<span style="color: #658b00">self</span>.labels[sid])] += <span style="color: #B452CD">1</span>
		<span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">self</span>.labelCodes[labelCodesCount.index(<span style="color: #658b00">max</span>(labelCodesCount))]

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">getInformationGain</span>(<span style="color: #658b00">self</span>, sampleIds, attributeId):
		gain = <span style="color: #658b00">self</span>.getEntropy(sampleIds)
		attributeVals = []
		attributeValsCount = []
		attributeValsIds = []
		<span style="color: #8B008B; font-weight: bold">for</span> sid <span style="color: #8B008B">in</span> sampleIds:
			val = <span style="color: #658b00">self</span>.sample[sid][attributeId]
			<span style="color: #8B008B; font-weight: bold">if</span> val <span style="color: #8B008B">not</span> <span style="color: #8B008B">in</span> attributeVals:
				attributeVals.append(val)
				attributeValsCount.append(<span style="color: #B452CD">0</span>)
				attributeValsIds.append([])
			vid = attributeVals.index(val)
			attributeValsCount[vid] += <span style="color: #B452CD">1</span>
			attributeValsIds[vid].append(sid)
		<span style="color: #228B22"># print(&quot;-gig&quot;, self.attributes[attributeId])</span>
		<span style="color: #8B008B; font-weight: bold">for</span> vc, vids <span style="color: #8B008B">in</span> <span style="color: #658b00">zip</span>(attributeValsCount, attributeValsIds):
			<span style="color: #228B22"># print(&quot;-gig&quot;, vids)</span>
			gain -= vc/<span style="color: #658b00">len</span>(sampleIds) * <span style="color: #658b00">self</span>.getEntropy(vids)
		<span style="color: #8B008B; font-weight: bold">return</span> gain

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">getAttributeMaxInformationGain</span>(<span style="color: #658b00">self</span>, sampleIds, attributeIds):
		attributesEntropy = [<span style="color: #B452CD">0</span>] * <span style="color: #658b00">len</span>(attributeIds)
		<span style="color: #8B008B; font-weight: bold">for</span> i, attId <span style="color: #8B008B">in</span> <span style="color: #658b00">zip</span>(<span style="color: #658b00">range</span>(<span style="color: #658b00">len</span>(attributeIds)), attributeIds):
			attributesEntropy[i] = <span style="color: #658b00">self</span>.getInformationGain(sampleIds, attId)
		maxId = attributeIds[attributesEntropy.index(<span style="color: #658b00">max</span>(attributesEntropy))]
		<span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">self</span>.attributes[maxId], maxId

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">isSingleLabeled</span>(<span style="color: #658b00">self</span>, sampleIds):
		label = <span style="color: #658b00">self</span>.labels[sampleIds[<span style="color: #B452CD">0</span>]]
		<span style="color: #8B008B; font-weight: bold">for</span> sid <span style="color: #8B008B">in</span> sampleIds:
			<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">self</span>.labels[sid] != label:
				<span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">False</span>
		<span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">True</span>

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">getLabel</span>(<span style="color: #658b00">self</span>, sampleId):
		<span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">self</span>.labels[sampleId]

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">id3</span>(<span style="color: #658b00">self</span>):
		sampleIds = [x <span style="color: #8B008B; font-weight: bold">for</span> x <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.sample))]
		attributeIds = [x <span style="color: #8B008B; font-weight: bold">for</span> x <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.attributes))]
		<span style="color: #658b00">self</span>.root = <span style="color: #658b00">self</span>.id3Recv(sampleIds, attributeIds, <span style="color: #658b00">self</span>.root)

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">id3Recv</span>(<span style="color: #658b00">self</span>, sampleIds, attributeIds, root):
		root = Node() <span style="color: #228B22"># Initialize current root</span>
		<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">self</span>.isSingleLabeled(sampleIds):
			root.value = <span style="color: #658b00">self</span>.labels[sampleIds[<span style="color: #B452CD">0</span>]]
			<span style="color: #8B008B; font-weight: bold">return</span> root
		<span style="color: #228B22"># print(attributeIds)</span>
		<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">len</span>(attributeIds) == <span style="color: #B452CD">0</span>:
			root.value = <span style="color: #658b00">self</span>.getDominantLabel(sampleIds)
			<span style="color: #8B008B; font-weight: bold">return</span> root
		bestAttrName, bestAttrId = <span style="color: #658b00">self</span>.getAttributeMaxInformationGain(
			sampleIds, attributeIds)
		<span style="color: #228B22"># print(bestAttrName)</span>
		root.value = bestAttrName
		root.childs = []  <span style="color: #228B22"># Create list of children</span>
		<span style="color: #8B008B; font-weight: bold">for</span> value <span style="color: #8B008B">in</span> <span style="color: #658b00">self</span>.getAttributeValues(sampleIds, bestAttrId):
			<span style="color: #228B22"># print(value)</span>
			child = Node()
			child.value = value
			root.childs.append(child)  <span style="color: #228B22"># Append new child node to current</span>
									   <span style="color: #228B22"># root</span>
			childSampleIds = []
			<span style="color: #8B008B; font-weight: bold">for</span> sid <span style="color: #8B008B">in</span> sampleIds:
				<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">self</span>.sample[sid][bestAttrId] == value:
					childSampleIds.append(sid)
			<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">len</span>(childSampleIds) == <span style="color: #B452CD">0</span>:
				child.next = <span style="color: #658b00">self</span>.getDominantLabel(sampleIds)
			<span style="color: #8B008B; font-weight: bold">else</span>:
				<span style="color: #228B22"># print(bestAttrName, bestAttrId)</span>
				<span style="color: #228B22"># print(attributeIds)</span>
				<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">len</span>(attributeIds) &gt; <span style="color: #B452CD">0</span> <span style="color: #8B008B">and</span> bestAttrId <span style="color: #8B008B">in</span> attributeIds:
					toRemove = attributeIds.index(bestAttrId)
					attributeIds.pop(toRemove)
				child.next = <span style="color: #658b00">self</span>.id3Recv(
					childSampleIds, attributeIds, child.next)
		<span style="color: #8B008B; font-weight: bold">return</span> root

	<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">printTree</span>(<span style="color: #658b00">self</span>):
		<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">self</span>.root:
			roots = deque()
			roots.append(<span style="color: #658b00">self</span>.root)
			<span style="color: #8B008B; font-weight: bold">while</span> <span style="color: #658b00">len</span>(roots) &gt; <span style="color: #B452CD">0</span>:
				root = roots.popleft()
				<span style="color: #8B008B; font-weight: bold">print</span>(root.value)
				<span style="color: #8B008B; font-weight: bold">if</span> root.childs:
					<span style="color: #8B008B; font-weight: bold">for</span> child <span style="color: #8B008B">in</span> root.childs:
						<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;({})&#39;</span>.format(child.value))
						roots.append(child.next)
				<span style="color: #8B008B; font-weight: bold">elif</span> root.next:
					<span style="color: #8B008B; font-weight: bold">print</span>(root.next)


<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">test</span>():
	f = <span style="color: #658b00">open</span>(<span style="color: #CD5555">&#39;DataFiles/rideclass.csv&#39;</span>)
	attributes = f.readline().split(<span style="color: #CD5555">&#39;,&#39;</span>)
	attributes = attributes[<span style="color: #B452CD">1</span>:<span style="color: #658b00">len</span>(attributes)-<span style="color: #B452CD">1</span>]
	<span style="color: #8B008B; font-weight: bold">print</span>(attributes)
	sample = f.readlines()
	f.close()
	<span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #658b00">len</span>(sample)):
		sample[i] = re.sub(<span style="color: #CD5555">&#39;\d+,&#39;</span>, <span style="color: #CD5555">&#39;&#39;</span>, sample[i])
		sample[i] = sample[i].strip().split(<span style="color: #CD5555">&#39;,&#39;</span>)
	labels = []
	<span style="color: #8B008B; font-weight: bold">for</span> s <span style="color: #8B008B">in</span> sample:
		labels.append(s.pop())
	<span style="color: #228B22"># print(sample)</span>
	<span style="color: #228B22"># print(labels)</span>
	decisionTree = DecisionTree(sample, attributes, labels)
	<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;System entropy {}&quot;</span>.format(decisionTree.entropy))
	decisionTree.id3()
	decisionTree.printTree()


<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #00688B">__name__</span> == <span style="color: #CD5555">&#39;__main__&#39;</span>:
	test()
</pre></div>
</section>


<section>
<h2 id="___sec24">Cancer Data again now with Decision Trees and other Methods </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span>  train_test_split 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> load_breast_cancer
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.svm</span> <span style="color: #8B008B; font-weight: bold">import</span> SVC
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LogisticRegression
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeClassifier

<span style="color: #228B22"># Load the data</span>
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data,cancer.target,random_state=<span style="color: #B452CD">0</span>)
<span style="color: #8B008B; font-weight: bold">print</span>(X_train.shape)
<span style="color: #8B008B; font-weight: bold">print</span>(X_test.shape)
<span style="color: #228B22"># Logistic Regression</span>
logreg = LogisticRegression(solver=<span style="color: #CD5555">&#39;lbfgs&#39;</span>)
logreg.fit(X_train, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Logistic Regression: {:.2f}&quot;</span>.format(logreg.score(X_test,y_test)))
<span style="color: #228B22"># Support vector machine</span>
svm = SVC(gamma=<span style="color: #CD5555">&#39;auto&#39;</span>, C=<span style="color: #B452CD">100</span>)
svm.fit(X_train, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with SVM: {:.2f}&quot;</span>.format(svm.score(X_test,y_test)))
<span style="color: #228B22"># Decision Trees</span>
deep_tree_clf = DecisionTreeClassifier(max_depth=<span style="color: #658b00">None</span>)
deep_tree_clf.fit(X_train, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Decision Trees: {:.2f}&quot;</span>.format(deep_tree_clf.score(X_test,y_test)))
<span style="color: #228B22">#now scale the data</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
<span style="color: #228B22"># Logistic Regression</span>
logreg.fit(X_train_scaled, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy Logistic Regression with scaled data: {:.2f}&quot;</span>.format(logreg.score(X_test_scaled,y_test)))
<span style="color: #228B22"># Support Vector Machine</span>
svm.fit(X_train_scaled, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy SVM with scaled data: {:.2f}&quot;</span>.format(logreg.score(X_test_scaled,y_test)))
<span style="color: #228B22"># Decision Trees</span>
deep_tree_clf.fit(X_train_scaled, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Decision Trees and scaled data: {:.2f}&quot;</span>.format(deep_tree_clf.score(X_test_scaled,y_test)))
</pre></div>
</section>


<section>
<h2 id="___sec25">Another example, the moons again </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">__future__</span> <span style="color: #8B008B; font-weight: bold">import</span> division, print_function, unicode_literals

<span style="color: #228B22"># Common imports</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">os</span>

<span style="color: #228B22"># to make this notebook&#39;s output stable across runs</span>
np.random.seed(<span style="color: #B452CD">42</span>)

<span style="color: #228B22"># To plot pretty figures</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.colors</span> <span style="color: #8B008B; font-weight: bold">import</span> ListedColormap
plt.rcParams[<span style="color: #CD5555">&#39;axes.labelsize&#39;</span>] = <span style="color: #B452CD">14</span>
plt.rcParams[<span style="color: #CD5555">&#39;xtick.labelsize&#39;</span>] = <span style="color: #B452CD">12</span>
plt.rcParams[<span style="color: #CD5555">&#39;ytick.labelsize&#39;</span>] = <span style="color: #B452CD">12</span>


<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.svm</span> <span style="color: #8B008B; font-weight: bold">import</span> SVC
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn</span> <span style="color: #8B008B; font-weight: bold">import</span> datasets
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> make_moons
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> export_graphviz

Xm, ym = make_moons(n_samples=<span style="color: #B452CD">100</span>, noise=<span style="color: #B452CD">0.25</span>, random_state=<span style="color: #B452CD">53</span>)

deep_tree_clf1 = DecisionTreeClassifier(random_state=<span style="color: #B452CD">42</span>)
deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=<span style="color: #B452CD">4</span>, random_state=<span style="color: #B452CD">42</span>)
deep_tree_clf1.fit(Xm, ym)
deep_tree_clf2.fit(Xm, ym)


<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">plot_decision_boundary</span>(clf, X, y, axes=[<span style="color: #B452CD">0</span>, <span style="color: #B452CD">7.5</span>, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">3</span>], iris=<span style="color: #658b00">True</span>, legend=<span style="color: #658b00">False</span>, plot_training=<span style="color: #658b00">True</span>):
    x1s = np.linspace(axes[<span style="color: #B452CD">0</span>], axes[<span style="color: #B452CD">1</span>], <span style="color: #B452CD">100</span>)
    x2s = np.linspace(axes[<span style="color: #B452CD">2</span>], axes[<span style="color: #B452CD">3</span>], <span style="color: #B452CD">100</span>)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)
    custom_cmap = ListedColormap([<span style="color: #CD5555">&#39;#fafab0&#39;</span>,<span style="color: #CD5555">&#39;#9898ff&#39;</span>,<span style="color: #CD5555">&#39;#a0faa0&#39;</span>])
    plt.contourf(x1, x2, y_pred, alpha=<span style="color: #B452CD">0.3</span>, cmap=custom_cmap)
    <span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> iris:
        custom_cmap2 = ListedColormap([<span style="color: #CD5555">&#39;#7d7d58&#39;</span>,<span style="color: #CD5555">&#39;#4c4c7f&#39;</span>,<span style="color: #CD5555">&#39;#507d50&#39;</span>])
        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=<span style="color: #B452CD">0.8</span>)
    <span style="color: #8B008B; font-weight: bold">if</span> plot_training:
        plt.plot(X[:, <span style="color: #B452CD">0</span>][y==<span style="color: #B452CD">0</span>], X[:, <span style="color: #B452CD">1</span>][y==<span style="color: #B452CD">0</span>], <span style="color: #CD5555">&quot;yo&quot;</span>, label=<span style="color: #CD5555">&quot;Iris-Setosa&quot;</span>)
        plt.plot(X[:, <span style="color: #B452CD">0</span>][y==<span style="color: #B452CD">1</span>], X[:, <span style="color: #B452CD">1</span>][y==<span style="color: #B452CD">1</span>], <span style="color: #CD5555">&quot;bs&quot;</span>, label=<span style="color: #CD5555">&quot;Iris-Versicolor&quot;</span>)
        plt.plot(X[:, <span style="color: #B452CD">0</span>][y==<span style="color: #B452CD">2</span>], X[:, <span style="color: #B452CD">1</span>][y==<span style="color: #B452CD">2</span>], <span style="color: #CD5555">&quot;g^&quot;</span>, label=<span style="color: #CD5555">&quot;Iris-Virginica&quot;</span>)
        plt.axis(axes)
    <span style="color: #8B008B; font-weight: bold">if</span> iris:
        plt.xlabel(<span style="color: #CD5555">&quot;Petal length&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)
        plt.ylabel(<span style="color: #CD5555">&quot;Petal width&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)
    <span style="color: #8B008B; font-weight: bold">else</span>:
        plt.xlabel(<span style="color: #CD5555">r&quot;$x_1$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
        plt.ylabel(<span style="color: #CD5555">r&quot;$x_2$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>, rotation=<span style="color: #B452CD">0</span>)
    <span style="color: #8B008B; font-weight: bold">if</span> legend:
        plt.legend(loc=<span style="color: #CD5555">&quot;lower right&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)
plt.figure(figsize=(<span style="color: #B452CD">11</span>, <span style="color: #B452CD">4</span>))
plt.subplot(<span style="color: #B452CD">121</span>)
plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-<span style="color: #B452CD">1.5</span>, <span style="color: #B452CD">2.5</span>, -<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1.5</span>], iris=<span style="color: #658b00">False</span>)
plt.title(<span style="color: #CD5555">&quot;No restrictions&quot;</span>, fontsize=<span style="color: #B452CD">16</span>)
plt.subplot(<span style="color: #B452CD">122</span>)
plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-<span style="color: #B452CD">1.5</span>, <span style="color: #B452CD">2.5</span>, -<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1.5</span>], iris=<span style="color: #658b00">False</span>)
plt.title(<span style="color: #CD5555">&quot;min_samples_leaf = {}&quot;</span>.format(deep_tree_clf2.min_samples_leaf), fontsize=<span style="color: #B452CD">14</span>)
plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec26">Playing around with regions </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>np.random.seed(<span style="color: #B452CD">6</span>)
Xs = np.random.rand(<span style="color: #B452CD">100</span>, <span style="color: #B452CD">2</span>) - <span style="color: #B452CD">0.5</span>
ys = (Xs[:, <span style="color: #B452CD">0</span>] &gt; <span style="color: #B452CD">0</span>).astype(np.float32) * <span style="color: #B452CD">2</span>

angle = np.pi/<span style="color: #B452CD">4</span>
rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])
Xsr = Xs.dot(rotation_matrix)

tree_clf_s = DecisionTreeClassifier(random_state=<span style="color: #B452CD">42</span>)
tree_clf_s.fit(Xs, ys)
tree_clf_sr = DecisionTreeClassifier(random_state=<span style="color: #B452CD">42</span>)
tree_clf_sr.fit(Xsr, ys)

plt.figure(figsize=(<span style="color: #B452CD">11</span>, <span style="color: #B452CD">4</span>))
plt.subplot(<span style="color: #B452CD">121</span>)
plot_decision_boundary(tree_clf_s, Xs, ys, axes=[-<span style="color: #B452CD">0.7</span>, <span style="color: #B452CD">0.7</span>, -<span style="color: #B452CD">0.7</span>, <span style="color: #B452CD">0.7</span>], iris=<span style="color: #658b00">False</span>)
plt.subplot(<span style="color: #B452CD">122</span>)
plot_decision_boundary(tree_clf_sr, Xsr, ys, axes=[-<span style="color: #B452CD">0.7</span>, <span style="color: #B452CD">0.7</span>, -<span style="color: #B452CD">0.7</span>, <span style="color: #B452CD">0.7</span>], iris=<span style="color: #658b00">False</span>)

plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec27">Regression trees </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #228B22"># Quadratic training set + noise</span>
np.random.seed(<span style="color: #B452CD">42</span>)
m = <span style="color: #B452CD">200</span>
X = np.random.rand(m, <span style="color: #B452CD">1</span>)
y = <span style="color: #B452CD">4</span> * (X - <span style="color: #B452CD">0.5</span>) ** <span style="color: #B452CD">2</span>
y = y + np.random.randn(m, <span style="color: #B452CD">1</span>) / <span style="color: #B452CD">10</span>
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(max_depth=<span style="color: #B452CD">2</span>, random_state=<span style="color: #B452CD">42</span>)
tree_reg.fit(X, y)
</pre></div>
</section>


<section>
<h2 id="___sec28">Final regressor code </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeRegressor

tree_reg1 = DecisionTreeRegressor(random_state=<span style="color: #B452CD">42</span>, max_depth=<span style="color: #B452CD">2</span>)
tree_reg2 = DecisionTreeRegressor(random_state=<span style="color: #B452CD">42</span>, max_depth=<span style="color: #B452CD">3</span>)
tree_reg1.fit(X, y)
tree_reg2.fit(X, y)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">plot_regression_predictions</span>(tree_reg, X, y, axes=[<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, -<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1</span>], ylabel=<span style="color: #CD5555">&quot;$y$&quot;</span>):
    x1 = np.linspace(axes[<span style="color: #B452CD">0</span>], axes[<span style="color: #B452CD">1</span>], <span style="color: #B452CD">500</span>).reshape(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
    y_pred = tree_reg.predict(x1)
    plt.axis(axes)
    plt.xlabel(<span style="color: #CD5555">&quot;$x_1$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
    <span style="color: #8B008B; font-weight: bold">if</span> ylabel:
        plt.ylabel(ylabel, fontsize=<span style="color: #B452CD">18</span>, rotation=<span style="color: #B452CD">0</span>)
    plt.plot(X, y, <span style="color: #CD5555">&quot;b.&quot;</span>)
    plt.plot(x1, y_pred, <span style="color: #CD5555">&quot;r.-&quot;</span>, linewidth=<span style="color: #B452CD">2</span>, label=<span style="color: #CD5555">r&quot;$\hat{y}$&quot;</span>)

plt.figure(figsize=(<span style="color: #B452CD">11</span>, <span style="color: #B452CD">4</span>))
plt.subplot(<span style="color: #B452CD">121</span>)
plot_regression_predictions(tree_reg1, X, y)
<span style="color: #8B008B; font-weight: bold">for</span> split, style <span style="color: #8B008B">in</span> ((<span style="color: #B452CD">0.1973</span>, <span style="color: #CD5555">&quot;k-&quot;</span>), (<span style="color: #B452CD">0.0917</span>, <span style="color: #CD5555">&quot;k--&quot;</span>), (<span style="color: #B452CD">0.7718</span>, <span style="color: #CD5555">&quot;k--&quot;</span>)):
    plt.plot([split, split], [-<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1</span>], style, linewidth=<span style="color: #B452CD">2</span>)
plt.text(<span style="color: #B452CD">0.21</span>, <span style="color: #B452CD">0.65</span>, <span style="color: #CD5555">&quot;Depth=0&quot;</span>, fontsize=<span style="color: #B452CD">15</span>)
plt.text(<span style="color: #B452CD">0.01</span>, <span style="color: #B452CD">0.2</span>, <span style="color: #CD5555">&quot;Depth=1&quot;</span>, fontsize=<span style="color: #B452CD">13</span>)
plt.text(<span style="color: #B452CD">0.65</span>, <span style="color: #B452CD">0.8</span>, <span style="color: #CD5555">&quot;Depth=1&quot;</span>, fontsize=<span style="color: #B452CD">13</span>)
plt.legend(loc=<span style="color: #CD5555">&quot;upper center&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.title(<span style="color: #CD5555">&quot;max_depth=2&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)

plt.subplot(<span style="color: #B452CD">122</span>)
plot_regression_predictions(tree_reg2, X, y, ylabel=<span style="color: #658b00">None</span>)
<span style="color: #8B008B; font-weight: bold">for</span> split, style <span style="color: #8B008B">in</span> ((<span style="color: #B452CD">0.1973</span>, <span style="color: #CD5555">&quot;k-&quot;</span>), (<span style="color: #B452CD">0.0917</span>, <span style="color: #CD5555">&quot;k--&quot;</span>), (<span style="color: #B452CD">0.7718</span>, <span style="color: #CD5555">&quot;k--&quot;</span>)):
    plt.plot([split, split], [-<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1</span>], style, linewidth=<span style="color: #B452CD">2</span>)
<span style="color: #8B008B; font-weight: bold">for</span> split <span style="color: #8B008B">in</span> (<span style="color: #B452CD">0.0458</span>, <span style="color: #B452CD">0.1298</span>, <span style="color: #B452CD">0.2873</span>, <span style="color: #B452CD">0.9040</span>):
    plt.plot([split, split], [-<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1</span>], <span style="color: #CD5555">&quot;k:&quot;</span>, linewidth=<span style="color: #B452CD">1</span>)
plt.text(<span style="color: #B452CD">0.3</span>, <span style="color: #B452CD">0.5</span>, <span style="color: #CD5555">&quot;Depth=2&quot;</span>, fontsize=<span style="color: #B452CD">13</span>)
plt.title(<span style="color: #CD5555">&quot;max_depth=3&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)

plt.show()
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>tree_reg1 = DecisionTreeRegressor(random_state=<span style="color: #B452CD">42</span>)
tree_reg2 = DecisionTreeRegressor(random_state=<span style="color: #B452CD">42</span>, min_samples_leaf=<span style="color: #B452CD">10</span>)
tree_reg1.fit(X, y)
tree_reg2.fit(X, y)

x1 = np.linspace(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, <span style="color: #B452CD">500</span>).reshape(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
y_pred1 = tree_reg1.predict(x1)
y_pred2 = tree_reg2.predict(x1)

plt.figure(figsize=(<span style="color: #B452CD">11</span>, <span style="color: #B452CD">4</span>))

plt.subplot(<span style="color: #B452CD">121</span>)
plt.plot(X, y, <span style="color: #CD5555">&quot;b.&quot;</span>)
plt.plot(x1, y_pred1, <span style="color: #CD5555">&quot;r.-&quot;</span>, linewidth=<span style="color: #B452CD">2</span>, label=<span style="color: #CD5555">r&quot;$\hat{y}$&quot;</span>)
plt.axis([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, -<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1.1</span>])
plt.xlabel(<span style="color: #CD5555">&quot;$x_1$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.ylabel(<span style="color: #CD5555">&quot;$y$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>, rotation=<span style="color: #B452CD">0</span>)
plt.legend(loc=<span style="color: #CD5555">&quot;upper center&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.title(<span style="color: #CD5555">&quot;No restrictions&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)

plt.subplot(<span style="color: #B452CD">122</span>)
plt.plot(X, y, <span style="color: #CD5555">&quot;b.&quot;</span>)
plt.plot(x1, y_pred2, <span style="color: #CD5555">&quot;r.-&quot;</span>, linewidth=<span style="color: #B452CD">2</span>, label=<span style="color: #CD5555">r&quot;$\hat{y}$&quot;</span>)
plt.axis([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">1</span>, -<span style="color: #B452CD">0.2</span>, <span style="color: #B452CD">1.1</span>])
plt.xlabel(<span style="color: #CD5555">&quot;$x_1$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
plt.title(<span style="color: #CD5555">&quot;min_samples_leaf={}&quot;</span>.format(tree_reg2.min_samples_leaf), fontsize=<span style="color: #B452CD">14</span>)

plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec29">Pros and cons of trees, pros </h2>

<ul>
<p><li> White box, easy to interpret model. Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches discussed earlier (think of support vector machines)</li>
<p><li> Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!</li>
<p><li> No feature normalization needed</li>
<p><li> Tree models can handle both continuous and categorical data (Classification and Regression Trees)</li>
<p><li> Can model nonlinear relationships</li>
<p><li> Can model interactions between the different descriptive features</li>
<p><li> Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small)</li>
</ul>
</section>


<section>
<h2 id="___sec30">Disadvantages </h2>

<ul>
<p><li> Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches</li>
<p><li> If continuous features are used the tree may become quite large and hence less interpretable</li>
<p><li> Decision trees are prone to overfit the training data and hence do not well generalize the data if no stopping criteria or improvements like pruning, boosting or bagging are implemented</li>
<p><li> Small changes in the data may lead to a completely different tree. This issue can be addressed by using ensemble methods like bagging, boosting or random forests</li>
<p><li> Unbalanced datasets where some target feature values occur much more frequently than others may lead to biased trees since the frequently occurring feature values are preferred over the less frequently occurring ones.</li> 
<p><li> If the number of features is relatively large (high dimensional) and the number of instances is relatively low, the tree might overfit the data</li>
<p><li> Features with many levels may be preferred over features with less levels since for them it is <em>more easy</em> to split the dataset such that the sub datasets only contain pure target feature values. This issue can be addressed by preferring for instance the information gain ratio as splitting criteria over information gain</li>
</ul>
<p>

However, by aggregating many decision trees, using methods like
bagging, random forests, and boosting, the predictive performance of
trees can be substantially improved.
</section>


<section>
<h2 id="___sec31">Ensemble Methods: From a Single Tree to Many Trees and Extreme Boosting, Meet the Jungle of Methods </h2>

<p>
As stated above and seen in many of the examples discussed here about
a single decision tree, we often end up overfitting our training
data. This normally means that we have a high variance. Can we reduce
the variance of a statistical learning method?

<p>
This leads us to a set of different methods that can combine different
machine learning algorithms or just use one of them to construct
forests and jungles of trees, homogeneous ones or heterogenous
ones. These methods are recognized by different names which we will
try to explain here. These are

<ol>
<p><li> Voting classifiers</li>
<p><li> Bagging and Pasting</li>
<p><li> Random forests</li>
<p><li> Boosting methods, from adaptive to Extreme Gradient Boosting (XGBoost)</li>
</ol>
<p>

We discuss these methods here.
</section>


<section>
<h2 id="___sec32">An Overview of Ensemble Methods  </h2>

<p>
<br /><br /><center><p><img src="DataFiles/ensembleoverview.png" align="bottom" width=600></p></center><br /><br />
</section>


<section>
<h2 id="___sec33">Bagging </h2>

<p>
The <b>plain</b> decision trees suffer from high
variance. This means that if we split the training data into two parts
at random, and fit a decision tree to both halves, the results that we
get could be quite different. In contrast, a procedure with low
variance will yield similar results if applied repeatedly to distinct
data sets; linear regression tends to have low variance, if the ratio
of \( n \) to \( p \) is moderately large.

<p>
<b>Bootstrap aggregation</b>, or just <b>bagging</b>, is a
general-purpose procedure for reducing the variance of a statistical
learning method.
</section>


<section>
<h2 id="___sec34">More bagging </h2>

<p>
Bagging typically results in improved accuracy
over prediction using a single tree. Unfortunately, however, it can be
difficult to interpret the resulting model. Recall that one of the
advantages of decision trees is the attractive and easily interpreted
diagram that results.

<p>
However, when we bag a large number of trees, it is no longer
possible to represent the resulting statistical learning procedure
using a single tree, and it is no longer clear which variables are
most important to the procedure. Thus, bagging improves prediction
accuracy at the expense of interpretability.  Although the collection
of bagged trees is much more difficult to interpret than a single
tree, one can obtain an overall summary of the importance of each
predictor using the MSE (for bagging regression trees) or the Gini
index (for bagging classification trees). In the case of bagging
regression trees, we can record the total amount that the MSE is
decreased due to splits over a given predictor, averaged over all \( B \) possible
trees. A large value indicates an important predictor. Similarly, in
the context of bagging classification trees, we can add up the total
amount that the Gini index  is decreased by splits over a given
predictor, averaged over all \( B \) trees.
</section>


<section>
<h2 id="___sec35">Simple Voting Example, head or tail </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>heads_proba = <span style="color: #B452CD">0.51</span>
coin_tosses = (np.random.rand(<span style="color: #B452CD">10000</span>, <span style="color: #B452CD">10</span>) &lt; heads_proba).astype(np.int32)
cumulative_heads_ratio = np.cumsum(coin_tosses, axis=<span style="color: #B452CD">0</span>) / np.arange(<span style="color: #B452CD">1</span>, <span style="color: #B452CD">10001</span>).reshape(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
plt.figure(figsize=(<span style="color: #B452CD">8</span>,<span style="color: #B452CD">3.5</span>))
plt.plot(cumulative_heads_ratio)
plt.plot([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">10000</span>], [<span style="color: #B452CD">0.51</span>, <span style="color: #B452CD">0.51</span>], <span style="color: #CD5555">&quot;k--&quot;</span>, linewidth=<span style="color: #B452CD">2</span>, label=<span style="color: #CD5555">&quot;51%&quot;</span>)
plt.plot([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">10000</span>], [<span style="color: #B452CD">0.5</span>, <span style="color: #B452CD">0.5</span>], <span style="color: #CD5555">&quot;k-&quot;</span>, label=<span style="color: #CD5555">&quot;50%&quot;</span>)
plt.xlabel(<span style="color: #CD5555">&quot;Number of coin tosses&quot;</span>)
plt.ylabel(<span style="color: #CD5555">&quot;Heads ratio&quot;</span>)
plt.legend(loc=<span style="color: #CD5555">&quot;lower right&quot;</span>)
plt.axis([<span style="color: #B452CD">0</span>, <span style="color: #B452CD">10000</span>, <span style="color: #B452CD">0.42</span>, <span style="color: #B452CD">0.58</span>])
save_fig(<span style="color: #CD5555">&quot;votingsimple&quot;</span>)
plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec36">Using the Voting Classifier </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> make_moons

X, y = make_moons(n_samples=<span style="color: #B452CD">500</span>, noise=<span style="color: #B452CD">0.30</span>, random_state=<span style="color: #B452CD">42</span>)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span style="color: #B452CD">42</span>)

<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> VotingClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LogisticRegression
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.svm</span> <span style="color: #8B008B; font-weight: bold">import</span> SVC

log_clf = LogisticRegression(solver=<span style="color: #CD5555">&quot;liblinear&quot;</span>, random_state=<span style="color: #B452CD">42</span>)
rnd_clf = RandomForestClassifier(n_estimators=<span style="color: #B452CD">10</span>, random_state=<span style="color: #B452CD">42</span>)
svm_clf = SVC(gamma=<span style="color: #CD5555">&quot;auto&quot;</span>, random_state=<span style="color: #B452CD">42</span>)

voting_clf = VotingClassifier(
    estimators=[(<span style="color: #CD5555">&#39;lr&#39;</span>, log_clf), (<span style="color: #CD5555">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #CD5555">&#39;svc&#39;</span>, svm_clf)],
    voting=<span style="color: #CD5555">&#39;hard&#39;</span>)

voting_clf.fit(X_train, y_train)

<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> accuracy_score

<span style="color: #8B008B; font-weight: bold">for</span> clf <span style="color: #8B008B">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    <span style="color: #8B008B; font-weight: bold">print</span>(clf.<span style="color: #00688B">__class__</span>.<span style="color: #00688B">__name__</span>, accuracy_score(y_test, y_pred))

log_clf = LogisticRegression(solver=<span style="color: #CD5555">&quot;liblinear&quot;</span>, random_state=<span style="color: #B452CD">42</span>)
rnd_clf = RandomForestClassifier(n_estimators=<span style="color: #B452CD">10</span>, random_state=<span style="color: #B452CD">42</span>)
svm_clf = SVC(gamma=<span style="color: #CD5555">&quot;auto&quot;</span>, probability=<span style="color: #658b00">True</span>, random_state=<span style="color: #B452CD">42</span>)

voting_clf = VotingClassifier(
    estimators=[(<span style="color: #CD5555">&#39;lr&#39;</span>, log_clf), (<span style="color: #CD5555">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #CD5555">&#39;svc&#39;</span>, svm_clf)],
    voting=<span style="color: #CD5555">&#39;soft&#39;</span>)
voting_clf.fit(X_train, y_train)

<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> accuracy_score

<span style="color: #8B008B; font-weight: bold">for</span> clf <span style="color: #8B008B">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    <span style="color: #8B008B; font-weight: bold">print</span>(clf.<span style="color: #00688B">__class__</span>.<span style="color: #00688B">__name__</span>, accuracy_score(y_test, y_pred))
</pre></div>
</section>


<section>
<h2 id="___sec37">Please, not the moons again! Voting and Bagging </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> make_moons

X, y = make_moons(n_samples=<span style="color: #B452CD">500</span>, noise=<span style="color: #B452CD">0.30</span>, random_state=<span style="color: #B452CD">42</span>)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span style="color: #B452CD">42</span>)
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> VotingClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LogisticRegression
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.svm</span> <span style="color: #8B008B; font-weight: bold">import</span> SVC

log_clf = LogisticRegression(random_state=<span style="color: #B452CD">42</span>)
rnd_clf = RandomForestClassifier(random_state=<span style="color: #B452CD">42</span>)
svm_clf = SVC(random_state=<span style="color: #B452CD">42</span>)

voting_clf = VotingClassifier(
    estimators=[(<span style="color: #CD5555">&#39;lr&#39;</span>, log_clf), (<span style="color: #CD5555">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #CD5555">&#39;svc&#39;</span>, svm_clf)],
    voting=<span style="color: #CD5555">&#39;hard&#39;</span>)
voting_clf.fit(X_train, y_train)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> accuracy_score

<span style="color: #8B008B; font-weight: bold">for</span> clf <span style="color: #8B008B">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    <span style="color: #8B008B; font-weight: bold">print</span>(clf.<span style="color: #00688B">__class__</span>.<span style="color: #00688B">__name__</span>, accuracy_score(y_test, y_pred))
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>log_clf = LogisticRegression(random_state=<span style="color: #B452CD">42</span>)
rnd_clf = RandomForestClassifier(random_state=<span style="color: #B452CD">42</span>)
svm_clf = SVC(probability=<span style="color: #658b00">True</span>, random_state=<span style="color: #B452CD">42</span>)

voting_clf = VotingClassifier(
    estimators=[(<span style="color: #CD5555">&#39;lr&#39;</span>, log_clf), (<span style="color: #CD5555">&#39;rf&#39;</span>, rnd_clf), (<span style="color: #CD5555">&#39;svc&#39;</span>, svm_clf)],
    voting=<span style="color: #CD5555">&#39;soft&#39;</span>)
voting_clf.fit(X_train, y_train)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> accuracy_score

<span style="color: #8B008B; font-weight: bold">for</span> clf <span style="color: #8B008B">in</span> (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    <span style="color: #8B008B; font-weight: bold">print</span>(clf.<span style="color: #00688B">__class__</span>.<span style="color: #00688B">__name__</span>, accuracy_score(y_test, y_pred))
</pre></div>
</section>


<section>
<h2 id="___sec38">Bagging Examples  </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> BaggingClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeClassifier

bag_clf = BaggingClassifier(
    DecisionTreeClassifier(random_state=<span style="color: #B452CD">42</span>), n_estimators=<span style="color: #B452CD">500</span>,
    max_samples=<span style="color: #B452CD">100</span>, bootstrap=<span style="color: #658b00">True</span>, n_jobs=-<span style="color: #B452CD">1</span>, random_state=<span style="color: #B452CD">42</span>)
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> accuracy_score
<span style="color: #8B008B; font-weight: bold">print</span>(accuracy_score(y_test, y_pred))
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>tree_clf = DecisionTreeClassifier(random_state=<span style="color: #B452CD">42</span>)
tree_clf.fit(X_train, y_train)
y_pred_tree = tree_clf.predict(X_test)
<span style="color: #8B008B; font-weight: bold">print</span>(accuracy_score(y_test, y_pred_tree))
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.colors</span> <span style="color: #8B008B; font-weight: bold">import</span> ListedColormap

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">plot_decision_boundary</span>(clf, X, y, axes=[-<span style="color: #B452CD">1.5</span>, <span style="color: #B452CD">2.5</span>, -<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1.5</span>], alpha=<span style="color: #B452CD">0.5</span>, contour=<span style="color: #658b00">True</span>):
    x1s = np.linspace(axes[<span style="color: #B452CD">0</span>], axes[<span style="color: #B452CD">1</span>], <span style="color: #B452CD">100</span>)
    x2s = np.linspace(axes[<span style="color: #B452CD">2</span>], axes[<span style="color: #B452CD">3</span>], <span style="color: #B452CD">100</span>)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)
    custom_cmap = ListedColormap([<span style="color: #CD5555">&#39;#fafab0&#39;</span>,<span style="color: #CD5555">&#39;#9898ff&#39;</span>,<span style="color: #CD5555">&#39;#a0faa0&#39;</span>])
    plt.contourf(x1, x2, y_pred, alpha=<span style="color: #B452CD">0.3</span>, cmap=custom_cmap)
    <span style="color: #8B008B; font-weight: bold">if</span> contour:
        custom_cmap2 = ListedColormap([<span style="color: #CD5555">&#39;#7d7d58&#39;</span>,<span style="color: #CD5555">&#39;#4c4c7f&#39;</span>,<span style="color: #CD5555">&#39;#507d50&#39;</span>])
        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=<span style="color: #B452CD">0.8</span>)
    plt.plot(X[:, <span style="color: #B452CD">0</span>][y==<span style="color: #B452CD">0</span>], X[:, <span style="color: #B452CD">1</span>][y==<span style="color: #B452CD">0</span>], <span style="color: #CD5555">&quot;yo&quot;</span>, alpha=alpha)
    plt.plot(X[:, <span style="color: #B452CD">0</span>][y==<span style="color: #B452CD">1</span>], X[:, <span style="color: #B452CD">1</span>][y==<span style="color: #B452CD">1</span>], <span style="color: #CD5555">&quot;bs&quot;</span>, alpha=alpha)
    plt.axis(axes)
    plt.xlabel(<span style="color: #CD5555">r&quot;$x_1$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>)
    plt.ylabel(<span style="color: #CD5555">r&quot;$x_2$&quot;</span>, fontsize=<span style="color: #B452CD">18</span>, rotation=<span style="color: #B452CD">0</span>)
plt.figure(figsize=(<span style="color: #B452CD">11</span>,<span style="color: #B452CD">4</span>))
plt.subplot(<span style="color: #B452CD">121</span>)
plot_decision_boundary(tree_clf, X, y)
plt.title(<span style="color: #CD5555">&quot;Decision Tree&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)
plt.subplot(<span style="color: #B452CD">122</span>)
plot_decision_boundary(bag_clf, X, y)
plt.title(<span style="color: #CD5555">&quot;Decision Trees with Bagging&quot;</span>, fontsize=<span style="color: #B452CD">14</span>)
save_fig(<span style="color: #CD5555">&quot;baggingtree&quot;</span>)
plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec39">Making your own Bootstrap: Changing the Level of the Decision Tree </h2>

<p>
Let us bring up our good old boostrap example from the linear regression lectures. We change the linerar regression algorithm with
a decision tree wth different depths and perform a bootstrap aggregate (in this case we as many bootstraps as data points \( n \)).
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.pipeline</span> <span style="color: #8B008B; font-weight: bold">import</span> make_pipeline
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.utils</span> <span style="color: #8B008B; font-weight: bold">import</span> resample
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeRegressor

n = <span style="color: #B452CD">100</span>
n_boostraps = <span style="color: #B452CD">100</span>
maxdepth = <span style="color: #B452CD">8</span>

<span style="color: #228B22"># Make data set.</span>
x = np.linspace(-<span style="color: #B452CD">3</span>, <span style="color: #B452CD">3</span>, n).reshape(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
y = np.exp(-x**<span style="color: #B452CD">2</span>) + <span style="color: #B452CD">1.5</span> * np.exp(-(x-<span style="color: #B452CD">2</span>)**<span style="color: #B452CD">2</span>)+ np.random.normal(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">0.1</span>, x.shape)
error = np.zeros(maxdepth)
bias = np.zeros(maxdepth)
variance = np.zeros(maxdepth)
polydegree = np.zeros(maxdepth)
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=<span style="color: #B452CD">0.2</span>)

<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

<span style="color: #228B22"># we produce a simple tree first as benchmark</span>
simpletree = DecisionTreeRegressor(max_depth=<span style="color: #B452CD">3</span>) 
simpletree.fit(X_train_scaled, y_train)
simpleprediction = simpletree.predict(X_test_scaled)
<span style="color: #8B008B; font-weight: bold">for</span> degree <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>,maxdepth):
    model = DecisionTreeRegressor(max_depth=degree) 
    y_pred = np.empty((y_test.shape[<span style="color: #B452CD">0</span>], n_boostraps))
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_boostraps):
        x_, y_ = resample(X_train_scaled, y_train)
        model.fit(x_, y_)
        y_pred[:, i] = model.predict(X_test_scaled)<span style="color: #228B22">#.ravel()</span>

    polydegree[degree] = degree
    error[degree] = np.mean( np.mean((y_test - y_pred)**<span style="color: #B452CD">2</span>, axis=<span style="color: #B452CD">1</span>, keepdims=<span style="color: #658b00">True</span>) )
    bias[degree] = np.mean( (y_test - np.mean(y_pred, axis=<span style="color: #B452CD">1</span>, keepdims=<span style="color: #658b00">True</span>))**<span style="color: #B452CD">2</span> )
    variance[degree] = np.mean( np.var(y_pred, axis=<span style="color: #B452CD">1</span>, keepdims=<span style="color: #658b00">True</span>) )
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Polynomial degree:&#39;</span>, degree)
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Error:&#39;</span>, error[degree])
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Bias^2:&#39;</span>, bias[degree])
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Var:&#39;</span>, variance[degree])
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;{} &gt;= {} + {} = {}&#39;</span>.format(error[degree], bias[degree], variance[degree], bias[degree]+variance[degree]))

mse_simpletree = np.mean( np.mean((y_test - simpleprediction)**<span style="color: #B452CD">2</span>)
plt.xlim(<span style="color: #B452CD">1</span>,maxdepth)
plt.plot(polydegree, error, label=<span style="color: #CD5555">&#39;MSE simple tree&#39;</span>)
plt.plot(polydegree, mse_simpletree, label=<span style="color: #CD5555">&#39;MSE for Bootstrap&#39;</span>)
plt.plot(polydegree, bias, label=<span style="color: #CD5555">&#39;bias&#39;</span>)
plt.plot(polydegree, variance, label=<span style="color: #CD5555">&#39;Variance&#39;</span>)
plt.legend()
save_fig(<span style="color: #CD5555">&quot;baggingboot&quot;</span>)
plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec40">Random forests </h2>

<p>
Random forests provide an improvement over bagged trees by way of a
small tweak that decorrelates the trees.

<p>
As in bagging, we build a
number of decision trees on bootstrapped training samples. But when
building these decision trees, each time a split in a tree is
considered, a random sample of \( m \) predictors is chosen as split
candidates from the full set of \( p \) predictors. The split is allowed to
use only one of those \( m \) predictors.

<p>
A fresh sample of \( m \) predictors is
taken at each split, and typically we choose

<p>&nbsp;<br>
$$
m\approx \sqrt{p}.
$$
<p>&nbsp;<br>

<p>
In building a random forest, at
each split in the tree, the algorithm is not even allowed to consider
a majority of the available predictors.

<p>
The reason for this is rather clever. Suppose that there is one very
strong predictor in the data set, along with a number of other
moderately strong predictors. Then in the collection of bagged
variable importance random forest trees, most or all of the trees will
use this strong predictor in the top split. Consequently, all of the
bagged trees will look quite similar to each other. Hence the
predictions from the bagged trees will be highly correlated.
Unfortunately, averaging many highly correlated quantities does not
lead to as large of a reduction in variance as averaging many
uncorrelated quanti- ties. In particular, this means that bagging will
not lead to a substantial reduction in variance over a single tree in
this setting.
</section>


<section>
<h2 id="___sec41">Random Forest Algorithm </h2>
The algorithm described here can be applied to both classification and regression problems.

<p>
We will grow of forest of say \( M \) trees.

<ol>
<p><li> For \( m=1:M \) we</li>

<ul>

<p><li> Draw a bootstrap sample of from the training data organized in our \( \boldsymbol{X} \) matrix.</li>

<p><li> We grow then a random forest tree \( T_m \) based on the bootstrapped data by repeating the steps outlined till we reach the maximum node size is reached</li>

<ol>

<p><li> we select \( m \le p \) varibales at random from the \( p \) predictors/features</li>

<p><li> pick the best split point among the \( m \) features using either the CART algorithm or the ID3 for classification and create a new node</li>

<p><li> split the node into daughter nodes</li>
</ol>
<p>
</ul>
<p><li> Output then the ensemble of trees \( \{T_m\}_1^{M} \) and make predictions for either a regression type of problem or a classification type of problem.</li> 
</ol>
</section>


<section>
<h2 id="___sec42">Random Forests Compared with other Methods on the Cancer Data </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span>  train_test_split 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> load_breast_cancer
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.svm</span> <span style="color: #8B008B; font-weight: bold">import</span> SVC
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> LogisticRegression
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.tree</span> <span style="color: #8B008B; font-weight: bold">import</span> DecisionTreeClassifier

<span style="color: #228B22"># Load the data</span>
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data,cancer.target,random_state=<span style="color: #B452CD">0</span>)
<span style="color: #8B008B; font-weight: bold">print</span>(X_train.shape)
<span style="color: #8B008B; font-weight: bold">print</span>(X_test.shape)
<span style="color: #228B22"># Logistic Regression</span>
logreg = LogisticRegression(solver=<span style="color: #CD5555">&#39;lbfgs&#39;</span>)
logreg.fit(X_train, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Logistic Regression: {:.2f}&quot;</span>.format(logreg.score(X_test,y_test)))
<span style="color: #228B22"># Support vector machine</span>
svm = SVC(gamma=<span style="color: #CD5555">&#39;auto&#39;</span>, C=<span style="color: #B452CD">100</span>)
svm.fit(X_train, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with SVM: {:.2f}&quot;</span>.format(svm.score(X_test,y_test)))
<span style="color: #228B22"># Decision Trees</span>
deep_tree_clf = DecisionTreeClassifier(max_depth=<span style="color: #658b00">None</span>)
deep_tree_clf.fit(X_train, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Decision Trees: {:.2f}&quot;</span>.format(deep_tree_clf.score(X_test,y_test)))
<span style="color: #228B22">#now scale the data</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
<span style="color: #228B22"># Logistic Regression</span>
logreg.fit(X_train_scaled, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy Logistic Regression with scaled data: {:.2f}&quot;</span>.format(logreg.score(X_test_scaled,y_test)))
<span style="color: #228B22"># Support Vector Machine</span>
svm.fit(X_train_scaled, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy SVM with scaled data: {:.2f}&quot;</span>.format(logreg.score(X_test_scaled,y_test)))
<span style="color: #228B22"># Decision Trees</span>
deep_tree_clf.fit(X_train_scaled, y_train)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Decision Trees and scaled data: {:.2f}&quot;</span>.format(deep_tree_clf.score(X_test_scaled,y_test)))


<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> RandomForestClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> LabelEncoder
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> cross_validate
<span style="color: #228B22"># Data set not specificied</span>
<span style="color: #228B22">#Instantiate the model with 500 trees and entropy as splitting criteria</span>
Random_Forest_model = RandomForestClassifier(n_estimators=<span style="color: #B452CD">500</span>,criterion=<span style="color: #CD5555">&quot;entropy&quot;</span>)
Random_Forest_model.fit(X_train_scaled, y_train)
<span style="color: #228B22">#Cross validation</span>
accuracy = cross_validate(Random_Forest_model,X_test_scaled,y_test,cv=<span style="color: #B452CD">10</span>)[<span style="color: #CD5555">&#39;test_score&#39;</span>]
<span style="color: #8B008B; font-weight: bold">print</span>(accuracy)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Random Forests and scaled data: {:.2f}&quot;</span>.format(Random_Forest_model.score(X_test_scaled,y_test)))


<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scikitplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">skplt</span>
y_pred = Random_Forest_model.predict(X_test_scaled)
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=<span style="color: #658b00">True</span>)
plt.show()
y_probas = Random_Forest_model.predict_proba(X_test_scaled)
skplt.metrics.plot_roc(y_test, y_probas)
plt.show()
skplt.metrics.plot_cumulative_gain(y_test, y_probas)
plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec43">Compare  Bagging on Trees with Random Forests  </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>bag_clf = BaggingClassifier(
    DecisionTreeClassifier(splitter=<span style="color: #CD5555">&quot;random&quot;</span>, max_leaf_nodes=<span style="color: #B452CD">16</span>, random_state=<span style="color: #B452CD">42</span>),
    n_estimators=<span style="color: #B452CD">500</span>, max_samples=<span style="color: #B452CD">1.0</span>, bootstrap=<span style="color: #658b00">True</span>, n_jobs=-<span style="color: #B452CD">1</span>, random_state=<span style="color: #B452CD">42</span>)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span>bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> RandomForestClassifier
rnd_clf = RandomForestClassifier(n_estimators=<span style="color: #B452CD">500</span>, max_leaf_nodes=<span style="color: #B452CD">16</span>, n_jobs=-<span style="color: #B452CD">1</span>, random_state=<span style="color: #B452CD">42</span>)
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)
np.sum(y_pred == y_pred_rf) / <span style="color: #658b00">len</span>(y_pred) 
</pre></div>
</section>


<section>
<h2 id="___sec44">Boosting, a Bird's Eye View </h2>

<p>
The basic idea is to combine weak classifiers in order to create a good
classifier. With a weak classifier we often intend a classifier which
produces results which are only slightly better than we would get by
random guesses.

<p>
This is done by applying in an iterative way a weak (or a standard
classifier like decision trees) to modify the data. In each iteration
we emphasize those observations which are misclassified by weighting
them with a factor.
</section>


<section>
<h2 id="___sec45">What is boosting? Additive Modelling/Iterative Fitting </h2>

<p>
Boosting is a way of fitting an additive expansion in a set of
elementary basis functions like for example some simple polynomials.
Assume for example that we have a function
<p>&nbsp;<br>
$$
f_M(x) = \sum_{i=1}^M \beta_m b(x;\gamma_m),
$$
<p>&nbsp;<br>

<p>
where \( \beta_m \) are the expansion parameters to be determined in a
minimization process and \( b(x;\gamma_m) \) are some simple functions of
the multivariable parameter \( x \) which is characterized by the
parameters \( \gamma_m \).

<p>
As an example, consider the Sigmoid function we used in logistic
regression. In that case, we can translate the function
\( b(x;\gamma_m) \) into the Sigmoid function

<p>&nbsp;<br>
$$
\sigma(t) = \frac{1}{1+\exp{(-t)}},
$$
<p>&nbsp;<br>

<p>
where \( t=\gamma_0+\gamma_1 x \) and the parameters \( \gamma_0 \) and
\( \gamma_1 \) where determined by the Logistic Regression fitting
algorithm.

<p>
As another example, consider the cost function we defined for linear regression
<p>&nbsp;<br>
$$
C(\boldsymbol{y},\boldsymbol{f}) = \frac{1}{n} \sum_{i=0}^{n-1}(y_i-f(x_i))^2.
$$
<p>&nbsp;<br>

<p>
In this case the function \( f(x) \) was replaced by the design matrix
\( \boldsymbol{X} \) and the unknown linear regression parameters \( \boldsymbol{\beta} \),
that is \( \boldsymbol{f}=\boldsymbol{X}\boldsymbol{\beta} \). In linear regression we could
simply invert a matrix and obtained the parameters \( \beta \) by

<p>&nbsp;<br>
$$
\boldsymbol{\beta}=\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.
$$
<p>&nbsp;<br>

<p>
In iterative fitting or additive modeling, we minimize the cost function with respect to the parameters \( \beta_m \) and \( \gamma_m \).
</section>


<section>
<h2 id="___sec46">Iterative Fitting, Regression and Squared-error Cost Function </h2>

<p>
The way we proceed is as follows (here we specialize to the squared-error cost function)

<ol>
<p><li> Establish a cost function, here \( {\cal C}(\boldsymbol{y},\boldsymbol{f}) = \frac{1}{n} \sum_{i=0}^{n-1}(y_i-f_M(x_i))^2 \) with \( f_M(x) = \sum_{i=1}^M \beta_m b(x;\gamma_m) \).</li>
<p><li> Initialize with a guess \( f_0(x) \). It could be one or even zero or some random numbers.</li>
<p><li> For \( m=1:M \)

<ol type="a"></li>
 <p><li> minimize \( \sum_{i=0}^{n-1}(y_i-f_{m-1}(x_i)-\beta b(x;\gamma))^2 \) wrt \( \gamma \) and \( \beta \)</li>
 <p><li> This gives the optimal values \( \beta_m \) and \( \gamma_m \)</li>
 <p><li> Determine then the new values \( f_m(x)=f_{m-1}(x) +\beta_m b(x;\gamma_m) \)</li>
</ol>
<p>
</ol>
<p>

We could use any of the algorithms we have discussed till now. If we
use trees, \( \gamma \) parameterizes the split variables and split points
at the internal nodes, and the predictions at the terminal nodes.
</section>


<section>
<h2 id="___sec47">Squared-Error Example and Iterative Fitting </h2>

<p>
To better understand what happens, let us develop the steps for the iterative fitting using the above squared error function.

<p>
For simplicity we assume also that our functions \( b(x;\gamma)=1+\gamma x \).

<p>
This means that for every iteration, we need to optimize

<p>&nbsp;<br>
$$
(\beta_m,\gamma_m)  \mathrm{argmin}_{\beta,\lambda}\hspace{0.1cm} \sum_{i=0}^{n-1}(y_i-f_{m-1}(x_i)-\beta b(x;\gamma))^2=\sum_{i=0}^{n-1}(y_i-f_{m-1}(x_i)-\beta(1+\gamma x_i))^2.
$$
<p>&nbsp;<br>

<p>
We start our iteration by simply setting \( f_0(x)=0 \). 
Taking the derivatives  with respect to \( \beta \) and \( \gamma \) we obtain
<p>&nbsp;<br>
$$
\frac{\partial {\cal C}}{\partial \beta} = -2\sum_{i}(1+\gamma x_i)(y_i-\beta(1+\gamma x_i))=0,
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
\frac{\partial {\cal C}}{\partial \gamma} =-2\sum_{i}\beta x_i(y_i-\beta(1+\gamma x_i))=0.
$$
<p>&nbsp;<br>

We can then rewrite these equations as (defining \( \boldsymbol{w}=\boldsymbol{e}+\gamma \boldsymbol{x}) \) with \( \boldsymbol{e} \) being the unit vector)
<p>&nbsp;<br>
$$
\gamma \boldsymbol{w}^T(\boldsymbol{y}-\beta\gamma \boldsymbol{w})=0,
$$
<p>&nbsp;<br>

which gives us \( \beta = \boldsymbol{w}^T\boldsymbol{y}/(\boldsymbol{w}^T\boldsymbol{w}) \). Similarly we have 
<p>&nbsp;<br>
$$
\beta\gamma \boldsymbol{x}^T(\boldsymbol{y}-\beta(1+\gamma \boldsymbol{x}))=0,
$$
<p>&nbsp;<br>

<p>
which leads to \( \gamma =(\boldsymbol{x}^T\boldsymbol{y}-\beta\boldsymbol{x}^T\boldsymbol{e})/(\beta\boldsymbol{x}^T\boldsymbol{x}) \).  Inserting
for \( \beta \) gives us an equation for \( \gamma \). This is a non-linear equation in the unknown \( \gamma \) and has to be solved numerically.

<p>
The solution to these two equations gives us in turn \( \beta_1 \) and \( \gamma_1 \) leading to the new expression for \( f_1(x) \) as
\( f_1(x) = \beta_1(1+\gamma_1x) \). Doing this \( M \) times results in our final estimate for the function \( f \).
</section>


<section>
<h2 id="___sec48">Iterative Fitting, Classification and AdaBoost </h2>

<p>
Let us consider a binary classification problem with two outcomes \( y_i \in \{-1,1\} \) and \( i=0,1,2,\dots,n-1 \) as our set of
observations. We define a classification function \( G(x) \) which produces a prediction taking any of the two values 
\( \{-1,1\} \).

<p>
The error rate of the training sample is then

<p>&nbsp;<br>
$$
\mathrm{\overline{err}}=\frac{1}{n} \sum_{i=0}^{n-1} I(y_i\ne G(x_i)). 
$$
<p>&nbsp;<br>

<p>
The iterative procedure starts with defining a weak classifier whose
error rate is barely better than random guessing.  The iterative
procedure in boosting is to sequentially apply a the weak
classification algorithm to repeatedly modified versions of the data
producing a sequence of weak classifiers \( G_m(x) \).

<p>
Here we will express our  function \( f(x) \) in terms of \( G(x) \). That is
<p>&nbsp;<br>
$$
f_M(x) = \sum_{i=1}^M \beta_m b(x;\gamma_m),
$$
<p>&nbsp;<br>

will be a function of 
<p>&nbsp;<br>
$$
G_M(x) = \mathrm{sign} \sum_{i=1}^M \alpha_m G_m(x).
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="___sec49">Adaptive Boosting, AdaBoost </h2>

<p>
In our iterative procedure we define thus
<p>&nbsp;<br>
$$
f_m(x) = f_{m-1}(x)+\beta_mG_m(x).
$$
<p>&nbsp;<br>

<p>
The simplest possible cost function which leads (also simple from a computational point of view) to the AdaBoost algorithm is the
exponential cost/loss function defined as
<p>&nbsp;<br>
$$
C(\boldsymbol{y},\boldsymbol{f}) = \sum_{i=0}^{n-1}\exp{(-y_i(f_{m-1}(x_i)+\beta G(x_i))}.
$$
<p>&nbsp;<br>

<p>
We optimize \( \beta \) and \( G \) for each value of \( m=1:M \) as we did in the regression case.
This is normally done in two steps. Let us however first rewrite the cost function as

<p>&nbsp;<br>
$$
C(\boldsymbol{y},\boldsymbol{f}) = \sum_{i=0}^{n-1}w_i^{m}\exp{(-y_i\beta G(x_i))},
$$
<p>&nbsp;<br>

where we have defined \( w_i^m= \exp{(-y_if_{m-1}(x_i))} \).
</section>


<section>
<h2 id="___sec50">Building up AdaBoost </h2>

<p>
First, for any \( \beta > 0 \), we optimize \( G \) by setting
<p>&nbsp;<br>
$$
G_m(x) = \mathrm{sign} \sum_{i=0}^{n-1} w_i^m I(y_i \ne G_(x_i)),
$$
<p>&nbsp;<br>

which is the classifier that minimizes the weighted error rate in predicting \( y \).

<p>
We can do this by rewriting
<p>&nbsp;<br>
$$
\exp{-(\beta)}\sum_{y_i=G(x_i)}w_i^m+\exp{(\beta)}\sum_{y_i\ne G(x_i)}w_i^m,
$$
<p>&nbsp;<br>

which can be rewritten as
<p>&nbsp;<br>
$$
(\exp{(\beta)}-\exp{-(\beta)})\sum_{i=0}^{n-1}w_i^mI(y_i\ne G(x_i))+\exp{(-\beta)}\sum_{i=0}^{n-1}w_i^m=0,
$$
<p>&nbsp;<br>

which leads to
<p>&nbsp;<br>
$$
\beta_m = \frac{1}{2}\log{\frac{1-\mathrm{\overline{err}}}{\mathrm{\overline{err}}}},
$$
<p>&nbsp;<br>

where we have redefined the error as 
<p>&nbsp;<br>
$$
\mathrm{\overline{err}}_m=\frac{1}{n}\frac{\sum_{i=0}^{n-1}w_i^mI(y_i\ne G(x_i)}{\sum_{i=0}^{n-1}w_i^m},
$$
<p>&nbsp;<br>

which leads to an update of
<p>&nbsp;<br>
$$
f_m(x) = f_{m-1}(x) +\beta_m G_m(x).
$$
<p>&nbsp;<br>

This leads to the new weights
<p>&nbsp;<br>
$$
w_i^{m+1} = w_i^m \exp{(-y_i\beta_m G_m(x_i))}
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="___sec51">Adaptive boosting: AdaBoost, Basic Algorithm </h2>

<p>
The algorithm here is rather straightforward. Assume that our weak
classifier is a decision tree and we consider a binary set of outputs
with \( y_i \in \{-1,1\} \) and \( i=0,1,2,\dots,n-1 \) as our set of
observations. Our design matrix is given in terms of the
feature/predictor vectors
\( \boldsymbol{X}=[\boldsymbol{x}_0\boldsymbol{x}_1\dots\boldsymbol{x}_{p-1}] \). Finally, we define also a
classifier determined by our data via a function \( G(x) \). This function tells us how well we are able to classify our outputs/targets \( \boldsymbol{y} \).

<p>
We have already defined the misclassification error \( \mathrm{err} \) as
<p>&nbsp;<br>
$$
\mathrm{err}=\frac{1}{n}\sum_{i=0}^{n-1}I(y_i\ne G(x_i)),
$$
<p>&nbsp;<br>

where the function \( I() \) is one if we misclassify and zero if we classify correctly.
</section>


<section>
<h2 id="___sec52">Basic Steps of AdaBoost </h2>

<p>
With the above definitions we are now ready to set up the algorithm for AdaBoost.
The basic idea is to set up weights which will be used to scale the correctly classified and the misclassified cases.

<ol>
<p><li> We start by initializing all weights to \( w_i = 1/n \), with \( i=0,1,2,\dots n-1 \). It is to see then that \( \sum_{i=0}^{n-1}w_i = 1 \).</li>
<p><li> We rewrite the misclassification error as</li> 
</ol>
<p>&nbsp;<br>
$$
\mathrm{\overline{err}}_m=\frac{\sum_{i=0}^{n-1}w_i^m I(y_i\ne G(x_i))}{\sum_{i=0}^{n-1}w_i},
$$
<p>&nbsp;<br>


<ol>
<p><li> Then we start looping over all attempts at classifying, namely we start an iterative process for \( m=1:M \), where \( M \) is the final number of classifications. Our given classifier could for example be a plain decision tree.

<ol type="a"></li>
 <p><li> Fit then a given classifier to the training using the weights \( w_i \).</li>
 <p><li> Compute then \( \mathrm{err} \) and figure out which events are classified properly and which are classified wrongly.</li>
 <p><li> Define a quantity \( \alpha_{m} = \log{(1-\mathrm{\overline{err}}_m)/\mathrm{\overline{err}}_m} \)</li>
 <p><li> Set the new weights to \( w_i = w_i\times \exp{(\alpha_m I(y_i\ne G(x_i)} \).</li>
</ol>
<p><li> Compute the new classifier \( G(x)= \sum_{i=0}^{n-1}\alpha_m I(y_i\ne G(x_i) \).</li>
</ol>
<p>

For the iterations with \( m \le 2 \) the weights are modified
individually at each steps. The obersvations which were misclassified
at iteration \( m-1 \) have a weight which is larger than those which were
classified properly. As this proceeds, the observations which were
difficult to classifiy correctly are given a larger influence. Each
new classification step \( m \) is then forced to concentrate on those
observations that are missed in the previous iterations.
</section>


<section>
<h2 id="___sec53">AdaBoost Examples </h2>

<p>
Using <b>Scikit-Learn</b> it is easy to appply the adaptive boosting algorithm, as done here.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> AdaBoostClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=<span style="color: #B452CD">1</span>), n_estimators=<span style="color: #B452CD">200</span>,
    algorithm=<span style="color: #CD5555">&quot;SAMME.R&quot;</span>, learning_rate=<span style="color: #B452CD">0.5</span>, random_state=<span style="color: #B452CD">42</span>)
ada_clf.fit(X_train, y_train)

<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> AdaBoostClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=<span style="color: #B452CD">1</span>), n_estimators=<span style="color: #B452CD">200</span>,
    algorithm=<span style="color: #CD5555">&quot;SAMME.R&quot;</span>, learning_rate=<span style="color: #B452CD">0.5</span>, random_state=<span style="color: #B452CD">42</span>)
ada_clf.fit(X_train_scaled, y_train)
y_pred = ada_clf.predict(X_test_scaled)
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=<span style="color: #658b00">True</span>)
plt.show()
y_probas = ada_clf.predict_proba(X_test_scaled)
skplt.metrics.plot_roc(y_test, y_probas)
plt.show()
skplt.metrics.plot_cumulative_gain(y_test, y_probas)
plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec54">Gradient boosting: Basics with Steepest Descent </h2>

<p>
Gradient boosting is again a similar technique to Adaptive boosting,
it combines so-called weak classifiers or regressors into a strong
method via a series of iterations.

<p>
In order to understand the method, let us illustrate its basics by
bringing back the essential steps in linear regression, where our cost
function was the least squares function.
</section>


<section>
<h2 id="___sec55">The Squared-Error again! Steepest Descent </h2>

<p>
We start again with our cost function \( {\cal C}(\boldsymbol{y}m\boldsymbol{f})=\sum_{i=0}^{n-1}{\cal L}(y_i, f(x_i)) \) where we want to minimize
This means that for every iteration, we need to optimize

<p>&nbsp;<br>
$$
(\hat{\boldsymbol{f}}) = \mathrm{argmin}_{\boldsymbol{f}}\hspace{0.1cm} \sum_{i=0}^{n-1}(y_i-f(x_i))^2.
$$
<p>&nbsp;<br>

<p>
We define a real function \( h_m(x) \) that defines our final function \( f_M(x) \) as
<p>&nbsp;<br>
$$
f_M(x) = \sum_{m=0}^M h_m(x).
$$
<p>&nbsp;<br>

<p>
In the steepest decent approach we approximate \( h_m(x) = -\rho_m g_m(x) \), where \( \rho_m \) is a scalar and \( g_m(x) \) the gradient defined as
<p>&nbsp;<br>
$$
g_m(x_i) = \left[ \frac{\partial {\cal L}(y_i, f(x_i))}{\partial f(x_i)}\right]_{f(x_i)=f_{m-1}(x_i)}.
$$
<p>&nbsp;<br>

<p>
With the new gradient we can update \( f_m(x) = f_{m-1}(x) -\rho_m g_m(x) \). Using the above squared-error function we see that
the gradient is \( g_m(x_i) = -2(y_i-f(x_i)) \).

<p>
Choosing \( f_0(x)=0 \) we obtain \( g_m(x) = -2y_i \) and inserting this into the minimization problem for the cost function we have
<p>&nbsp;<br>
$$
(\rho_1) = \mathrm{argmin}_{\rho}\hspace{0.1cm} \sum_{i=0}^{n-1}(y_i+2\rho y_i)^2.
$$
<p>&nbsp;<br>
</section>


<section>
<h2 id="___sec56">Steepest Descent Example </h2>

<p>
Optimizing with respect to \( \rho \) we obtain (taking the derivative) that \( \rho_1 = -1/2 \). We have then that
<p>&nbsp;<br>
$$
f_1(x) = f_{0}(x) -\rho_1 g_1(x)=-y_i.
$$
<p>&nbsp;<br>

We can then proceed and compute
<p>&nbsp;<br>
$$
g_2(x_i) = \left[ \frac{\partial {\cal L}(y_i, f(x_i))}{\partial f(x_i)}\right]_{f(x_i)=f_{1}(x_i)=y_i}=-4y_i,
$$
<p>&nbsp;<br>

and find a new value for \( \rho_2=-1/2 \) and continue till we have reached \( m=M \). We can modify the steepest descent method, or steepest boosting, by introducing what is called <b>gradient boosting</b>.
</section>


<section>
<h2 id="___sec57">Gradient Boosting, algorithm </h2>

<p>
Suppose we have a cost function \( C(f)=\sum_{i=0}^{n-1}L(y_i, f(x_i)) \) where \( y_i \) is our target and \( f(x_i) \) the function which is meant to model \( y_i \). The above cost function could be our standard  squared-error  function
<p>&nbsp;<br>
$$
C(\boldsymbol{y},\boldsymbol{f})=\sum_{i=0}^{n-1}(y_i-f(x_i))^2.
$$
<p>&nbsp;<br>

<p>
The way we proceed in an iterative fashion is to

<ol>
<p><li> Initialize our estimate \( f_0(x) \).</li>
<p><li> For \( m=1:M \), we

<ol type="a"></li>
 <p><li> compute the negative gradient vector \( \boldsymbol{u}_m = -\partial C(\boldsymbol{y},\boldsymbol{f})/\partial \boldsymbol{f}(x) \) at \( f(x) = f_{m-1}(x) \);</li>
 <p><li> fit the so-called base-learner to the negative gradient \( h_m(u_m,x) \);</li>
 <p><li> update the estimate \( f_m(x) = f_{m-1}(x)+\nu h_m(u_m,x) \);</li>
</ol>
<p><li> The final estimate is then \( f_M(x) = \sum_{m=1}^M\nu h_m(u_m,x) \).</li>
</ol>
</section>


<section>
<h2 id="___sec58">Gradient Boosting Example, Regression </h2>

<p>
We discuss here the difference between the steepest descent approach and gradient boosting by repeating our simple regression example above.
</section>


<section>
<h2 id="___sec59">Gradient Boosting, Examples of Regression </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> GradientBoostingRegressor
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scikitplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">skplt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> mean_squared_error

n = <span style="color: #B452CD">100</span>
maxdegree = <span style="color: #B452CD">6</span>

<span style="color: #228B22"># Make data set.</span>
x = np.linspace(-<span style="color: #B452CD">3</span>, <span style="color: #B452CD">3</span>, n).reshape(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
y = np.exp(-x**<span style="color: #B452CD">2</span>) + <span style="color: #B452CD">1.5</span> * np.exp(-(x-<span style="color: #B452CD">2</span>)**<span style="color: #B452CD">2</span>)+ np.random.normal(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">0.1</span>, x.shape)

error = np.zeros(maxdegree)
bias = np.zeros(maxdegree)
variance = np.zeros(maxdegree)
polydegree = np.zeros(maxdegree)
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=<span style="color: #B452CD">0.2</span>)
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

<span style="color: #8B008B; font-weight: bold">for</span> degree <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>,maxdegree):
    model = GradientBoostingRegressor(max_depth=degree, n_estimators=<span style="color: #B452CD">100</span>, learning_rate=<span style="color: #B452CD">1.0</span>)  
    model.fit(X_train_scaled,y_train)
    y_pred = model.predict(X_test_scaled)
    polydegree[degree] = degree
    error[degree] = np.mean( np.mean((y_test - y_pred)**<span style="color: #B452CD">2</span>) )
    bias[degree] = np.mean( (y_test - np.mean(y_pred))**<span style="color: #B452CD">2</span> )
    variance[degree] = np.mean( np.var(y_pred) )
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Max depth:&#39;</span>, degree)
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Error:&#39;</span>, error[degree])
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Bias^2:&#39;</span>, bias[degree])
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Var:&#39;</span>, variance[degree])
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;{} &gt;= {} + {} = {}&#39;</span>.format(error[degree], bias[degree], variance[degree], bias[degree]+variance[degree]))

plt.xlim(<span style="color: #B452CD">1</span>,maxdegree-<span style="color: #B452CD">1</span>)
plt.plot(polydegree, error, label=<span style="color: #CD5555">&#39;Error&#39;</span>)
plt.plot(polydegree, bias, label=<span style="color: #CD5555">&#39;bias&#39;</span>)
plt.plot(polydegree, variance, label=<span style="color: #CD5555">&#39;Variance&#39;</span>)
plt.legend()
save_fig(<span style="color: #CD5555">&quot;gdregression&quot;</span>)
plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec60">Gradient Boosting, Classification Example </h2>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span>  train_test_split 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> load_breast_cancer
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scikitplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">skplt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.ensemble</span> <span style="color: #8B008B; font-weight: bold">import</span> GradientBoostingClassifier
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> cross_validate

<span style="color: #228B22"># Load the data</span>
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data,cancer.target,random_state=<span style="color: #B452CD">0</span>)
<span style="color: #8B008B; font-weight: bold">print</span>(X_train.shape)
<span style="color: #8B008B; font-weight: bold">print</span>(X_test.shape)
<span style="color: #228B22">#now scale the data</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

gd_clf = GradientBoostingClassifier(max_depth=<span style="color: #B452CD">3</span>, n_estimators=<span style="color: #B452CD">100</span>, learning_rate=<span style="color: #B452CD">1.0</span>)  
gd_clf.fit(X_train_scaled, y_train)
<span style="color: #228B22">#Cross validation</span>
accuracy = cross_validate(gd_clf,X_test_scaled,y_test,cv=<span style="color: #B452CD">10</span>)[<span style="color: #CD5555">&#39;test_score&#39;</span>]
<span style="color: #8B008B; font-weight: bold">print</span>(accuracy)
<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Random Forests and scaled data: {:.2f}&quot;</span>.format(gd_clf.score(X_test_scaled,y_test)))

<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scikitplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">skplt</span>
y_pred = gd_clf.predict(X_test_scaled)
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=<span style="color: #658b00">True</span>)
save_fig(<span style="color: #CD5555">&quot;gdclassiffierconfusion&quot;</span>)
plt.show()
y_probas = gd_clf.predict_proba(X_test_scaled)
skplt.metrics.plot_roc(y_test, y_probas)
save_fig(<span style="color: #CD5555">&quot;gdclassiffierroc&quot;</span>)
plt.show()
skplt.metrics.plot_cumulative_gain(y_test, y_probas)
save_fig(<span style="color: #CD5555">&quot;gdclassiffiercgain&quot;</span>)
plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec61">XGBoost: Extreme Gradient Boosting </h2>

<p>
<a href="https://github.com/dmlc/xgboost" target="_blank">XGBoost</a> or Extreme Gradient
Boosting, is an optimized distributed gradient boosting library
designed to be highly efficient, flexible and portable. It implements
machine learning algorithms under the Gradient Boosting
framework. XGBoost provides a parallel tree boosting that solve many
data science problems in a fast and accurate way. See the <a href="https://arxiv.org/abs/1603.02754" target="_blank">article by Chen and Guestrin</a>.

<p>
The authors design and build a highly scalable end-to-end tree
boosting system. It has  a theoretically justified weighted quantile
sketch for efficient proposal calculation. It introduces a novel sparsity-aware algorithm for parallel tree learning and an effective cache-aware block structure for out-of-core tree learning.

<p>
It is now the algorithm which wins essentially all ML competitions!!!
</section>


<section>
<h2 id="___sec62">Regression Case </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> train_test_split
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">xgboost</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">xgb</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scikitplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">skplt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.metrics</span> <span style="color: #8B008B; font-weight: bold">import</span> mean_squared_error

n = <span style="color: #B452CD">100</span>
maxdegree = <span style="color: #B452CD">6</span>

<span style="color: #228B22"># Make data set.</span>
x = np.linspace(-<span style="color: #B452CD">3</span>, <span style="color: #B452CD">3</span>, n).reshape(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">1</span>)
y = np.exp(-x**<span style="color: #B452CD">2</span>) + <span style="color: #B452CD">1.5</span> * np.exp(-(x-<span style="color: #B452CD">2</span>)**<span style="color: #B452CD">2</span>)+ np.random.normal(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">0.1</span>, x.shape)

error = np.zeros(maxdegree)
bias = np.zeros(maxdegree)
variance = np.zeros(maxdegree)
polydegree = np.zeros(maxdegree)
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=<span style="color: #B452CD">0.2</span>)
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

<span style="color: #8B008B; font-weight: bold">for</span> degree <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(maxdegree):
    model =  xgb.XGBRegressor(objective =<span style="color: #CD5555">&#39;reg:squarederror&#39;</span>, colsaobjective =<span style="color: #CD5555">&#39;reg:squarederror&#39;</span>, colsample_bytree = <span style="color: #B452CD">0.3</span>, learning_rate = <span style="color: #B452CD">0.1</span>,max_depth = degree, alpha = <span style="color: #B452CD">10</span>, n_estimators = <span style="color: #B452CD">200</span>)

    model.fit(X_train_scaled,y_train)
    y_pred = model.predict(X_test_scaled)
    polydegree[degree] = degree
    error[degree] = np.mean( np.mean((y_test - y_pred)**<span style="color: #B452CD">2</span>) )
    bias[degree] = np.mean( (y_test - np.mean(y_pred))**<span style="color: #B452CD">2</span> )
    variance[degree] = np.mean( np.var(y_pred) )
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Max depth:&#39;</span>, degree)
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Error:&#39;</span>, error[degree])
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Bias^2:&#39;</span>, bias[degree])
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;Var:&#39;</span>, variance[degree])
    <span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&#39;{} &gt;= {} + {} = {}&#39;</span>.format(error[degree], bias[degree], variance[degree], bias[degree]+variance[degree]))

plt.xlim(<span style="color: #B452CD">1</span>,maxdegree-<span style="color: #B452CD">1</span>)
plt.plot(polydegree, error, label=<span style="color: #CD5555">&#39;Error&#39;</span>)
plt.plot(polydegree, bias, label=<span style="color: #CD5555">&#39;bias&#39;</span>)
plt.plot(polydegree, variance, label=<span style="color: #CD5555">&#39;Variance&#39;</span>)
plt.legend()
plt.show()
</pre></div>
</section>


<section>
<h2 id="___sec63">Xgboost on the Cancer Data </h2>

<p>
As you will see from the confusion matrix below, XGBoots does an excellent job on the Wisconsin cancer data and outperforms essentially all agorithms we have discussed till now. 
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span></span><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span>  train_test_split 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.datasets</span> <span style="color: #8B008B; font-weight: bold">import</span> load_breast_cancer
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> LabelEncoder
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.model_selection</span> <span style="color: #8B008B; font-weight: bold">import</span> cross_validate
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scikitplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">skplt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">xgboost</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">xgb</span>
<span style="color: #228B22"># Load the data</span>
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data,cancer.target,random_state=<span style="color: #B452CD">0</span>)
<span style="color: #8B008B; font-weight: bold">print</span>(X_train.shape)
<span style="color: #8B008B; font-weight: bold">print</span>(X_test.shape)
<span style="color: #228B22">#now scale the data</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.preprocessing</span> <span style="color: #8B008B; font-weight: bold">import</span> StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

xg_clf = xgb.XGBClassifier()
xg_clf.fit(X_train_scaled,y_train)

y_test = xg_clf.predict(X_test_scaled)

<span style="color: #8B008B; font-weight: bold">print</span>(<span style="color: #CD5555">&quot;Test set accuracy with Random Forests and scaled data: {:.2f}&quot;</span>.format(xg_clf.score(X_test_scaled,y_test)))

<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scikitplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">skplt</span>
y_pred = xg_clf.predict(X_test_scaled)
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=<span style="color: #658b00">True</span>)
save_fig(<span style="color: #CD5555">&quot;xdclassiffierconfusion&quot;</span>)
plt.show()
y_probas = xg_clf.predict_proba(X_test_scaled)
skplt.metrics.plot_roc(y_test, y_probas)
save_fig(<span style="color: #CD5555">&quot;xdclassiffierroc&quot;</span>)
plt.show()
skplt.metrics.plot_cumulative_gain(y_test, y_probas)
save_fig(<span style="color: #CD5555">&quot;gdclassiffiercgain&quot;</span>)
plt.show()


xgb.plot_tree(xg_clf,num_trees=<span style="color: #B452CD">0</span>)
plt.rcParams[<span style="color: #CD5555">&#39;figure.figsize&#39;</span>] = [<span style="color: #B452CD">50</span>, <span style="color: #B452CD">10</span>]
save_fig(<span style="color: #CD5555">&quot;xgtree&quot;</span>)
plt.show()

xgb.plot_importance(xg_clf)
plt.rcParams[<span style="color: #CD5555">&#39;figure.figsize&#39;</span>] = [<span style="color: #B452CD">5</span>, <span style="color: #B452CD">5</span>]
save_fig(<span style="color: #CD5555">&quot;xgparams&quot;</span>)
plt.show()
</pre></div>
</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

    // Display navigation controls in the bottom right corner
    controls: true,

    // Display progress bar (below the horiz. slider)
    progress: true,

    // Display the page number of the current slide
    slideNumber: true,

    // Push each slide change to the browser history
    history: false,

    // Enable keyboard shortcuts for navigation
    keyboard: true,

    // Enable the slide overview mode
    overview: true,

    // Vertical centering of slides
    //center: true,
    center: false,

    // Enables touch navigation on devices with touch input
    touch: true,

    // Loop the presentation
    loop: false,

    // Change the presentation direction to be RTL
    rtl: false,

    // Turns fragments on and off globally
    fragments: true,

    // Flags if the presentation is running in an embedded mode,
    // i.e. contained within a limited portion of the screen
    embedded: false,

    // Number of milliseconds between automatically proceeding to the
    // next slide, disabled when set to 0, this value can be overwritten
    // by using a data-autoslide attribute on your slides
    autoSlide: 0,

    // Stop auto-sliding after user input
    autoSlideStoppable: true,

    // Enable slide navigation via mouse wheel
    mouseWheel: false,

    // Hides the address bar on mobile devices
    hideAddressBar: true,

    // Opens links in an iframe preview overlay
    previewLinks: false,

    // Transition style
    transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Transition speed
    transitionSpeed: 'default', // default/fast/slow

    // Transition style for full page slide backgrounds
    backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

    // Number of slides away from the current that are visible
    viewDistance: 3,

    // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

    // Parallax background size
    //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

    theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
    dependencies: [
        // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
        { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

        // Interpret Markdown in <section> elements
        { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

        // Syntax highlight for <code> elements
        { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

        // Zoom in and out with Alt+click
        { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

        // Speaker notes
        { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

        // Remote control your reveal.js presentation using a touch device
        //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

        // MathJax
        //{ src: 'reveal.js/plugin/math/math.js', async: true }
    ]
});

Reveal.initialize({

    // The "normal" size of the presentation, aspect ratio will be preserved
    // when the presentation is scaled to fit different resolutions. Can be
    // specified using percentage units.
    width: 1170,  // original: 960,
    height: 700,

    // Factor of the display size that should remain empty around the content
    margin: 0.1,

    // Bounds for smallest/largest possible scale to apply to content
    minScale: 0.2,
    maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
     end footer logo -->



</body>
</html>
