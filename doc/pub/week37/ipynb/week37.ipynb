{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d842e7e1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week37.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Week 37: Gradient descent methods -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd52479",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Week 37: Gradient descent methods\n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo, Norway\n",
    "\n",
    "Date: **September 8-12, 2025**\n",
    "\n",
    "<!-- todo add link to videos and add link to Van Wieringens notes -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b6141",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Plans for week 37, lecture Monday\n",
    "\n",
    "**Plans and material  for the lecture on Monday September 8.**\n",
    "\n",
    "The family of gradient descent methods\n",
    "1. Plain gradient descent (constant learning rate), reminder from last week with examples using OLS and Ridge\n",
    "\n",
    "2. Improving gradient descent with momentum\n",
    "\n",
    "3. Introducing stochastic gradient descent\n",
    "\n",
    "4. More advanced updates of the learning rate: ADAgrad, RMSprop and ADAM\n",
    "\n",
    "5. [Video of Lecture](https://youtu.be/SuxK68tj-V8)\n",
    "\n",
    "6. [Whiteboard notes](https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2025/FYSSTKweek37.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd264b1c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Readings and Videos:\n",
    "1. Recommended: Goodfellow et al, Deep Learning, introduction to gradient descent, see sections 4.3-4.5  at <https://www.deeplearningbook.org/contents/numerical.html> and chapter 8.3-8.5 at <https://www.deeplearningbook.org/contents/optimization.html>\n",
    "\n",
    "2. Rashcka et al, pages 37-44 and pages 278-283 with focus on linear regression.\n",
    "\n",
    "3. Video on gradient descent at <https://www.youtube.com/watch?v=sDv4f4s2SB8>\n",
    "\n",
    "4. Video on Stochastic gradient descent at <https://www.youtube.com/watch?v=vMh0zPT0tLI>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608927bc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Material for lecture Monday September 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60640670",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradient descent and revisiting Ordinary Least Squares from last week\n",
    "\n",
    "Last week we started with  linear regression as a case study for the gradient descent\n",
    "methods. Linear regression is a great test case for the gradient\n",
    "descent methods discussed in the lectures since it has several\n",
    "desirable properties such as:\n",
    "\n",
    "1. An analytical solution (recall homework sets for week 35).\n",
    "\n",
    "2. The gradient can be computed analytically.\n",
    "\n",
    "3. The cost function is convex which guarantees that gradient descent converges for small enough learning rates\n",
    "\n",
    "We revisit an example similar to what we had in the first homework set. We have a function  of the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947b67ee",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = 2*np.random.rand(m,1)\n",
    "y = 4+3*x+np.random.randn(m,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a787eca",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $x_i \\in [0,1] $ is chosen randomly using a uniform distribution. Additionally we have a stochastic noise chosen according to a normal distribution $\\cal {N}(0,1)$. \n",
    "The linear regression model is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e84ac7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "h_\\theta(x) = \\boldsymbol{y} = \\theta_0 + \\theta_1 x,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c217e",
   "metadata": {
    "editable": true
   },
   "source": [
    "such that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b145d4eb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{y}_i = \\theta_0 + \\theta_1 x_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6d60d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradient descent example\n",
    "\n",
    "Let $\\mathbf{y} = (y_1,\\cdots,y_n)^T$, $\\mathbf{\\boldsymbol{y}} = (\\boldsymbol{y}_1,\\cdots,\\boldsymbol{y}_n)^T$ and $\\theta = (\\theta_0, \\theta_1)^T$\n",
    "\n",
    "It is convenient to write $\\mathbf{\\boldsymbol{y}} = X\\theta$ where $X \\in \\mathbb{R}^{100 \\times 2} $ is the design matrix given by (we keep the intercept here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deafba0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "X \\equiv \\begin{bmatrix}\n",
    "1 & x_1  \\\\\n",
    "\\vdots & \\vdots  \\\\\n",
    "1 & x_{100} &  \\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ac423",
   "metadata": {
    "editable": true
   },
   "source": [
    "The cost/loss/risk function is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7232b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\theta) = \\frac{1}{n}||X\\theta-\\mathbf{y}||_{2}^{2} = \\frac{1}{n}\\sum_{i=1}^{100}\\left[ (\\theta_0 + \\theta_1 x_i)^2 - 2 y_i (\\theta_0 + \\theta_1 x_i) + y_i^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194af20",
   "metadata": {
    "editable": true
   },
   "source": [
    "and we want to find $\\theta$ such that $C(\\theta)$ is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58d823",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The derivative of the cost/loss function\n",
    "\n",
    "Computing $\\partial C(\\theta) / \\partial \\theta_0$ and $\\partial C(\\theta) / \\partial \\theta_1$ we can show  that the gradient can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10129d02",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\theta} C(\\theta) = \\frac{2}{n}\\begin{bmatrix} \\sum_{i=1}^{100} \\left(\\theta_0+\\theta_1x_i-y_i\\right) \\\\\n",
    "\\sum_{i=1}^{100}\\left( x_i (\\theta_0+\\theta_1x_i)-y_ix_i\\right) \\\\\n",
    "\\end{bmatrix} = \\frac{2}{n}X^T(X\\theta - \\mathbf{y}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd07523",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $X$ is the design matrix defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda7e01",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Hessian matrix\n",
    "The Hessian matrix of $C(\\theta)$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64bdd1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{H} \\equiv \\begin{bmatrix}\n",
    "\\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0^2} & \\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0 \\partial \\theta_1}  \\\\\n",
    "\\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0 \\partial \\theta_1} & \\frac{\\partial^2 C(\\theta)}{\\partial \\theta_1^2} &  \\\\\n",
    "\\end{bmatrix} = \\frac{2}{n}X^T X.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f4c5d",
   "metadata": {
    "editable": true
   },
   "source": [
    "This result implies that $C(\\theta)$ is a convex function since the matrix $X^T X$ always is positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed73a8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple program\n",
    "\n",
    "We can now write a program that minimizes $C(\\theta)$ using the gradient descent method with a constant learning rate $\\eta$ according to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70ad9b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\eta \\nabla_\\theta C(\\theta_k), \\ k=0,1,\\cdots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbef92d",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can use the expression we computed for the gradient and let use a\n",
    "$\\theta_0$ be chosen randomly and let $\\eta = 0.001$. Stop iterating\n",
    "when $||\\nabla_\\theta C(\\theta_k) || \\leq \\epsilon = 10^{-8}$. **Note that the code below does not include the latter stop criterion**.\n",
    "\n",
    "And finally we can compare our solution for $\\theta$ with the analytic result given by \n",
    "$\\theta= (X^TX)^{-1} X^T \\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0728a369",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradient Descent Example\n",
    "\n",
    "Here our simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48d43f0",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Importing various packages\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "\n",
    "# the number of datapoints\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* X.T @ X\n",
    "# Get the eigenvalues\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta_linreg = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(theta_linreg)\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradient = (2.0/n)*X.T @ (X @ theta-y)\n",
    "    theta -= eta*gradient\n",
    "\n",
    "print(theta)\n",
    "xnew = np.array([[0],[2]])\n",
    "xbnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = xbnew.dot(theta)\n",
    "ypredict2 = xbnew.dot(theta_linreg)\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Gradient descent example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c6ed1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradient descent and Ridge\n",
    "\n",
    "We have also discussed Ridge regression where the loss function contains a regularized term given by the $L_2$ norm of $\\theta$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ce6e3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C_{\\text{ridge}}(\\theta) = \\frac{1}{n}||X\\theta -\\mathbf{y}||^2 + \\lambda ||\\theta||^2, \\ \\lambda \\geq 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0de7c2",
   "metadata": {
    "editable": true
   },
   "source": [
    "In order to minimize $C_{\\text{ridge}}(\\theta)$ using GD we adjust the gradient as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c0dea",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_\\theta C_{\\text{ridge}}(\\theta)  = \\frac{2}{n}\\begin{bmatrix} \\sum_{i=1}^{100} \\left(\\theta_0+\\theta_1x_i-y_i\\right) \\\\\n",
    "\\sum_{i=1}^{100}\\left( x_i (\\theta_0+\\theta_1x_i)-y_ix_i\\right) \\\\\n",
    "\\end{bmatrix} + 2\\lambda\\begin{bmatrix} \\theta_0 \\\\ \\theta_1\\end{bmatrix} = 2 (\\frac{1}{n}X^T(X\\theta - \\mathbf{y})+\\lambda \\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb07f6",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can easily extend our program to minimize $C_{\\text{ridge}}(\\theta)$ using gradient descent and compare with the analytical solution given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d6c64",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{\\text{ridge}} = \\left(X^T X + n\\lambda I_{2 \\times 2} \\right)^{-1} X^T \\mathbf{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd65db",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Hessian matrix for Ridge Regression\n",
    "The Hessian matrix of Ridge Regression for our simple example  is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5a4d1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{H} \\equiv \\begin{bmatrix}\n",
    "\\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0^2} & \\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0 \\partial \\theta_1}  \\\\\n",
    "\\frac{\\partial^2 C(\\theta)}{\\partial \\theta_0 \\partial \\theta_1} & \\frac{\\partial^2 C(\\theta)}{\\partial \\theta_1^2} &  \\\\\n",
    "\\end{bmatrix} = \\frac{2}{n}X^T X+2\\lambda\\boldsymbol{I}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178c97e",
   "metadata": {
    "editable": true
   },
   "source": [
    "This implies that the Hessian matrix  is positive definite, hence the stationary point is a\n",
    "minimum.\n",
    "Note that the Ridge cost function is convex being  a sum of two convex\n",
    "functions. Therefore, the stationary point is a global\n",
    "minimum of this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853aec7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Program example for gradient descent with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81740e7b",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "\n",
    "# the number of datapoints\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "\n",
    "#Ridge parameter lambda\n",
    "lmbda  = 0.001\n",
    "Id = n*lmbda* np.eye(XT_X.shape[0])\n",
    "\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X+2*lmbda* np.eye(XT_X.shape[0])\n",
    "# Get the eigenvalues\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "\n",
    "theta_linreg = np.linalg.inv(XT_X+Id) @ X.T @ y\n",
    "print(theta_linreg)\n",
    "# Start plain gradient descent\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 100\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = 2.0/n*X.T @ (X @ (theta)-y)+2*lmbda*theta\n",
    "    theta -= eta*gradients\n",
    "\n",
    "print(theta)\n",
    "ypredict = X @ theta\n",
    "ypredict2 = X @ theta_linreg\n",
    "plt.plot(x, ypredict, \"r-\")\n",
    "plt.plot(x, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Gradient descent example for Ridge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b6e08",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using gradient descent methods, limitations\n",
    "\n",
    "* **Gradient descent (GD) finds local minima of our function**. Since the GD algorithm is deterministic, if it converges, it will converge to a local minimum of our cost/loss/risk function. Because in ML we are often dealing with extremely rugged landscapes with many local minima, this can lead to poor performance.\n",
    "\n",
    "* **GD is sensitive to initial conditions**. One consequence of the local nature of GD is that initial conditions matter. Depending on where one starts, one will end up at a different local minima. Therefore, it is very important to think about how one initializes the training process. This is true for GD as well as more complicated variants of GD.\n",
    "\n",
    "* **Gradients are computationally expensive to calculate for large datasets**. In many cases in statistics and ML, the cost/loss/risk function is a sum of terms, with one term for each data point. For example, in linear regression, $E \\propto \\sum_{i=1}^n (y_i - \\mathbf{w}^T\\cdot\\mathbf{x}_i)^2$; for logistic regression, the square error is replaced by the cross entropy. To calculate the gradient we have to sum over *all* $n$ data points. Doing this at every GD step becomes extremely computationally expensive. An ingenious solution to this, is to calculate the gradients using small subsets of the data called \"mini batches\". This has the added benefit of introducing stochasticity into our algorithm.\n",
    "\n",
    "* **GD is very sensitive to choices of learning rates**. GD is extremely sensitive to the choice of learning rates. If the learning rate is very small, the training process take an extremely long time. For larger learning rates, GD can diverge and give poor results. Furthermore, depending on what the local landscape looks like, we have to modify the learning rates to ensure convergence. Ideally, we would *adaptively* choose the learning rates to match the landscape.\n",
    "\n",
    "* **GD treats all directions in parameter space uniformly.** Another major drawback of GD is that unlike Newton's method, the learning rate for GD is the same in all directions in parameter space. For this reason, the maximum learning rate is set by the behavior of the steepest direction and this can significantly slow down training. Ideally, we would like to take large steps in flat directions and small steps in steep directions. Since we are exploring rugged landscapes where curvatures change, this requires us to keep track of not only the gradient but second derivatives. The ideal scenario would be to calculate the Hessian but this proves to be too computationally expensive. \n",
    "\n",
    "* GD can take exponential time to escape saddle points, even with random initialization. As we mentioned, GD is extremely sensitive to initial condition since it determines the particular local minimum GD would eventually reach. However, even with a good initialization scheme, through the introduction of randomness, GD can still take exponential time to escape saddle points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9be1a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Momentum based GD\n",
    "\n",
    "We discuss here some simple examples where we introduce what is called\n",
    "'memory'about previous steps, or what is normally called momentum\n",
    "gradient descent.\n",
    "For the mathematical details, see whiteboad notes from lecture on September 8, 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1267e6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Improving gradient descent with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494e82a7",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import arange\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# objective function\n",
    "def objective(x):\n",
    "\treturn x**2.0\n",
    " \n",
    "# derivative of objective function\n",
    "def derivative(x):\n",
    "\treturn x * 2.0\n",
    " \n",
    "# gradient descent algorithm\n",
    "def gradient_descent(objective, derivative, bounds, n_iter, step_size):\n",
    "\t# track all solutions\n",
    "\tsolutions, scores = list(), list()\n",
    "\t# generate an initial point\n",
    "\tsolution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "\t# run the gradient descent\n",
    "\tfor i in range(n_iter):\n",
    "\t\t# calculate gradient\n",
    "\t\tgradient = derivative(solution)\n",
    "\t\t# take a step\n",
    "\t\tsolution = solution - step_size * gradient\n",
    "\t\t# evaluate candidate point\n",
    "\t\tsolution_eval = objective(solution)\n",
    "\t\t# store solution\n",
    "\t\tsolutions.append(solution)\n",
    "\t\tscores.append(solution_eval)\n",
    "\t\t# report progress\n",
    "\t\tprint('>%d f(%s) = %.5f' % (i, solution, solution_eval))\n",
    "\treturn [solutions, scores]\n",
    " \n",
    "# seed the pseudo random number generator\n",
    "seed(4)\n",
    "# define range for input\n",
    "bounds = asarray([[-1.0, 1.0]])\n",
    "# define the total iterations\n",
    "n_iter = 30\n",
    "# define the step size\n",
    "step_size = 0.1\n",
    "# perform the gradient descent search\n",
    "solutions, scores = gradient_descent(objective, derivative, bounds, n_iter, step_size)\n",
    "# sample input range uniformly at 0.1 increments\n",
    "inputs = arange(bounds[0,0], bounds[0,1]+0.1, 0.1)\n",
    "# compute targets\n",
    "results = objective(inputs)\n",
    "# create a line plot of input vs result\n",
    "pyplot.plot(inputs, results)\n",
    "# plot the solutions found\n",
    "pyplot.plot(solutions, scores, '.-', color='red')\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46858c7c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Same code but now with momentum gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a917123",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import arange\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# objective function\n",
    "def objective(x):\n",
    "\treturn x**2.0\n",
    " \n",
    "# derivative of objective function\n",
    "def derivative(x):\n",
    "\treturn x * 2.0\n",
    " \n",
    "# gradient descent algorithm\n",
    "def gradient_descent(objective, derivative, bounds, n_iter, step_size, momentum):\n",
    "\t# track all solutions\n",
    "\tsolutions, scores = list(), list()\n",
    "\t# generate an initial point\n",
    "\tsolution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "\t# keep track of the change\n",
    "\tchange = 0.0\n",
    "\t# run the gradient descent\n",
    "\tfor i in range(n_iter):\n",
    "\t\t# calculate gradient\n",
    "\t\tgradient = derivative(solution)\n",
    "\t\t# calculate update\n",
    "\t\tnew_change = step_size * gradient + momentum * change\n",
    "\t\t# take a step\n",
    "\t\tsolution = solution - new_change\n",
    "\t\t# save the change\n",
    "\t\tchange = new_change\n",
    "\t\t# evaluate candidate point\n",
    "\t\tsolution_eval = objective(solution)\n",
    "\t\t# store solution\n",
    "\t\tsolutions.append(solution)\n",
    "\t\tscores.append(solution_eval)\n",
    "\t\t# report progress\n",
    "\t\tprint('>%d f(%s) = %.5f' % (i, solution, solution_eval))\n",
    "\treturn [solutions, scores]\n",
    " \n",
    "# seed the pseudo random number generator\n",
    "seed(4)\n",
    "# define range for input\n",
    "bounds = asarray([[-1.0, 1.0]])\n",
    "# define the total iterations\n",
    "n_iter = 30\n",
    "# define the step size\n",
    "step_size = 0.1\n",
    "# define momentum\n",
    "momentum = 0.3\n",
    "# perform the gradient descent search with momentum\n",
    "solutions, scores = gradient_descent(objective, derivative, bounds, n_iter, step_size, momentum)\n",
    "# sample input range uniformly at 0.1 increments\n",
    "inputs = arange(bounds[0,0], bounds[0,1]+0.1, 0.1)\n",
    "# compute targets\n",
    "results = objective(inputs)\n",
    "# create a line plot of input vs result\n",
    "pyplot.plot(inputs, results)\n",
    "# plot the solutions found\n",
    "pyplot.plot(solutions, scores, '.-', color='red')\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b2aa8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Overview video on Stochastic Gradient Descent (SGD)\n",
    "\n",
    "[What is Stochastic Gradient Descent](https://www.youtube.com/watch?v=vMh0zPT0tLI&ab_channel=StatQuestwithJoshStarmer)\n",
    "There are several reasons for using stochastic gradient descent. Some of these are:\n",
    "\n",
    "1. Efficiency: Updates weights more frequently using a single or a small batch of samples, which speeds up convergence.\n",
    "\n",
    "2. Hopefully avoid Local Minima\n",
    "\n",
    "3. Memory Usage: Requires less memory compared to computing gradients for the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dacb8ef",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Batches and mini-batches\n",
    "\n",
    "In gradient descent we compute the cost function and its gradient for all data points we have.\n",
    "\n",
    "In large-scale applications such as the [ILSVRC challenge](https://www.image-net.org/challenges/LSVRC/), the\n",
    "training data can have on order of millions of examples. Hence, it\n",
    "seems wasteful to compute the full cost function over the entire\n",
    "training set in order to perform only a single parameter update. A\n",
    "very common approach to addressing this challenge is to compute the\n",
    "gradient over batches of the training data. For example, a typical batch could contain some thousand  examples from\n",
    "an  entire training set of several millions. This batch is then used to\n",
    "perform a parameter update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9add4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Pros and cons\n",
    "\n",
    "1. Speed: SGD is faster than gradient descent because it uses only one training example per iteration, whereas gradient descent requires the entire dataset. This speed advantage becomes more significant as the size of the dataset increases.\n",
    "\n",
    "2. Convergence: Gradient descent has a more predictable convergence behaviour because it uses the average gradient of the entire dataset. In contrast, SGD’s convergence behaviour can be more erratic due to its random sampling of individual training examples.\n",
    "\n",
    "3. Memory: Gradient descent requires more memory than SGD because it must store the entire dataset for each iteration. SGD only needs to store the current training example, making it more memory-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5168cc9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Convergence rates\n",
    "\n",
    "1. Stochastic Gradient Descent has a faster convergence rate due to the use of single training examples in each iteration.\n",
    "\n",
    "2. Gradient Descent as a slower convergence rate, as it uses the entire dataset for each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47321307",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "In general, stochastic Gradient Descent is Less accurate than gradient\n",
    "descent, as it calculates the gradient on single examples, which may\n",
    "not accurately represent the overall dataset.  Gradient Descent is\n",
    "more accurate because it uses the average gradient calculated over the\n",
    "entire dataset.\n",
    "\n",
    "There are other disadvantages to using SGD. The main drawback is that\n",
    "its convergence behaviour can be more erratic due to the random\n",
    "sampling of individual training examples. This can lead to less\n",
    "accurate results, as the algorithm may not converge to the true\n",
    "minimum of the cost function. Additionally, the learning rate, which\n",
    "determines the step size of each update to the model’s parameters,\n",
    "must be carefully chosen to ensure convergence.\n",
    "\n",
    "It is however the method of choice in deep learning algorithms where\n",
    "SGD is often used in combination with other optimization techniques,\n",
    "such as momentum or adaptive learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f44d6b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "In stochastic gradient descent, the extreme case is the case where we\n",
    "have only one batch, that is we include the whole data set.\n",
    "\n",
    "This process is called Stochastic Gradient\n",
    "Descent (SGD) (or also sometimes on-line gradient descent). This is\n",
    "relatively less common to see because in practice due to vectorized\n",
    "code optimizations it can be computationally much more efficient to\n",
    "evaluate the gradient for 100 examples, than the gradient for one\n",
    "example 100 times. Even though SGD technically refers to using a\n",
    "single example at a time to evaluate the gradient, you will hear\n",
    "people use the term SGD even when referring to mini-batch gradient\n",
    "descent (i.e. mentions of MGD for “Minibatch Gradient Descent”, or BGD\n",
    "for “Batch gradient descent” are rare to see), where it is usually\n",
    "assumed that mini-batches are used. The size of the mini-batch is a\n",
    "hyperparameter but it is not very common to cross-validate or bootstrap it. It is\n",
    "usually based on memory constraints (if any), or set to some value,\n",
    "e.g. 32, 64 or 128. We use powers of 2 in practice because many\n",
    "vectorized operation implementations work faster when their inputs are\n",
    "sized in powers of 2.\n",
    "\n",
    "In our notes with  SGD we mean stochastic gradient descent with mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898ef421",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent (SGD) and variants thereof address some of\n",
    "the shortcomings of the Gradient descent method discussed above.\n",
    "\n",
    "The underlying idea of SGD comes from the observation that the cost\n",
    "function, which we want to minimize, can almost always be written as a\n",
    "sum over $n$ data points $\\{\\mathbf{x}_i\\}_{i=1}^n$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e827950",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\mathbf{\\theta}) = \\sum_{i=1}^n c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e99546",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Computation of gradients\n",
    "\n",
    "This in turn means that the gradient can be\n",
    "computed as a sum over $i$-gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92afe6c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_\\theta C(\\mathbf{\\theta}) = \\sum_i^n \\nabla_\\theta c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a4aca",
   "metadata": {
    "editable": true
   },
   "source": [
    "Stochasticity/randomness is introduced by only taking the\n",
    "gradient on a subset of the data called minibatches.  If there are $n$\n",
    "data points and the size of each minibatch is $M$, there will be $n/M$\n",
    "minibatches. We denote these minibatches by $B_k$ where\n",
    "$k=1,\\cdots,n/M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884cc0d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## SGD example\n",
    "As an example, suppose we have $10$ data points $(\\mathbf{x}_1,\\cdots, \\mathbf{x}_{10})$ \n",
    "and we choose to have $M=5$ minibathces,\n",
    "then each minibatch contains two data points. In particular we have\n",
    "$B_1 = (\\mathbf{x}_1,\\mathbf{x}_2), \\cdots, B_5 =\n",
    "(\\mathbf{x}_9,\\mathbf{x}_{10})$. Note that if you choose $M=1$ you\n",
    "have only a single batch with all data points and on the other extreme,\n",
    "you may choose $M=n$ resulting in a minibatch for each datapoint, i.e\n",
    "$B_k = \\mathbf{x}_k$.\n",
    "\n",
    "The idea is now to approximate the gradient by replacing the sum over\n",
    "all data points with a sum over the data points in one the minibatches\n",
    "picked at random in each gradient descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392aeed0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\theta}\n",
    "C(\\mathbf{\\theta}) = \\sum_{i=1}^n \\nabla_\\theta c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\theta}) \\rightarrow \\sum_{i \\in B_k}^n \\nabla_\\theta\n",
    "c_i(\\mathbf{x}_i, \\mathbf{\\theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04581249",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The gradient step\n",
    "\n",
    "Thus a gradient descent step now looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21077a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{j+1} = \\theta_j - \\eta_j \\sum_{i \\in B_k}^n \\nabla_\\theta c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bed668",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $k$ is picked at random with equal\n",
    "probability from $[1,n/M]$. An iteration over the number of\n",
    "minibathces (n/M) is commonly referred to as an epoch. Thus it is\n",
    "typical to choose a number of epochs and for each epoch iterate over\n",
    "the number of minibatches, as exemplified in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c15b282",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602bda4c",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "n = 100 #100 datapoints \n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "n_epochs = 10 #number of epochs\n",
    "\n",
    "j = 0\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    for i in range(m):\n",
    "        k = np.random.randint(m) #Pick the k-th minibatch at random\n",
    "        #Compute the gradient using the data in minibatch Bk\n",
    "        #Compute new suggestion for \n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332831a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "Taking the gradient only on a subset of the data has two important\n",
    "benefits. First, it introduces randomness which decreases the chance\n",
    "that our opmization scheme gets stuck in a local minima. Second, if\n",
    "the size of the minibatches are small relative to the number of\n",
    "datapoints ($M <  n$), the computation of the gradient is much\n",
    "cheaper since we sum over the datapoints in the $k-th$ minibatch and not\n",
    "all $n$ datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187eb27c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## When do we stop?\n",
    "\n",
    "A natural question is when do we stop the search for a new minimum?\n",
    "One possibility is to compute the full gradient after a given number\n",
    "of epochs and check if the norm of the gradient is smaller than some\n",
    "threshold and stop if true. However, the condition that the gradient\n",
    "is zero is valid also for local minima, so this would only tell us\n",
    "that we are close to a local/global minimum. However, we could also\n",
    "evaluate the cost function at this point, store the result and\n",
    "continue the search. If the test kicks in at a later stage we can\n",
    "compare the values of the cost function and keep the $\\theta$ that\n",
    "gave the lowest value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbdbb5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Slightly different approach\n",
    "\n",
    "Another approach is to let the step length $\\eta_j$ depend on the\n",
    "number of epochs in such a way that it becomes very small after a\n",
    "reasonable time such that we do not move at all. Such approaches are\n",
    "also called scaling. There are many such ways to [scale the learning\n",
    "rate](https://towardsdatascience.com/gradient-descent-the-learning-rate-and-the-importance-of-feature-scaling-6c0b416596e1)\n",
    "and [discussions here](https://www.jmlr.org/papers/volume23/20-1258/20-1258.pdf). See\n",
    "also\n",
    "<https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1>\n",
    "for a discussion of different scaling functions for the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea8e21",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Time decay rate\n",
    "\n",
    "As an example, let $e = 0,1,2,3,\\cdots$ denote the current epoch and let $t_0, t_1 > 0$ be two fixed numbers. Furthermore, let $t = e \\cdot m + i$ where $m$ is the number of minibatches and $i=0,\\cdots,m-1$. Then the function $$\\eta_j(t; t_0, t_1) = \\frac{t_0}{t+t_1} $$ goes to zero as the number of epochs gets large. I.e. we start with a step length $\\eta_j (0; t_0, t_1) = t_0/t_1$ which decays in *time* $t$.\n",
    "\n",
    "In this way we can fix the number of epochs, compute $\\theta$ and\n",
    "evaluate the cost function at the end. Repeating the computation will\n",
    "give a different result since the scheme is random by design. Then we\n",
    "pick the final $\\theta$ that gives the lowest value of the cost\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77a60fcd",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def step_length(t,t0,t1):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "n = 100 #100 datapoints \n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "n_epochs = 500 #number of epochs\n",
    "t0 = 1.0\n",
    "t1 = 10\n",
    "\n",
    "eta_j = t0/t1\n",
    "j = 0\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    for i in range(m):\n",
    "        k = np.random.randint(m) #Pick the k-th minibatch at random\n",
    "        #Compute the gradient using the data in minibatch Bk\n",
    "        #Compute new suggestion for theta\n",
    "        t = epoch*m+i\n",
    "        eta_j = step_length(t,t0,t1)\n",
    "        j += 1\n",
    "\n",
    "print(\"eta_j after %d epochs: %g\" % (n_epochs,eta_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b030b80c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Code with a Number of Minibatches which varies\n",
    "\n",
    "In the code here we vary the number of mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bdf875b",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "from math import exp, sqrt\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = 2.0/n*X.T @ ((X @ theta)-y)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "# Can you figure out a better way of setting up the contributions to each batch?\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (2.0/M)* xi.T @ ((xi @ theta)-yi)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        theta = theta - eta*gradients\n",
    "print(\"theta from own sdg\")\n",
    "print(theta)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cebd9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Replace or not\n",
    "\n",
    "In the above code, we have use replacement in setting up the\n",
    "mini-batches. The discussion\n",
    "[here](https://sebastianraschka.com/faq/docs/sgd-methods.html) may be\n",
    "useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9011a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## SGD vs Full-Batch GD: Convergence Speed and Memory Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c85da0",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Theoretical Convergence Speed and convex optimization\n",
    "\n",
    "Consider minimizing an empirical cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df0f80",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\theta) =\\frac{1}{N}\\sum_{i=1}^N l_i(\\theta),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02b845",
   "metadata": {
    "editable": true
   },
   "source": [
    "where each $l_i(\\theta)$ is a\n",
    "differentiable loss term. Gradient Descent (GD) updates parameters\n",
    "using the full gradient $\\nabla C(\\theta)$, while Stochastic Gradient\n",
    "Descent (SGD) uses a single sample (or mini-batch) gradient $\\nabla\n",
    "l_i(\\theta)$ selected at random. In equation form, one GD step is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21997f1a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{t+1} = \\theta_t-\\eta \\nabla C(\\theta_t) =\\theta_t -\\eta \\frac{1}{N}\\sum_{i=1}^N \\nabla l_i(\\theta_t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdefe165",
   "metadata": {
    "editable": true
   },
   "source": [
    "whereas one SGD step is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac200d56",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{t+1} = \\theta_t -\\eta \\nabla l_{i_t}(\\theta_t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3edfb3",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $i_t$ randomly chosen. On smooth convex problems, GD and SGD both\n",
    "converge to the global minimum, but their rates differ. GD can take\n",
    "larger, more stable steps since it uses the exact gradient, achieving\n",
    "an error that decreases on the order of $O(1/t)$ per iteration for\n",
    "convex objectives (and even exponentially fast for strongly convex\n",
    "cases). In contrast, plain SGD has more variance in each step, leading\n",
    "to sublinear convergence in expectation – typically $O(1/\\sqrt{t})$\n",
    "for general convex objectives (\\thetaith appropriate diminishing step\n",
    "sizes) . Intuitively, GD’s trajectory is smoother and more\n",
    "predictable, while SGD’s path oscillates due to noise but costs far\n",
    "less per iteration, enabling many more updates in the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe05c0d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Strongly Convex Case\n",
    "\n",
    "If $C(\\theta)$ is strongly convex and $L$-smooth (so GD enjoys linear\n",
    "convergence), the gap $C(\\theta_t)-C(\\theta^*)$ for GD shrinks as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae403f1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\theta_t) - C(\\theta^* ) \\le \\Big(1 - \\frac{\\mu}{L}\\Big)^t [C(\\theta_0)-C(\\theta^*)],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44272171",
   "metadata": {
    "editable": true
   },
   "source": [
    "a geometric (linear) convergence per iteration . Achieving an\n",
    "$\\epsilon$-accurate solution thus takes on the order of\n",
    "$\\log(1/\\epsilon)$ iterations for GD. However, each GD iteration costs\n",
    "$O(N)$ gradient evaluations. SGD cannot exploit strong convexity to\n",
    "obtain a linear rate – instead, with a properly decaying step size\n",
    "(e.g. $\\eta_t = \\frac{1}{\\mu t}$) or iterate averaging, SGD attains an\n",
    "$O(1/t)$ convergence rate in expectation . For example, one result\n",
    "of Moulines and  Bach 2011, see <https://papers.nips.cc/paper_files/paper/2011/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html> shows that with $\\eta_t = \\Theta(1/t)$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde29ef",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbb{E}[C(\\theta_t) - C(\\theta^*)] = O(1/t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77f20e",
   "metadata": {
    "editable": true
   },
   "source": [
    "for strongly convex, smooth $F$ . This $1/t$ rate is slower per\n",
    "iteration than GD’s exponential decay, but each SGD iteration is $N$\n",
    "times cheaper. In fact, to reach error $\\epsilon$, plain SGD needs on\n",
    "the order of $T=O(1/\\epsilon)$ iterations (sub-linear convergence),\n",
    "while GD needs $O(\\log(1/\\epsilon))$ iterations. When accounting for\n",
    "cost-per-iteration, GD requires $O(N \\log(1/\\epsilon))$ total gradient\n",
    "computations versus SGD’s $O(1/\\epsilon)$ single-sample\n",
    "computations. In large-scale regimes (huge $N$), SGD can be\n",
    "faster in wall-clock time because $N \\log(1/\\epsilon)$ may far exceed\n",
    "$1/\\epsilon$ for reasonable accuracy levels. In other words,\n",
    "with millions of data points, one epoch of GD (one full gradient) is\n",
    "extremely costly, whereas SGD can make $N$ cheap updates in the time\n",
    "GD makes one – often yielding a good solution faster in practice, even\n",
    "though SGD’s asymptotic error decays more slowly. As one lecture\n",
    "succinctly puts it: “SGD can be super effective in terms of iteration\n",
    "cost and memory, but SGD is slow to converge and can’t adapt to strong\n",
    "convexity” . Thus, the break-even point depends on $N$ and the desired\n",
    "accuracy: for moderate accuracy on very large $N$, SGD’s cheaper\n",
    "updates win; for extremely high precision (very small $\\epsilon$) on a\n",
    "modest $N$, GD’s fast convergence per step can be advantageous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479bd97",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Non-Convex Problems\n",
    "\n",
    "In non-convex optimization (e.g. deep neural networks), neither GD nor\n",
    "SGD guarantees global minima, but SGD often displays faster progress\n",
    "in finding useful minima. Theoretical results here are weaker, usually\n",
    "showing convergence to a stationary point $\\theta$ ($|\\nabla C|$ is\n",
    "small) in expectation. For example, GD might require $O(1/\\epsilon^2)$\n",
    "iterations to ensure $|\\nabla C(\\theta)| < \\epsilon$, and SGD typically has\n",
    "similar polynomial complexity (often worse due to gradient\n",
    "noise). However, a noteworthy difference is that SGD’s stochasticity\n",
    "can help escape saddle points or poor local minima. Random gradient\n",
    "fluctuations act like implicit noise, helping the iterate “jump” out\n",
    "of flat saddle regions where full-batch GD could stagnate . In fact,\n",
    "research has shown that adding noise to GD can guarantee escaping\n",
    "saddle points in polynomial time, and the inherent noise in SGD often\n",
    "serves this role. Empirically, this means SGD can sometimes find a\n",
    "lower loss basin faster, whereas full-batch GD might get “stuck” near\n",
    "saddle points or need a very small learning rate to navigate complex\n",
    "error surfaces . Overall, in modern high-dimensional machine learning,\n",
    "SGD (or mini-batch SGD) is the workhorse for large non-convex problems\n",
    "because it converges to good solutions much faster in practice,\n",
    "despite the lack of a linear convergence guarantee. Full-batch GD is\n",
    "rarely used on large neural networks, as it would require tiny steps\n",
    "to avoid divergence and is extremely slow per iteration ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea65c9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Memory Usage and Scalability\n",
    "\n",
    "A major advantage of SGD is its memory efficiency in handling large\n",
    "datasets. Full-batch GD requires access to the entire training set for\n",
    "each iteration, which often means the whole dataset (or a large\n",
    "subset) must reside in memory to compute $\\nabla C(\\theta)$ . This results\n",
    "in memory usage that scales linearly with the dataset size $N$. For\n",
    "instance, if each training sample is large (e.g. high-dimensional\n",
    "features), computing a full gradient may require storing a substantial\n",
    "portion of the data or all intermediate gradients until they are\n",
    "aggregated. In contrast, SGD needs only a single (or a small\n",
    "mini-batch of) training example(s) in memory at any time . The\n",
    "algorithm processes one sample (or mini-batch) at a time and\n",
    "immediately updates the model, discarding that sample before moving to\n",
    "the next. This streaming approach means that memory footprint is\n",
    "essentially independent of $N$ (apart from storing the model\n",
    "parameters themselves). As one source notes, gradient descent\n",
    "“requires more memory than SGD” because it “must store the entire\n",
    "dataset for each iteration,” whereas SGD “only needs to store the\n",
    "current training example” . In practical terms, if you have a dataset\n",
    "of size, say, 1 million examples, full-batch GD would need memory for\n",
    "all million every step, while SGD could be implemented to load just\n",
    "one example at a time – a crucial benefit if data are too large to fit\n",
    "in RAM or GPU memory. This scalability makes SGD suitable for\n",
    "large-scale learning: as long as you can stream data from disk, SGD\n",
    "can handle arbitrarily large datasets with fixed memory. In fact, SGD\n",
    "“does not need to remember which examples were visited” in the past,\n",
    "allowing it to run in an online fashion on infinite data streams\n",
    ". Full-batch GD, on the other hand, would require multiple passes\n",
    "through a giant dataset per update (or a complex distributed memory\n",
    "system), which is often infeasible.\n",
    "\n",
    "There is also a secondary memory effect: computing a full-batch\n",
    "gradient in deep learning requires storing all intermediate\n",
    "activations for backpropagation across the entire batch. A very large\n",
    "batch (approaching the full dataset) might exhaust GPU memory due to\n",
    "the need to hold activation gradients for thousands or millions of\n",
    "examples simultaneously. SGD/minibatches mitigate this by splitting\n",
    "the workload – e.g. with a mini-batch of size 32 or 256, memory use\n",
    "stays bounded, whereas a full-batch (size = $N$) forward/backward pass\n",
    "could not even be executed if $N$ is huge. Techniques like gradient\n",
    "accumulation exist to simulate large-batch GD by summing many\n",
    "small-batch gradients – but these still process data in manageable\n",
    "chunks to avoid memory overflow. In summary, memory complexity for GD\n",
    "grows with $N$, while for SGD it remains $O(1)$ w.r.t. dataset size\n",
    "(only the model and perhaps a mini-batch reside in memory) . This is a\n",
    "key reason why batch GD “does not scale” to very large data and why\n",
    "virtually all large-scale machine learning algorithms rely on\n",
    "stochastic or mini-batch methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3fe4c4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Empirical Evidence: Convergence Time and Memory in Practice\n",
    "\n",
    "Empirical studies strongly support the theoretical trade-offs\n",
    "above. In large-scale machine learning tasks, SGD often converges to a\n",
    "good solution much faster in wall-clock time than full-batch GD, and\n",
    "it uses far less memory. For example, Bottou & Bousquet (2008)\n",
    "analyzed learning time under a fixed computational budget and\n",
    "concluded that when data is abundant, it’s better to use a faster\n",
    "(even if less precise) optimization method to process more examples in\n",
    "the same time . This analysis showed that for large-scale problems,\n",
    "processing more data with SGD yields lower error than spending the\n",
    "time to do exact (batch) optimization on fewer data . In other words,\n",
    "if you have a time budget, it’s often optimal to accept slightly\n",
    "slower convergence per step (as with SGD) in exchange for being able\n",
    "to use many more training samples in that time. This phenomenon is\n",
    "borne out by experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d08c69",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Deep Neural Networks\n",
    "\n",
    "In modern deep learning, full-batch GD is so slow that it is rarely\n",
    "attempted; instead, mini-batch SGD is standard. A recent study\n",
    "demonstrated that it is possible to train a ResNet-50 on ImageNet\n",
    "using full-batch gradient descent, but it required careful tuning\n",
    "(e.g. gradient clipping, tiny learning rates) and vast computational\n",
    "resources – and even then, each full-batch update was extremely\n",
    "expensive.\n",
    "\n",
    "Using a huge batch\n",
    "(closer to full GD) tends to slow down convergence if the learning\n",
    "rate is not scaled up, and often encounters optimization difficulties\n",
    "(plateaus) that small batches avoid.\n",
    "Empirically, small or medium\n",
    "batch SGD finds minima in fewer clock hours because it can rapidly\n",
    "loop over the data with gradient noise aiding exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b549d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Memory constraints\n",
    "\n",
    "From a memory standpoint, practitioners note that batch GD becomes\n",
    "infeasible on large data. For example, if one tried to do full-batch\n",
    "training on a dataset that doesn’t fit in RAM or GPU memory, the\n",
    "program would resort to heavy disk I/O or simply crash. SGD\n",
    "circumvents this by processing mini-batches. Even in cases where data\n",
    "does fit in memory, using a full batch can spike memory usage due to\n",
    "storing all gradients. One empirical observation is that mini-batch\n",
    "training has a “lower, fluctuating usage pattern” of memory, whereas\n",
    "full-batch loading “quickly consumes memory (often exceeding limits)”\n",
    ". This is especially relevant for graph neural networks or other\n",
    "models where a “batch” may include a huge chunk of a graph: full-batch\n",
    "gradient computation can exhaust GPU memory, whereas mini-batch\n",
    "methods keep memory usage manageable .\n",
    "\n",
    "In summary, SGD converges faster than full-batch GD in terms of actual\n",
    "training time for large-scale problems, provided we measure\n",
    "convergence as reaching a good-enough solution. Theoretical bounds\n",
    "show SGD needs more iterations, but because it performs many more\n",
    "updates per unit time (and requires far less memory), it often\n",
    "achieves lower loss in a given time frame than GD. Full-batch GD might\n",
    "take slightly fewer iterations in theory, but each iteration is so\n",
    "costly that it is “slower… especially for large datasets” . Meanwhile,\n",
    "memory scaling strongly favors SGD: GD’s memory cost grows with\n",
    "dataset size, making it impractical beyond a point, whereas SGD’s\n",
    "memory use is modest and mostly constant w.r.t. $N$ . These\n",
    "differences have made SGD (and mini-batch variants) the de facto\n",
    "choice for training large machine learning models, from logistic\n",
    "regression on millions of examples to deep neural networks with\n",
    "billions of parameters. The consensus in both research and practice is\n",
    "that for large-scale or high-dimensional tasks, SGD-type methods\n",
    "converge quicker per unit of computation and handle memory constraints\n",
    "better than standard full-batch gradient descent ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2661e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Second moment of the gradient\n",
    "\n",
    "In stochastic gradient descent, with and without momentum, we still\n",
    "have to specify a schedule for tuning the learning rates $\\eta_t$\n",
    "as a function of time.  As discussed in the context of Newton's\n",
    "method, this presents a number of dilemmas. The learning rate is\n",
    "limited by the steepest direction which can change depending on the\n",
    "current position in the landscape. To circumvent this problem, ideally\n",
    "our algorithm would keep track of curvature and take large steps in\n",
    "shallow, flat directions and small steps in steep, narrow directions.\n",
    "Second-order methods accomplish this by calculating or approximating\n",
    "the Hessian and normalizing the learning rate by the\n",
    "curvature. However, this is very computationally expensive for\n",
    "extremely large models. Ideally, we would like to be able to\n",
    "adaptively change the step size to match the landscape without paying\n",
    "the steep computational price of calculating or approximating\n",
    "Hessians.\n",
    "\n",
    "During the last decade a number of methods have been introduced that accomplish\n",
    "this by tracking not only the gradient, but also the second moment of\n",
    "the gradient. These methods include AdaGrad, AdaDelta, Root Mean Squared Propagation (RMS-Prop), and\n",
    "[ADAM](https://arxiv.org/abs/1412.6980)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2106298",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Challenge: Choosing a Fixed Learning Rate\n",
    "A fixed $\\eta$ is hard to get right:\n",
    "1. If $\\eta$ is too large, the updates can overshoot the minimum, causing oscillations or divergence\n",
    "\n",
    "2. If $\\eta$ is too small, convergence is very slow (many iterations to make progress)\n",
    "\n",
    "In practice, one often uses trial-and-error or schedules (decaying $\\eta$ over time) to find a workable balance.\n",
    "For a function with steep directions and flat directions, a single global $\\eta$ may be inappropriate:\n",
    "1. Steep coordinates require a smaller step size to avoid oscillation.\n",
    "\n",
    "2. Flat/shallow coordinates could use a larger step to speed up progress.\n",
    "\n",
    "3. This issue is pronounced in high-dimensional problems with **sparse or varying-scale features** – we need a method to adjust step sizesper feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a053c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Motivation for Adaptive Step Sizes\n",
    "\n",
    "1. Instead of a fixed global $\\eta$, use an **adaptive learning rate** for each parameter that depends on the history of gradients.\n",
    "\n",
    "2. Parameters that have large accumulated gradient magnitude should get smaller steps (they've been changing a lot), whereas parameters with small or infrequent gradients can have larger relative steps.\n",
    "\n",
    "3. This is especially useful for sparse features: Rarely active features accumulate little gradient, so their learning rate remains comparatively high, ensuring they are not neglected\n",
    "\n",
    "4. Conversely, frequently active features accumulate large gradient sums, and their learning rate automatically decreases, preventing too-large updates\n",
    "\n",
    "5. Several algorithms implement this idea (AdaGrad, RMSProp, AdaDelta, Adam, etc.). We will derive **AdaGrad**, one of the first adaptive methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0924df8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## AdaGrad algorithm, taken from [Goodfellow et al](https://www.deeplearningbook.org/contents/optimization.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/adagrad.png, width=600 frac=0.8] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/adagrad.png\" width=\"600\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743f26d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Derivation of the AdaGrad Algorithm\n",
    "\n",
    "**Accumulating Gradient History.**\n",
    "\n",
    "1. AdaGrad maintains a running sum of squared gradients for each parameter (coordinate)\n",
    "\n",
    "2. Let $g_t = \\nabla C_{i_t}(x_t)$ be the gradient at step $t$ (or a subgradient for nondifferentiable cases).\n",
    "\n",
    "3. Initialize $r_0 = 0$ (an all-zero vector in $\\mathbb{R}^d$).\n",
    "\n",
    "4. At each iteration $t$, update the accumulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b5d6a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "r_t = r_{t-1} + g_t \\circ g_t,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e2738",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. Here  $g_t \\circ g_t$ denotes element-wise square of the gradient vector. $g_t^{(j)} = g_{t-1}^{(j)} + (g_{t,j})^2$ for each parameter $j$.\n",
    "\n",
    "2. We can view $H_t = \\mathrm{diag}(r_t)$ as a diagonal matrix of past squared gradients. Initially $H_0 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753de13",
   "metadata": {
    "editable": true
   },
   "source": [
    "## AdaGrad Update Rule Derivation\n",
    "\n",
    "We scale the gradient by the inverse square root of the accumulated matrix $H_t$. The AdaGrad update at step $t$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db67ba3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{t+1} =\\theta_t - \\eta H_t^{-1/2} g_t,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831e978",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $H_t^{-1/2}$ is the diagonal matrix with entries $(r_{t}^{(1)})^{-1/2}, \\dots, (r_{t}^{(d)})^{-1/2}$\n",
    "In coordinates, this means each parameter $j$ has an individual step size:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7758a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{t+1,j} =\\theta_{t,j} -\\frac{\\eta}{\\sqrt{r_{t,j}}}g_{t,j}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62a4ff",
   "metadata": {
    "editable": true
   },
   "source": [
    "In practice we add a small constant $\\epsilon$ in the denominator for numerical stability to avoid division by zero:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a2b948",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{t+1,j}= \\theta_{t,j}-\\frac{\\eta}{\\sqrt{\\epsilon + r_{t,j}}}g_{t,j}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f269e80",
   "metadata": {
    "editable": true
   },
   "source": [
    "Equivalently, the effective learning rate for parameter $j$ at time $t$ is $\\displaystyle \\alpha_{t,j} = \\frac{\\eta}{\\sqrt{\\epsilon + r_{t,j}}}$. This decreases over time as $r_{t,j}$ grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec584c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## AdaGrad Properties\n",
    "\n",
    "1. AdaGrad automatically tunes the step size for each parameter. Parameters with more *volatile or large gradients* get smaller steps, and those with *small or infrequent gradients* get relatively larger steps\n",
    "\n",
    "2. No manual schedule needed: The accumulation $r_t$ keeps increasing (or stays the same if gradient is zero), so step sizes $\\eta/\\sqrt{r_t}$ are non-increasing. This has a similar effect to a learning rate schedule, but individualized per coordinate.\n",
    "\n",
    "3. Sparse data benefit: For very sparse features, $r_{t,j}$ grows slowly, so that feature’s parameter retains a higher learning rate for longer, allowing it to make significant updates when it does get a gradient signal\n",
    "\n",
    "4. Convergence: In convex optimization, AdaGrad can be shown to achieve a sub-linear convergence rate  comparable to the best fixed learning rate tuned for the problem\n",
    "\n",
    "It effectively reduces the need to tune $\\eta$ by hand.\n",
    "1. Limitations: Because $r_t$ accumulates without bound, AdaGrad’s learning rates can become extremely small over long training, potentially slowing progress. (Later variants like RMSProp, AdaDelta, Adam address this by modifying the accumulation rule.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b741016",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RMSProp: Adaptive Learning Rates\n",
    "\n",
    "Addresses AdaGrad’s diminishing learning rate issue.\n",
    "Uses a decaying average of squared gradients (instead of a cumulative sum):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76108e75",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_t = \\rho v_{t-1} + (1-\\rho)(\\nabla C(\\theta_t))^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a3353",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\rho$ typically $0.9$ (or $0.99$).\n",
    "1. Update: $\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} \\nabla C(\\theta_t)$.\n",
    "\n",
    "2. Recent gradients have more weight, so $v_t$ adapts to the current landscape.\n",
    "\n",
    "3. Avoids AdaGrad’s “infinite memory” problem – learning rate does not continuously decay to zero.\n",
    "\n",
    "RMSProp was first proposed in lecture notes by Geoff Hinton, 2012 - unpublished.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a76ae",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RMSProp algorithm, taken from [Goodfellow et al](https://www.deeplearningbook.org/contents/optimization.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/rmsprop.png, width=600 frac=0.8] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/rmsprop.png\" width=\"600\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5fd82e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Adam Optimizer\n",
    "\n",
    "Why combine Momentum and RMSProp? Motivation for Adam: Adaptive Moment Estimation (Adam) was introduced by Kingma an Ba (2014) to combine the benefits of momentum and RMSProp.\n",
    "\n",
    "1. Fast convergence by smoothing gradients (accelerates in long-term gradient direction).\n",
    "\n",
    "2. Adaptive rates (RMSProp): Per-dimension learning rate scaling for stability (handles different feature scales, sparse gradients).\n",
    "\n",
    "3. Adam uses both: maintains moving averages of both first moment (gradients) and second moment (squared gradients)\n",
    "\n",
    "4. Additionally, includes a mechanism to correct the bias in these moving averages (crucial in early iterations)\n",
    "\n",
    "**Result**: Adam is robust, achieves faster convergence with less tuning, and often outperforms SGD (with momentum) in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cda2f6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## [ADAM optimizer](https://arxiv.org/abs/1412.6980)\n",
    "\n",
    "In [ADAM](https://arxiv.org/abs/1412.6980), we keep a running average of\n",
    "both the first and second moment of the gradient and use this\n",
    "information to adaptively change the learning rate for different\n",
    "parameters.  The method is efficient when working with large\n",
    "problems involving lots data and/or parameters.  It is a combination of the\n",
    "gradient descent with momentum algorithm and the RMSprop algorithm\n",
    "discussed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69310c2b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Why Combine Momentum and RMSProp?\n",
    "\n",
    "1. Momentum: Fast convergence by smoothing gradients (accelerates in long-term gradient direction).\n",
    "\n",
    "2. Adaptive rates (RMSProp): Per-dimension learning rate scaling for stability (handles different feature scales, sparse gradients).\n",
    "\n",
    "3. Adam uses both: maintains moving averages of both first moment (gradients) and second moment (squared gradients)\n",
    "\n",
    "4. Additionally, includes a mechanism to correct the bias in these moving averages (crucial in early iterations)\n",
    "\n",
    "Result: Adam is robust, achieves faster convergence with less tuning, and often outperforms SGD (with momentum) in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b8734",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Adam: Exponential Moving Averages (Moments)\n",
    "Adam maintains two moving averages at each time step $t$ for each parameter $w$:\n",
    "**First moment (mean) $m_t$.**\n",
    "\n",
    "The Momentum term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ce6bf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "m_t = \\beta_1m_{t-1} + (1-\\beta_1)\\, \\nabla C(\\theta_t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba64fd6",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Second moment (uncentered variance) $v_t$.**\n",
    "\n",
    "The RMS term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1a9ee",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "v_t = \\beta_2v_{t-1} + (1-\\beta_2)(\\nabla C(\\theta_t))^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aae51f",
   "metadata": {
    "editable": true
   },
   "source": [
    "with typical $\\beta_1 = 0.9$, $\\beta_2 = 0.999$. Initialize $m_0 = 0$, $v_0 = 0$.\n",
    "\n",
    "  These are **biased** estimators of the true first and second moment of the gradients, especially at the start (since $m_0,v_0$ are zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38adfadd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Adam: Bias Correction\n",
    "To counteract initialization bias in $m_t, v_t$, Adam computes bias-corrected estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484156fb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\qquad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1d0c2",
   "metadata": {
    "editable": true
   },
   "source": [
    "* When $t$ is small, $1-\\beta_i^t \\approx 0$, so $\\hat{m}_t, \\hat{v}_t$ significantly larger than raw $m_t, v_t$, compensating for the initial zero bias.\n",
    "\n",
    "* As $t$ increases, $1-\\beta_i^t \\to 1$, and $\\hat{m}_t, \\hat{v}_t$ converge to $m_t, v_t$.\n",
    "\n",
    "* Bias correction is important for Adam’s stability in early iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d5568",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Adam: Update Rule Derivation\n",
    "Finally, Adam updates parameters using the bias-corrected moments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb873c1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_{t+1} =\\theta_t -\\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon}\\hat{m}_t,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1129f6",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\epsilon$ is a small constant (e.g. $10^{-8}$) to prevent division by zero.\n",
    "Breaking it down:\n",
    "1. Compute gradient $\\nabla C(\\theta_t)$.\n",
    "\n",
    "2. Update first moment $m_t$ and second moment $v_t$ (exponential moving averages).\n",
    "\n",
    "3. Bias-correct: $\\hat{m}_t = m_t/(1-\\beta_1^t)$, $\\; \\hat{v}_t = v_t/(1-\\beta_2^t)$.\n",
    "\n",
    "4. Compute step: $\\Delta \\theta_t = \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$.\n",
    "\n",
    "5. Update parameters: $\\theta_{t+1} = \\theta_t - \\alpha\\, \\Delta \\theta_t$.\n",
    "\n",
    "This is the Adam update rule as given in the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15ce48",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Adam vs. AdaGrad and RMSProp\n",
    "\n",
    "1. AdaGrad: Uses per-coordinate scaling like Adam, but no momentum. Tends to slow down too much due to cumulative history (no forgetting)\n",
    "\n",
    "2. RMSProp: Uses moving average of squared gradients (like Adam’s $v_t$) to maintain adaptive learning rates, but does not include momentum or bias-correction.\n",
    "\n",
    "3. Adam: Effectively RMSProp + Momentum + Bias-correction\n",
    "\n",
    "  * Momentum ($m_t$) provides acceleration and smoother convergence.\n",
    "\n",
    "  * Adaptive $v_t$ scaling moderates the step size per dimension.\n",
    "\n",
    "  * Bias correction (absent in AdaGrad/RMSProp) ensures robust estimates early on.\n",
    "\n",
    "In practice, Adam often yields faster convergence and better tuning stability than RMSProp or AdaGrad alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb65e2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Adaptivity Across Dimensions\n",
    "\n",
    "1. Adam adapts the step size \\emph{per coordinate}: parameters with larger gradient variance get smaller effective steps, those with smaller or sparse gradients get larger steps.\n",
    "\n",
    "2. This per-dimension adaptivity is inherited from AdaGrad/RMSProp and helps handle ill-conditioned or sparse problems.\n",
    "\n",
    "3. Meanwhile, momentum (first moment) allows Adam to continue making progress even if gradients become small or noisy, by leveraging accumulated direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3862c40",
   "metadata": {
    "editable": true
   },
   "source": [
    "## ADAM algorithm, taken from [Goodfellow et al](https://www.deeplearningbook.org/contents/optimization.html)\n",
    "\n",
    "<!-- dom:FIGURE: [figures/adam.png, width=600 frac=0.8] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/adam.png\" width=\"600\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa2b35",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Algorithms and codes for Adagrad, RMSprop and Adam\n",
    "\n",
    "The algorithms we have implemented are well described in the text by [Goodfellow, Bengio and Courville, chapter 8](https://www.deeplearningbook.org/contents/optimization.html).\n",
    "\n",
    "The codes which implement these algorithms are discussed below here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de27d3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Practical tips\n",
    "\n",
    "* **Randomize the data when making mini-batches**. It is always important to randomly shuffle the data when forming mini-batches. Otherwise, the gradient descent method can fit spurious correlations resulting from the order in which data is presented.\n",
    "\n",
    "* **Transform your inputs**. Learning becomes difficult when our landscape has a mixture of steep and flat directions. One simple trick for minimizing these situations is to standardize the data by subtracting the mean and normalizing the variance of input variables. Whenever possible, also decorrelate the inputs. To understand why this is helpful, consider the case of linear regression. It is easy to show that for the squared error cost function, the Hessian of the cost function is just the correlation matrix between the inputs. Thus, by standardizing the inputs, we are ensuring that the landscape looks homogeneous in all directions in parameter space. Since most deep networks can be viewed as linear transformations followed by a non-linearity at each layer, we expect this intuition to hold beyond the linear case.\n",
    "\n",
    "* **Monitor the out-of-sample performance.** Always monitor the performance of your model on a validation set (a small portion of the training data that is held out of the training process to serve as a proxy for the test set. If the validation error starts increasing, then the model is beginning to overfit. Terminate the learning process. This *early stopping* significantly improves performance in many settings.\n",
    "\n",
    "* **Adaptive optimization methods don't always have good generalization.** Recent studies have shown that adaptive methods such as ADAM, RMSPorp, and AdaGrad tend to have poor generalization compared to SGD or SGD with momentum, particularly in the high-dimensional limit (i.e. the number of parameters exceeds the number of data points). Although it is not clear at this stage why these methods perform so well in training deep neural networks, simpler procedures like properly-tuned SGD may work as well or better in these applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a1a601",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Sneaking in automatic differentiation using Autograd\n",
    "\n",
    "In the examples here we take the liberty of sneaking in automatic\n",
    "differentiation (without having discussed the mathematics).  In\n",
    "project 1 you will write the gradients as discussed above, that is\n",
    "hard-coding the gradients.  By introducing automatic differentiation\n",
    "via the library **autograd**, which is now replaced by **JAX**, we have\n",
    "more flexibility in setting up alternative cost functions.\n",
    "\n",
    "The\n",
    "first example shows results with ordinary leats squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c721352d",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients for OLS\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(theta):\n",
    "    return (1.0/n)*np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "# define the gradient\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = training_gradient(theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36cec47",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Same code but now with momentum gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc5df7eb",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients for OLS\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(theta):\n",
    "    return (1.0/n)*np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x#+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 30\n",
    "\n",
    "# define the gradient\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = training_gradient(theta)\n",
    "    theta -= eta*gradients\n",
    "    print(iter,gradients[0],gradients[1])\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "# Now improve with momentum gradient descent\n",
    "change = 0.0\n",
    "delta_momentum = 0.3\n",
    "for iter in range(Niterations):\n",
    "    # calculate gradient\n",
    "    gradients = training_gradient(theta)\n",
    "    # calculate update\n",
    "    new_change = eta*gradients+delta_momentum*change\n",
    "    # take a step\n",
    "    theta -= new_change\n",
    "    # save the change\n",
    "    change = new_change\n",
    "    print(iter,gradients[0],gradients[1])\n",
    "print(\"theta from own gd wth momentum\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27af70",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Including Stochastic Gradient Descent with Autograd\n",
    "\n",
    "In this code we include the stochastic gradient descent approach\n",
    "discussed above. Note here that we specify which argument we are\n",
    "taking the derivative with respect to when using **autograd**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adef9763",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using SGD\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = (1.0/n)*training_gradient(y, X, theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "# Can you figure out a better way of setting up the contributions to each batch?\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        theta = theta - eta*gradients\n",
    "print(\"theta from own sdg\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310fe5b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Same code but now with momentum gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcf65acf",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using SGD\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 100\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = (1.0/n)*training_gradient(y, X, theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "change = 0.0\n",
    "delta_momentum = 0.3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+delta_momentum*change\n",
    "        # take a step\n",
    "        theta -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "print(\"theta from own sdg with momentum\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2c550",
   "metadata": {
    "editable": true
   },
   "source": [
    "## But none of these can compete with Newton's method\n",
    "\n",
    "Note that we here have introduced automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "300a02a4",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Using Newton's method\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(theta):\n",
    "    return (1.0/n)*np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+5*x*x\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "# Note that here the Hessian does not depend on the parameters theta\n",
    "invH = np.linalg.pinv(H)\n",
    "theta = np.random.randn(3,1)\n",
    "Niterations = 5\n",
    "# define the gradient\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = training_gradient(theta)\n",
    "    theta -= invH @ gradients\n",
    "    print(iter,gradients[0],gradients[1])\n",
    "print(\"theta from own Newton code\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb5fd26",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Similar (second order function now) problem but now with AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "030efc5d",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using AdaGrad and Stochastic Gradient descent\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 1000\n",
    "x = np.random.rand(n,1)\n",
    "y = 2.0+3*x +4*x*x\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "\n",
    "# Value for learning rate\n",
    "eta = 0.01\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        Giter += gradients*gradients\n",
    "        update = gradients*eta/(delta+np.sqrt(Giter))\n",
    "        theta -= update\n",
    "print(\"theta from own AdaGrad\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66850bb7",
   "metadata": {
    "editable": true
   },
   "source": [
    "Running this code we note an almost perfect agreement with the results from matrix inversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1608bcf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RMSprop for adaptive learning rate with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba7d8f7",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using RMSprop  and Stochastic Gradient descent\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 1000\n",
    "x = np.random.rand(n,1)\n",
    "y = 2.0+3*x +4*x*x# +np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "\n",
    "# Value for learning rate\n",
    "eta = 0.01\n",
    "# Value for parameter rho\n",
    "rho = 0.99\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = 0.0\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "\t# Accumulated gradient\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        Giter = (rho*Giter+(1-rho)*gradients*gradients)\n",
    "\t# Taking the diagonal only and inverting\n",
    "        update = gradients*eta/(delta+np.sqrt(Giter))\n",
    "\t# Hadamard product\n",
    "        theta -= update\n",
    "print(\"theta from own RMSprop\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503f74b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## And finally [ADAM](https://arxiv.org/pdf/1412.6980.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2a2732a",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using RMSprop  and Stochastic Gradient descent\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 1000\n",
    "x = np.random.rand(n,1)\n",
    "y = 2.0+3*x +4*x*x# +np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "\n",
    "# Value for learning rate\n",
    "eta = 0.01\n",
    "# Value for parameters theta1 and theta2, see https://arxiv.org/abs/1412.6980\n",
    "theta1 = 0.9\n",
    "theta2 = 0.999\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-7\n",
    "iter = 0\n",
    "for epoch in range(n_epochs):\n",
    "    first_moment = 0.0\n",
    "    second_moment = 0.0\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        # Computing moments first\n",
    "        first_moment = theta1*first_moment + (1-theta1)*gradients\n",
    "        second_moment = theta2*second_moment+(1-theta2)*gradients*gradients\n",
    "        first_term = first_moment/(1.0-theta1**iter)\n",
    "        second_term = second_moment/(1.0-theta2**iter)\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        update = eta*first_term/(np.sqrt(second_term)+delta)\n",
    "        theta -= update\n",
    "print(\"theta from own ADAM\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8475863",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Material for the lab sessions\n",
    "\n",
    "1. Exercise set for week 37 and reminder on scaling (from lab sessions of week 35)\n",
    "\n",
    "2. Work on project 1\n",
    "<!-- * [Video of exercise sessions week 37](https://youtu.be/bK4AEcTu-oM) -->\n",
    "\n",
    "For more discussions of Ridge regression and calculation of averages, [Wessel van Wieringen's](https://arxiv.org/abs/1509.09169) article is highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d0717",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Reminder on different scaling methods\n",
    "\n",
    "Before fitting a regression model, it is good practice to normalize or\n",
    "standardize the features. This ensures all features are on a\n",
    "comparable scale, which is especially important when using\n",
    "regularization. In the exercises this week we will perform standardization, scaling each\n",
    "feature to have mean 0 and standard deviation 1.\n",
    "\n",
    "Here we compute the mean and standard deviation of each column (feature) in our design/feature matrix $\\boldsymbol{X}$.\n",
    "Then we subtract the mean and divide by the standard deviation for each feature.\n",
    "\n",
    "In the example here we\n",
    "we will also center the target $\\boldsymbol{y}$ to mean $0$. Centering $\\boldsymbol{y}$\n",
    "(and each feature) means the model does not require a separate intercept\n",
    "term, the data is shifted such that the intercept is effectively 0\n",
    ". (In practice, one could include an intercept in the model and not\n",
    "penalize it, but here we simplify by centering.)\n",
    "Choose $n=100$ data points and set up $\\boldsymbol{x}, $\\boldsymbol{y}$ and the design matrix $\\boldsymbol{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46375144",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Standardize features (zero mean, unit variance for each feature)\n",
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "X_std[X_std == 0] = 1  # safeguard to avoid division by zero for constant features\n",
    "X_norm = (X - X_mean) / X_std\n",
    "\n",
    "# Center the target to zero mean (optional, to simplify intercept handling)\n",
    "y_mean = ?\n",
    "y_centered = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39426ccf",
   "metadata": {
    "editable": true
   },
   "source": [
    "Do we need to center the values of $y$?\n",
    "\n",
    "After this preprocessing, each column of $\\boldsymbol{X}_{\\mathrm{norm}}$ has mean zero and standard deviation $1$\n",
    "and $\\boldsymbol{y}_{\\mathrm{centered}}$ has mean 0. This can make the optimization landscape\n",
    "nicer and ensures the regularization penalty $\\lambda \\sum_j\n",
    "\\theta_j^2$ in Ridge regression treats each coefficient fairly (since features are on the\n",
    "same scale)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7fe27f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Functionality in Scikit-Learn\n",
    "\n",
    "**Scikit-Learn** has several functions which allow us to rescale the\n",
    "data, normally resulting in much better results in terms of various\n",
    "accuracy scores.  The **StandardScaler** function in **Scikit-Learn**\n",
    "ensures that for each feature/predictor we study the mean value is\n",
    "zero and the variance is one (every column in the design/feature\n",
    "matrix).  This scaling has the drawback that it does not ensure that\n",
    "we have a particular maximum or minimum in our data set. Another\n",
    "function included in **Scikit-Learn** is the **MinMaxScaler** which\n",
    "ensures that all features are exactly between $0$ and $1$. The"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd48e39",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More preprocessing\n",
    "\n",
    "The **Normalizer** scales each data\n",
    "point such that the feature vector has a euclidean length of one. In other words, it\n",
    "projects a data point on the circle (or sphere in the case of higher dimensions) with a\n",
    "radius of 1. This means every data point is scaled by a different number (by the\n",
    "inverse of it’s length).\n",
    "This normalization is often used when only the direction (or angle) of the data matters,\n",
    "not the length of the feature vector.\n",
    "\n",
    "The **RobustScaler** works similarly to the StandardScaler in that it\n",
    "ensures statistical properties for each feature that guarantee that\n",
    "they are on the same scale. However, the RobustScaler uses the median\n",
    "and quartiles, instead of mean and variance. This makes the\n",
    "RobustScaler ignore data points that are very different from the rest\n",
    "(like measurement errors). These odd data points are also called\n",
    "outliers, and might often lead to trouble for other scaling\n",
    "techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c60a0a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Frequently used scaling functions\n",
    "\n",
    "Many features are often scaled using standardization to improve performance. In **Scikit-Learn** this is given by the **StandardScaler** function as discussed above. It is easy however to write your own. \n",
    "Mathematically, this involves subtracting the mean and divide by the standard deviation over the data set, for each feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6eaa0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "x_j^{(i)} \\rightarrow \\frac{x_j^{(i)} - \\overline{x}_j}{\\sigma(x_j)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25135896",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\overline{x}_j$ and $\\sigma(x_j)$ are the mean and standard deviation, respectively,  of the feature $x_j$.\n",
    "This ensures that each feature has zero mean and unit standard deviation.  For data sets where  we do not have the standard deviation or don't wish to calculate it,  it is then common to simply set it to one.\n",
    "\n",
    "Keep in mind that when you transform your data set before training a model, the same transformation needs to be done\n",
    "on your eventual new data set  before making a prediction. If we translate this into a Python code, it would could be implemented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "469ca11e",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Model training, we compute the mean value of y and X\n",
    "y_train_mean = np.mean(y_train)\n",
    "X_train_mean = np.mean(X_train,axis=0)\n",
    "X_train = X_train - X_train_mean\n",
    "y_train = y_train - y_train_mean\n",
    "\n",
    "# The we fit our model with the training data\n",
    "trained_model = some_model.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "#Model prediction, we need also to transform our data set used for the prediction.\n",
    "X_test = X_test - X_train_mean #Use mean from training data\n",
    "y_pred = trained_model(X_test)\n",
    "y_pred = y_pred + y_train_mean\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33722029",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let us try to understand what this may imply mathematically when we\n",
    "subtract the mean values, also known as *zero centering*. For\n",
    "simplicity, we will focus on  ordinary regression, as done in the above example.\n",
    "\n",
    "The cost/loss function  for regression is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe27291e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\theta_0, \\theta_1, ... , \\theta_{p-1}) = \\frac{1}{n}\\sum_{i=0}^{n} \\left(y_i - \\theta_0 - \\sum_{j=1}^{p-1} X_{ij}\\theta_j\\right)^2,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead1167d",
   "metadata": {
    "editable": true
   },
   "source": [
    "Recall also that we use the squared value. This expression can lead to an\n",
    "increased penalty for higher differences between predicted and\n",
    "output/target values.\n",
    "\n",
    "What we have done is to single out the $\\theta_0$ term in the\n",
    "definition of the mean squared error (MSE).  The design matrix $X$\n",
    "does in this case not contain any intercept column.  When we take the\n",
    "derivative with respect to $\\theta_0$, we want the derivative to obey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2efb706",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial \\theta_j} = 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65333100",
   "metadata": {
    "editable": true
   },
   "source": [
    "for all $j$. For $\\theta_0$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fde497c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial \\theta_0} = -\\frac{2}{n}\\sum_{i=0}^{n-1} \\left(y_i - \\theta_0 - \\sum_{j=1}^{p-1} X_{ij} \\theta_j\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ce562",
   "metadata": {
    "editable": true
   },
   "source": [
    "Multiplying away the constant $2/n$, we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63a6f8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sum_{i=0}^{n-1} \\theta_0 = \\sum_{i=0}^{n-1}y_i - \\sum_{i=0}^{n-1} \\sum_{j=1}^{p-1} X_{ij} \\theta_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba0a6e4",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let us specialize first to the case where we have only two parameters $\\theta_0$ and $\\theta_1$.\n",
    "Our result for $\\theta_0$ simplifies then to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b377f93",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "n\\theta_0 = \\sum_{i=0}^{n-1}y_i - \\sum_{i=0}^{n-1} X_{i1} \\theta_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e9d08",
   "metadata": {
    "editable": true
   },
   "source": [
    "We obtain then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84784b8e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_0 = \\frac{1}{n}\\sum_{i=0}^{n-1}y_i - \\theta_1\\frac{1}{n}\\sum_{i=0}^{n-1} X_{i1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c6e5a",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce9763",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mu_{\\boldsymbol{x}_1}=\\frac{1}{n}\\sum_{i=0}^{n-1} X_{i1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1842a",
   "metadata": {
    "editable": true
   },
   "source": [
    "and the mean value of the outputs as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12163e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mu_y=\\frac{1}{n}\\sum_{i=0}^{n-1}y_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097e9ab",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239422b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_0 = \\mu_y - \\theta_1\\mu_{\\boldsymbol{x}_1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9778bb",
   "metadata": {
    "editable": true
   },
   "source": [
    "In the general case with more parameters than $\\theta_0$ and $\\theta_1$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179b77b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_0 = \\frac{1}{n}\\sum_{i=0}^{n-1}y_i - \\frac{1}{n}\\sum_{i=0}^{n-1}\\sum_{j=1}^{p-1} X_{ij}\\theta_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2f56e",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can rewrite the latter equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aa9739",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\theta_0 = \\frac{1}{n}\\sum_{i=0}^{n-1}y_i - \\sum_{j=1}^{p-1} \\mu_{\\boldsymbol{x}_j}\\theta_j,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d270cb13",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52457b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mu_{\\boldsymbol{x}_j}=\\frac{1}{n}\\sum_{i=0}^{n-1} X_{ij},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98105d",
   "metadata": {
    "editable": true
   },
   "source": [
    "the mean value for all elements of the column vector $\\boldsymbol{x}_j$.\n",
    "\n",
    "Replacing $y_i$ with $y_i - y_i - \\overline{\\boldsymbol{y}}$ and centering also our design matrix results in a cost function (in vector-matrix disguise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82302f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\theta}) = (\\boldsymbol{\\tilde{y}} - \\tilde{X}\\boldsymbol{\\theta})^T(\\boldsymbol{\\tilde{y}} - \\tilde{X}\\boldsymbol{\\theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a07a10",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we minimize with respect to $\\boldsymbol{\\theta}$ we have then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea19374e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} = (\\tilde{X}^T\\tilde{X})^{-1}\\tilde{X}^T\\boldsymbol{\\tilde{y}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd1361",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\boldsymbol{\\tilde{y}} = \\boldsymbol{y} - \\overline{\\boldsymbol{y}}$\n",
    "and $\\tilde{X}_{ij} = X_{ij} - \\frac{1}{n}\\sum_{k=0}^{n-1}X_{kj}$.\n",
    "\n",
    "For Ridge regression we need to add $\\lambda \\boldsymbol{\\theta}^T\\boldsymbol{\\theta}$ to the cost function and get then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a52f34",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} = (\\tilde{X}^T\\tilde{X} + \\lambda I)^{-1}\\tilde{X}^T\\boldsymbol{\\tilde{y}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6807dc",
   "metadata": {
    "editable": true
   },
   "source": [
    "What does this mean? And why do we insist on all this? Let us look at some examples.\n",
    "\n",
    "This code shows a simple first-order fit to a data set using the above transformed data, where we consider the role of the intercept first, by either excluding it or including it (*code example thanks to  Øyvind Sigmundson Schøyen*). Here our scaling of the data is done by subtracting the mean values only.\n",
    "Note also that we do not split the data into training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ed0cafc",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True theta: [2, 0.5, 3.7]\n",
      "Fitted theta: [2.08376632 0.19569961 3.97898392]\n",
      "Sklearn fitted theta: [2.08376632 0.19569961 3.97898392]\n",
      "MSE with intercept column\n",
      "0.004113634617443139\n",
      "MSE with intercept column from SKL\n",
      "0.004113634617443147\n",
      "Manual intercept: 2.083766322923899\n",
      "Fitted theta (without intercept): [0.19569961 3.97898392]\n",
      "Sklearn intercept: 2.0837663229239043\n",
      "Sklearn fitted theta (without intercept): [0.19569961 3.97898392]\n",
      "MSE with Manual intercept\n",
      "0.00411363461744314\n",
      "MSE with Sklearn intercept\n",
      "0.004113634617443131\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "np.random.seed(2021)\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "\n",
    "def fit_theta(X, y):\n",
    "    return np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "\n",
    "true_theta = [2, 0.5, 3.7]\n",
    "\n",
    "x = np.linspace(0, 1, 11)\n",
    "y = np.sum(\n",
    "    np.asarray([x ** p * b for p, b in enumerate(true_theta)]), axis=0\n",
    ") + 0.1 * np.random.normal(size=len(x))\n",
    "\n",
    "degree = 3\n",
    "X = np.zeros((len(x), degree))\n",
    "\n",
    "# Include the intercept in the design matrix\n",
    "for p in range(degree):\n",
    "    X[:, p] = x ** p\n",
    "\n",
    "theta = fit_theta(X, y)\n",
    "\n",
    "# Intercept is included in the design matrix\n",
    "skl = LinearRegression(fit_intercept=False).fit(X, y)\n",
    "\n",
    "print(f\"True theta: {true_theta}\")\n",
    "print(f\"Fitted theta: {theta}\")\n",
    "print(f\"Sklearn fitted theta: {skl.coef_}\")\n",
    "ypredictOwn = X @ theta\n",
    "ypredictSKL = skl.predict(X)\n",
    "print(f\"MSE with intercept column\")\n",
    "print(MSE(y,ypredictOwn))\n",
    "print(f\"MSE with intercept column from SKL\")\n",
    "print(MSE(y,ypredictSKL))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, label=\"Data\")\n",
    "plt.plot(x, X @ theta, label=\"Fit\")\n",
    "plt.plot(x, skl.predict(X), label=\"Sklearn (fit_intercept=False)\")\n",
    "\n",
    "\n",
    "# Do not include the intercept in the design matrix\n",
    "X = np.zeros((len(x), degree - 1))\n",
    "\n",
    "for p in range(degree - 1):\n",
    "    X[:, p] = x ** (p + 1)\n",
    "\n",
    "# Intercept is not included in the design matrix\n",
    "skl = LinearRegression(fit_intercept=True).fit(X, y)\n",
    "\n",
    "# Use centered values for X and y when computing coefficients\n",
    "y_offset = np.average(y, axis=0)\n",
    "X_offset = np.average(X, axis=0)\n",
    "\n",
    "theta = fit_theta(X - X_offset, y-y_offset)\n",
    "intercept = np.mean(y_offset - X_offset @ theta)\n",
    "\n",
    "print(f\"Manual intercept: {intercept}\")\n",
    "print(f\"Fitted theta (without intercept): {theta}\")\n",
    "print(f\"Sklearn intercept: {skl.intercept_}\")\n",
    "print(f\"Sklearn fitted theta (without intercept): {skl.coef_}\")\n",
    "ypredictOwn = X @ theta\n",
    "ypredictSKL = skl.predict(X)\n",
    "print(f\"MSE with Manual intercept\")\n",
    "print(MSE(y,ypredictOwn+intercept))\n",
    "print(f\"MSE with Sklearn intercept\")\n",
    "print(MSE(y,ypredictSKL))\n",
    "\n",
    "plt.plot(x, X @ theta + intercept, \"--\", label=\"Fit (manual intercept)\")\n",
    "plt.plot(x, skl.predict(X), \"--\", label=\"Sklearn (fit_intercept=True)\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72dbb49",
   "metadata": {
    "editable": true
   },
   "source": [
    "The intercept is the value of our output/target variable\n",
    "when all our features are zero and our function crosses the $y$-axis (for a one-dimensional case). \n",
    "\n",
    "Printing the MSE, we see first that both methods give the same MSE, as\n",
    "they should.  However, when we move to for example Ridge regression,\n",
    "the way we treat the intercept may give a larger or smaller MSE,\n",
    "meaning that the MSE can be penalized by the value of the\n",
    "intercept. Not including the intercept in the fit, means that the\n",
    "regularization term does not include $\\theta_0$. For different values\n",
    "of $\\lambda$, this may lead to different MSE values. \n",
    "\n",
    "To remind the reader, the regularization term, with the intercept in Ridge regression, is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7759b1f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\lambda \\vert\\vert \\boldsymbol{\\theta} \\vert\\vert_2^2 = \\lambda \\sum_{j=0}^{p-1}\\theta_j^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ecd6e",
   "metadata": {
    "editable": true
   },
   "source": [
    "but when we take out the intercept, this equation becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae897f1e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\lambda \\vert\\vert \\boldsymbol{\\theta} \\vert\\vert_2^2 = \\lambda \\sum_{j=1}^{p-1}\\theta_j^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c41f7f",
   "metadata": {
    "editable": true
   },
   "source": [
    "For Lasso regression we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa013cc4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\lambda \\vert\\vert \\boldsymbol{\\theta} \\vert\\vert_1 = \\lambda \\sum_{j=1}^{p-1}\\vert\\theta_j\\vert.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b24be",
   "metadata": {
    "editable": true
   },
   "source": [
    "It means that, when scaling the design matrix and the outputs/targets,\n",
    "by subtracting the mean values, we have an optimization problem which\n",
    "is not penalized by the intercept. The MSE value can then be smaller\n",
    "since it focuses only on the remaining quantities. If we however bring\n",
    "back the intercept, we will get a MSE which then contains the\n",
    "intercept.\n",
    "\n",
    "Armed with this wisdom, we attempt first to simply set the intercept equal to **False** in our implementation of Ridge regression for our well-known  vanilla data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9b1fa0",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta values for own Ridge implementation\n",
      "[ 1.03032441e+00  6.28336218e-02 -6.24175744e-01  5.21169159e-02\n",
      "  2.80847477e-01  2.12552073e-01  8.13220608e-02 -1.69634577e-02\n",
      " -6.50846112e-02 -7.38962192e-02 -5.94226022e-02 -3.50227564e-02\n",
      " -9.80609616e-03  1.08299273e-02  2.41882037e-02  2.93492130e-02\n",
      "  2.64742912e-02  1.63249532e-02 -5.01831251e-05 -2.15098090e-02]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[ 1.03032441e+00  6.28336218e-02 -6.24175744e-01  5.21169159e-02\n",
      "  2.80847477e-01  2.12552073e-01  8.13220608e-02 -1.69634577e-02\n",
      " -6.50846112e-02 -7.38962192e-02 -5.94226022e-02 -3.50227564e-02\n",
      " -9.80609615e-03  1.08299273e-02  2.41882037e-02  2.93492130e-02\n",
      "  2.64742912e-02  1.63249532e-02 -5.01831207e-05 -2.15098090e-02]\n",
      "MSE values for own Ridge implementation\n",
      "4.3632959215700067e-07\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "4.363295916323784e-07\n",
      "Theta values for own Ridge implementation\n",
      "[ 1.03630548 -0.01963611 -0.37900111 -0.07062318  0.12182967  0.16343471\n",
      "  0.13003291  0.07490892  0.02365049 -0.01449782 -0.03814292 -0.04909093\n",
      " -0.05009826 -0.04389027 -0.03279636 -0.01866537 -0.00289724  0.01348565\n",
      "  0.02976145  0.04543942]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[ 1.03630548 -0.01963611 -0.37900111 -0.07062318  0.12182967  0.16343471\n",
      "  0.13003291  0.07490892  0.02365049 -0.01449782 -0.03814292 -0.04909093\n",
      " -0.05009826 -0.04389027 -0.03279636 -0.01866537 -0.00289724  0.01348565\n",
      "  0.02976145  0.04543942]\n",
      "MSE values for own Ridge implementation\n",
      "5.194042827197027e-06\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "5.1940428268204826e-06\n",
      "Theta values for own Ridge implementation\n",
      "[ 1.04220758 -0.10931453 -0.17641709 -0.06020587  0.02208512  0.05789007\n",
      "  0.06491736  0.05785343  0.04537385  0.03196357  0.01969145  0.00934499\n",
      "  0.00107405 -0.00526348 -0.00992331 -0.01318643 -0.01531845 -0.01655318\n",
      " -0.01708852 -0.01708781]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[ 1.04220758 -0.10931453 -0.17641709 -0.06020587  0.02208512  0.05789007\n",
      "  0.06491736  0.05785343  0.04537385  0.03196357  0.01969145  0.00934499\n",
      "  0.00107405 -0.00526348 -0.00992331 -0.01318643 -0.01531845 -0.01655318\n",
      " -0.01708852 -0.01708781]\n",
      "MSE values for own Ridge implementation\n",
      "2.0940821989643363e-05\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "2.094082198961999e-05\n",
      "Theta values for own Ridge implementation\n",
      "[ 1.01219292 -0.06043581 -0.10391807 -0.05651951 -0.01898855  0.00312361\n",
      "  0.01463049  0.01975848  0.02123176  0.02068067  0.01905883  0.01691985\n",
      "  0.01458337  0.01223198  0.00996754  0.00784393  0.00588657  0.00410387\n",
      "  0.00249435  0.00105081]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[ 1.01219292 -0.06043581 -0.10391807 -0.05651951 -0.01898855  0.00312361\n",
      "  0.01463049  0.01975848  0.02123176  0.02068067  0.01905883  0.01691985\n",
      "  0.01458337  0.01223198  0.00996754  0.00784393  0.00588657  0.00410387\n",
      "  0.00249435  0.00105081]\n",
      "MSE values for own Ridge implementation\n",
      "0.0003153514830957865\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "0.00031535148309580783\n",
      "Theta values for own Ridge implementation\n",
      "[ 8.38916861e-01  1.31276579e-01  8.97497404e-03 -1.72271878e-02\n",
      " -2.11744554e-02 -1.91492986e-02 -1.57201944e-02 -1.23002365e-02\n",
      " -9.30466214e-03 -6.81048318e-03 -4.78184120e-03 -3.15130074e-03\n",
      " -1.84923989e-03 -8.13661243e-04  7.46984697e-06  6.56636616e-04\n",
      "  1.16805821e-03  1.56912044e-03  1.88168312e-03  2.12318726e-03]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[ 8.38916861e-01  1.31276579e-01  8.97497404e-03 -1.72271878e-02\n",
      " -2.11744554e-02 -1.91492986e-02 -1.57201944e-02 -1.23002365e-02\n",
      " -9.30466214e-03 -6.81048318e-03 -4.78184120e-03 -3.15130074e-03\n",
      " -1.84923989e-03 -8.13661243e-04  7.46984697e-06  6.56636616e-04\n",
      "  1.16805821e-03  1.56912044e-03  1.88168312e-03  2.12318726e-03]\n",
      "MSE values for own Ridge implementation\n",
      "0.015072388895177157\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "0.0150723888951771\n",
      "Theta values for own Ridge implementation\n",
      "[0.37396662 0.14174745 0.0764924  0.04892055 0.03447512 0.02586427\n",
      " 0.02024962 0.01633913 0.01347916 0.0113104  0.0096208  0.00827728\n",
      " 0.00719176 0.00630331 0.00556826 0.0049544  0.00443743 0.0039987\n",
      " 0.0036237  0.003301  ]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[0.37396662 0.14174745 0.0764924  0.04892055 0.03447512 0.02586427\n",
      " 0.02024962 0.01633913 0.01347916 0.0113104  0.0096208  0.00827728\n",
      " 0.00719176 0.00630331 0.00556826 0.0049544  0.00443743 0.0039987\n",
      " 0.0036237  0.003301  ]\n",
      "MSE values for own Ridge implementation\n",
      "0.26409315307910036\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "0.26409315307910025\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "\n",
    "# A seed just to ensure that the random numbers are the same for every run.\n",
    "# Useful for eventual debugging.\n",
    "np.random.seed(3155)\n",
    "\n",
    "n = 100\n",
    "x = np.random.rand(n)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)\n",
    "\n",
    "Maxpolydegree = 20\n",
    "X = np.zeros((n,Maxpolydegree))\n",
    "#We include explicitely the intercept column\n",
    "for degree in range(Maxpolydegree):\n",
    "    X[:,degree] = x**degree\n",
    "# We split the data in test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "p = Maxpolydegree\n",
    "I = np.eye(p,p)\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 6\n",
    "MSEOwnRidgePredict = np.zeros(nlambdas)\n",
    "MSERidgePredict = np.zeros(nlambdas)\n",
    "lambdas = np.logspace(-4, 2, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "    OwnRidgeTheta = np.linalg.pinv(X_train.T @ X_train+lmb*I) @ X_train.T @ y_train\n",
    "    # Note: we include the intercept column and no scaling\n",
    "    RegRidge = linear_model.Ridge(lmb,fit_intercept=False)\n",
    "    RegRidge.fit(X_train,y_train)\n",
    "    # and then make the prediction\n",
    "    ytildeOwnRidge = X_train @ OwnRidgeTheta\n",
    "    ypredictOwnRidge = X_test @ OwnRidgeTheta\n",
    "    ytildeRidge = RegRidge.predict(X_train)\n",
    "    ypredictRidge = RegRidge.predict(X_test)\n",
    "    MSEOwnRidgePredict[i] = MSE(y_test,ypredictOwnRidge)\n",
    "    MSERidgePredict[i] = MSE(y_test,ypredictRidge)\n",
    "    print(\"Theta values for own Ridge implementation\")\n",
    "    print(OwnRidgeTheta)\n",
    "    print(\"Theta values for Scikit-Learn Ridge implementation\")\n",
    "    print(RegRidge.coef_)\n",
    "    print(\"MSE values for own Ridge implementation\")\n",
    "    print(MSEOwnRidgePredict[i])\n",
    "    print(\"MSE values for Scikit-Learn Ridge implementation\")\n",
    "    print(MSERidgePredict[i])\n",
    "\n",
    "# Now plot the results\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), MSEOwnRidgePredict, 'r', label = 'MSE own Ridge Test')\n",
    "plt.plot(np.log10(lambdas), MSERidgePredict, 'g', label = 'MSE Ridge Test')\n",
    "\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5ca37",
   "metadata": {
    "editable": true
   },
   "source": [
    "The results here agree when we force **Scikit-Learn**'s Ridge function to include the first column in our design matrix.\n",
    "We see that the results agree very well. Here we have thus explicitely included the intercept column in the design matrix.\n",
    "What happens if we do not include the intercept in our fit?\n",
    "Let us see how we can change this code by zero centering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a731e32c",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta values for own Ridge implementation\n",
      "[ 6.21437387e-02 -6.21799480e-01  4.97375876e-02  2.80186212e-01\n",
      "  2.13204296e-01  8.21806408e-02 -1.64261842e-02 -6.49525927e-02\n",
      " -7.40621002e-02 -5.97380172e-02 -3.53631498e-02 -1.00892602e-02\n",
      "  1.06457090e-02  2.41129954e-02  2.93712126e-02  2.65682547e-02\n",
      "  1.64586608e-02  8.86820363e-05 -2.13996549e-02]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[ 6.21437387e-02 -6.21799480e-01  4.97375876e-02  2.80186212e-01\n",
      "  2.13204296e-01  8.21806408e-02 -1.64261842e-02 -6.49525927e-02\n",
      " -7.40621002e-02 -5.97380172e-02 -3.53631498e-02 -1.00892602e-02\n",
      "  1.06457090e-02  2.41129954e-02  2.93712126e-02  2.65682547e-02\n",
      "  1.64586608e-02  8.86820368e-05 -2.13996549e-02]\n",
      "Intercept from own implementation:\n",
      "1.0303792191716266\n",
      "Intercept from Scikit-Learn Ridge implementation\n",
      "1.030379219171611\n",
      "MSE values for own Ridge implementation\n",
      "4.5384372495623863e-07\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "4.538437249513786e-07\n",
      "Theta values for own Ridge implementation\n",
      "[-0.02347793 -0.37069922 -0.07332612  0.11841713  0.16193346  0.1301137\n",
      "  0.07578485  0.02473008 -0.01354652 -0.03746205 -0.048708   -0.04998023\n",
      " -0.04397732 -0.03302123 -0.01896364 -0.00321213  0.01320154  0.02954601\n",
      "  0.04532178]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[-0.02347793 -0.37069922 -0.07332612  0.11841713  0.16193346  0.1301137\n",
      "  0.07578485  0.02473008 -0.01354652 -0.03746205 -0.048708   -0.04998023\n",
      " -0.04397732 -0.03302123 -0.01896364 -0.00321213  0.01320154  0.02954601\n",
      "  0.04532178]\n",
      "Intercept from own implementation:\n",
      "1.0367447638630856\n",
      "Intercept from Scikit-Learn Ridge implementation\n",
      "1.0367447638630596\n",
      "MSE values for own Ridge implementation\n",
      "5.753660262133006e-06\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "5.753660262105462e-06\n",
      "Theta values for own Ridge implementation\n",
      "[-0.12806426 -0.15525674 -0.05487426  0.0191961   0.05293494  0.0605132\n",
      "  0.05479386  0.04368354  0.03139359  0.01993241  0.01011079  0.00212882\n",
      " -0.00410226 -0.00879058 -0.01217801 -0.0144992  -0.01596432 -0.01675341\n",
      " -0.01701673]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[-0.12806426 -0.15525674 -0.05487426  0.0191961   0.05293494  0.0605132\n",
      "  0.05479386  0.04368354  0.03139359  0.01993241  0.01011079  0.00212882\n",
      " -0.00410226 -0.00879058 -0.01217801 -0.0144992  -0.01596432 -0.01675341\n",
      " -0.01701673]\n",
      "Intercept from own implementation:\n",
      "1.0457044830369082\n",
      "Intercept from Scikit-Learn Ridge implementation\n",
      "1.0457044830369118\n",
      "MSE values for own Ridge implementation\n",
      "2.891180548496047e-05\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "2.891180548496749e-05\n",
      "Theta values for own Ridge implementation\n",
      "[-0.13729887 -0.08409267 -0.03383964 -0.00374257  0.01182892  0.01878432\n",
      "  0.02095086  0.02056419  0.01888753  0.01662493  0.01416442  0.01171602\n",
      "  0.00938921  0.00723663  0.0052788   0.00351837  0.00194828  0.00055647\n",
      " -0.00067131]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[-0.13729887 -0.08409267 -0.03383964 -0.00374257  0.01182892  0.01878432\n",
      "  0.02095086  0.02056419  0.01888753  0.01662493  0.01416442  0.01171602\n",
      "  0.00938921  0.00723663  0.0052788   0.00351837  0.00194828  0.00055647\n",
      " -0.00067131]\n",
      "Intercept from own implementation:\n",
      "1.0395764153518305\n",
      "Intercept from Scikit-Learn Ridge implementation\n",
      "1.0395764153518303\n",
      "MSE values for own Ridge implementation\n",
      "6.344515538040167e-05\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "6.344515538040063e-05\n",
      "Theta values for own Ridge implementation\n",
      "[-0.05814585 -0.04390414 -0.02861232 -0.01777492 -0.01046009 -0.00550644\n",
      " -0.00210846  0.00024979  0.00189794  0.00305065  0.00385122  0.00439783\n",
      "  0.00475926  0.00498468  0.00510982  0.00516093  0.00515753  0.00511424\n",
      "  0.00504205]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[-0.05814585 -0.04390414 -0.02861232 -0.01777492 -0.01046009 -0.00550644\n",
      " -0.00210846  0.00024979  0.00189794  0.00305065  0.00385122  0.00439783\n",
      "  0.00475926  0.00498468  0.00510982  0.00516093  0.00515753  0.00511424\n",
      "  0.00504205]\n",
      "Intercept from own implementation:\n",
      "1.0036921863049417\n",
      "Intercept from Scikit-Learn Ridge implementation\n",
      "1.0036921863049417\n",
      "MSE values for own Ridge implementation\n",
      "0.0008213907109028527\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "0.000821390710902852\n",
      "Theta values for own Ridge implementation\n",
      "[-0.00952707 -0.00855426 -0.00684557 -0.00542639 -0.00433676 -0.00350466\n",
      " -0.00286296 -0.00236187 -0.0019659  -0.00164965 -0.0013947  -0.00118745\n",
      " -0.0010177  -0.0008777  -0.00076148 -0.00066442 -0.00058287 -0.00051397\n",
      " -0.00045544]\n",
      "Theta values for Scikit-Learn Ridge implementation\n",
      "[-0.00952707 -0.00855426 -0.00684557 -0.00542639 -0.00433676 -0.00350466\n",
      " -0.00286296 -0.00236187 -0.0019659  -0.00164965 -0.0013947  -0.00118745\n",
      " -0.0010177  -0.0008777  -0.00076148 -0.00066442 -0.00058287 -0.00051397\n",
      " -0.00045544]\n",
      "Intercept from own implementation:\n",
      "0.9661173541241802\n",
      "Intercept from Scikit-Learn Ridge implementation\n",
      "0.9661173541241802\n",
      "MSE values for own Ridge implementation\n",
      "0.0031252083411001533\n",
      "MSE values for Scikit-Learn Ridge implementation\n",
      "0.0031252083411001533\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "# A seed just to ensure that the random numbers are the same for every run.\n",
    "# Useful for eventual debugging.\n",
    "np.random.seed(3155)\n",
    "\n",
    "n = 100\n",
    "x = np.random.rand(n)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)\n",
    "\n",
    "Maxpolydegree = 20\n",
    "X = np.zeros((n,Maxpolydegree-1))\n",
    "\n",
    "for degree in range(1,Maxpolydegree): #No intercept column\n",
    "    X[:,degree-1] = x**(degree)\n",
    "\n",
    "# We split the data in test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#For our own implementation, we will need to deal with the intercept by centering the design matrix and the target variable\n",
    "\n",
    "X_train_mean = np.mean(X_train,axis=0)\n",
    "#Center by removing mean from each feature\n",
    "X_train_scaled = X_train - X_train_mean \n",
    "X_test_scaled = X_test - X_train_mean\n",
    "#The model intercept (called y_scaler) is given by the mean of the target variable (IF X is centered)\n",
    "#Remove the intercept from the training data.\n",
    "y_scaler = np.mean(y_train)           \n",
    "y_train_scaled = y_train - y_scaler   \n",
    "\n",
    "p = Maxpolydegree-1\n",
    "I = np.eye(p,p)\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 6\n",
    "MSEOwnRidgePredict = np.zeros(nlambdas)\n",
    "MSERidgePredict = np.zeros(nlambdas)\n",
    "\n",
    "lambdas = np.logspace(-4, 2, nlambdas)\n",
    "for i in range(nlambdas):\n",
    "    lmb = lambdas[i]\n",
    "    OwnRidgeTheta = np.linalg.pinv(X_train_scaled.T @ X_train_scaled+lmb*I) @ X_train_scaled.T @ (y_train_scaled)\n",
    "    intercept_ = y_scaler - X_train_mean@OwnRidgeTheta #The intercept can be shifted so the model can predict on uncentered data\n",
    "    #Add intercept to prediction\n",
    "    ypredictOwnRidge = X_test_scaled @ OwnRidgeTheta + y_scaler \n",
    "    RegRidge = linear_model.Ridge(lmb)\n",
    "    RegRidge.fit(X_train,y_train)\n",
    "    ypredictRidge = RegRidge.predict(X_test)\n",
    "    MSEOwnRidgePredict[i] = MSE(y_test,ypredictOwnRidge)\n",
    "    MSERidgePredict[i] = MSE(y_test,ypredictRidge)\n",
    "    print(\"Theta values for own Ridge implementation\")\n",
    "    print(OwnRidgeTheta) #Intercept is given by mean of target variable\n",
    "    print(\"Theta values for Scikit-Learn Ridge implementation\")\n",
    "    print(RegRidge.coef_)\n",
    "    print('Intercept from own implementation:')\n",
    "    print(intercept_)\n",
    "    print('Intercept from Scikit-Learn Ridge implementation')\n",
    "    print(RegRidge.intercept_)\n",
    "    print(\"MSE values for own Ridge implementation\")\n",
    "    print(MSEOwnRidgePredict[i])\n",
    "    print(\"MSE values for Scikit-Learn Ridge implementation\")\n",
    "    print(MSERidgePredict[i])\n",
    "\n",
    "\n",
    "# Now plot the results\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lambdas), MSEOwnRidgePredict, 'b--', label = 'MSE own Ridge Test')\n",
    "plt.plot(np.log10(lambdas), MSERidgePredict, 'g--', label = 'MSE SL Ridge Test')\n",
    "plt.xlabel('log10(lambda)')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea197d8",
   "metadata": {
    "editable": true
   },
   "source": [
    "We see here, when compared to the code which includes explicitely the\n",
    "intercept column, that our MSE value is actually smaller. This is\n",
    "because the regularization term does not include the intercept value\n",
    "$\\theta_0$ in the fitting.  This applies to Lasso regularization as\n",
    "well.  It means that our optimization is now done only with the\n",
    "centered matrix and/or vector that enter the fitting procedure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
