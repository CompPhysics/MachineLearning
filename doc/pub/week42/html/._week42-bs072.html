<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week42.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week42-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 42 Constructing a Neural Network code with examples">
<title>Week 42 Constructing a Neural Network code with examples</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week42.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week42-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Lecture October 14, 2024', 2, None, 'lecture-october-14-2024'),
              ('Material for the active learning sessions on Tuesday and '
               'Wednesday',
               2,
               None,
               'material-for-the-active-learning-sessions-on-tuesday-and-wednesday'),
              ('Writing a code which implements a feed-forward neural network',
               2,
               None,
               'writing-a-code-which-implements-a-feed-forward-neural-network'),
              ('Mathematics of deep learning',
               2,
               None,
               'mathematics-of-deep-learning'),
              ('Reminder on books with hands-on material and codes',
               2,
               None,
               'reminder-on-books-with-hands-on-material-and-codes'),
              ('Reading recommendations', 2, None, 'reading-recommendations'),
              ('Simpler examples first, and automatic differentiation',
               2,
               None,
               'simpler-examples-first-and-automatic-differentiation'),
              ('Reminder on the chain rule and gradients',
               2,
               None,
               'reminder-on-the-chain-rule-and-gradients'),
              ('Multivariable functions', 2, None, 'multivariable-functions'),
              ('Automatic differentiation through examples',
               2,
               None,
               'automatic-differentiation-through-examples'),
              ('Simple example', 2, None, 'simple-example'),
              ('Smarter way of evaluating the above function',
               2,
               None,
               'smarter-way-of-evaluating-the-above-function'),
              ('Reducing the number of operations',
               2,
               None,
               'reducing-the-number-of-operations'),
              ('Chain rule, forward and reverse modes',
               2,
               None,
               'chain-rule-forward-and-reverse-modes'),
              ('Forward and reverse modes',
               2,
               None,
               'forward-and-reverse-modes'),
              ('More complicated function',
               2,
               None,
               'more-complicated-function'),
              ('Counting the number of floating point operations',
               2,
               None,
               'counting-the-number-of-floating-point-operations'),
              ('Defining intermediate operations',
               2,
               None,
               'defining-intermediate-operations'),
              ('New expression for the derivative',
               2,
               None,
               'new-expression-for-the-derivative'),
              ('Final derivatives', 2, None, 'final-derivatives'),
              ('In general not this simple',
               2,
               None,
               'in-general-not-this-simple'),
              ('Automatic differentiation',
               2,
               None,
               'automatic-differentiation'),
              ('Chain rule', 2, None, 'chain-rule'),
              ('First network example, simple percepetron with one input',
               2,
               None,
               'first-network-example-simple-percepetron-with-one-input'),
              ('Layout of a simple neural network with no hidden layer',
               2,
               None,
               'layout-of-a-simple-neural-network-with-no-hidden-layer'),
              ('Optimizing the parameters',
               2,
               None,
               'optimizing-the-parameters'),
              ('Adding a hidden layer', 2, None, 'adding-a-hidden-layer'),
              ('Layout of a simple neural network with one hidden layer',
               2,
               None,
               'layout-of-a-simple-neural-network-with-one-hidden-layer'),
              ('The derivatives', 2, None, 'the-derivatives'),
              ('Important observations', 2, None, 'important-observations'),
              ('The training', 2, None, 'the-training'),
              ('Code example', 2, None, 'code-example'),
              ('Exercise 1: Including more data',
               2,
               None,
               'exercise-1-including-more-data'),
              ('Simple neural network and the  back propagation equations',
               2,
               None,
               'simple-neural-network-and-the-back-propagation-equations'),
              ('Layout of a simple neural network with two input nodes, one  '
               'hidden layer and one output node',
               2,
               None,
               'layout-of-a-simple-neural-network-with-two-input-nodes-one-hidden-layer-and-one-output-node'),
              ('The ouput layer', 2, None, 'the-ouput-layer'),
              ('Compact expressions', 2, None, 'compact-expressions'),
              ('Output layer', 2, None, 'output-layer'),
              ('Explicit derivatives', 2, None, 'explicit-derivatives'),
              ('Derivatives of the hidden layer',
               2,
               None,
               'derivatives-of-the-hidden-layer'),
              ('Final expression', 2, None, 'final-expression'),
              ('Completing the list', 2, None, 'completing-the-list'),
              ('Final expressions for the biases of the hidden layer',
               2,
               None,
               'final-expressions-for-the-biases-of-the-hidden-layer'),
              ('Gradient expressions', 2, None, 'gradient-expressions'),
              ('Exercise 2: Extended program',
               2,
               None,
               'exercise-2-extended-program'),
              ('Setting up the equations for a neural network',
               2,
               None,
               'setting-up-the-equations-for-a-neural-network'),
              ('Layout of a neural network with three hidden layers',
               2,
               None,
               'layout-of-a-neural-network-with-three-hidden-layers'),
              ('Definitions', 2, None, 'definitions'),
              ('Inputs to the activation function',
               2,
               None,
               'inputs-to-the-activation-function'),
              ('Derivatives and the chain rule',
               2,
               None,
               'derivatives-and-the-chain-rule'),
              ('Derivative of the cost function',
               2,
               None,
               'derivative-of-the-cost-function'),
              ('The  back propagation equations for a neural network',
               2,
               None,
               'the-back-propagation-equations-for-a-neural-network'),
              ('Analyzing the last results',
               2,
               None,
               'analyzing-the-last-results'),
              ('More considerations', 2, None, 'more-considerations'),
              ('Derivatives in terms of $z_j^L$',
               2,
               None,
               'derivatives-in-terms-of-z-j-l'),
              ('Bringing it together', 2, None, 'bringing-it-together'),
              ('Final back propagating equation',
               2,
               None,
               'final-back-propagating-equation'),
              ('Using the chain rule and summing over all $k$ entries',
               2,
               None,
               'using-the-chain-rule-and-summing-over-all-k-entries'),
              ('Setting up the back propagation algorithm',
               2,
               None,
               'setting-up-the-back-propagation-algorithm'),
              ('Setting up the back propagation algorithm, part 2',
               2,
               None,
               'setting-up-the-back-propagation-algorithm-part-2'),
              ('Setting up the Back propagation algorithm, part 3',
               2,
               None,
               'setting-up-the-back-propagation-algorithm-part-3'),
              ('Updating the gradients', 2, None, 'updating-the-gradients'),
              ('Activation functions', 3, None, 'activation-functions'),
              ('Activation functions, Logistic and Hyperbolic ones',
               3,
               None,
               'activation-functions-logistic-and-hyperbolic-ones'),
              ('Relevance', 3, None, 'relevance'),
              ('Fine-tuning neural network hyperparameters',
               2,
               None,
               'fine-tuning-neural-network-hyperparameters'),
              ('Hidden layers', 2, None, 'hidden-layers'),
              ('Vanishing gradients', 2, None, 'vanishing-gradients'),
              ('Exploding gradients', 2, None, 'exploding-gradients'),
              ('Is the Logistic activation function (Sigmoid)  our choice?',
               2,
               None,
               'is-the-logistic-activation-function-sigmoid-our-choice'),
              ('Logistic function as the root of problems',
               2,
               None,
               'logistic-function-as-the-root-of-problems'),
              ('The derivative of the Logistic funtion',
               2,
               None,
               'the-derivative-of-the-logistic-funtion'),
              ('Insights from the paper by Glorot and Bengio',
               2,
               None,
               'insights-from-the-paper-by-glorot-and-bengio'),
              ('The RELU function family', 2, None, 'the-relu-function-family'),
              ('ELU function', 2, None, 'elu-function'),
              ('Which activation function should we use?',
               2,
               None,
               'which-activation-function-should-we-use'),
              ('More on activation functions, output layers',
               2,
               None,
               'more-on-activation-functions-output-layers'),
              ('Batch Normalization', 2, None, 'batch-normalization'),
              ('Dropout', 2, None, 'dropout'),
              ('Gradient Clipping', 2, None, 'gradient-clipping'),
              ('A top-down perspective on Neural networks',
               2,
               None,
               'a-top-down-perspective-on-neural-networks'),
              ('More top-down perspectives',
               2,
               None,
               'more-top-down-perspectives'),
              ('Limitations of supervised learning with deep networks',
               2,
               None,
               'limitations-of-supervised-learning-with-deep-networks'),
              ('Limitations of NNs', 2, None, 'limitations-of-nns'),
              ('Homogeneous data', 2, None, 'homogeneous-data'),
              ('More limitations', 2, None, 'more-limitations'),
              ('Setting up the back-propagation algorithm',
               2,
               None,
               'setting-up-the-back-propagation-algorithm'),
              ('Setting up a Multi-layer perceptron model for classification',
               2,
               None,
               'setting-up-a-multi-layer-perceptron-model-for-classification'),
              ('Defining the cost function',
               2,
               None,
               'defining-the-cost-function'),
              ('Example: binary classification problem',
               2,
               None,
               'example-binary-classification-problem'),
              ('The Softmax function', 2, None, 'the-softmax-function'),
              ('Developing a code for doing neural networks with back '
               'propagation',
               2,
               None,
               'developing-a-code-for-doing-neural-networks-with-back-propagation'),
              ('Collect and pre-process data',
               2,
               None,
               'collect-and-pre-process-data'),
              ('Train and test datasets', 2, None, 'train-and-test-datasets'),
              ('Define model and architecture',
               2,
               None,
               'define-model-and-architecture'),
              ('Layers', 2, None, 'layers'),
              ('Weights and biases', 2, None, 'weights-and-biases'),
              ('Feed-forward pass', 2, None, 'feed-forward-pass'),
              ('Matrix multiplications', 2, None, 'matrix-multiplications'),
              ('Choose cost function and optimizer',
               2,
               None,
               'choose-cost-function-and-optimizer'),
              ('Optimizing the cost function',
               2,
               None,
               'optimizing-the-cost-function'),
              ('Regularization', 2, None, 'regularization'),
              ('Matrix  multiplication', 2, None, 'matrix-multiplication'),
              ('Improving performance', 2, None, 'improving-performance'),
              ('Full object-oriented implementation',
               2,
               None,
               'full-object-oriented-implementation'),
              ('Evaluate model performance on test data',
               2,
               None,
               'evaluate-model-performance-on-test-data'),
              ('Adjust hyperparameters', 2, None, 'adjust-hyperparameters'),
              ('Visualization', 2, None, 'visualization'),
              ('scikit-learn implementation',
               2,
               None,
               'scikit-learn-implementation'),
              ('Visualization', 2, None, 'visualization'),
              ('Building neural networks in Tensorflow and Keras',
               2,
               None,
               'building-neural-networks-in-tensorflow-and-keras'),
              ('Tensorflow', 2, None, 'tensorflow'),
              ('Using Keras', 2, None, 'using-keras'),
              ('Collect and pre-process data',
               2,
               None,
               'collect-and-pre-process-data'),
              ('The Breast Cancer Data, now with Keras',
               2,
               None,
               'the-breast-cancer-data-now-with-keras'),
              ('Building a neural network code',
               2,
               None,
               'building-a-neural-network-code'),
              ('Learning rate methods', 3, None, 'learning-rate-methods'),
              ('Usage of the above learning rate schedulers',
               3,
               None,
               'usage-of-the-above-learning-rate-schedulers'),
              ('Cost functions', 3, None, 'cost-functions'),
              ('Activation functions', 3, None, 'activation-functions'),
              ('The Neural Network', 3, None, 'the-neural-network'),
              ('Multiclass classification',
               3,
               None,
               'multiclass-classification'),
              ('Testing the XOR gate and other gates',
               2,
               None,
               'testing-the-xor-gate-and-other-gates')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week42-bs.html">Week 42 Constructing a Neural Network code with examples</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week42-bs001.html#lecture-october-14-2024" style="font-size: 80%;"><b>Lecture October 14, 2024</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs002.html#material-for-the-active-learning-sessions-on-tuesday-and-wednesday" style="font-size: 80%;"><b>Material for the active learning sessions on Tuesday and Wednesday</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs003.html#writing-a-code-which-implements-a-feed-forward-neural-network" style="font-size: 80%;"><b>Writing a code which implements a feed-forward neural network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs004.html#mathematics-of-deep-learning" style="font-size: 80%;"><b>Mathematics of deep learning</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs005.html#reminder-on-books-with-hands-on-material-and-codes" style="font-size: 80%;"><b>Reminder on books with hands-on material and codes</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs006.html#reading-recommendations" style="font-size: 80%;"><b>Reading recommendations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs007.html#simpler-examples-first-and-automatic-differentiation" style="font-size: 80%;"><b>Simpler examples first, and automatic differentiation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs008.html#reminder-on-the-chain-rule-and-gradients" style="font-size: 80%;"><b>Reminder on the chain rule and gradients</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs009.html#multivariable-functions" style="font-size: 80%;"><b>Multivariable functions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs010.html#automatic-differentiation-through-examples" style="font-size: 80%;"><b>Automatic differentiation through examples</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs011.html#simple-example" style="font-size: 80%;"><b>Simple example</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs012.html#smarter-way-of-evaluating-the-above-function" style="font-size: 80%;"><b>Smarter way of evaluating the above function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs013.html#reducing-the-number-of-operations" style="font-size: 80%;"><b>Reducing the number of operations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs014.html#chain-rule-forward-and-reverse-modes" style="font-size: 80%;"><b>Chain rule, forward and reverse modes</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs015.html#forward-and-reverse-modes" style="font-size: 80%;"><b>Forward and reverse modes</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs016.html#more-complicated-function" style="font-size: 80%;"><b>More complicated function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs017.html#counting-the-number-of-floating-point-operations" style="font-size: 80%;"><b>Counting the number of floating point operations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs018.html#defining-intermediate-operations" style="font-size: 80%;"><b>Defining intermediate operations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs019.html#new-expression-for-the-derivative" style="font-size: 80%;"><b>New expression for the derivative</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs020.html#final-derivatives" style="font-size: 80%;"><b>Final derivatives</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs021.html#in-general-not-this-simple" style="font-size: 80%;"><b>In general not this simple</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs022.html#automatic-differentiation" style="font-size: 80%;"><b>Automatic differentiation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs023.html#chain-rule" style="font-size: 80%;"><b>Chain rule</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs024.html#first-network-example-simple-percepetron-with-one-input" style="font-size: 80%;"><b>First network example, simple percepetron with one input</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs025.html#layout-of-a-simple-neural-network-with-no-hidden-layer" style="font-size: 80%;"><b>Layout of a simple neural network with no hidden layer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs026.html#optimizing-the-parameters" style="font-size: 80%;"><b>Optimizing the parameters</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs027.html#adding-a-hidden-layer" style="font-size: 80%;"><b>Adding a hidden layer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs028.html#layout-of-a-simple-neural-network-with-one-hidden-layer" style="font-size: 80%;"><b>Layout of a simple neural network with one hidden layer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs029.html#the-derivatives" style="font-size: 80%;"><b>The derivatives</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs030.html#important-observations" style="font-size: 80%;"><b>Important observations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs031.html#the-training" style="font-size: 80%;"><b>The training</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs032.html#code-example" style="font-size: 80%;"><b>Code example</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs033.html#exercise-1-including-more-data" style="font-size: 80%;"><b>Exercise 1: Including more data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs034.html#simple-neural-network-and-the-back-propagation-equations" style="font-size: 80%;"><b>Simple neural network and the  back propagation equations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs035.html#layout-of-a-simple-neural-network-with-two-input-nodes-one-hidden-layer-and-one-output-node" style="font-size: 80%;"><b>Layout of a simple neural network with two input nodes, one  hidden layer and one output node</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs036.html#the-ouput-layer" style="font-size: 80%;"><b>The ouput layer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs037.html#compact-expressions" style="font-size: 80%;"><b>Compact expressions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs038.html#output-layer" style="font-size: 80%;"><b>Output layer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs039.html#explicit-derivatives" style="font-size: 80%;"><b>Explicit derivatives</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs040.html#derivatives-of-the-hidden-layer" style="font-size: 80%;"><b>Derivatives of the hidden layer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs041.html#final-expression" style="font-size: 80%;"><b>Final expression</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs042.html#completing-the-list" style="font-size: 80%;"><b>Completing the list</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs043.html#final-expressions-for-the-biases-of-the-hidden-layer" style="font-size: 80%;"><b>Final expressions for the biases of the hidden layer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs044.html#gradient-expressions" style="font-size: 80%;"><b>Gradient expressions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs045.html#exercise-2-extended-program" style="font-size: 80%;"><b>Exercise 2: Extended program</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs046.html#setting-up-the-equations-for-a-neural-network" style="font-size: 80%;"><b>Setting up the equations for a neural network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs047.html#layout-of-a-neural-network-with-three-hidden-layers" style="font-size: 80%;"><b>Layout of a neural network with three hidden layers</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs048.html#definitions" style="font-size: 80%;"><b>Definitions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs049.html#inputs-to-the-activation-function" style="font-size: 80%;"><b>Inputs to the activation function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs050.html#derivatives-and-the-chain-rule" style="font-size: 80%;"><b>Derivatives and the chain rule</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs051.html#derivative-of-the-cost-function" style="font-size: 80%;"><b>Derivative of the cost function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs052.html#the-back-propagation-equations-for-a-neural-network" style="font-size: 80%;"><b>The  back propagation equations for a neural network</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs053.html#analyzing-the-last-results" style="font-size: 80%;"><b>Analyzing the last results</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs054.html#more-considerations" style="font-size: 80%;"><b>More considerations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs055.html#derivatives-in-terms-of-z-j-l" style="font-size: 80%;"><b>Derivatives in terms of \( z_j^L \)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs056.html#bringing-it-together" style="font-size: 80%;"><b>Bringing it together</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs057.html#final-back-propagating-equation" style="font-size: 80%;"><b>Final back propagating equation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs058.html#using-the-chain-rule-and-summing-over-all-k-entries" style="font-size: 80%;"><b>Using the chain rule and summing over all \( k \) entries</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs087.html#setting-up-the-back-propagation-algorithm" style="font-size: 80%;"><b>Setting up the back propagation algorithm</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs060.html#setting-up-the-back-propagation-algorithm-part-2" style="font-size: 80%;"><b>Setting up the back propagation algorithm, part 2</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs061.html#setting-up-the-back-propagation-algorithm-part-3" style="font-size: 80%;"><b>Setting up the Back propagation algorithm, part 3</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs062.html#updating-the-gradients" style="font-size: 80%;"><b>Updating the gradients</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs116.html#activation-functions" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs064.html#activation-functions-logistic-and-hyperbolic-ones" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions, Logistic and Hyperbolic ones</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs065.html#relevance" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Relevance</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs066.html#fine-tuning-neural-network-hyperparameters" style="font-size: 80%;"><b>Fine-tuning neural network hyperparameters</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs067.html#hidden-layers" style="font-size: 80%;"><b>Hidden layers</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs068.html#vanishing-gradients" style="font-size: 80%;"><b>Vanishing gradients</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs069.html#exploding-gradients" style="font-size: 80%;"><b>Exploding gradients</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs070.html#is-the-logistic-activation-function-sigmoid-our-choice" style="font-size: 80%;"><b>Is the Logistic activation function (Sigmoid)  our choice?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs071.html#logistic-function-as-the-root-of-problems" style="font-size: 80%;"><b>Logistic function as the root of problems</b></a></li>
     <!-- navigation toc: --> <li><a href="#the-derivative-of-the-logistic-funtion" style="font-size: 80%;"><b>The derivative of the Logistic funtion</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs073.html#insights-from-the-paper-by-glorot-and-bengio" style="font-size: 80%;"><b>Insights from the paper by Glorot and Bengio</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs074.html#the-relu-function-family" style="font-size: 80%;"><b>The RELU function family</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs075.html#elu-function" style="font-size: 80%;"><b>ELU function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs076.html#which-activation-function-should-we-use" style="font-size: 80%;"><b>Which activation function should we use?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs077.html#more-on-activation-functions-output-layers" style="font-size: 80%;"><b>More on activation functions, output layers</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs078.html#batch-normalization" style="font-size: 80%;"><b>Batch Normalization</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs079.html#dropout" style="font-size: 80%;"><b>Dropout</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs080.html#gradient-clipping" style="font-size: 80%;"><b>Gradient Clipping</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs081.html#a-top-down-perspective-on-neural-networks" style="font-size: 80%;"><b>A top-down perspective on Neural networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs082.html#more-top-down-perspectives" style="font-size: 80%;"><b>More top-down perspectives</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs083.html#limitations-of-supervised-learning-with-deep-networks" style="font-size: 80%;"><b>Limitations of supervised learning with deep networks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs084.html#limitations-of-nns" style="font-size: 80%;"><b>Limitations of NNs</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs085.html#homogeneous-data" style="font-size: 80%;"><b>Homogeneous data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs086.html#more-limitations" style="font-size: 80%;"><b>More limitations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs087.html#setting-up-the-back-propagation-algorithm" style="font-size: 80%;"><b>Setting up the back-propagation algorithm</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs088.html#setting-up-a-multi-layer-perceptron-model-for-classification" style="font-size: 80%;"><b>Setting up a Multi-layer perceptron model for classification</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs089.html#defining-the-cost-function" style="font-size: 80%;"><b>Defining the cost function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs090.html#example-binary-classification-problem" style="font-size: 80%;"><b>Example: binary classification problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs091.html#the-softmax-function" style="font-size: 80%;"><b>The Softmax function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs092.html#developing-a-code-for-doing-neural-networks-with-back-propagation" style="font-size: 80%;"><b>Developing a code for doing neural networks with back propagation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs114.html#collect-and-pre-process-data" style="font-size: 80%;"><b>Collect and pre-process data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs094.html#train-and-test-datasets" style="font-size: 80%;"><b>Train and test datasets</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs095.html#define-model-and-architecture" style="font-size: 80%;"><b>Define model and architecture</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs096.html#layers" style="font-size: 80%;"><b>Layers</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs097.html#weights-and-biases" style="font-size: 80%;"><b>Weights and biases</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs098.html#feed-forward-pass" style="font-size: 80%;"><b>Feed-forward pass</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs099.html#matrix-multiplications" style="font-size: 80%;"><b>Matrix multiplications</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs100.html#choose-cost-function-and-optimizer" style="font-size: 80%;"><b>Choose cost function and optimizer</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs101.html#optimizing-the-cost-function" style="font-size: 80%;"><b>Optimizing the cost function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs102.html#regularization" style="font-size: 80%;"><b>Regularization</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs103.html#matrix-multiplication" style="font-size: 80%;"><b>Matrix  multiplication</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs104.html#improving-performance" style="font-size: 80%;"><b>Improving performance</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs105.html#full-object-oriented-implementation" style="font-size: 80%;"><b>Full object-oriented implementation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs106.html#evaluate-model-performance-on-test-data" style="font-size: 80%;"><b>Evaluate model performance on test data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs107.html#adjust-hyperparameters" style="font-size: 80%;"><b>Adjust hyperparameters</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs110.html#visualization" style="font-size: 80%;"><b>Visualization</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs109.html#scikit-learn-implementation" style="font-size: 80%;"><b>scikit-learn implementation</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs110.html#visualization" style="font-size: 80%;"><b>Visualization</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs111.html#building-neural-networks-in-tensorflow-and-keras" style="font-size: 80%;"><b>Building neural networks in Tensorflow and Keras</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs112.html#tensorflow" style="font-size: 80%;"><b>Tensorflow</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs113.html#using-keras" style="font-size: 80%;"><b>Using Keras</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs114.html#collect-and-pre-process-data" style="font-size: 80%;"><b>Collect and pre-process data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs115.html#the-breast-cancer-data-now-with-keras" style="font-size: 80%;"><b>The Breast Cancer Data, now with Keras</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs116.html#building-a-neural-network-code" style="font-size: 80%;"><b>Building a neural network code</b></a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs116.html#learning-rate-methods" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Learning rate methods</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs116.html#usage-of-the-above-learning-rate-schedulers" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Usage of the above learning rate schedulers</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs116.html#cost-functions" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Cost functions</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs116.html#activation-functions" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Activation functions</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs116.html#the-neural-network" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;The Neural Network</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs116.html#multiclass-classification" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Multiclass classification</a></li>
     <!-- navigation toc: --> <li><a href="._week42-bs117.html#testing-the-xor-gate-and-other-gates" style="font-size: 80%;"><b>Testing the XOR gate and other gates</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0072"></a>
<!-- !split -->
<h2 id="the-derivative-of-the-logistic-funtion" class="anchor">The derivative of the Logistic funtion </h2>

<p>Looking at the logistic activation function, when inputs become large
(negative or positive), the function saturates at 0 or 1, with a
derivative extremely close to 0. Thus when backpropagation kicks in,
it has virtually no gradient to propagate back through the network,
and what little gradient exists keeps getting diluted as
backpropagation progresses down through the top layers, so there is
really nothing left for the lower layers.
</p>

<p>In their paper, Glorot and Bengio propose a way to significantly
alleviate this problem. We need the signal to flow properly in both
directions: in the forward direction when making predictions, and in
the reverse direction when backpropagating gradients. We don&#8217;t want
the signal to die out, nor do we want it to explode and saturate. For
the signal to flow properly, the authors argue that we need the
variance of the outputs of each layer to be equal to the variance of
its inputs, and we also need the gradients to have equal variance
before and after flowing through a layer in the reverse direction.
</p>

<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week42-bs071.html">&laquo;</a></li>
  <li><a href="._week42-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week42-bs064.html">65</a></li>
  <li><a href="._week42-bs065.html">66</a></li>
  <li><a href="._week42-bs066.html">67</a></li>
  <li><a href="._week42-bs067.html">68</a></li>
  <li><a href="._week42-bs068.html">69</a></li>
  <li><a href="._week42-bs069.html">70</a></li>
  <li><a href="._week42-bs070.html">71</a></li>
  <li><a href="._week42-bs071.html">72</a></li>
  <li class="active"><a href="._week42-bs072.html">73</a></li>
  <li><a href="._week42-bs073.html">74</a></li>
  <li><a href="._week42-bs074.html">75</a></li>
  <li><a href="._week42-bs075.html">76</a></li>
  <li><a href="._week42-bs076.html">77</a></li>
  <li><a href="._week42-bs077.html">78</a></li>
  <li><a href="._week42-bs078.html">79</a></li>
  <li><a href="._week42-bs079.html">80</a></li>
  <li><a href="._week42-bs080.html">81</a></li>
  <li><a href="._week42-bs081.html">82</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week42-bs117.html">118</a></li>
  <li><a href="._week42-bs073.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>

