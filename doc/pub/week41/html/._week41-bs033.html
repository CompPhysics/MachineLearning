<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 41 Tensor flow and Deep Learning, Convolutional Neural Networks">

<title>Week 41 Tensor flow and Deep Learning, Convolutional Neural Networks</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 40', 2, None, '___sec0'),
              ('Overview video for week 41', 2, None, '___sec1'),
              ('Setting up the Back propagation algorithm', 2, None, '___sec2'),
              ('Setting up a Multi-layer perceptron model for classification',
               2,
               None,
               '___sec3'),
              ('Defining the cost function', 2, None, '___sec4'),
              ('Example: binary classification problem', 2, None, '___sec5'),
              ('The Softmax function', 2, None, '___sec6'),
              ('Developing a code for doing neural networks with back '
               'propagation',
               2,
               None,
               '___sec7'),
              ('Collect and pre-process data', 2, None, '___sec8'),
              ('Train and test datasets', 2, None, '___sec9'),
              ('Define model and architecture', 2, None, '___sec10'),
              ('Layers', 2, None, '___sec11'),
              ('Weights and biases', 2, None, '___sec12'),
              ('Feed-forward pass', 2, None, '___sec13'),
              ('Matrix multiplications', 2, None, '___sec14'),
              ('Choose cost function and optimizer', 2, None, '___sec15'),
              ('Optimizing the cost function', 2, None, '___sec16'),
              ('Regularization', 2, None, '___sec17'),
              ('Matrix  multiplication', 2, None, '___sec18'),
              ('Improving performance', 2, None, '___sec19'),
              ('Full object-oriented implementation', 2, None, '___sec20'),
              ('Evaluate model performance on test data', 2, None, '___sec21'),
              ('Adjust hyperparameters', 2, None, '___sec22'),
              ('Visualization', 2, None, '___sec23'),
              ('scikit-learn implementation', 2, None, '___sec24'),
              ('Visualization', 2, None, '___sec25'),
              ('Building neural networks in Tensorflow and Keras',
               2,
               None,
               '___sec26'),
              ('Tensorflow', 2, None, '___sec27'),
              ('Collect and pre-process data', 2, None, '___sec28'),
              ('Using TensorFlow backend', 2, None, '___sec29'),
              ('Optimizing and using gradient descent', 2, None, '___sec30'),
              ('Using Keras', 2, None, '___sec31'),
              ('Which activation function should I use?', 2, None, '___sec32'),
              ('Is the Logistic activation function (Sigmoid)  our choice?',
               2,
               None,
               '___sec33'),
              ('The derivative of the Logistic funtion', 2, None, '___sec34'),
              ('The RELU function family', 2, None, '___sec35'),
              ('Which activation function should we use?', 2, None, '___sec36'),
              ('A top-down perspective on Neural networks',
               2,
               None,
               '___sec37'),
              ('Limitations of supervised learning with deep networks',
               2,
               None,
               '___sec38'),
              ('Convolutional Neural Networks (recognizing images)',
               2,
               None,
               '___sec39'),
              ('Regular NNs don’t scale well to full images',
               2,
               None,
               '___sec40'),
              ('3D volumes of neurons', 2, None, '___sec41'),
              ('Layers used to build CNNs', 2, None, '___sec42'),
              ('Transforming images', 2, None, '___sec43'),
              ('CNNs in brief', 2, None, '___sec44'),
              ('CNNs in more detail, building convolutional neural networks in '
               'Tensorflow and Keras',
               2,
               None,
               '___sec45'),
              ('Setting it up', 2, None, '___sec46'),
              ('The MNIST dataset again', 2, None, '___sec47'),
              ('Strong correlations', 2, None, '___sec48'),
              ('Layers of a CNN', 2, None, '___sec49'),
              ('Systematic reduction', 2, None, '___sec50'),
              ('Prerequisites: Collect and pre-process data',
               2,
               None,
               '___sec51'),
              ('Importing Keras and Tensorflow', 2, None, '___sec52'),
              ('Using TensorFlow backend', 2, None, '___sec53'),
              ('Train the model', 2, None, '___sec54'),
              ('Visualizing the results', 2, None, '___sec55'),
              ('Running with Keras', 2, None, '___sec56'),
              ('Final part', 2, None, '___sec57'),
              ('Final visualization', 2, None, '___sec58'),
              ('Fun links', 2, None, '___sec59')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week41-bs.html">Week 41 Tensor flow and Deep Learning, Convolutional Neural Networks</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week41-bs001.html#___sec0" style="font-size: 80%;">Plan for week 40</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs002.html#___sec1" style="font-size: 80%;">Overview video for week 41</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs003.html#___sec2" style="font-size: 80%;">Setting up the Back propagation algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs004.html#___sec3" style="font-size: 80%;">Setting up a Multi-layer perceptron model for classification</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs005.html#___sec4" style="font-size: 80%;">Defining the cost function</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs006.html#___sec5" style="font-size: 80%;">Example: binary classification problem</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs007.html#___sec6" style="font-size: 80%;">The Softmax function</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs008.html#___sec7" style="font-size: 80%;">Developing a code for doing neural networks with back propagation</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs009.html#___sec8" style="font-size: 80%;">Collect and pre-process data</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs010.html#___sec9" style="font-size: 80%;">Train and test datasets</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs011.html#___sec10" style="font-size: 80%;">Define model and architecture</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs012.html#___sec11" style="font-size: 80%;">Layers</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs013.html#___sec12" style="font-size: 80%;">Weights and biases</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs014.html#___sec13" style="font-size: 80%;">Feed-forward pass</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs015.html#___sec14" style="font-size: 80%;">Matrix multiplications</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs016.html#___sec15" style="font-size: 80%;">Choose cost function and optimizer</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs017.html#___sec16" style="font-size: 80%;">Optimizing the cost function</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs018.html#___sec17" style="font-size: 80%;">Regularization</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs019.html#___sec18" style="font-size: 80%;">Matrix  multiplication</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs020.html#___sec19" style="font-size: 80%;">Improving performance</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs021.html#___sec20" style="font-size: 80%;">Full object-oriented implementation</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs022.html#___sec21" style="font-size: 80%;">Evaluate model performance on test data</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs023.html#___sec22" style="font-size: 80%;">Adjust hyperparameters</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs024.html#___sec23" style="font-size: 80%;">Visualization</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs025.html#___sec24" style="font-size: 80%;">scikit-learn implementation</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs026.html#___sec25" style="font-size: 80%;">Visualization</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs027.html#___sec26" style="font-size: 80%;">Building neural networks in Tensorflow and Keras</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs028.html#___sec27" style="font-size: 80%;">Tensorflow</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs029.html#___sec28" style="font-size: 80%;">Collect and pre-process data</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs030.html#___sec29" style="font-size: 80%;">Using TensorFlow backend</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs031.html#___sec30" style="font-size: 80%;">Optimizing and using gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs032.html#___sec31" style="font-size: 80%;">Using Keras</a></li>
     <!-- navigation toc: --> <li><a href="#___sec32" style="font-size: 80%;">Which activation function should I use?</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs034.html#___sec33" style="font-size: 80%;">Is the Logistic activation function (Sigmoid)  our choice?</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs035.html#___sec34" style="font-size: 80%;">The derivative of the Logistic funtion</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs036.html#___sec35" style="font-size: 80%;">The RELU function family</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs037.html#___sec36" style="font-size: 80%;">Which activation function should we use?</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs038.html#___sec37" style="font-size: 80%;">A top-down perspective on Neural networks</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs039.html#___sec38" style="font-size: 80%;">Limitations of supervised learning with deep networks</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs040.html#___sec39" style="font-size: 80%;">Convolutional Neural Networks (recognizing images)</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs041.html#___sec40" style="font-size: 80%;">Regular NNs don’t scale well to full images</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs042.html#___sec41" style="font-size: 80%;">3D volumes of neurons</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs043.html#___sec42" style="font-size: 80%;">Layers used to build CNNs</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs044.html#___sec43" style="font-size: 80%;">Transforming images</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs045.html#___sec44" style="font-size: 80%;">CNNs in brief</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs046.html#___sec45" style="font-size: 80%;">CNNs in more detail, building convolutional neural networks in Tensorflow and Keras</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs047.html#___sec46" style="font-size: 80%;">Setting it up</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs048.html#___sec47" style="font-size: 80%;">The MNIST dataset again</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs049.html#___sec48" style="font-size: 80%;">Strong correlations</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs050.html#___sec49" style="font-size: 80%;">Layers of a CNN</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs051.html#___sec50" style="font-size: 80%;">Systematic reduction</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs052.html#___sec51" style="font-size: 80%;">Prerequisites: Collect and pre-process data</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs053.html#___sec52" style="font-size: 80%;">Importing Keras and Tensorflow</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs054.html#___sec53" style="font-size: 80%;">Using TensorFlow backend</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs055.html#___sec54" style="font-size: 80%;">Train the model</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs056.html#___sec55" style="font-size: 80%;">Visualizing the results</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs057.html#___sec56" style="font-size: 80%;">Running with Keras</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs058.html#___sec57" style="font-size: 80%;">Final part</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs059.html#___sec58" style="font-size: 80%;">Final visualization</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs060.html#___sec59" style="font-size: 80%;">Fun links</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0033"></a>
<!-- !split  -->

<h2 id="___sec32" class="anchor">Which activation function should I use? </h2>

<p>
The Back propagation algorithm we derived above works by going from
the output layer to the input layer, propagating the error gradient on
the way. Once the algorithm has computed the gradient of the cost
function with regards to each parameter in the network, it uses these
gradients to update each parameter with a Gradient Descent (GD) step.

<p>
Unfortunately for us, the gradients often get smaller and smaller as the
algorithm progresses down to the first hidden layers. As a result, the
GD update leaves the lower layer connection weights
virtually unchanged, and training never converges to a good
solution. This is known in the literature as 
<b>the vanishing gradients problem</b>.

<p>
In other cases, the opposite can happen, namely the the gradients can grow bigger and
bigger. The result is that many of the layers get large updates of the 
weights the
algorithm diverges. This is the <b>exploding gradients problem</b>, which is
mostly encountered in recurrent neural networks. More generally, deep
neural networks suffer from unstable gradients, different layers may
learn at widely different speeds

<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week41-bs032.html">&laquo;</a></li>
  <li><a href="._week41-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week41-bs025.html">26</a></li>
  <li><a href="._week41-bs026.html">27</a></li>
  <li><a href="._week41-bs027.html">28</a></li>
  <li><a href="._week41-bs028.html">29</a></li>
  <li><a href="._week41-bs029.html">30</a></li>
  <li><a href="._week41-bs030.html">31</a></li>
  <li><a href="._week41-bs031.html">32</a></li>
  <li><a href="._week41-bs032.html">33</a></li>
  <li class="active"><a href="._week41-bs033.html">34</a></li>
  <li><a href="._week41-bs034.html">35</a></li>
  <li><a href="._week41-bs035.html">36</a></li>
  <li><a href="._week41-bs036.html">37</a></li>
  <li><a href="._week41-bs037.html">38</a></li>
  <li><a href="._week41-bs038.html">39</a></li>
  <li><a href="._week41-bs039.html">40</a></li>
  <li><a href="._week41-bs040.html">41</a></li>
  <li><a href="._week41-bs041.html">42</a></li>
  <li><a href="._week41-bs042.html">43</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week41-bs060.html">61</a></li>
  <li><a href="._week41-bs034.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    

