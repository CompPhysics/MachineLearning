<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 41 Tensor flow and Deep Learning, Convolutional Neural Networks">

<title>Week 41 Tensor flow and Deep Learning, Convolutional Neural Networks</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 40', 2, None, '___sec0'),
              ('Overview video for week 41', 2, None, '___sec1'),
              ('Setting up the Back propagation algorithm', 2, None, '___sec2'),
              ('Setting up a Multi-layer perceptron model for classification',
               2,
               None,
               '___sec3'),
              ('Defining the cost function', 2, None, '___sec4'),
              ('Example: binary classification problem', 2, None, '___sec5'),
              ('The Softmax function', 2, None, '___sec6'),
              ('Developing a code for doing neural networks with back '
               'propagation',
               2,
               None,
               '___sec7'),
              ('Collect and pre-process data', 2, None, '___sec8'),
              ('Train and test datasets', 2, None, '___sec9'),
              ('Define model and architecture', 2, None, '___sec10'),
              ('Layers', 2, None, '___sec11'),
              ('Weights and biases', 2, None, '___sec12'),
              ('Feed-forward pass', 2, None, '___sec13'),
              ('Matrix multiplications', 2, None, '___sec14'),
              ('Choose cost function and optimizer', 2, None, '___sec15'),
              ('Optimizing the cost function', 2, None, '___sec16'),
              ('Regularization', 2, None, '___sec17'),
              ('Matrix  multiplication', 2, None, '___sec18'),
              ('Improving performance', 2, None, '___sec19'),
              ('Full object-oriented implementation', 2, None, '___sec20'),
              ('Evaluate model performance on test data', 2, None, '___sec21'),
              ('Adjust hyperparameters', 2, None, '___sec22'),
              ('Visualization', 2, None, '___sec23'),
              ('scikit-learn implementation', 2, None, '___sec24'),
              ('Visualization', 2, None, '___sec25'),
              ('Building neural networks in Tensorflow and Keras',
               2,
               None,
               '___sec26'),
              ('Tensorflow', 2, None, '___sec27'),
              ('Collect and pre-process data', 2, None, '___sec28'),
              ('Using TensorFlow backend', 2, None, '___sec29'),
              ('Optimizing and using gradient descent', 2, None, '___sec30'),
              ('Using Keras', 2, None, '___sec31'),
              ('Which activation function should I use?', 2, None, '___sec32'),
              ('Is the Logistic activation function (Sigmoid)  our choice?',
               2,
               None,
               '___sec33'),
              ('The derivative of the Logistic funtion', 2, None, '___sec34'),
              ('The RELU function family', 2, None, '___sec35'),
              ('Which activation function should we use?', 2, None, '___sec36'),
              ('A top-down perspective on Neural networks',
               2,
               None,
               '___sec37'),
              ('Limitations of supervised learning with deep networks',
               2,
               None,
               '___sec38'),
              ('Convolutional Neural Networks (recognizing images)',
               2,
               None,
               '___sec39'),
              ('Regular NNs don’t scale well to full images',
               2,
               None,
               '___sec40'),
              ('3D volumes of neurons', 2, None, '___sec41'),
              ('Layers used to build CNNs', 2, None, '___sec42'),
              ('Transforming images', 2, None, '___sec43'),
              ('CNNs in brief', 2, None, '___sec44'),
              ('CNNs in more detail, building convolutional neural networks in '
               'Tensorflow and Keras',
               2,
               None,
               '___sec45'),
              ('Setting it up', 2, None, '___sec46'),
              ('The MNIST dataset again', 2, None, '___sec47'),
              ('Strong correlations', 2, None, '___sec48'),
              ('Layers of a CNN', 2, None, '___sec49'),
              ('Systematic reduction', 2, None, '___sec50'),
              ('Prerequisites: Collect and pre-process data',
               2,
               None,
               '___sec51'),
              ('Importing Keras and Tensorflow', 2, None, '___sec52'),
              ('Using TensorFlow backend', 2, None, '___sec53'),
              ('Train the model', 2, None, '___sec54'),
              ('Visualizing the results', 2, None, '___sec55'),
              ('Running with Keras', 2, None, '___sec56'),
              ('Final part', 2, None, '___sec57'),
              ('Final visualization', 2, None, '___sec58'),
              ('Fun links', 2, None, '___sec59')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week41-bs.html">Week 41 Tensor flow and Deep Learning, Convolutional Neural Networks</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week41-bs001.html#___sec0" style="font-size: 80%;">Plan for week 40</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs002.html#___sec1" style="font-size: 80%;">Overview video for week 41</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs003.html#___sec2" style="font-size: 80%;">Setting up the Back propagation algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs004.html#___sec3" style="font-size: 80%;">Setting up a Multi-layer perceptron model for classification</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs005.html#___sec4" style="font-size: 80%;">Defining the cost function</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs006.html#___sec5" style="font-size: 80%;">Example: binary classification problem</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs007.html#___sec6" style="font-size: 80%;">The Softmax function</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs008.html#___sec7" style="font-size: 80%;">Developing a code for doing neural networks with back propagation</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs009.html#___sec8" style="font-size: 80%;">Collect and pre-process data</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs010.html#___sec9" style="font-size: 80%;">Train and test datasets</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs011.html#___sec10" style="font-size: 80%;">Define model and architecture</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs012.html#___sec11" style="font-size: 80%;">Layers</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs013.html#___sec12" style="font-size: 80%;">Weights and biases</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs014.html#___sec13" style="font-size: 80%;">Feed-forward pass</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs015.html#___sec14" style="font-size: 80%;">Matrix multiplications</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs016.html#___sec15" style="font-size: 80%;">Choose cost function and optimizer</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs017.html#___sec16" style="font-size: 80%;">Optimizing the cost function</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs018.html#___sec17" style="font-size: 80%;">Regularization</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs019.html#___sec18" style="font-size: 80%;">Matrix  multiplication</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs020.html#___sec19" style="font-size: 80%;">Improving performance</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs021.html#___sec20" style="font-size: 80%;">Full object-oriented implementation</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs022.html#___sec21" style="font-size: 80%;">Evaluate model performance on test data</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs023.html#___sec22" style="font-size: 80%;">Adjust hyperparameters</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs024.html#___sec23" style="font-size: 80%;">Visualization</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs025.html#___sec24" style="font-size: 80%;">scikit-learn implementation</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs026.html#___sec25" style="font-size: 80%;">Visualization</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs027.html#___sec26" style="font-size: 80%;">Building neural networks in Tensorflow and Keras</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs028.html#___sec27" style="font-size: 80%;">Tensorflow</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs029.html#___sec28" style="font-size: 80%;">Collect and pre-process data</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs030.html#___sec29" style="font-size: 80%;">Using TensorFlow backend</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs031.html#___sec30" style="font-size: 80%;">Optimizing and using gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs032.html#___sec31" style="font-size: 80%;">Using Keras</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs033.html#___sec32" style="font-size: 80%;">Which activation function should I use?</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs034.html#___sec33" style="font-size: 80%;">Is the Logistic activation function (Sigmoid)  our choice?</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs035.html#___sec34" style="font-size: 80%;">The derivative of the Logistic funtion</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs036.html#___sec35" style="font-size: 80%;">The RELU function family</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs037.html#___sec36" style="font-size: 80%;">Which activation function should we use?</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs038.html#___sec37" style="font-size: 80%;">A top-down perspective on Neural networks</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs039.html#___sec38" style="font-size: 80%;">Limitations of supervised learning with deep networks</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs040.html#___sec39" style="font-size: 80%;">Convolutional Neural Networks (recognizing images)</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs041.html#___sec40" style="font-size: 80%;">Regular NNs don’t scale well to full images</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs042.html#___sec41" style="font-size: 80%;">3D volumes of neurons</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs043.html#___sec42" style="font-size: 80%;">Layers used to build CNNs</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs044.html#___sec43" style="font-size: 80%;">Transforming images</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs045.html#___sec44" style="font-size: 80%;">CNNs in brief</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs046.html#___sec45" style="font-size: 80%;">CNNs in more detail, building convolutional neural networks in Tensorflow and Keras</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs047.html#___sec46" style="font-size: 80%;">Setting it up</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs048.html#___sec47" style="font-size: 80%;">The MNIST dataset again</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs049.html#___sec48" style="font-size: 80%;">Strong correlations</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs050.html#___sec49" style="font-size: 80%;">Layers of a CNN</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs051.html#___sec50" style="font-size: 80%;">Systematic reduction</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs052.html#___sec51" style="font-size: 80%;">Prerequisites: Collect and pre-process data</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs053.html#___sec52" style="font-size: 80%;">Importing Keras and Tensorflow</a></li>
     <!-- navigation toc: --> <li><a href="#___sec53" style="font-size: 80%;">Using TensorFlow backend</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs055.html#___sec54" style="font-size: 80%;">Train the model</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs056.html#___sec55" style="font-size: 80%;">Visualizing the results</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs057.html#___sec56" style="font-size: 80%;">Running with Keras</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs058.html#___sec57" style="font-size: 80%;">Final part</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs059.html#___sec58" style="font-size: 80%;">Final visualization</a></li>
     <!-- navigation toc: --> <li><a href="._week41-bs060.html#___sec59" style="font-size: 80%;">Fun links</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0054"></a>
<!-- !split -->

<h2 id="___sec53" class="anchor">Using TensorFlow backend </h2>

<p>
We need to define model and architecture and choose cost function and optmizer.
<p>

<!-- code=text (!bc pycid) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span>import tensorflow as tf

class ConvolutionalNeuralNetworkTensorflow:
    def __init__(
            self,
            X_train,
            Y_train,
            X_test,
            Y_test,
            n_filters=10,
            n_neurons_connected=50,
            n_categories=10,
            receptive_field=3,
            stride=1,
            padding=1,
            epochs=10,
            batch_size=100,
            eta=0.1,
            lmbd=0.0):
        
        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name=&#39;global_step&#39;)
        
        self.X_train = X_train
        self.Y_train = Y_train
        self.X_test = X_test
        self.Y_test = Y_test
        
        self.n_inputs, self.input_width, self.input_height, self.depth = X_train.shape
        
        self.n_filters = n_filters
        self.n_downsampled = int(self.input_width*self.input_height*n_filters / 4)
        self.n_neurons_connected = n_neurons_connected
        self.n_categories = n_categories
        
        self.receptive_field = receptive_field
        self.stride = stride
        self.strides = [stride, stride, stride, stride]
        self.padding = padding
        
        self.epochs = epochs
        self.batch_size = batch_size
        self.iterations = self.n_inputs // self.batch_size
        self.eta = eta
        self.lmbd = lmbd
        
        self.create_placeholders()
        self.create_CNN()
        self.create_loss()
        self.create_optimiser()
        self.create_accuracy()
    
    def create_placeholders(self):
        with tf.name_scope(&#39;data&#39;):
            self.X = tf.placeholder(tf.float32, shape=(None, self.input_width, self.input_height, self.depth), name=&#39;X_data&#39;)
            self.Y = tf.placeholder(tf.float32, shape=(None, self.n_categories), name=&#39;Y_data&#39;)
    
    def create_CNN(self):
        with tf.name_scope(&#39;CNN&#39;):
            
            # Convolutional layer
            self.W_conv = self.weight_variable([self.receptive_field, self.receptive_field, self.depth, self.n_filters], name=&#39;conv&#39;, dtype=tf.float32)
            b_conv = self.weight_variable([self.n_filters], name=&#39;conv&#39;, dtype=tf.float32)
            z_conv = tf.nn.conv2d(self.X, self.W_conv, self.strides, padding=&#39;SAME&#39;, name=&#39;conv&#39;) + b_conv
            a_conv = tf.nn.relu(z_conv)
            
            # 2x2 max pooling
            a_pool = tf.nn.max_pool(a_conv, [1, 2, 2, 1], [1, 2, 2, 1], padding=&#39;SAME&#39;, name=&#39;pool&#39;)
            
            # Fully connected layer
            a_pool_flat = tf.reshape(a_pool, [-1, self.n_downsampled])
            self.W_fc = self.weight_variable([self.n_downsampled, self.n_neurons_connected], name=&#39;fc&#39;, dtype=tf.float32)
            b_fc = self.bias_variable([self.n_neurons_connected], name=&#39;fc&#39;, dtype=tf.float32)
            a_fc = tf.nn.relu(tf.matmul(a_pool_flat, self.W_fc) + b_fc)
            
            # Output layer
            self.W_out = self.weight_variable([self.n_neurons_connected, self.n_categories], name=&#39;out&#39;, dtype=tf.float32)
            b_out = self.bias_variable([self.n_categories], name=&#39;out&#39;, dtype=tf.float32)
            self.z_out = tf.matmul(a_fc, self.W_out) + b_out
    
    def create_loss(self):
        with tf.name_scope(&#39;loss&#39;):
            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.Y, logits=self.z_out))
            
            regularizer_loss_conv = tf.nn.l2_loss(self.W_conv)
            regularizer_loss_fc = tf.nn.l2_loss(self.W_fc)
            regularizer_loss_out = tf.nn.l2_loss(self.W_out)
            regularizer_loss = self.lmbd*(regularizer_loss_conv + regularizer_loss_fc + regularizer_loss_out)
            
            self.loss = softmax_loss + regularizer_loss

    def create_accuracy(self):
        with tf.name_scope(&#39;accuracy&#39;):
            probabilities = tf.nn.softmax(self.z_out)
            predictions = tf.argmax(probabilities, 1)
            labels = tf.argmax(self.Y, 1)
            
            correct_predictions = tf.equal(predictions, labels)
            correct_predictions = tf.cast(correct_predictions, tf.float32)
            self.accuracy = tf.reduce_mean(correct_predictions)
    
    def create_optimiser(self):
        with tf.name_scope(&#39;optimizer&#39;):
            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.eta).minimize(self.loss, global_step=self.global_step)
            
    def weight_variable(self, shape, name=&#39;&#39;, dtype=tf.float32):
        initial = tf.truncated_normal(shape, stddev=0.1)
        return tf.Variable(initial, name=name, dtype=dtype)
    
    def bias_variable(self, shape, name=&#39;&#39;, dtype=tf.float32):
        initial = tf.constant(0.1, shape=shape)
        return tf.Variable(initial, name=name, dtype=dtype)

    def fit(self):
        data_indices = np.arange(self.n_inputs)

        with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
            for i in range(self.epochs):
                for j in range(self.iterations):
                    chosen_datapoints = np.random.choice(data_indices, size=self.batch_size, replace=False)
                    batch_X, batch_Y = self.X_train[chosen_datapoints], self.Y_train[chosen_datapoints]
            
                    sess.run([CNN.loss, CNN.optimizer],
                        feed_dict={CNN.X: batch_X,
                                   CNN.Y: batch_Y})
                    accuracy = sess.run(CNN.accuracy,
                        feed_dict={CNN.X: batch_X,
                                   CNN.Y: batch_Y})
                    step = sess.run(CNN.global_step)
    
            self.train_loss, self.train_accuracy = sess.run([CNN.loss, CNN.accuracy],
                feed_dict={CNN.X: self.X_train,
                           CNN.Y: self.Y_train})
        
            self.test_loss, self.test_accuracy = sess.run([CNN.loss, CNN.accuracy],
                feed_dict={CNN.X: self.X_test,
                           CNN.Y: self.Y_test})
</pre></div>
<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week41-bs053.html">&laquo;</a></li>
  <li><a href="._week41-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week41-bs046.html">47</a></li>
  <li><a href="._week41-bs047.html">48</a></li>
  <li><a href="._week41-bs048.html">49</a></li>
  <li><a href="._week41-bs049.html">50</a></li>
  <li><a href="._week41-bs050.html">51</a></li>
  <li><a href="._week41-bs051.html">52</a></li>
  <li><a href="._week41-bs052.html">53</a></li>
  <li><a href="._week41-bs053.html">54</a></li>
  <li class="active"><a href="._week41-bs054.html">55</a></li>
  <li><a href="._week41-bs055.html">56</a></li>
  <li><a href="._week41-bs056.html">57</a></li>
  <li><a href="._week41-bs057.html">58</a></li>
  <li><a href="._week41-bs058.html">59</a></li>
  <li><a href="._week41-bs059.html">60</a></li>
  <li><a href="._week41-bs060.html">61</a></li>
  <li><a href="._week41-bs055.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    

