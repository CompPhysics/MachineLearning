<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week47.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week47-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 47: Recurrent neural networks and Autoencoders">
<title>Week 47: Recurrent neural networks and Autoencoders</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week47.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week47-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 47', 2, None, 'plan-for-week-47'),
              ('Reading recommendations RNNs',
               2,
               None,
               'reading-recommendations-rnns'),
              ('TensorFlow examples', 2, None, 'tensorflow-examples'),
              ('Reading recommendations: Autoencoders (AE)',
               2,
               None,
               'reading-recommendations-autoencoders-ae'),
              ('What is a recurrent NN?', 2, None, 'what-is-a-recurrent-nn'),
              ('Why RNNs?', 2, None, 'why-rnns'),
              ('More whys', 2, None, 'more-whys'),
              ('RNNs in more detail', 2, None, 'rnns-in-more-detail'),
              ('RNNs in more detail, part 2',
               2,
               None,
               'rnns-in-more-detail-part-2'),
              ('RNNs in more detail, part 3',
               2,
               None,
               'rnns-in-more-detail-part-3'),
              ('RNNs in more detail, part 4',
               2,
               None,
               'rnns-in-more-detail-part-4'),
              ('RNNs in more detail, part 5',
               2,
               None,
               'rnns-in-more-detail-part-5'),
              ('RNNs in more detail, part 6',
               2,
               None,
               'rnns-in-more-detail-part-6'),
              ('RNNs in more detail, part 7',
               2,
               None,
               'rnns-in-more-detail-part-7'),
              ('RNN Forward Pass Equations',
               2,
               None,
               'rnn-forward-pass-equations'),
              ('Unrolled RNN in Time', 2, None, 'unrolled-rnn-in-time'),
              ('Example Task: Character-level RNN Classification',
               2,
               None,
               'example-task-character-level-rnn-classification'),
              ('PyTorch: Defining a Simple RNN, using Tensorflow',
               2,
               None,
               'pytorch-defining-a-simple-rnn-using-tensorflow'),
              ('Similar example using PyTorch',
               2,
               None,
               'similar-example-using-pytorch'),
              ('Backpropagation Through Time (BPTT) and Gradients',
               2,
               None,
               'backpropagation-through-time-bptt-and-gradients'),
              ('Truncated BPTT and Gradient Clipping',
               2,
               None,
               'truncated-bptt-and-gradient-clipping'),
              ('Applications of Simple RNNs',
               2,
               None,
               'applications-of-simple-rnns'),
              ('Sequence Modeling Tasks', 2, None, 'sequence-modeling-tasks'),
              ('Other Sequence Applications',
               2,
               None,
               'other-sequence-applications'),
              ('Training and Practical Tips',
               2,
               None,
               'training-and-practical-tips'),
              ('Limitations and Considerations',
               2,
               None,
               'limitations-and-considerations'),
              ('PyTorch RNN Time Series Example',
               2,
               None,
               'pytorch-rnn-time-series-example'),
              ('Tensorflow (Keras) RNN Time Series Example',
               2,
               None,
               'tensorflow-keras-rnn-time-series-example'),
              ('The mathematics of RNNs, the basic architecture',
               2,
               None,
               'the-mathematics-of-rnns-the-basic-architecture'),
              ('Gating mechanism: Long Short Term Memory (LSTM)',
               2,
               None,
               'gating-mechanism-long-short-term-memory-lstm'),
              ('Implementing a memory cell in a neural network',
               2,
               None,
               'implementing-a-memory-cell-in-a-neural-network'),
              ('LSTM details', 2, None, 'lstm-details'),
              ('Basic layout (All figures from Raschka *et al.,*)',
               2,
               None,
               'basic-layout-all-figures-from-raschka-et-al'),
              ('LSTM details', 2, None, 'lstm-details'),
              ('Comparing with a standard  RNN',
               2,
               None,
               'comparing-with-a-standard-rnn'),
              ('LSTM details I', 2, None, 'lstm-details-i'),
              ('LSTM details II', 2, None, 'lstm-details-ii'),
              ('LSTM details III', 2, None, 'lstm-details-iii'),
              ('Forget gate', 2, None, 'forget-gate'),
              ('The forget gate', 2, None, 'the-forget-gate'),
              ('Basic layout', 2, None, 'basic-layout'),
              ('Input gate', 2, None, 'input-gate'),
              ('Short summary', 2, None, 'short-summary'),
              ('Forget and input', 2, None, 'forget-and-input'),
              ('Basic layout', 2, None, 'basic-layout'),
              ('Output gate', 2, None, 'output-gate'),
              ('Summary of LSTM', 2, None, 'summary-of-lstm'),
              ('LSTM implementation using TensorFlow',
               2,
               None,
               'lstm-implementation-using-tensorflow'),
              ('And the corresponding one with PyTorch',
               2,
               None,
               'and-the-corresponding-one-with-pytorch'),
              ('Dynamical ordinary differential equation',
               2,
               None,
               'dynamical-ordinary-differential-equation'),
              ('The Runge-Kutta-4 code', 2, None, 'the-runge-kutta-4-code'),
              ('Using the above data to train an RNN',
               2,
               None,
               'using-the-above-data-to-train-an-rnn'),
              ('Similar code using PyTorch',
               2,
               None,
               'similar-code-using-pytorch'),
              ('Autoencoders: Overarching view',
               2,
               None,
               'autoencoders-overarching-view'),
              ('Powerful detectors', 2, None, 'powerful-detectors'),
              ('First introduction of AEs',
               2,
               None,
               'first-introduction-of-aes'),
              ('Autoencoder structure', 2, None, 'autoencoder-structure'),
              ('Schematic image of an Autoencoder',
               2,
               None,
               'schematic-image-of-an-autoencoder'),
              ('More on the structure', 2, None, 'more-on-the-structure'),
              ('Decoder part', 2, None, 'decoder-part'),
              ('Typical AEs', 2, None, 'typical-aes'),
              ('Feed Forward Autoencoder', 2, None, 'feed-forward-autoencoder'),
              ('Mirroring', 2, None, 'mirroring'),
              ('Output of middle layer', 2, None, 'output-of-middle-layer'),
              ('Activation Function of the Output Layer',
               2,
               None,
               'activation-function-of-the-output-layer'),
              ('ReLU', 2, None, 'relu'),
              ('Sigmoid', 2, None, 'sigmoid'),
              ('Cost/Loss Function', 2, None, 'cost-loss-function'),
              ('Binary Cross-Entropy', 2, None, 'binary-cross-entropy'),
              ('Reconstruction Error', 2, None, 'reconstruction-error'),
              ('Implementation using TensorFlow',
               2,
               None,
               'implementation-using-tensorflow'),
              ('Implementation using PyTorch',
               2,
               None,
               'implementation-using-pytorch'),
              ('Dimensionality reduction and links with Principal component '
               'analysis',
               2,
               None,
               'dimensionality-reduction-and-links-with-principal-component-analysis'),
              ('Linear functions', 2, None, 'linear-functions'),
              ('AE mean-squared error', 2, None, 'ae-mean-squared-error'),
              ('Dimensionality reduction',
               2,
               None,
               'dimensionality-reduction')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week47-bs.html">Week 47: Recurrent neural networks and Autoencoders</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week47-bs001.html#plan-for-week-47" style="font-size: 80%;">Plan for week 47</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs002.html#reading-recommendations-rnns" style="font-size: 80%;">Reading recommendations RNNs</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs003.html#tensorflow-examples" style="font-size: 80%;">TensorFlow examples</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs004.html#reading-recommendations-autoencoders-ae" style="font-size: 80%;">Reading recommendations: Autoencoders (AE)</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs005.html#what-is-a-recurrent-nn" style="font-size: 80%;">What is a recurrent NN?</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs006.html#why-rnns" style="font-size: 80%;">Why RNNs?</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs007.html#more-whys" style="font-size: 80%;">More whys</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs008.html#rnns-in-more-detail" style="font-size: 80%;">RNNs in more detail</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs009.html#rnns-in-more-detail-part-2" style="font-size: 80%;">RNNs in more detail, part 2</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs010.html#rnns-in-more-detail-part-3" style="font-size: 80%;">RNNs in more detail, part 3</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs011.html#rnns-in-more-detail-part-4" style="font-size: 80%;">RNNs in more detail, part 4</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs012.html#rnns-in-more-detail-part-5" style="font-size: 80%;">RNNs in more detail, part 5</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs013.html#rnns-in-more-detail-part-6" style="font-size: 80%;">RNNs in more detail, part 6</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs014.html#rnns-in-more-detail-part-7" style="font-size: 80%;">RNNs in more detail, part 7</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs015.html#rnn-forward-pass-equations" style="font-size: 80%;">RNN Forward Pass Equations</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs016.html#unrolled-rnn-in-time" style="font-size: 80%;">Unrolled RNN in Time</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs017.html#example-task-character-level-rnn-classification" style="font-size: 80%;">Example Task: Character-level RNN Classification</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs018.html#pytorch-defining-a-simple-rnn-using-tensorflow" style="font-size: 80%;">PyTorch: Defining a Simple RNN, using Tensorflow</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs019.html#similar-example-using-pytorch" style="font-size: 80%;">Similar example using PyTorch</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs020.html#backpropagation-through-time-bptt-and-gradients" style="font-size: 80%;">Backpropagation Through Time (BPTT) and Gradients</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs021.html#truncated-bptt-and-gradient-clipping" style="font-size: 80%;">Truncated BPTT and Gradient Clipping</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs022.html#applications-of-simple-rnns" style="font-size: 80%;">Applications of Simple RNNs</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs023.html#sequence-modeling-tasks" style="font-size: 80%;">Sequence Modeling Tasks</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs024.html#other-sequence-applications" style="font-size: 80%;">Other Sequence Applications</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs025.html#training-and-practical-tips" style="font-size: 80%;">Training and Practical Tips</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs026.html#limitations-and-considerations" style="font-size: 80%;">Limitations and Considerations</a></li>
     <!-- navigation toc: --> <li><a href="#pytorch-rnn-time-series-example" style="font-size: 80%;">PyTorch RNN Time Series Example</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs028.html#tensorflow-keras-rnn-time-series-example" style="font-size: 80%;">Tensorflow (Keras) RNN Time Series Example</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs029.html#the-mathematics-of-rnns-the-basic-architecture" style="font-size: 80%;">The mathematics of RNNs, the basic architecture</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs030.html#gating-mechanism-long-short-term-memory-lstm" style="font-size: 80%;">Gating mechanism: Long Short Term Memory (LSTM)</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs031.html#implementing-a-memory-cell-in-a-neural-network" style="font-size: 80%;">Implementing a memory cell in a neural network</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs034.html#lstm-details" style="font-size: 80%;">LSTM details</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs033.html#basic-layout-all-figures-from-raschka-et-al" style="font-size: 80%;">Basic layout (All figures from Raschka <em>et al.,</em>)</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs034.html#lstm-details" style="font-size: 80%;">LSTM details</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs035.html#comparing-with-a-standard-rnn" style="font-size: 80%;">Comparing with a standard  RNN</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs036.html#lstm-details-i" style="font-size: 80%;">LSTM details I</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs037.html#lstm-details-ii" style="font-size: 80%;">LSTM details II</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs038.html#lstm-details-iii" style="font-size: 80%;">LSTM details III</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs039.html#forget-gate" style="font-size: 80%;">Forget gate</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs040.html#the-forget-gate" style="font-size: 80%;">The forget gate</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs045.html#basic-layout" style="font-size: 80%;">Basic layout</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs042.html#input-gate" style="font-size: 80%;">Input gate</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs043.html#short-summary" style="font-size: 80%;">Short summary</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs044.html#forget-and-input" style="font-size: 80%;">Forget and input</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs045.html#basic-layout" style="font-size: 80%;">Basic layout</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs046.html#output-gate" style="font-size: 80%;">Output gate</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs047.html#summary-of-lstm" style="font-size: 80%;">Summary of LSTM</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs048.html#lstm-implementation-using-tensorflow" style="font-size: 80%;">LSTM implementation using TensorFlow</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs049.html#and-the-corresponding-one-with-pytorch" style="font-size: 80%;">And the corresponding one with PyTorch</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs050.html#dynamical-ordinary-differential-equation" style="font-size: 80%;">Dynamical ordinary differential equation</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs051.html#the-runge-kutta-4-code" style="font-size: 80%;">The Runge-Kutta-4 code</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs052.html#using-the-above-data-to-train-an-rnn" style="font-size: 80%;">Using the above data to train an RNN</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs053.html#similar-code-using-pytorch" style="font-size: 80%;">Similar code using PyTorch</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs054.html#autoencoders-overarching-view" style="font-size: 80%;">Autoencoders: Overarching view</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs055.html#powerful-detectors" style="font-size: 80%;">Powerful detectors</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs056.html#first-introduction-of-aes" style="font-size: 80%;">First introduction of AEs</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs057.html#autoencoder-structure" style="font-size: 80%;">Autoencoder structure</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs058.html#schematic-image-of-an-autoencoder" style="font-size: 80%;">Schematic image of an Autoencoder</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs059.html#more-on-the-structure" style="font-size: 80%;">More on the structure</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs060.html#decoder-part" style="font-size: 80%;">Decoder part</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs061.html#typical-aes" style="font-size: 80%;">Typical AEs</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs062.html#feed-forward-autoencoder" style="font-size: 80%;">Feed Forward Autoencoder</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs063.html#mirroring" style="font-size: 80%;">Mirroring</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs064.html#output-of-middle-layer" style="font-size: 80%;">Output of middle layer</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs065.html#activation-function-of-the-output-layer" style="font-size: 80%;">Activation Function of the Output Layer</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs066.html#relu" style="font-size: 80%;">ReLU</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs067.html#sigmoid" style="font-size: 80%;">Sigmoid</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs068.html#cost-loss-function" style="font-size: 80%;">Cost/Loss Function</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs069.html#binary-cross-entropy" style="font-size: 80%;">Binary Cross-Entropy</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs070.html#reconstruction-error" style="font-size: 80%;">Reconstruction Error</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs071.html#implementation-using-tensorflow" style="font-size: 80%;">Implementation using TensorFlow</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs072.html#implementation-using-pytorch" style="font-size: 80%;">Implementation using PyTorch</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs073.html#dimensionality-reduction-and-links-with-principal-component-analysis" style="font-size: 80%;">Dimensionality reduction and links with Principal component analysis</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs074.html#linear-functions" style="font-size: 80%;">Linear functions</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs075.html#ae-mean-squared-error" style="font-size: 80%;">AE mean-squared error</a></li>
     <!-- navigation toc: --> <li><a href="._week47-bs076.html#dimensionality-reduction" style="font-size: 80%;">Dimensionality reduction</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0027"></a>
<!-- !split -->
<h2 id="pytorch-rnn-time-series-example" class="anchor">PyTorch RNN Time Series Example </h2>

<p>We first implement a simple RNN in PyTorch to forecast a univariate
time series (a sine wave). The steps are: (1) generate synthetic data
and form input/output sequences; (2) define an nn.RNN model; (3) train
the model with MSE loss and an optimizer; (4) evaluate on a held-out
test set. For example, using a sine wave as in prior tutorials &#65532;, we
create sliding windows of length seq_length. The code below shows each
step. We use nn.RNN (the basic recurrent layer) followed by a linear
output. The training loop (with MSELoss and Adam) updates the model to
minimize prediction error &#65532;.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch</span> <span style="color: #008000; font-weight: bold">import</span> nn, optim

<span style="color: #408080; font-style: italic"># 1. Data preparation: generate a sine wave and create input-output sequences</span>
time_steps <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>, <span style="color: #666666">100</span>, <span style="color: #666666">500</span>)
data <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sin(time_steps)                   <span style="color: #408080; font-style: italic"># shape (500,)</span>
seq_length <span style="color: #666666">=</span> <span style="color: #666666">20</span>
X, y <span style="color: #666666">=</span> [], []
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(data) <span style="color: #666666">-</span> seq_length):
    X<span style="color: #666666">.</span>append(data[i:i<span style="color: #666666">+</span>seq_length])         <span style="color: #408080; font-style: italic"># sequence of length seq_length</span>
    y<span style="color: #666666">.</span>append(data[i<span style="color: #666666">+</span>seq_length])           <span style="color: #408080; font-style: italic"># next value to predict</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(X)                            <span style="color: #408080; font-style: italic"># shape (480, seq_length)</span>
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(y)                            <span style="color: #408080; font-style: italic"># shape (480,)</span>
<span style="color: #408080; font-style: italic"># Add feature dimension (1) for the RNN input</span>
X <span style="color: #666666">=</span> X[<span style="color: #666666">...</span>, <span style="color: #008000; font-weight: bold">None</span>]                           <span style="color: #408080; font-style: italic"># shape (480, seq_length, 1)</span>
y <span style="color: #666666">=</span> y[<span style="color: #666666">...</span>, <span style="color: #008000; font-weight: bold">None</span>]                           <span style="color: #408080; font-style: italic"># shape (480, 1)</span>

<span style="color: #408080; font-style: italic"># Split into train/test sets (80/20 split)</span>
train_size <span style="color: #666666">=</span> <span style="color: #008000">int</span>(<span style="color: #666666">0.8</span> <span style="color: #666666">*</span> <span style="color: #008000">len</span>(X))
X_train <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(X[:train_size], dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)
y_train <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(y[:train_size], dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)
X_test  <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(X[train_size:],  dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)
y_test  <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(y[train_size:],  dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)

<span style="color: #408080; font-style: italic"># 2. Model definition: simple RNN followed by a linear layer</span>
<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">SimpleRNNModel</span>(nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, input_size<span style="color: #666666">=1</span>, hidden_size<span style="color: #666666">=16</span>, num_layers<span style="color: #666666">=1</span>):
        <span style="color: #008000">super</span>(SimpleRNNModel, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #408080; font-style: italic"># nn.RNN for sequential data (batch_first=True expects (batch, seq_len, features))</span>
        <span style="color: #008000">self</span><span style="color: #666666">.</span>rnn <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>RNN(input_size, hidden_size, num_layers, batch_first<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>fc <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>Linear(hidden_size, <span style="color: #666666">1</span>)    <span style="color: #408080; font-style: italic"># output layer for prediction</span>

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        out, _ <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>rnn(x)                 <span style="color: #408080; font-style: italic"># out: (batch, seq_len, hidden_size)</span>
        out <span style="color: #666666">=</span> out[:, <span style="color: #666666">-1</span>, :]                  <span style="color: #408080; font-style: italic"># take output of last time step</span>
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>fc(out)                 <span style="color: #408080; font-style: italic"># linear layer to 1D output</span>

model <span style="color: #666666">=</span> SimpleRNNModel(input_size<span style="color: #666666">=1</span>, hidden_size<span style="color: #666666">=16</span>, num_layers<span style="color: #666666">=1</span>)
<span style="color: #008000">print</span>(model)  <span style="color: #408080; font-style: italic"># print model summary (structure)</span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Model Explanation: Here input$\_$size=1 because each time step has one
feature. The RNN hidden state has size 16, and batch$\_$first=True means
input tensors have shape (batch, seq$\_$len, features). We take the last
RNN output and feed it through a linear layer to predict the next
value .
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># 3. Training loop: MSE loss and Adam optimizer</span>
criterion <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>MSELoss()                  <span style="color: #408080; font-style: italic"># mean squared error loss</span>
optimizer <span style="color: #666666">=</span> optim<span style="color: #666666">.</span>Adam(model<span style="color: #666666">.</span>parameters(), lr<span style="color: #666666">=0.01</span>)

epochs <span style="color: #666666">=</span> <span style="color: #666666">50</span>
<span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>, epochs<span style="color: #666666">+1</span>):
    model<span style="color: #666666">.</span>train()
    optimizer<span style="color: #666666">.</span>zero_grad()
    output <span style="color: #666666">=</span> model(X_train)               <span style="color: #408080; font-style: italic"># forward pass</span>
    loss <span style="color: #666666">=</span> criterion(output, y_train)     <span style="color: #408080; font-style: italic"># compute training loss</span>
    loss<span style="color: #666666">.</span>backward()                       <span style="color: #408080; font-style: italic"># backpropagate</span>
    optimizer<span style="color: #666666">.</span>step()                      <span style="color: #408080; font-style: italic"># update weights</span>
    <span style="color: #008000; font-weight: bold">if</span> epoch <span style="color: #666666">%</span> <span style="color: #666666">10</span> <span style="color: #666666">==</span> <span style="color: #666666">0</span>:
        <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Epoch </span><span style="color: #BB6688; font-weight: bold">{</span>epoch<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">/</span><span style="color: #BB6688; font-weight: bold">{</span>epochs<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">, Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss<span style="color: #666666">.</span>item()<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Training Details: We train for 50 epochs, printing the training loss
every 10 epochs. As training proceeds, the loss (MSE) typically
decreases, indicating the RNN is learning the sine-wave pattern &#65532;.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># 4. Evaluation on test set</span>
model<span style="color: #666666">.</span>eval()
<span style="color: #008000; font-weight: bold">with</span> torch<span style="color: #666666">.</span>no_grad():
    pred <span style="color: #666666">=</span> model(X_test)
    test_loss <span style="color: #666666">=</span> criterion(pred, y_test)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Test Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>test_loss<span style="color: #666666">.</span>item()<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)

<span style="color: #408080; font-style: italic"># (Optional) View a few actual vs. predicted values</span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Actual:&quot;</span>, y_test[:<span style="color: #666666">5</span>]<span style="color: #666666">.</span>flatten()<span style="color: #666666">.</span>numpy())
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Pred : &quot;</span>, pred[:<span style="color: #666666">5</span>]<span style="color: #666666">.</span>flatten()<span style="color: #666666">.</span>numpy())
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Evaluation: We switch to eval mode and compute loss on the test
set. The lower test loss indicates how well the model generalizes. The
code prints a few sample predictions against actual values for
qualitative assessment. 
</p>

<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week47-bs026.html">&laquo;</a></li>
  <li><a href="._week47-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week47-bs019.html">20</a></li>
  <li><a href="._week47-bs020.html">21</a></li>
  <li><a href="._week47-bs021.html">22</a></li>
  <li><a href="._week47-bs022.html">23</a></li>
  <li><a href="._week47-bs023.html">24</a></li>
  <li><a href="._week47-bs024.html">25</a></li>
  <li><a href="._week47-bs025.html">26</a></li>
  <li><a href="._week47-bs026.html">27</a></li>
  <li class="active"><a href="._week47-bs027.html">28</a></li>
  <li><a href="._week47-bs028.html">29</a></li>
  <li><a href="._week47-bs029.html">30</a></li>
  <li><a href="._week47-bs030.html">31</a></li>
  <li><a href="._week47-bs031.html">32</a></li>
  <li><a href="._week47-bs032.html">33</a></li>
  <li><a href="._week47-bs033.html">34</a></li>
  <li><a href="._week47-bs034.html">35</a></li>
  <li><a href="._week47-bs035.html">36</a></li>
  <li><a href="._week47-bs036.html">37</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week47-bs076.html">77</a></li>
  <li><a href="._week47-bs028.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>

