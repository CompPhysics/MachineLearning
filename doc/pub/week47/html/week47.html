<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week47.do.txt --pygments_html_style=default --html_style=bloodish --html_links_in_new_window --html_output=week47 --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 47: Recurrent neural networks and Autoencoders">
<title>Week 47: Recurrent neural networks and Autoencoders</title>
<style type="text/css">
/* bloodish style */
body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em; color: #8A0808; }
h2 { font-size: 1.6em; color: #8A0808; }
h3 { font-size: 1.4em; color: #8A0808; }
h4 { font-size: 1.2em; color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa; }div.highlight {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    line-height: 1.21429em;
}
div.cell {
    width: 100%;
    padding: 5px 5px 5px 0;
    margin: 0;
    outline: none;
}
div.input {
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.inner_cell {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
    box-flex: 1;
    flex: 1;
}
div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 4px;
    background: #f7f7f7;
    line-height: 1.21429em;
}
div.input_area > div.highlight {
    margin: .4em;
    border: none;
    padding: 0;
    background-color: transparent;
}
div.output_wrapper {
    position: relative;
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
.output {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
div.output_area {
    padding: 0;
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.output_subarea {
    padding: .4em .4em 0 .4em;
    box-flex: 1;
    flex: 1;
}
div.output_text {
    text-align: left;
    color: #000;
    line-height: 1.21429em;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #bababa;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #f8f8f8;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_question.png); }
div { text-align: justify; text-justify: inter-word; }
.tab {
  padding-left: 1.5em;
}
div.toc p,a {
  line-height: 1.3;
  margin-top: 1.1;
  margin-bottom: 1.1;
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 47', 2, None, 'plan-for-week-47'),
              ('Reading recommendations', 2, None, 'reading-recommendations'),
              ('TensorFlow examples', 2, None, 'tensorflow-examples'),
              ('What is a recurrent NN?', 2, None, 'what-is-a-recurrent-nn'),
              ('Why RNNs?', 2, None, 'why-rnns'),
              ('RNNs in more detail', 2, None, 'rnns-in-more-detail'),
              ('RNNs in more detail, part 2',
               2,
               None,
               'rnns-in-more-detail-part-2'),
              ('RNNs in more detail, part 3',
               2,
               None,
               'rnns-in-more-detail-part-3'),
              ('RNNs in more detail, part 4',
               2,
               None,
               'rnns-in-more-detail-part-4'),
              ('RNNs in more detail, part 5',
               2,
               None,
               'rnns-in-more-detail-part-5'),
              ('RNNs in more detail, part 6',
               2,
               None,
               'rnns-in-more-detail-part-6'),
              ('RNNs in more detail, part 7',
               2,
               None,
               'rnns-in-more-detail-part-7'),
              ('The mathematics of RNNs, the basic architecture',
               2,
               None,
               'the-mathematics-of-rnns-the-basic-architecture'),
              ('Gating mechanism: Long Short Term Memory (LSTM)',
               2,
               None,
               'gating-mechanism-long-short-term-memory-lstm'),
              ('Implementing a memory cell in a neural network',
               2,
               None,
               'implementing-a-memory-cell-in-a-neural-network'),
              ('LSTM details', 2, None, 'lstm-details'),
              ('Basic layout (All figures from Raschka *et al.,*)',
               2,
               None,
               'basic-layout-all-figures-from-raschka-et-al'),
              ('LSTM details', 2, None, 'lstm-details'),
              ('Comparing with a standard  RNN',
               2,
               None,
               'comparing-with-a-standard-rnn'),
              ('LSTM details I', 2, None, 'lstm-details-i'),
              ('LSTM details II', 2, None, 'lstm-details-ii'),
              ('LSTM details III', 2, None, 'lstm-details-iii'),
              ('Forget gate', 2, None, 'forget-gate'),
              ('The forget gate', 2, None, 'the-forget-gate'),
              ('Basic layout', 2, None, 'basic-layout'),
              ('Input gate', 2, None, 'input-gate'),
              ('Short summary', 2, None, 'short-summary'),
              ('Forget and input', 2, None, 'forget-and-input'),
              ('Basic layout', 2, None, 'basic-layout'),
              ('Output gate', 2, None, 'output-gate'),
              ('Summary of LSTM', 2, None, 'summary-of-lstm'),
              ('LSTM implementation using TensorFlow',
               2,
               None,
               'lstm-implementation-using-tensorflow'),
              ('And the corresponding one with PyTorch',
               2,
               None,
               'and-the-corresponding-one-with-pytorch'),
              ('Dynamical ordinary differential equation',
               2,
               None,
               'dynamical-ordinary-differential-equation'),
              ('The Runge-Kutta-4 code', 2, None, 'the-runge-kutta-4-code'),
              ('Using the above data to train an RNN',
               2,
               None,
               'using-the-above-data-to-train-an-rnn')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- ------------------- main content ---------------------- -->
<center>
<h1>Week 47: Recurrent neural networks and Autoencoders</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> 
</center>
<!-- institution -->
<center>
<b>Department of Physics, University of Oslo, Norway</b>
</center>
<br>
<center>
<h4>November 17-21, 2025</h4>
</center> <!-- date -->
<br>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="plan-for-week-47">Plan for week 47 </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Plans for the lecture Monday 18 November, with video suggestions etc</b>
<p>
<ol>
<li> Recurrent neural networks, code examples and long-short-term memory</li>
<li> Readings and Videos:</li>
<li> These lecture notes at <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week47/ipynb/week47.ipynb" target="_blank"><tt>https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week47/ipynb/week47.ipynb</tt></a></li>
<li> See also lecture notes from week 46 at <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week46/ipynb/week46.ipynb" target="_blank"><tt>https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week46/ipynb/week46.ipynb</tt></a>. The lecture on Monday starts with a repetition on recurrent neural networks. The second lecture starts with basics of autoenconders.
<!-- o Video of lecture at <a href="https://youtu.be/RIHzmLv05DA" target="_blank"><tt>https://youtu.be/RIHzmLv05DA</tt></a> -->
<!-- o Whiteboard notes at <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2024/NotesNovember18.pdf" target="_blank"><tt>https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2024/NotesNovember18.pdf</tt></a> -->
<!-- * <a href="https://youtu.be/SpWXsvn5I9E" target="_blank">Video of Lecture</a> --></li>
</ol>
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b>Lab sessions on Tuesday and Wednesday</b>
<p>
<ol>
<li> Work and Discussion of project 3</li>
<li> Last weekly exercise</li>
</ol>
</div>
  

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="reading-recommendations">Reading recommendations </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> For RNNs, see Goodfellow et al chapter 10, see <a href="https://www.deeplearningbook.org/contents/rnn.html" target="_blank"><tt>https://www.deeplearningbook.org/contents/rnn.html</tt></a>.</li>
<li> Reading suggestions for implementation of RNNs in PyTorch: see Rashcka et al.'s chapter 15 and GitHub site at <a href="https://github.com/rasbt/machine-learning-book/tree/main/ch15" target="_blank"><tt>https://github.com/rasbt/machine-learning-book/tree/main/ch15</tt></a>.</li>
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="tensorflow-examples">TensorFlow examples </h2>
<p>For TensorFlow (using Keras) implementations, we recommend</p>
<ol>
<li> David Foster, Generative Deep Learning with TensorFlow, see chapter 5 at <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html" target="_blank"><tt>https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html</tt></a></li>
<li> Joseph Babcock and Raghav Bali Generative AI with Python and their GitHub link, chapters 2 and  3 at <a href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2" target="_blank"><tt>https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2</tt></a></li>  
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="what-is-a-recurrent-nn">What is a recurrent NN? </h2>

<p>A recurrent neural network (RNN), as opposed to a regular fully
connected neural network (FCNN) or just neural network (NN), has
layers that are connected to themselves.
</p>

<p>In an FCNN there are no connections between nodes in a single
layer. For instance, \( (h_1^1 \) is not connected to \( (h_2^1 \). In
addition, the input and output are always of a fixed length.
</p>

<p>In an RNN, however, this is no longer the case. Nodes in the hidden
layers are connected to themselves.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="why-rnns">Why RNNs? </h2>

<p>Recurrent neural networks work very well when working with
sequential data, that is data where the order matters. In a regular
fully connected network, the order of input doesn't really matter.
</p>

<p>Another property of  RNNs is that they can handle variable input
and output. Consider again the simplified breast cancer dataset. If you
have trained a regular FCNN on the dataset with the two features, it
makes no sense to suddenly add a third feature. The network would not
know what to do with it, and would reject such inputs with three
features (or any other number of features that isn't two, for that
matter).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail">RNNs in more detail  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN2.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-2">RNNs in more detail, part 2  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN3.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-3">RNNs in more detail, part 3  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN4.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-4">RNNs in more detail, part 4  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN5.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-5">RNNs in more detail, part 5  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN6.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-6">RNNs in more detail, part 6  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN7.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-7">RNNs in more detail, part 7  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN8.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-mathematics-of-rnns-the-basic-architecture">The mathematics of RNNs, the basic architecture  </h2>

<p>See notebook at <a href="https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week7/ipynb/rnnmath.ipynb" target="_blank"><tt>https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week7/ipynb/rnnmath.ipynb</tt></a></p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="gating-mechanism-long-short-term-memory-lstm">Gating mechanism: Long Short Term Memory (LSTM) </h2>

<p>Besides a simple recurrent neural network layer, as discussed above, there are two other
commonly used types of recurrent neural network layers: Long Short
Term Memory (LSTM) and Gated Recurrent Unit (GRU).  For a short
introduction to these layers see <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_blank"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>
and <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_blank"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>.
</p>

<p>LSTM uses a memory cell for 
modeling long-range dependencies and avoid vanishing gradient
 problems.
Capable of modeling longer term dependencies by having
memory cells and gates that controls the information flow along
with the memory cells.
</p>

<ol>
<li> Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).</li>
<li> They designed a memory cell using logistic and linear units with multiplicative interactions.</li>
<li> Information gets into the cell whenever its &#8220;write&#8221; gate is on.</li>
<li> The information stays in the cell so long as its <b>keep</b> gate is on.</li>
<li> Information can be read from the cell by turning on its <b>read</b> gate.</li> 
</ol>
<p>The LSTM were first introduced to overcome the vanishing gradient problem.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="implementing-a-memory-cell-in-a-neural-network">Implementing a memory cell in a neural network </h2>

<p>To preserve information for a long time in
the activities of an RNN, we use a circuit
that implements an analog memory cell.
</p>

<ol>
<li> A linear unit that has a self-link with a weight of 1 will maintain its state.</li>
<li> Information is stored in the cell by activating its write gate.</li>
<li> Information is retrieved by activating the read gate.</li>
<li> We can backpropagate through this circuit because logistics are have nice derivatives.</li> 
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-details">LSTM details </h2>

<p>The LSTM is a unit cell that is made of three gates:</p>
<ol>
<li> the input gate,</li>
<li> the forget gate,</li>
<li> and the output gate.</li>
</ol>
<p>It also introduces a cell state \( c \), which can be thought of as the
long-term memory, and a hidden state \( h \) which can be thought of as
the short-term memory.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="basic-layout-all-figures-from-raschka-et-al">Basic layout (All figures from Raschka <em>et al.,</em>) </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM1.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-details">LSTM details </h2>

<p>The first stage is called the forget gate, where we combine the input
at (say, time \( t \)), and the hidden cell state input at \( t-1 \), passing
it through the Sigmoid activation function and then performing an
element-wise multiplication, denoted by \( \odot \).
</p>

<p>Mathematically we have (see also figure below)</p>
$$
\mathbf{f}^{(t)} = \sigma(W_{fx}\mathbf{x}^{(t)} + W_{fh}\mathbf{h}^{(t-1)} + \mathbf{b}_f)
$$

<p>where the $W$s are the weights to be trained.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="comparing-with-a-standard-rnn">Comparing with a standard  RNN  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM2.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-details-i">LSTM details I </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM3.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-details-ii">LSTM details II  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM4.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-details-iii">LSTM details III  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM5.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="forget-gate">Forget gate  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM6.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-forget-gate">The forget gate </h2>

<p>The naming forget gate stems from the fact that  the Sigmoid activation function's
outputs are very close to \( 0 \) if the argument for the function is very
negative, and \( 1 \) if the argument is very positive. Hence we can
control the amount of information we want to take from the long-term
memory.
</p>
$$
\mathbf{f}^{(t)} = \sigma(W_{fx}\mathbf{x}^{(t)} + W_{fh}\mathbf{h}^{(t-1)} + \mathbf{b}_f)
$$

<p>where the $W$s are the weights to be trained.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="basic-layout">Basic layout </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM7.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="input-gate">Input gate </h2>

<p>The next stage is the input gate, which consists of both a Sigmoid
function (\( \sigma_i \)), which decide what percentage of the input will
be stored in the long-term memory, and the \( \tanh_i \) function, which
decide what is the full memory that can be stored in the long term
memory. When these results are calculated and multiplied together, it
is added to the cell state or stored in the long-term memory, denoted
as \( \oplus \). 
</p>

<p>We have</p>
$$
\mathbf{i}^{(t)} = \sigma_g(W_{ix}\mathbf{x}^{(t)} + W_{ih}\mathbf{h}^{(t-1)} + \mathbf{b}_i),
$$

<p>and</p>
$$
\mathbf{g}^{(t)} = \tanh(W_{gx}\mathbf{x}^{(t)} + W_{gh}\mathbf{h}^{(t-1)} + \mathbf{b}_g),
$$

<p>again the $W$s are the weights to train.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="short-summary">Short summary  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM8.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="forget-and-input">Forget and input </h2>

<p>The forget gate and the input gate together also update the cell state with the following equation, </p>
$$
\mathbf{c}^{(t)} = \mathbf{f}^{(t)} \otimes \mathbf{c}^{(t-1)} + \mathbf{i}^{(t)} \otimes \mathbf{g}^{(t)},
$$

<p>where \( f^{(t)} \) and \( i^{(t)} \) are the outputs of the forget gate and the input gate, respectively.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="basic-layout">Basic layout </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM9.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="output-gate">Output gate </h2>

<p>The final stage of the LSTM is the output gate, and its purpose is to
update the short-term memory.  To achieve this, we take the newly
generated long-term memory and process it through a hyperbolic tangent
(\( \tanh \)) function creating a potential new short-term memory. We then
multiply this potential memory by the output of the Sigmoid function
(\( \sigma_o \)). This multiplication generates the final output as well
as the input for the next hidden cell (\( h^{\langle t \rangle} \)) within
the LSTM cell.
</p>

<p>We have </p>
$$
\begin{aligned}
\mathbf{o}^{(t)} &= \sigma_g(W_o\mathbf{x}^{(t)} + U_o\mathbf{h}^{(t-1)} + \mathbf{b}_o), \\
\mathbf{h}^{(t)} &= \mathbf{o}^{(t)} \otimes \sigma_h(\mathbf{c}^{(t)}). \\
\end{aligned}
$$

<p>where \( \mathbf{W_o,U_o} \) are the weights of the output gate and \( \mathbf{b_o} \) is the bias of the output gate.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="summary-of-lstm">Summary of LSTM </h2>

<p>LSTMs provide a basic approach for modeling long-range dependencies in sequences.
If you wish to read more, see <b>An Empirical Exploration of Recurrent Network Architectures</b>, authored
by Rafal Jozefowicz <em>et al.,</em>  Proceedings of ICML, 2342-2350, 2015).
</p>

<p>An important recent development are the so-called <b>gated recurrent units (GRU)</b>, see for example the article
by Junyoung Chung <em>et al.,</em>, at URL:"https://arxiv.org/abs/1412.3555.
This article is an excellent read if you are interested in learning
more about these modern RNN architectures
</p>

<p>The GRUs have a simpler
architecture than LSTMs. This leads to computationally more efficient methods, while their
performance in some tasks, such as polyphonic music modeling, is comparable to LSTMs.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-implementation-using-tensorflow">LSTM implementation using TensorFlow </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">Key points:</span>
<span style="color: #BA2121; font-style: italic">1. The input images (28x28 pixels) are treated as sequences of 28 timesteps with 28 features each</span>
<span style="color: #BA2121; font-style: italic">2. The LSTM layer processes this sequential data</span>
<span style="color: #BA2121; font-style: italic">3. A final dense layer with softmax activation handles the classification</span>
<span style="color: #BA2121; font-style: italic">4. Typical accuracy ranges between 95-98% (lower than CNNs but reasonable for demonstration)</span>

<span style="color: #BA2121; font-style: italic">Note: LSTMs are not typically used for image classification (CNNs are more efficient), but this demonstrates how to adapt them for such tasks. Training might take longer compared to CNN architectures.</span>

<span style="color: #BA2121; font-style: italic">To improve performance, you could:</span>
<span style="color: #BA2121; font-style: italic">1. Add more LSTM layers</span>
<span style="color: #BA2121; font-style: italic">2. Use Bidirectional LSTMs</span>
<span style="color: #BA2121; font-style: italic">3. Increase the number of units</span>
<span style="color: #BA2121; font-style: italic">4. Add dropout for regularization</span>
<span style="color: #BA2121; font-style: italic">5. Use learning rate scheduling</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.models</span> <span style="color: #008000; font-weight: bold">import</span> Sequential
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.layers</span> <span style="color: #008000; font-weight: bold">import</span> LSTM, Dense
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.utils</span> <span style="color: #008000; font-weight: bold">import</span> to_categorical

<span style="color: #408080; font-style: italic"># Load and preprocess data</span>
(x_train, y_train), (x_test, y_test) <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>datasets<span style="color: #666666">.</span>mnist<span style="color: #666666">.</span>load_data()

<span style="color: #408080; font-style: italic"># Normalize pixel values to [0, 1]</span>
x_train <span style="color: #666666">=</span> x_train<span style="color: #666666">.</span>astype(<span style="color: #BA2121">&#39;float32&#39;</span>) <span style="color: #666666">/</span> <span style="color: #666666">255.0</span>
x_test <span style="color: #666666">=</span> x_test<span style="color: #666666">.</span>astype(<span style="color: #BA2121">&#39;float32&#39;</span>) <span style="color: #666666">/</span> <span style="color: #666666">255.0</span>

<span style="color: #408080; font-style: italic"># Reshape data for LSTM (samples, timesteps, features)</span>
<span style="color: #408080; font-style: italic"># MNIST images are 28x28, so we treat each image as 28 timesteps of 28 features</span>
x_train <span style="color: #666666">=</span> x_train<span style="color: #666666">.</span>reshape((<span style="color: #666666">-1</span>, <span style="color: #666666">28</span>, <span style="color: #666666">28</span>))
x_test <span style="color: #666666">=</span> x_test<span style="color: #666666">.</span>reshape((<span style="color: #666666">-1</span>, <span style="color: #666666">28</span>, <span style="color: #666666">28</span>))

<span style="color: #408080; font-style: italic"># Convert labels to one-hot encoding</span>
y_train <span style="color: #666666">=</span> to_categorical(y_train, <span style="color: #666666">10</span>)
y_test <span style="color: #666666">=</span> to_categorical(y_test, <span style="color: #666666">10</span>)

<span style="color: #408080; font-style: italic"># Build LSTM model</span>
model <span style="color: #666666">=</span> Sequential()
model<span style="color: #666666">.</span>add(LSTM(<span style="color: #666666">128</span>, input_shape<span style="color: #666666">=</span>(<span style="color: #666666">28</span>, <span style="color: #666666">28</span>)))  <span style="color: #408080; font-style: italic"># 128 LSTM units</span>
model<span style="color: #666666">.</span>add(Dense(<span style="color: #666666">10</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;softmax&#39;</span>))

<span style="color: #408080; font-style: italic"># Compile the model</span>
model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;categorical_crossentropy&#39;</span>,
             optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&#39;adam&#39;</span>,
             metrics<span style="color: #666666">=</span>[<span style="color: #BA2121">&#39;accuracy&#39;</span>])

<span style="color: #408080; font-style: italic"># Display model summary</span>
model<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># Train the model</span>
history <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(x_train, y_train,
                   batch_size<span style="color: #666666">=64</span>,
                   epochs<span style="color: #666666">=10</span>,
                   validation_split<span style="color: #666666">=0.2</span>)

<span style="color: #408080; font-style: italic"># Evaluate on test data</span>
test_loss, test_acc <span style="color: #666666">=</span> model<span style="color: #666666">.</span>evaluate(x_test, y_test, verbose<span style="color: #666666">=2</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">Test accuracy: </span><span style="color: #BB6688; font-weight: bold">{</span>test_acc<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="and-the-corresponding-one-with-pytorch">And the corresponding one with PyTorch </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">Key components:</span>
<span style="color: #BA2121; font-style: italic">1. **Data Handling**: Uses PyTorch DataLoader with MNIST dataset</span>
<span style="color: #BA2121; font-style: italic">2. **LSTM Architecture**:</span>
<span style="color: #BA2121; font-style: italic">  - Input sequence of 28 timesteps (image rows)</span>
<span style="color: #BA2121; font-style: italic">  - 128 hidden units in LSTM layer</span>
<span style="color: #BA2121; font-style: italic">  - Fully connected layer for classification</span>
<span style="color: #BA2121; font-style: italic">3. **Training**:</span>
<span style="color: #BA2121; font-style: italic">  - Cross-entropy loss</span>
<span style="color: #BA2121; font-style: italic">  - Adam optimizer</span>
<span style="color: #BA2121; font-style: italic">  - Automatic GPU utilization if available</span>

<span style="color: #BA2121; font-style: italic">This implementation typically achieves **97-98% accuracy** after 10 epochs. The main differences from the TensorFlow/Keras version:</span>
<span style="color: #BA2121; font-style: italic">- Explicit device management (CPU/GPU)</span>
<span style="color: #BA2121; font-style: italic">- Manual training loop</span>
<span style="color: #BA2121; font-style: italic">- Different data loading pipeline</span>
<span style="color: #BA2121; font-style: italic">- More explicit tensor reshaping</span>

<span style="color: #BA2121; font-style: italic">To improve performance, you could:</span>
<span style="color: #BA2121; font-style: italic">1. Add dropout regularization</span>
<span style="color: #BA2121; font-style: italic">2. Use bidirectional LSTM</span>
<span style="color: #BA2121; font-style: italic">3. Implement learning rate scheduling</span>
<span style="color: #BA2121; font-style: italic">4. Add batch normalization</span>
<span style="color: #BA2121; font-style: italic">5. Increase model capacity (more layers/units)</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.nn</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">nn</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.optim</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">optim</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torchvision</span> <span style="color: #008000; font-weight: bold">import</span> datasets, transforms
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch.utils.data</span> <span style="color: #008000; font-weight: bold">import</span> DataLoader

<span style="color: #408080; font-style: italic"># Hyperparameters</span>
input_size <span style="color: #666666">=</span> <span style="color: #666666">28</span>     <span style="color: #408080; font-style: italic"># Number of features (pixels per row)</span>
hidden_size <span style="color: #666666">=</span> <span style="color: #666666">128</span>   <span style="color: #408080; font-style: italic"># LSTM hidden state size</span>
num_classes <span style="color: #666666">=</span> <span style="color: #666666">10</span>    <span style="color: #408080; font-style: italic"># Digits 0-9</span>
num_epochs <span style="color: #666666">=</span> <span style="color: #666666">10</span>     <span style="color: #408080; font-style: italic"># Training iterations</span>
batch_size <span style="color: #666666">=</span> <span style="color: #666666">64</span>     <span style="color: #408080; font-style: italic"># Batch size</span>
learning_rate <span style="color: #666666">=</span> <span style="color: #666666">0.001</span>

<span style="color: #408080; font-style: italic"># Device configuration</span>
device <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>device(<span style="color: #BA2121">&#39;cuda&#39;</span> <span style="color: #008000; font-weight: bold">if</span> torch<span style="color: #666666">.</span>cuda<span style="color: #666666">.</span>is_available() <span style="color: #008000; font-weight: bold">else</span> <span style="color: #BA2121">&#39;cpu&#39;</span>)

<span style="color: #408080; font-style: italic"># MNIST dataset</span>
transform <span style="color: #666666">=</span> transforms<span style="color: #666666">.</span>Compose([
   transforms<span style="color: #666666">.</span>ToTensor(),
   transforms<span style="color: #666666">.</span>Normalize((<span style="color: #666666">0.1307</span>,), (<span style="color: #666666">0.3081</span>,))  <span style="color: #408080; font-style: italic"># MNIST mean and std</span>
])

train_dataset <span style="color: #666666">=</span> datasets<span style="color: #666666">.</span>MNIST(root<span style="color: #666666">=</span><span style="color: #BA2121">&#39;./data&#39;</span>,
                              train<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,
                              transform<span style="color: #666666">=</span>transform,
                              download<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)

test_dataset <span style="color: #666666">=</span> datasets<span style="color: #666666">.</span>MNIST(root<span style="color: #666666">=</span><span style="color: #BA2121">&#39;./data&#39;</span>,
                             train<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                             transform<span style="color: #666666">=</span>transform)

train_loader <span style="color: #666666">=</span> DataLoader(dataset<span style="color: #666666">=</span>train_dataset,
                         batch_size<span style="color: #666666">=</span>batch_size,
                         shuffle<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)

test_loader <span style="color: #666666">=</span> DataLoader(dataset<span style="color: #666666">=</span>test_dataset,
                        batch_size<span style="color: #666666">=</span>batch_size,
                        shuffle<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)

<span style="color: #408080; font-style: italic"># LSTM model</span>
<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">LSTMModel</span>(nn<span style="color: #666666">.</span>Module):
   <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, input_size, hidden_size, num_classes):
       <span style="color: #008000">super</span>(LSTMModel, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
       <span style="color: #008000">self</span><span style="color: #666666">.</span>hidden_size <span style="color: #666666">=</span> hidden_size
       <span style="color: #008000">self</span><span style="color: #666666">.</span>lstm <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>LSTM(input_size, hidden_size, batch_first<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
       <span style="color: #008000">self</span><span style="color: #666666">.</span>fc <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>Linear(hidden_size, num_classes)

   <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
       <span style="color: #408080; font-style: italic"># Reshape input to (batch_size, sequence_length, input_size)</span>
       x <span style="color: #666666">=</span> x<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">28</span>, <span style="color: #666666">28</span>)

       <span style="color: #408080; font-style: italic"># Forward propagate LSTM</span>
       out, _ <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>lstm(x)  <span style="color: #408080; font-style: italic"># out: (batch_size, seq_length, hidden_size)</span>

       <span style="color: #408080; font-style: italic"># Decode the hidden state of the last time step</span>
       out <span style="color: #666666">=</span> out[:, <span style="color: #666666">-1</span>, :]
       out <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>fc(out)
       <span style="color: #008000; font-weight: bold">return</span> out

<span style="color: #408080; font-style: italic"># Initialize model</span>
model <span style="color: #666666">=</span> LSTMModel(input_size, hidden_size, num_classes)<span style="color: #666666">.</span>to(device)

<span style="color: #408080; font-style: italic"># Loss and optimizer</span>
criterion <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>CrossEntropyLoss()
optimizer <span style="color: #666666">=</span> optim<span style="color: #666666">.</span>Adam(model<span style="color: #666666">.</span>parameters(), lr<span style="color: #666666">=</span>learning_rate)

<span style="color: #408080; font-style: italic"># Training loop</span>
total_step <span style="color: #666666">=</span> <span style="color: #008000">len</span>(train_loader)
<span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(num_epochs):
   model<span style="color: #666666">.</span>train()
   <span style="color: #008000; font-weight: bold">for</span> i, (images, labels) <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(train_loader):
       images <span style="color: #666666">=</span> images<span style="color: #666666">.</span>to(device)
       labels <span style="color: #666666">=</span> labels<span style="color: #666666">.</span>to(device)

       <span style="color: #408080; font-style: italic"># Forward pass</span>
       outputs <span style="color: #666666">=</span> model(images)
       loss <span style="color: #666666">=</span> criterion(outputs, labels)

       <span style="color: #408080; font-style: italic"># Backward and optimize</span>
       optimizer<span style="color: #666666">.</span>zero_grad()
       loss<span style="color: #666666">.</span>backward()
       optimizer<span style="color: #666666">.</span>step()

       <span style="color: #008000; font-weight: bold">if</span> (i<span style="color: #666666">+1</span>) <span style="color: #666666">%</span> <span style="color: #666666">100</span> <span style="color: #666666">==</span> <span style="color: #666666">0</span>:
           <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Epoch [</span><span style="color: #BB6688; font-weight: bold">{</span>epoch<span style="color: #666666">+1</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">/</span><span style="color: #BB6688; font-weight: bold">{</span>num_epochs<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">], Step [</span><span style="color: #BB6688; font-weight: bold">{</span>i<span style="color: #666666">+1</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">/</span><span style="color: #BB6688; font-weight: bold">{</span>total_step<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">], Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss<span style="color: #666666">.</span>item()<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)

   <span style="color: #408080; font-style: italic"># Test the model</span>
   model<span style="color: #666666">.</span>eval()
   <span style="color: #008000; font-weight: bold">with</span> torch<span style="color: #666666">.</span>no_grad():
       correct <span style="color: #666666">=</span> <span style="color: #666666">0</span>
       total <span style="color: #666666">=</span> <span style="color: #666666">0</span>
       <span style="color: #008000; font-weight: bold">for</span> images, labels <span style="color: #AA22FF; font-weight: bold">in</span> test_loader:
           images <span style="color: #666666">=</span> images<span style="color: #666666">.</span>to(device)
           labels <span style="color: #666666">=</span> labels<span style="color: #666666">.</span>to(device)
           outputs <span style="color: #666666">=</span> model(images)
           _, predicted <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>max(outputs<span style="color: #666666">.</span>data, <span style="color: #666666">1</span>)
           total <span style="color: #666666">+=</span> labels<span style="color: #666666">.</span>size(<span style="color: #666666">0</span>)
           correct <span style="color: #666666">+=</span> (predicted <span style="color: #666666">==</span> labels)<span style="color: #666666">.</span>sum()<span style="color: #666666">.</span>item()

       <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Test Accuracy: </span><span style="color: #BB6688; font-weight: bold">{</span><span style="color: #666666">100</span> <span style="color: #666666">*</span> correct <span style="color: #666666">/</span> total<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.2f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">%&#39;</span>)

<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Training finished.&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="dynamical-ordinary-differential-equation">Dynamical ordinary differential equation </h2>

<p>Let us illustrate how we could train an RNN using data from the
solution of a well-known differential equation, namely Newton's
equation for oscillatory motion for an object being forced into
harmonic oscillations by an applied external force.
</p>

<p>We will start with the basic algorithm for solving this type of
equations using the Runge-Kutta-4 approach. The first code example is
a standalone differential equation solver. It yields positions and
velocities as function of time, starting with an initial time \( t_0 \)
and ending with a final time.
</p>

<p>The data the program produces will in turn be used to train an RNN for
a selected number of training data. With a trained RNN, we will then
use the network to make predictions for data not included in the
training. That is, we will train a model which should be able to
reproduce velocities and positions not included in training data.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-runge-kutta-4-code">The Runge-Kutta-4 code </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">math</span> <span style="color: #008000; font-weight: bold">import</span> <span style="color: #666666">*</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>

<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">SpringForce</span>(v,x,t):
<span style="color: #408080; font-style: italic">#   note here that we have divided by mass and we return the acceleration</span>
    <span style="color: #008000; font-weight: bold">return</span>  <span style="color: #666666">-2*</span>gamma<span style="color: #666666">*</span>v<span style="color: #666666">-</span>x<span style="color: #666666">+</span>Ftilde<span style="color: #666666">*</span>cos(t<span style="color: #666666">*</span>Omegatilde)


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">RK4</span>(v,x,t,n,Force):
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n<span style="color: #666666">-1</span>):
<span style="color: #408080; font-style: italic"># Setting up k1</span>
        k1x <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>v[i]
        k1v <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>Force(v[i],x[i],t[i])
<span style="color: #408080; font-style: italic"># Setting up k2</span>
        vv <span style="color: #666666">=</span> v[i]<span style="color: #666666">+</span>k1v<span style="color: #666666">*0.5</span>
        xx <span style="color: #666666">=</span> x[i]<span style="color: #666666">+</span>k1x<span style="color: #666666">*0.5</span>
        k2x <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>vv
        k2v <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>Force(vv,xx,t[i]<span style="color: #666666">+</span>DeltaT<span style="color: #666666">*0.5</span>)
<span style="color: #408080; font-style: italic"># Setting up k3</span>
        vv <span style="color: #666666">=</span> v[i]<span style="color: #666666">+</span>k2v<span style="color: #666666">*0.5</span>
        xx <span style="color: #666666">=</span> x[i]<span style="color: #666666">+</span>k2x<span style="color: #666666">*0.5</span>
        k3x <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>vv
        k3v <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>Force(vv,xx,t[i]<span style="color: #666666">+</span>DeltaT<span style="color: #666666">*0.5</span>)
<span style="color: #408080; font-style: italic"># Setting up k4</span>
        vv <span style="color: #666666">=</span> v[i]<span style="color: #666666">+</span>k3v
        xx <span style="color: #666666">=</span> x[i]<span style="color: #666666">+</span>k3x
        k4x <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>vv
        k4v <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>Force(vv,xx,t[i]<span style="color: #666666">+</span>DeltaT)
<span style="color: #408080; font-style: italic"># Final result</span>
        x[i<span style="color: #666666">+1</span>] <span style="color: #666666">=</span> x[i]<span style="color: #666666">+</span>(k1x<span style="color: #666666">+2*</span>k2x<span style="color: #666666">+2*</span>k3x<span style="color: #666666">+</span>k4x)<span style="color: #666666">/6.</span>
        v[i<span style="color: #666666">+1</span>] <span style="color: #666666">=</span> v[i]<span style="color: #666666">+</span>(k1v<span style="color: #666666">+2*</span>k2v<span style="color: #666666">+2*</span>k3v<span style="color: #666666">+</span>k4v)<span style="color: #666666">/6.</span>
        t[i<span style="color: #666666">+1</span>] <span style="color: #666666">=</span> t[i] <span style="color: #666666">+</span> DeltaT


<span style="color: #408080; font-style: italic"># Main part begins here</span>

DeltaT <span style="color: #666666">=</span> <span style="color: #666666">0.001</span>
<span style="color: #408080; font-style: italic">#set up arrays </span>
tfinal <span style="color: #666666">=</span> <span style="color: #666666">20</span> <span style="color: #408080; font-style: italic"># in dimensionless time</span>
n <span style="color: #666666">=</span> ceil(tfinal<span style="color: #666666">/</span>DeltaT)
<span style="color: #408080; font-style: italic"># set up arrays for t, v, and x</span>
t <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n)
v <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n)
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n)
<span style="color: #408080; font-style: italic"># Initial conditions (can change to more than one dim)</span>
x0 <span style="color: #666666">=</span>  <span style="color: #666666">1.0</span> 
v0 <span style="color: #666666">=</span> <span style="color: #666666">0.0</span>
x[<span style="color: #666666">0</span>] <span style="color: #666666">=</span> x0
v[<span style="color: #666666">0</span>] <span style="color: #666666">=</span> v0
gamma <span style="color: #666666">=</span> <span style="color: #666666">0.2</span>
Omegatilde <span style="color: #666666">=</span> <span style="color: #666666">0.5</span>
Ftilde <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>
<span style="color: #408080; font-style: italic"># Start integrating using Euler&#39;s method</span>
<span style="color: #408080; font-style: italic"># Note that we define the force function as a SpringForce</span>
RK4(v,x,t,n,SpringForce)

<span style="color: #408080; font-style: italic"># Plot position as function of time    </span>
fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots()
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&#39;x[m]&#39;</span>)
ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&#39;t[s]&#39;</span>)
ax<span style="color: #666666">.</span>plot(t, x)
fig<span style="color: #666666">.</span>tight_layout()
save_fig(<span style="color: #BA2121">&quot;ForcedBlockRK4&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="using-the-above-data-to-train-an-rnn">Using the above data to train an RNN </h2>

<p>In the code here we have reworked the previous example in order to generate data that can be handled by recurrent neural networks in order to train our model.</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.nn</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">nn</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch.utils.data</span> <span style="color: #008000; font-weight: bold">import</span> Dataset, DataLoader
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>


<span style="color: #408080; font-style: italic"># Newton&#39;s equation for harmonic oscillations with external force</span>

<span style="color: #408080; font-style: italic"># Global parameters</span>
gamma <span style="color: #666666">=</span> <span style="color: #666666">0.2</span>        <span style="color: #408080; font-style: italic"># damping</span>
Omegatilde <span style="color: #666666">=</span> <span style="color: #666666">0.5</span>   <span style="color: #408080; font-style: italic"># driving frequency</span>
Ftilde <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>       <span style="color: #408080; font-style: italic"># driving amplitude</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">spring_force</span>(v, x, t):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    SpringForce:</span>
<span style="color: #BA2121; font-style: italic">    note: divided by mass =&gt; returns acceleration</span>
<span style="color: #BA2121; font-style: italic">    a = -2*gamma*v - x + Ftilde*cos(Omegatilde * t)</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">-2.0</span> <span style="color: #666666">*</span> gamma <span style="color: #666666">*</span> v <span style="color: #666666">-</span> x <span style="color: #666666">+</span> Ftilde <span style="color: #666666">*</span> np<span style="color: #666666">.</span>cos(Omegatilde <span style="color: #666666">*</span> t)


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">rk4_trajectory</span>(DeltaT<span style="color: #666666">=0.001</span>, tfinal<span style="color: #666666">=20.0</span>, x0<span style="color: #666666">=1.0</span>, v0<span style="color: #666666">=0.0</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Returns t, x, v arrays.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    n <span style="color: #666666">=</span> <span style="color: #008000">int</span>(np<span style="color: #666666">.</span>ceil(tfinal <span style="color: #666666">/</span> DeltaT))

    t <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n, dtype<span style="color: #666666">=</span>np<span style="color: #666666">.</span>float32)
    x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n, dtype<span style="color: #666666">=</span>np<span style="color: #666666">.</span>float32)
    v <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n, dtype<span style="color: #666666">=</span>np<span style="color: #666666">.</span>float32)

    x[<span style="color: #666666">0</span>] <span style="color: #666666">=</span> x0
    v[<span style="color: #666666">0</span>] <span style="color: #666666">=</span> v0

    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n <span style="color: #666666">-</span> <span style="color: #666666">1</span>):
        <span style="color: #408080; font-style: italic"># k1</span>
        k1x <span style="color: #666666">=</span> DeltaT <span style="color: #666666">*</span> v[i]
        k1v <span style="color: #666666">=</span> DeltaT <span style="color: #666666">*</span> spring_force(v[i], x[i], t[i])

        <span style="color: #408080; font-style: italic"># k2</span>
        vv <span style="color: #666666">=</span> v[i] <span style="color: #666666">+</span> <span style="color: #666666">0.5</span> <span style="color: #666666">*</span> k1v
        xx <span style="color: #666666">=</span> x[i] <span style="color: #666666">+</span> <span style="color: #666666">0.5</span> <span style="color: #666666">*</span> k1x
        k2x <span style="color: #666666">=</span> DeltaT <span style="color: #666666">*</span> vv
        k2v <span style="color: #666666">=</span> DeltaT <span style="color: #666666">*</span> spring_force(vv, xx, t[i] <span style="color: #666666">+</span> <span style="color: #666666">0.5</span> <span style="color: #666666">*</span> DeltaT)

        <span style="color: #408080; font-style: italic"># k3</span>
        vv <span style="color: #666666">=</span> v[i] <span style="color: #666666">+</span> <span style="color: #666666">0.5</span> <span style="color: #666666">*</span> k2v
        xx <span style="color: #666666">=</span> x[i] <span style="color: #666666">+</span> <span style="color: #666666">0.5</span> <span style="color: #666666">*</span> k2x
        k3x <span style="color: #666666">=</span> DeltaT <span style="color: #666666">*</span> vv
        k3v <span style="color: #666666">=</span> DeltaT <span style="color: #666666">*</span> spring_force(vv, xx, t[i] <span style="color: #666666">+</span> <span style="color: #666666">0.5</span> <span style="color: #666666">*</span> DeltaT)

        <span style="color: #408080; font-style: italic"># k4</span>
        vv <span style="color: #666666">=</span> v[i] <span style="color: #666666">+</span> k3v
        xx <span style="color: #666666">=</span> x[i] <span style="color: #666666">+</span> k3x
        k4x <span style="color: #666666">=</span> DeltaT <span style="color: #666666">*</span> vv
        k4v <span style="color: #666666">=</span> DeltaT <span style="color: #666666">*</span> spring_force(vv, xx, t[i] <span style="color: #666666">+</span> DeltaT)

        <span style="color: #408080; font-style: italic"># Update</span>
        x[i <span style="color: #666666">+</span> <span style="color: #666666">1</span>] <span style="color: #666666">=</span> x[i] <span style="color: #666666">+</span> (k1x <span style="color: #666666">+</span> <span style="color: #666666">2.0</span> <span style="color: #666666">*</span> k2x <span style="color: #666666">+</span> <span style="color: #666666">2.0</span> <span style="color: #666666">*</span> k3x <span style="color: #666666">+</span> k4x) <span style="color: #666666">/</span> <span style="color: #666666">6.0</span>
        v[i <span style="color: #666666">+</span> <span style="color: #666666">1</span>] <span style="color: #666666">=</span> v[i] <span style="color: #666666">+</span> (k1v <span style="color: #666666">+</span> <span style="color: #666666">2.0</span> <span style="color: #666666">*</span> k2v <span style="color: #666666">+</span> <span style="color: #666666">2.0</span> <span style="color: #666666">*</span> k3v <span style="color: #666666">+</span> k4v) <span style="color: #666666">/</span> <span style="color: #666666">6.0</span>
        t[i <span style="color: #666666">+</span> <span style="color: #666666">1</span>] <span style="color: #666666">=</span> t[i] <span style="color: #666666">+</span> DeltaT

    <span style="color: #008000; font-weight: bold">return</span> t, x, v


<span style="color: #408080; font-style: italic"># Sequence generation for RNN training</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">create_sequences</span>(x, seq_len):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Given a 1D array x (e.g., position as a function of time),</span>
<span style="color: #BA2121; font-style: italic">    create input/target sequences for next-step prediction.</span>

<span style="color: #BA2121; font-style: italic">    Inputs:  [x_i, x_{i+1}, ..., x_{i+seq_len-1}]</span>
<span style="color: #BA2121; font-style: italic">    Targets: [x_{i+1}, ..., x_{i+seq_len}]</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
    xs <span style="color: #666666">=</span> []
    ys <span style="color: #666666">=</span> []
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(x) <span style="color: #666666">-</span> seq_len):
        seq_x <span style="color: #666666">=</span> x[i : i <span style="color: #666666">+</span> seq_len]
        seq_y <span style="color: #666666">=</span> x[i <span style="color: #666666">+</span> <span style="color: #666666">1</span> : i <span style="color: #666666">+</span> seq_len <span style="color: #666666">+</span> <span style="color: #666666">1</span>]  <span style="color: #408080; font-style: italic"># shifted by one step</span>
        xs<span style="color: #666666">.</span>append(seq_x)
        ys<span style="color: #666666">.</span>append(seq_y)

    xs <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(xs, dtype<span style="color: #666666">=</span>np<span style="color: #666666">.</span>float32)      <span style="color: #408080; font-style: italic"># shape: (num_samples, seq_len)</span>
    ys <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(ys, dtype<span style="color: #666666">=</span>np<span style="color: #666666">.</span>float32)      <span style="color: #408080; font-style: italic"># shape: (num_samples, seq_len)</span>
    <span style="color: #408080; font-style: italic"># Add feature dimension (1 feature: the position x)</span>
    xs <span style="color: #666666">=</span> np<span style="color: #666666">.</span>expand_dims(xs, axis<span style="color: #666666">=-1</span>)         <span style="color: #408080; font-style: italic"># (num_samples, seq_len, 1)</span>
    ys <span style="color: #666666">=</span> np<span style="color: #666666">.</span>expand_dims(ys, axis<span style="color: #666666">=-1</span>)         <span style="color: #408080; font-style: italic"># (num_samples, seq_len, 1)</span>
    <span style="color: #008000; font-weight: bold">return</span> xs, ys


<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">OscillatorDataset</span>(Dataset):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, seq_len<span style="color: #666666">=50</span>, DeltaT<span style="color: #666666">=0.001</span>, tfinal<span style="color: #666666">=20.0</span>, x0<span style="color: #666666">=1.0</span>, v0<span style="color: #666666">=0.0</span>):
        t, x, v <span style="color: #666666">=</span> rk4_trajectory(DeltaT<span style="color: #666666">=</span>DeltaT, tfinal<span style="color: #666666">=</span>tfinal, x0<span style="color: #666666">=</span>x0, v0<span style="color: #666666">=</span>v0)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>t <span style="color: #666666">=</span> t
        <span style="color: #008000">self</span><span style="color: #666666">.</span>x <span style="color: #666666">=</span> x
        <span style="color: #008000">self</span><span style="color: #666666">.</span>v <span style="color: #666666">=</span> v
        xs, ys <span style="color: #666666">=</span> create_sequences(x, seq_len<span style="color: #666666">=</span>seq_len)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>inputs <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>from_numpy(xs)  <span style="color: #408080; font-style: italic"># (N, seq_len, 1)</span>
        <span style="color: #008000">self</span><span style="color: #666666">.</span>targets <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>from_numpy(ys) <span style="color: #408080; font-style: italic"># (N, seq_len, 1)</span>

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__len__</span>(<span style="color: #008000">self</span>):
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>inputs<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>]

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__getitem__</span>(<span style="color: #008000">self</span>, idx):
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>inputs[idx], <span style="color: #008000">self</span><span style="color: #666666">.</span>targets[idx]


<span style="color: #408080; font-style: italic"># RNN model (LSTM-based in this example)</span>

<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">RNNPredictor</span>(nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, input_size<span style="color: #666666">=1</span>, hidden_size<span style="color: #666666">=32</span>, num_layers<span style="color: #666666">=1</span>, output_size<span style="color: #666666">=1</span>):
        <span style="color: #008000">super</span>()<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #008000">self</span><span style="color: #666666">.</span>lstm <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>LSTM(input_size<span style="color: #666666">=</span>input_size,
                            hidden_size<span style="color: #666666">=</span>hidden_size,
                            num_layers<span style="color: #666666">=</span>num_layers,
                            batch_first<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>fc <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>Linear(hidden_size, output_size)

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        <span style="color: #408080; font-style: italic"># x: (batch, seq_len, input_size)</span>
        out, _ <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>lstm(x)   <span style="color: #408080; font-style: italic"># out: (batch, seq_len, hidden_size)</span>
        out <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>fc(out)      <span style="color: #408080; font-style: italic"># (batch, seq_len, output_size)</span>
        <span style="color: #008000; font-weight: bold">return</span> out


<span style="color: #408080; font-style: italic"># Training part</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">train_model</span>(
    seq_len<span style="color: #666666">=50</span>,
    DeltaT<span style="color: #666666">=0.001</span>,
    tfinal<span style="color: #666666">=20.0</span>,
    batch_size<span style="color: #666666">=64</span>,
    num_epochs<span style="color: #666666">=10</span>,
    hidden_size<span style="color: #666666">=64</span>,
    lr<span style="color: #666666">=1e-3</span>,
    device<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>,
):
    <span style="color: #008000; font-weight: bold">if</span> device <span style="color: #AA22FF; font-weight: bold">is</span> <span style="color: #008000; font-weight: bold">None</span>:
        device <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>device(<span style="color: #BA2121">&quot;cuda&quot;</span> <span style="color: #008000; font-weight: bold">if</span> torch<span style="color: #666666">.</span>cuda<span style="color: #666666">.</span>is_available() <span style="color: #008000; font-weight: bold">else</span> <span style="color: #BA2121">&quot;cpu&quot;</span>)
    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Using device: </span><span style="color: #BB6688; font-weight: bold">{</span>device<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)

    <span style="color: #408080; font-style: italic"># Dataset &amp; DataLoader</span>
    dataset <span style="color: #666666">=</span> OscillatorDataset(seq_len<span style="color: #666666">=</span>seq_len, DeltaT<span style="color: #666666">=</span>DeltaT, tfinal<span style="color: #666666">=</span>tfinal)
    train_loader <span style="color: #666666">=</span> DataLoader(dataset, batch_size<span style="color: #666666">=</span>batch_size, shuffle<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)

    <span style="color: #408080; font-style: italic"># Model, loss, optimizer</span>
    model <span style="color: #666666">=</span> RNNPredictor(input_size<span style="color: #666666">=1</span>, hidden_size<span style="color: #666666">=</span>hidden_size, output_size<span style="color: #666666">=1</span>)
    model<span style="color: #666666">.</span>to(device)

    criterion <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>MSELoss()
    optimizer <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>optim<span style="color: #666666">.</span>Adam(model<span style="color: #666666">.</span>parameters(), lr<span style="color: #666666">=</span>lr)

    <span style="color: #408080; font-style: italic"># Training loop</span>
    model<span style="color: #666666">.</span>train()
    <span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(num_epochs):
        epoch_loss <span style="color: #666666">=</span> <span style="color: #666666">0.0</span>
        <span style="color: #008000; font-weight: bold">for</span> batch_x, batch_y <span style="color: #AA22FF; font-weight: bold">in</span> train_loader:
            batch_x <span style="color: #666666">=</span> batch_x<span style="color: #666666">.</span>to(device)
            batch_y <span style="color: #666666">=</span> batch_y<span style="color: #666666">.</span>to(device)

            optimizer<span style="color: #666666">.</span>zero_grad()
            outputs <span style="color: #666666">=</span> model(batch_x)
            loss <span style="color: #666666">=</span> criterion(outputs, batch_y)
            loss<span style="color: #666666">.</span>backward()
            optimizer<span style="color: #666666">.</span>step()

            epoch_loss <span style="color: #666666">+=</span> loss<span style="color: #666666">.</span>item() <span style="color: #666666">*</span> batch_x<span style="color: #666666">.</span>size(<span style="color: #666666">0</span>)

        epoch_loss <span style="color: #666666">/=</span> <span style="color: #008000">len</span>(train_loader<span style="color: #666666">.</span>dataset)
        <span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Epoch </span><span style="color: #BB6688; font-weight: bold">{</span>epoch<span style="color: #666666">+1</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">/</span><span style="color: #BB6688; font-weight: bold">{</span>num_epochs<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">, Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>epoch_loss<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.6f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)

    <span style="color: #008000; font-weight: bold">return</span> model, dataset


<span style="color: #408080; font-style: italic"># Evaluation / visualization</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">evaluate_and_plot</span>(model, dataset, seq_len<span style="color: #666666">=50</span>, device<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">None</span>):
    <span style="color: #008000; font-weight: bold">if</span> device <span style="color: #AA22FF; font-weight: bold">is</span> <span style="color: #008000; font-weight: bold">None</span>:
        device <span style="color: #666666">=</span> <span style="color: #008000">next</span>(model<span style="color: #666666">.</span>parameters())<span style="color: #666666">.</span>device

    model<span style="color: #666666">.</span>eval()
    <span style="color: #008000; font-weight: bold">with</span> torch<span style="color: #666666">.</span>no_grad():
        <span style="color: #408080; font-style: italic"># Take a single sequence from the dataset</span>
        x_seq, y_seq <span style="color: #666666">=</span> dataset[<span style="color: #666666">0</span>]  <span style="color: #408080; font-style: italic"># shapes: (seq_len, 1), (seq_len, 1)</span>
        x_input <span style="color: #666666">=</span> x_seq<span style="color: #666666">.</span>unsqueeze(<span style="color: #666666">0</span>)<span style="color: #666666">.</span>to(device)  <span style="color: #408080; font-style: italic"># (1, seq_len, 1)</span>
        <span style="color: #408080; font-style: italic"># Model prediction (next-step for whole sequence)</span>
        y_pred <span style="color: #666666">=</span> model(x_input)<span style="color: #666666">.</span>cpu()<span style="color: #666666">.</span>numpy()<span style="color: #666666">.</span>squeeze(<span style="color: #666666">-1</span>)<span style="color: #666666">.</span>squeeze(<span style="color: #666666">0</span>)  <span style="color: #408080; font-style: italic"># (seq_len,)</span>
        <span style="color: #408080; font-style: italic"># True target</span>
        y_true <span style="color: #666666">=</span> y_seq<span style="color: #666666">.</span>numpy()<span style="color: #666666">.</span>squeeze(<span style="color: #666666">-1</span>)  <span style="color: #408080; font-style: italic"># (seq_len,)</span>
        <span style="color: #408080; font-style: italic"># Plot comparison</span>
        plt<span style="color: #666666">.</span>figure()
        plt<span style="color: #666666">.</span>plot(y_true, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;True x(t+t)&quot;</span>, linestyle<span style="color: #666666">=</span><span style="color: #BA2121">&quot;-&quot;</span>)
        plt<span style="color: #666666">.</span>plot(y_pred, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Predicted x(t+t)&quot;</span>, linestyle<span style="color: #666666">=</span><span style="color: #BA2121">&quot;--&quot;</span>)
        plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;Time step in sequence&quot;</span>)
        plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;Position&quot;</span>)
        plt<span style="color: #666666">.</span>legend()
        plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;RNN next-step prediction on oscillator trajectory&quot;</span>)
        plt<span style="color: #666666">.</span>tight_layout()
        plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># This is the main part of the code where we define the network</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #19177C">__name__</span> <span style="color: #666666">==</span> <span style="color: #BA2121">&quot;__main__&quot;</span>:
    <span style="color: #408080; font-style: italic"># Hyperparameters can be tweaked as you like</span>
    seq_len <span style="color: #666666">=</span> <span style="color: #666666">50</span>
    DeltaT <span style="color: #666666">=</span> <span style="color: #666666">0.001</span>
    tfinal <span style="color: #666666">=</span> <span style="color: #666666">20.0</span>
    num_epochs <span style="color: #666666">=</span> <span style="color: #666666">10</span>
    batch_size <span style="color: #666666">=</span> <span style="color: #666666">64</span>
    hidden_size <span style="color: #666666">=</span> <span style="color: #666666">64</span>
    lr <span style="color: #666666">=</span> <span style="color: #666666">1e-3</span>

    model, dataset <span style="color: #666666">=</span> train_model(
        seq_len<span style="color: #666666">=</span>seq_len,
        DeltaT<span style="color: #666666">=</span>DeltaT,
        tfinal<span style="color: #666666">=</span>tfinal,
        batch_size<span style="color: #666666">=</span>batch_size,
        num_epochs<span style="color: #666666">=</span>num_epochs,
        hidden_size<span style="color: #666666">=</span>hidden_size,
        lr<span style="color: #666666">=</span>lr,
    )

    evaluate_and_plot(model, dataset, seq_len<span style="color: #666666">=</span>seq_len)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<!-- ------------------- end of main content --------------- -->
<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2025, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</body>
</html>

