<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week47.do.txt --pygments_html_style=default --html_style=bloodish --html_links_in_new_window --html_output=week47 --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 47: Recurrent neural networks and Autoencoders">
<title>Week 47: Recurrent neural networks and Autoencoders</title>
<style type="text/css">
/* bloodish style */
body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em; color: #8A0808; }
h2 { font-size: 1.6em; color: #8A0808; }
h3 { font-size: 1.4em; color: #8A0808; }
h4 { font-size: 1.2em; color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa; }div.highlight {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    line-height: 1.21429em;
}
div.cell {
    width: 100%;
    padding: 5px 5px 5px 0;
    margin: 0;
    outline: none;
}
div.input {
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.inner_cell {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
    box-flex: 1;
    flex: 1;
}
div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 4px;
    background: #f7f7f7;
    line-height: 1.21429em;
}
div.input_area > div.highlight {
    margin: .4em;
    border: none;
    padding: 0;
    background-color: transparent;
}
div.output_wrapper {
    position: relative;
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
.output {
    box-orient: vertical;
    box-align: stretch;
    display: flex;
    flex-direction: column;
    align-items: stretch;
}
div.output_area {
    padding: 0;
    page-break-inside: avoid;
    box-orient: horizontal;
    box-align: stretch;
    display: flex;
    flex-direction: row;
    align-items: stretch;
}
div.output_subarea {
    padding: .4em .4em 0 .4em;
    box-flex: 1;
    flex: 1;
}
div.output_text {
    text-align: left;
    color: #000;
    line-height: 1.21429em;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #bababa;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #f8f8f8;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/doconce/doconce/master/bundled/html_images/small_gray_question.png); }
div { text-align: justify; text-justify: inter-word; }
.tab {
  padding-left: 1.5em;
}
div.toc p,a {
  line-height: 1.3;
  margin-top: 1.1;
  margin-bottom: 1.1;
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Plan for week 47', 2, None, 'plan-for-week-47'),
              ('Reading recommendations RNNs',
               2,
               None,
               'reading-recommendations-rnns'),
              ('TensorFlow examples', 2, None, 'tensorflow-examples'),
              ('Reading recommendations: Autoencoders (AE)',
               2,
               None,
               'reading-recommendations-autoencoders-ae'),
              ('What is a recurrent NN?', 2, None, 'what-is-a-recurrent-nn'),
              ('Why RNNs?', 2, None, 'why-rnns'),
              ('More whys', 2, None, 'more-whys'),
              ('RNNs in more detail', 2, None, 'rnns-in-more-detail'),
              ('RNNs in more detail, part 2',
               2,
               None,
               'rnns-in-more-detail-part-2'),
              ('RNNs in more detail, part 3',
               2,
               None,
               'rnns-in-more-detail-part-3'),
              ('RNNs in more detail, part 4',
               2,
               None,
               'rnns-in-more-detail-part-4'),
              ('RNNs in more detail, part 5',
               2,
               None,
               'rnns-in-more-detail-part-5'),
              ('RNNs in more detail, part 6',
               2,
               None,
               'rnns-in-more-detail-part-6'),
              ('RNNs in more detail, part 7',
               2,
               None,
               'rnns-in-more-detail-part-7'),
              ('RNN Forward Pass Equations',
               2,
               None,
               'rnn-forward-pass-equations'),
              ('Unrolled RNN in Time', 2, None, 'unrolled-rnn-in-time'),
              ('Example Task: Character-level RNN Classification',
               2,
               None,
               'example-task-character-level-rnn-classification'),
              ('PyTorch: Defining a Simple RNN, using Tensorflow',
               2,
               None,
               'pytorch-defining-a-simple-rnn-using-tensorflow'),
              ('Similar example using PyTorch',
               2,
               None,
               'similar-example-using-pytorch'),
              ('Backpropagation Through Time (BPTT) and Gradients',
               2,
               None,
               'backpropagation-through-time-bptt-and-gradients'),
              ('Truncated BPTT and Gradient Clipping',
               2,
               None,
               'truncated-bptt-and-gradient-clipping'),
              ('Applications of Simple RNNs',
               2,
               None,
               'applications-of-simple-rnns'),
              ('Sequence Modeling Tasks', 2, None, 'sequence-modeling-tasks'),
              ('Other Sequence Applications',
               2,
               None,
               'other-sequence-applications'),
              ('Training and Practical Tips',
               2,
               None,
               'training-and-practical-tips'),
              ('Limitations and Considerations',
               2,
               None,
               'limitations-and-considerations'),
              ('PyTorch RNN Time Series Example',
               2,
               None,
               'pytorch-rnn-time-series-example'),
              ('Tensorflow (Keras) RNN Time Series Example',
               2,
               None,
               'tensorflow-keras-rnn-time-series-example'),
              ('The mathematics of RNNs, the basic architecture',
               2,
               None,
               'the-mathematics-of-rnns-the-basic-architecture'),
              ('Gating mechanism: Long Short Term Memory (LSTM)',
               2,
               None,
               'gating-mechanism-long-short-term-memory-lstm'),
              ('Implementing a memory cell in a neural network',
               2,
               None,
               'implementing-a-memory-cell-in-a-neural-network'),
              ('LSTM details', 2, None, 'lstm-details'),
              ('Basic layout (All figures from Raschka *et al.,*)',
               2,
               None,
               'basic-layout-all-figures-from-raschka-et-al'),
              ('LSTM details', 2, None, 'lstm-details'),
              ('Comparing with a standard  RNN',
               2,
               None,
               'comparing-with-a-standard-rnn'),
              ('LSTM details I', 2, None, 'lstm-details-i'),
              ('LSTM details II', 2, None, 'lstm-details-ii'),
              ('LSTM details III', 2, None, 'lstm-details-iii'),
              ('Forget gate', 2, None, 'forget-gate'),
              ('The forget gate', 2, None, 'the-forget-gate'),
              ('Basic layout', 2, None, 'basic-layout'),
              ('Input gate', 2, None, 'input-gate'),
              ('Short summary', 2, None, 'short-summary'),
              ('Forget and input', 2, None, 'forget-and-input'),
              ('Basic layout', 2, None, 'basic-layout'),
              ('Output gate', 2, None, 'output-gate'),
              ('Summary of LSTM', 2, None, 'summary-of-lstm'),
              ('LSTM implementation using TensorFlow',
               2,
               None,
               'lstm-implementation-using-tensorflow'),
              ('And the corresponding one with PyTorch',
               2,
               None,
               'and-the-corresponding-one-with-pytorch'),
              ('Dynamical ordinary differential equation',
               2,
               None,
               'dynamical-ordinary-differential-equation'),
              ('The Runge-Kutta-4 code', 2, None, 'the-runge-kutta-4-code'),
              ('Using the above data to train an RNN',
               2,
               None,
               'using-the-above-data-to-train-an-rnn'),
              ('Similar code using PyTorch',
               2,
               None,
               'similar-code-using-pytorch'),
              ('Autoencoders: Overarching view',
               2,
               None,
               'autoencoders-overarching-view'),
              ('Powerful detectors', 2, None, 'powerful-detectors'),
              ('First introduction of AEs',
               2,
               None,
               'first-introduction-of-aes'),
              ('Autoencoder structure', 2, None, 'autoencoder-structure'),
              ('Schematic image of an Autoencoder',
               2,
               None,
               'schematic-image-of-an-autoencoder'),
              ('More on the structure', 2, None, 'more-on-the-structure'),
              ('Decoder part', 2, None, 'decoder-part'),
              ('Typical AEs', 2, None, 'typical-aes'),
              ('Feed Forward Autoencoder', 2, None, 'feed-forward-autoencoder'),
              ('Mirroring', 2, None, 'mirroring'),
              ('Output of middle layer', 2, None, 'output-of-middle-layer'),
              ('Activation Function of the Output Layer',
               2,
               None,
               'activation-function-of-the-output-layer'),
              ('ReLU', 2, None, 'relu'),
              ('Sigmoid', 2, None, 'sigmoid'),
              ('Cost/Loss Function', 2, None, 'cost-loss-function'),
              ('Binary Cross-Entropy', 2, None, 'binary-cross-entropy'),
              ('Reconstruction Error', 2, None, 'reconstruction-error'),
              ('Implementation using TensorFlow',
               2,
               None,
               'implementation-using-tensorflow'),
              ('Implementation using PyTorch',
               2,
               None,
               'implementation-using-pytorch'),
              ('Dimensionality reduction and links with Principal component '
               'analysis',
               2,
               None,
               'dimensionality-reduction-and-links-with-principal-component-analysis'),
              ('Linear functions', 2, None, 'linear-functions'),
              ('AE mean-squared error', 2, None, 'ae-mean-squared-error'),
              ('Dimensionality reduction',
               2,
               None,
               'dimensionality-reduction')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- ------------------- main content ---------------------- -->
<center>
<h1>Week 47: Recurrent neural networks and Autoencoders</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> 
</center>
<!-- institution -->
<center>
<b>Department of Physics, University of Oslo, Norway</b>
</center>
<br>
<center>
<h4>November 17-21, 2025</h4>
</center> <!-- date -->
<br>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="plan-for-week-47">Plan for week 47 </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Plans for the lecture Monday 17 November, with video suggestions etc</b>
<p>
<ol>
<li> Recurrent neural networks, code examples and long-short-term memory</li>
<li> Autoencoders (last topic this semester)</li>
<li> Last lecture: November 24, note error in time planner.
<!-- o Video of lecture at <a href="https://youtu.be/RIHzmLv05DA" target="_blank"><tt>https://youtu.be/RIHzmLv05DA</tt></a> -->
<!-- o Whiteboard notes at <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2024/NotesNovember18.pdf" target="_blank"><tt>https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2024/NotesNovember18.pdf</tt></a> -->
<!-- * <a href="https://youtu.be/SpWXsvn5I9E" target="_blank">Video of Lecture</a> --></li>
</ol>
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b>Lab sessions on Tuesday and Wednesday</b>
<p>
<ol>
<li> Work and Discussion of project 3</li>
<li> Last weekly exercise with deadline November 28, available from (early morning) Tuesday November 18.</li>
<li> Last lab sessions: November 25 and 26</li>
</ol>
</div>
  

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="reading-recommendations-rnns">Reading recommendations RNNs </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> These lecture notes at <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week47/ipynb/week47.ipynb" target="_blank"><tt>https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week47/ipynb/week47.ipynb</tt></a></li>
<li> See also lecture notes from week 46 at <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week46/ipynb/week46.ipynb" target="_blank"><tt>https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week46/ipynb/week46.ipynb</tt></a>. The lecture on Monday starts with a repetition on recurrent neural networks. The second lecture starts with basics of autoenconders.</li>
<li> For RNNs, see Goodfellow et al chapter 10, see <a href="https://www.deeplearningbook.org/contents/rnn.html" target="_blank"><tt>https://www.deeplearningbook.org/contents/rnn.html</tt></a>.</li>
<li> Reading suggestions for implementation of RNNs in PyTorch: see Rashcka et al.'s chapter 15 and GitHub site at <a href="https://github.com/rasbt/machine-learning-book/tree/main/ch15" target="_blank"><tt>https://github.com/rasbt/machine-learning-book/tree/main/ch15</tt></a>.</li>
<li> RNN video at <a href="https://youtu.be/PCgrgHgy26c?feature=shared" target="_blank"><tt>https://youtu.be/PCgrgHgy26c?feature=shared</tt></a></li>
<li> New xLSTM, see Beck et al <a href="https://arxiv.org/abs/2405.04517" target="_blank"><tt>https://arxiv.org/abs/2405.04517</tt></a>. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="tensorflow-examples">TensorFlow examples </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>For TensorFlow (using Keras) implementations, we recommend</p>
<ol>
<li> David Foster, Generative Deep Learning with TensorFlow, see chapter 5 at <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html" target="_blank"><tt>https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html</tt></a></li>
<li> Joseph Babcock and Raghav Bali Generative AI with Python and their GitHub link, chapters 2 and  3 at <a href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2" target="_blank"><tt>https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2</tt></a></li>  
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="reading-recommendations-autoencoders-ae">Reading recommendations: Autoencoders (AE) </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> Goodfellow et al chapter 14, see <a href="https://www.deeplearningbook.org/contents/autoencoders.html" target="_blank"><tt>https://www.deeplearningbook.org/contents/autoencoders.html</tt></a></li>
<li> Rashcka et al. Their chapter 17 contains a brief introduction only.</li>
<li> Deep Learning Tutorial on AEs from Stanford University at <a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/" target="_blank"><tt>http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/</tt></a></li>
<li> Building AEs in Keras at <a href="https://blog.keras.io/building-autoencoders-in-keras.html" target="_blank"><tt>https://blog.keras.io/building-autoencoders-in-keras.html</tt></a></li>
<li> Introduction to AEs in TensorFlow at <a href="https://www.tensorflow.org/tutorials/generative/autoencoder" target="_blank"><tt>https://www.tensorflow.org/tutorials/generative/autoencoder</tt></a></li>
<li> Grosse, University of Toronto, Lecture on AEs at <a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec20.pdf" target="_blank"><tt>http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec20.pdf</tt></a></li>
<li> Bank et al on AEs at <a href="https://arxiv.org/abs/2003.05991" target="_blank"><tt>https://arxiv.org/abs/2003.05991</tt></a></li>  
<li> Baldi and Hornik, Neural networks and principal component analysis: Learning from examples without local minima, Neural Networks 2, 53 (1989)</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="what-is-a-recurrent-nn">What is a recurrent NN? </h2>

<p>A recurrent neural network (RNN), as opposed to a regular fully
connected neural network (FCNN) or just neural network (NN), has
layers that are connected to themselves.
</p>

<p>In an FCNN there are no connections between nodes in a single
layer. For instance, \( (h_1^1 \) is not connected to \( (h_2^1 \). In
addition, the input and output are always of a fixed length.
</p>

<p>In an RNN, however, this is no longer the case. Nodes in the hidden
layers are connected to themselves.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="why-rnns">Why RNNs? </h2>

<p>Recurrent neural networks work very well when working with
sequential data, that is data where the order matters. In a regular
fully connected network, the order of input doesn't really matter.
</p>

<p>Another property of  RNNs is that they can handle variable input
and output. Consider again the simplified breast cancer dataset. If you
have trained a regular FCNN on the dataset with the two features, it
makes no sense to suddenly add a third feature. The network would not
know what to do with it, and would reject such inputs with three
features (or any other number of features that isn't two, for that
matter).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-whys">More whys </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> Traditional feedforward networks process fixed-size inputs and ignore temporal order. RNNs incorporate recurrence to handle sequential data like time series or language &#65532;.</li>
<li> At each time step, an RNN cell processes input x_t and a hidden state h_{t-1} from the previous step, producing a new hidden state h_t and (optionally) an output y_t.</li>
<li> This hidden state acts as a &#8220;memory&#8221; carrying information forward. For example, predicting stock prices or words in a sentence relies on past inputs &#65532; &#65532;.</li>
<li> RNNs share parameters across time steps, so they can generalize patterns regardless of sequence length &#65532;.</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail">RNNs in more detail  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN2.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-2">RNNs in more detail, part 2  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN3.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-3">RNNs in more detail, part 3  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN4.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-4">RNNs in more detail, part 4  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN5.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-5">RNNs in more detail, part 5  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN6.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-6">RNNs in more detail, part 6  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN7.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnns-in-more-detail-part-7">RNNs in more detail, part 7  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN8.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="rnn-forward-pass-equations">RNN Forward Pass Equations </h2>

<p>For a simple (vanilla) RNN with one hidden layer and no bias, the state update and output are:</p>
$$
\mathbf{h}_t = \sigma(\mathbf{W}_{xh}\mathbf{x}_t + \mathbf{W}_{hh}\mathbf{h}_{t-1})\,,\quad \mathbf{y}_t = \mathbf{W}_{yh}\mathbf{h}_t,
$$

<p>where \( \sigma \) is an activation (e.g. tanh or ReLU) &#65532;.</p>

<p>In matrix form,</p>
$$
\mathbf{W}_{xh}\in\mathbb{R}^{h\times d}, \mathbf{W}_{hh}\in\mathbb{R}^{h\times h}, \mathbf{W}_{yh}\in\mathbb{R}^{q\times h},
$$

<p>for input dimension  \( d \), hidden dimension \( h \), output dimension \( q \).</p>

<p>Because the same \( \mathbf{W} \) are used each step, gradients during training will propagate through time.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="unrolled-rnn-in-time">Unrolled RNN in Time </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> Input \( x_1,x_2,x_3,\dots \) feed sequentially; the hidden state flows from one step to the next, capturing past context.</li>
<li> After processing the final input \( x_T \), the network can make a prediction (many-to-one) or outputs can be produced at each step (many-to-many).</li>
<li> Unrolling clarifies that training an RNN is like training a deep feedforward network of depth T, with recurrent connections tying layers together.</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="example-task-character-level-rnn-classification">Example Task: Character-level RNN Classification </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> A classic example: feed a name (sequence of characters) one char at a time, and classify its language of origin.</li>
<li> At each step, the RNN outputs a hidden state; we use the final hidden state to predict the class of the entire sequence.</li>
<li> A character-level RNN reads words as a series of characters&#8212;outputting a prediction and &#8216;hidden state&#8217; at each step, feeding the previous hidden state into the next step. We take the final prediction to be the output&#8221; &#65532;.</li>
<li> This illustrates sequence-to-one modeling: every output depends on all previous inputs.</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="pytorch-defining-a-simple-rnn-using-tensorflow">PyTorch: Defining a Simple RNN, using Tensorflow </h2>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>

<span style="color: #408080; font-style: italic"># -----------------------</span>
<span style="color: #408080; font-style: italic"># 1. Hyperparameters</span>
<span style="color: #408080; font-style: italic"># -----------------------</span>
input_size <span style="color: #666666">=</span> <span style="color: #666666">10</span>        <span style="color: #408080; font-style: italic"># Dimensionality of each time step</span>
hidden_size <span style="color: #666666">=</span> <span style="color: #666666">20</span>       <span style="color: #408080; font-style: italic"># Number of recurrent units</span>
num_classes <span style="color: #666666">=</span> <span style="color: #666666">2</span>        <span style="color: #408080; font-style: italic"># Binary classification</span>
sequence_length <span style="color: #666666">=</span> <span style="color: #666666">5</span>     <span style="color: #408080; font-style: italic"># Sequence length</span>
batch_size <span style="color: #666666">=</span> <span style="color: #666666">16</span>

<span style="color: #408080; font-style: italic"># -----------------------</span>
<span style="color: #408080; font-style: italic"># 2. Dummy dataset</span>
<span style="color: #408080; font-style: italic">#    X: [batch, seq, features]</span>
<span style="color: #408080; font-style: italic">#    y: [batch]</span>
<span style="color: #408080; font-style: italic"># -----------------------</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(batch_size, sequence_length, input_size)<span style="color: #666666">.</span>astype(np<span style="color: #666666">.</span>float32)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randint(<span style="color: #666666">0</span>, num_classes, size<span style="color: #666666">=</span>(batch_size,))

<span style="color: #408080; font-style: italic"># -----------------------</span>
<span style="color: #408080; font-style: italic"># 3. Build simple RNN model</span>
<span style="color: #408080; font-style: italic"># -----------------------</span>
model <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>Sequential([
    tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>layers<span style="color: #666666">.</span>SimpleRNN(
        units<span style="color: #666666">=</span>hidden_size,
        activation<span style="color: #666666">=</span><span style="color: #BA2121">&quot;tanh&quot;</span>,
        return_sequences<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,   <span style="color: #408080; font-style: italic"># Only final hidden state</span>
        input_shape<span style="color: #666666">=</span>(sequence_length, input_size)
    ),
    tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>layers<span style="color: #666666">.</span>Dense(num_classes)
])

model<span style="color: #666666">.</span>compile(
    optimizer<span style="color: #666666">=</span>tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>optimizers<span style="color: #666666">.</span>Adam(<span style="color: #666666">1e-3</span>),
    loss<span style="color: #666666">=</span>tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>losses<span style="color: #666666">.</span>SparseCategoricalCrossentropy(from_logits<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>),
    metrics<span style="color: #666666">=</span>[<span style="color: #BA2121">&quot;accuracy&quot;</span>]
)

<span style="color: #408080; font-style: italic"># -----------------------</span>
<span style="color: #408080; font-style: italic"># 4. Train the model</span>
<span style="color: #408080; font-style: italic"># -----------------------</span>
history <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(
    X, y,
    epochs<span style="color: #666666">=5</span>,
    batch_size<span style="color: #666666">=</span>batch_size,
    verbose<span style="color: #666666">=1</span>
)

<span style="color: #408080; font-style: italic"># -----------------------</span>
<span style="color: #408080; font-style: italic"># 5. Evaluate</span>
<span style="color: #408080; font-style: italic"># -----------------------</span>
logits <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(X)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Logits from model:</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&quot;</span>, logits)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>This recurrent neural network uses the TensorFlow/Keras SimpleRNN, which is the counterpart to PyTorch&#8217;s nn.RNN.
In this code we have used
</p>
<ol>
<li> return_sequences=False makes it output only the last hidden state, which is fed to the classifier. Also, we have</li>
<li> from_logits=True matches the PyTorch CrossEntropyLoss.</li>
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="similar-example-using-pytorch">Similar example using PyTorch </h2>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.nn</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">nn</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.optim</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">optim</span>

<span style="color: #408080; font-style: italic"># -----------------------</span>
<span style="color: #408080; font-style: italic"># 1. Hyperparameters</span>
<span style="color: #408080; font-style: italic"># -----------------------</span>
input_size <span style="color: #666666">=</span> <span style="color: #666666">10</span>
hidden_size <span style="color: #666666">=</span> <span style="color: #666666">20</span>
num_layers <span style="color: #666666">=</span> <span style="color: #666666">1</span>
num_classes <span style="color: #666666">=</span> <span style="color: #666666">2</span>
sequence_length <span style="color: #666666">=</span> <span style="color: #666666">5</span>
batch_size <span style="color: #666666">=</span> <span style="color: #666666">16</span>
lr <span style="color: #666666">=</span> <span style="color: #666666">1e-3</span>

<span style="color: #408080; font-style: italic"># -----------------------</span>
<span style="color: #408080; font-style: italic"># 2. Dummy dataset</span>
<span style="color: #408080; font-style: italic"># -----------------------</span>
X <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>randn(batch_size, sequence_length, input_size)
y <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>randint(<span style="color: #666666">0</span>, num_classes, (batch_size,))

<span style="color: #408080; font-style: italic"># -----------------------</span>
<span style="color: #408080; font-style: italic"># 3. Simple RNN model</span>
<span style="color: #408080; font-style: italic"># -----------------------</span>
<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">SimpleRNN</span>(nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, input_size, hidden_size, num_layers, num_classes):
        <span style="color: #008000">super</span>(SimpleRNN, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #008000">self</span><span style="color: #666666">.</span>rnn <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>RNN(
            input_size<span style="color: #666666">=</span>input_size,
            hidden_size<span style="color: #666666">=</span>hidden_size,
            num_layers<span style="color: #666666">=</span>num_layers,
            batch_first<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,
            nonlinearity<span style="color: #666666">=</span><span style="color: #BA2121">&quot;tanh&quot;</span>
        )
        <span style="color: #008000">self</span><span style="color: #666666">.</span>fc <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>Linear(hidden_size, num_classes)

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        out, h_n <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>rnn(x)   <span style="color: #408080; font-style: italic"># out: [batch, seq, hidden]</span>

        <span style="color: #408080; font-style: italic"># ---- FIX: take only the last time-step tensor ----</span>
        last_hidden <span style="color: #666666">=</span> out[:, <span style="color: #666666">-1</span>, :]  <span style="color: #408080; font-style: italic"># [batch, hidden]</span>

        logits <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>fc(last_hidden)
        <span style="color: #008000; font-weight: bold">return</span> logits

model <span style="color: #666666">=</span> SimpleRNN(input_size, hidden_size, num_layers, num_classes)

criterion <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>CrossEntropyLoss()
optimizer <span style="color: #666666">=</span> optim<span style="color: #666666">.</span>Adam(model<span style="color: #666666">.</span>parameters(), lr<span style="color: #666666">=</span>lr)

<span style="color: #408080; font-style: italic"># -----------------------</span>
<span style="color: #408080; font-style: italic"># 4. Training step</span>
<span style="color: #408080; font-style: italic"># -----------------------</span>
model<span style="color: #666666">.</span>train()
optimizer<span style="color: #666666">.</span>zero_grad()

logits <span style="color: #666666">=</span> model(X)
loss <span style="color: #666666">=</span> criterion(logits, y)
loss<span style="color: #666666">.</span>backward()
optimizer<span style="color: #666666">.</span>step()

<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss<span style="color: #666666">.</span>item()<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="backpropagation-through-time-bptt-and-gradients">Backpropagation Through Time (BPTT) and Gradients </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Backpropagation Through Time (BPTT)</b>
<p>
<ol>
<li> Training an RNN involves computing gradients through time by unfolding the network: treat the unrolled RNN as a very deep feedforward net.</li>
<li> We compute the loss \( L = \frac{1}{T}\sum_{t=1}^T \ell(y_t,\hat y_t) \) and backpropagate from \( t=T \) down to \( t=1. \)</li>
<li> The computational graphs in the figures below shows how each hidden state depends on inputs and parameters across time &#65532;.</li>
<li> BPTT applies the chain rule along this graph, accumulating gradients from each time step into the shared parameters.</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="truncated-bptt-and-gradient-clipping">Truncated BPTT and Gradient Clipping </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> Truncated BPTT: Instead of backpropagating through all T steps, we may backpropagate through a fixed window of length \( \tau \). This approximates the full gradient and reduces computation.</li>
<li> Concretely, one computes gradients up to \( \tau \) steps and treats gradients beyond as zero. This still allows learning short-term patterns efficiently.</li>
<li> Gradient Clipping: Cap the gradient norm to a maximum value to prevent explosion. For example in PyTorch:</li>
</ol>
<p>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) ensures \( \|\nabla\|\le 1 \).</p>
<ol>
<li> These techniques help stabilize training, but the fundamental vanishing problem motivates using alternative RNN cells (LSTM/GRU) in practice (see below).</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="applications-of-simple-rnns">Applications of Simple RNNs </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> Forecasting: RNNs can predict future values from historical data. Example tasks include stock prices, weather patterns, or any temporal signal &#65532;.</li>
<li> By feeding in sequence \( \{x_1,x_2,\dots,x_T\} \), an RNN can output a prediction \( y_T \) (one-step ahead) or even a full sequence \( \{y_2,\dots,y_{T+1}\} \).</li>
<li> Unlike linear models, RNNs can capture complex temporal patterns (trends, seasonality, autocorrelation) in a data-driven way &#65532;.</li>
<li> Preprocessing (normalization, sliding windows) is important. Split data into train/test by time (no shuffling).</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="sequence-modeling-tasks">Sequence Modeling Tasks </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> Many-to-One: Classify or predict one value from an entire sequence (e.g., sentiment analysis of a movie review, or classifying a time series). We use the final hidden state as a summary of the sequence.</li>
<li> Many-to-Many (Prediction): Predict an output at each time step (e.g., language modeling or sequential regression). RNN outputs are used at each step.</li>
<li> Encoder&#8211;Decoder (Seq2Seq): (Advanced) Map input sequences to output sequences of different lengths. Though typically LSTM-based, it is s conceptually possible with simple RNNs.</li>
<li> RNNs also apply to physics and biology: e.g., modeling dynamical systems, protein sequences, or neuroscience time series. Any domain with sequential data can use RNN-based modeling.</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="other-sequence-applications">Other Sequence Applications </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> Sequence Classification: Use RNN hidden state for class labels. For example, classify a time series into anomaly vs normal.</li>
<li> Sequence Labeling: Predict labels at each time step (e.g. part-of-speech tagging). The RNN outputs a vector at each step passed through a classification layer.</li>
<li> Language and Text: (Advanced) Character or word-level models use RNNs to generate text or classify documents. E.g., predicting next character from previous ones (RNN language model) &#65532;.</li>
<li> Physically Motivated Data: RNNs can model dynamical systems (e.g., rolling ball trajectories, neuron spikes over time, climate data). They learn temporal patterns directly from data without explicit equations.</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="training-and-practical-tips">Training and Practical Tips </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> Loss Functions: Use MSE for regression tasks, cross-entropy for classification tasks. Sum or average losses over time steps as needed.</li>
<li> Batching Sequences: Handle variable-length sequences by padding or using masking. PyTorch pack_padded_sequence or Keras masking can help.</li>
<li> Optimization: Standard optimizers (SGD, Adam) work. Learning rate may need tuning due to sequential correlations.</li>
<li> Initial Hidden State: Usually initialized to zeros. Can also learn an initial state or carry hidden state across batches for very long sequences (stateful=True in Keras).</li>
<li> Regularization: Dropout can be applied to inputs or recurrent states (PyTorch/RNN has dropout option; Keras has dropout/recurrent_dropout).</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="limitations-and-considerations">Limitations and Considerations </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<li> Vanishing Gradients: Simple RNNs have fundamental difficulty learning long-term dependencies due to gradient decay &#65532;.</li>
<li> Capacity: Without gates, RNNs may struggle with tasks requiring remembering far-back inputs. Training can be slow as it&#8217;s inherently sequential.</li>
<li> Alternatives: In practice, gated RNNs (LSTM/GRU) or Transformers are often used for long-range dependencies. However, simple RNNs are still instructive and sometimes sufficient for short sequences &#65532; &#65532;.</li>
<li> Regularization: Weight decay or dropout (on inputs/states) can help generalization but must be applied carefully due to temporal correlations.</li>
<li> Statefulness: For very long sequences, one can preserve hidden state across batches (stateful RNN) to avoid resetting memory.</li>
</ol>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="pytorch-rnn-time-series-example">PyTorch RNN Time Series Example </h2>

<p>We first implement a simple RNN in PyTorch to forecast a univariate
time series (a sine wave). The steps are: (1) generate synthetic data
and form input/output sequences; (2) define an nn.RNN model; (3) train
the model with MSE loss and an optimizer; (4) evaluate on a held-out
test set. For example, using a sine wave as in prior tutorials &#65532;, we
create sliding windows of length seq_length. The code below shows each
step. We use nn.RNN (the basic recurrent layer) followed by a linear
output. The training loop (with MSELoss and Adam) updates the model to
minimize prediction error &#65532;.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch</span> <span style="color: #008000; font-weight: bold">import</span> nn, optim

<span style="color: #408080; font-style: italic"># 1. Data preparation: generate a sine wave and create input-output sequences</span>
time_steps <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>, <span style="color: #666666">100</span>, <span style="color: #666666">500</span>)
data <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sin(time_steps)                   <span style="color: #408080; font-style: italic"># shape (500,)</span>
seq_length <span style="color: #666666">=</span> <span style="color: #666666">20</span>
X, y <span style="color: #666666">=</span> [], []
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(data) <span style="color: #666666">-</span> seq_length):
    X<span style="color: #666666">.</span>append(data[i:i<span style="color: #666666">+</span>seq_length])         <span style="color: #408080; font-style: italic"># sequence of length seq_length</span>
    y<span style="color: #666666">.</span>append(data[i<span style="color: #666666">+</span>seq_length])           <span style="color: #408080; font-style: italic"># next value to predict</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(X)                            <span style="color: #408080; font-style: italic"># shape (480, seq_length)</span>
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(y)                            <span style="color: #408080; font-style: italic"># shape (480,)</span>
<span style="color: #408080; font-style: italic"># Add feature dimension (1) for the RNN input</span>
X <span style="color: #666666">=</span> X[<span style="color: #666666">...</span>, <span style="color: #008000; font-weight: bold">None</span>]                           <span style="color: #408080; font-style: italic"># shape (480, seq_length, 1)</span>
y <span style="color: #666666">=</span> y[<span style="color: #666666">...</span>, <span style="color: #008000; font-weight: bold">None</span>]                           <span style="color: #408080; font-style: italic"># shape (480, 1)</span>

<span style="color: #408080; font-style: italic"># Split into train/test sets (80/20 split)</span>
train_size <span style="color: #666666">=</span> <span style="color: #008000">int</span>(<span style="color: #666666">0.8</span> <span style="color: #666666">*</span> <span style="color: #008000">len</span>(X))
X_train <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(X[:train_size], dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)
y_train <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(y[:train_size], dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)
X_test  <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(X[train_size:],  dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)
y_test  <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(y[train_size:],  dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)

<span style="color: #408080; font-style: italic"># 2. Model definition: simple RNN followed by a linear layer</span>
<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">SimpleRNNModel</span>(nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, input_size<span style="color: #666666">=1</span>, hidden_size<span style="color: #666666">=16</span>, num_layers<span style="color: #666666">=1</span>):
        <span style="color: #008000">super</span>(SimpleRNNModel, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #408080; font-style: italic"># nn.RNN for sequential data (batch_first=True expects (batch, seq_len, features))</span>
        <span style="color: #008000">self</span><span style="color: #666666">.</span>rnn <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>RNN(input_size, hidden_size, num_layers, batch_first<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>fc <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>Linear(hidden_size, <span style="color: #666666">1</span>)    <span style="color: #408080; font-style: italic"># output layer for prediction</span>

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        out, _ <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>rnn(x)                 <span style="color: #408080; font-style: italic"># out: (batch, seq_len, hidden_size)</span>
        out <span style="color: #666666">=</span> out[:, <span style="color: #666666">-1</span>, :]                  <span style="color: #408080; font-style: italic"># take output of last time step</span>
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>fc(out)                 <span style="color: #408080; font-style: italic"># linear layer to 1D output</span>

model <span style="color: #666666">=</span> SimpleRNNModel(input_size<span style="color: #666666">=1</span>, hidden_size<span style="color: #666666">=16</span>, num_layers<span style="color: #666666">=1</span>)
<span style="color: #008000">print</span>(model)  <span style="color: #408080; font-style: italic"># print model summary (structure)</span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Model Explanation: Here input$\_$size=1 because each time step has one
feature. The RNN hidden state has size 16, and batch$\_$first=True means
input tensors have shape (batch, seq$\_$len, features). We take the last
RNN output and feed it through a linear layer to predict the next
value .
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># 3. Training loop: MSE loss and Adam optimizer</span>
criterion <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>MSELoss()                  <span style="color: #408080; font-style: italic"># mean squared error loss</span>
optimizer <span style="color: #666666">=</span> optim<span style="color: #666666">.</span>Adam(model<span style="color: #666666">.</span>parameters(), lr<span style="color: #666666">=0.01</span>)

epochs <span style="color: #666666">=</span> <span style="color: #666666">50</span>
<span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">1</span>, epochs<span style="color: #666666">+1</span>):
    model<span style="color: #666666">.</span>train()
    optimizer<span style="color: #666666">.</span>zero_grad()
    output <span style="color: #666666">=</span> model(X_train)               <span style="color: #408080; font-style: italic"># forward pass</span>
    loss <span style="color: #666666">=</span> criterion(output, y_train)     <span style="color: #408080; font-style: italic"># compute training loss</span>
    loss<span style="color: #666666">.</span>backward()                       <span style="color: #408080; font-style: italic"># backpropagate</span>
    optimizer<span style="color: #666666">.</span>step()                      <span style="color: #408080; font-style: italic"># update weights</span>
    <span style="color: #008000; font-weight: bold">if</span> epoch <span style="color: #666666">%</span> <span style="color: #666666">10</span> <span style="color: #666666">==</span> <span style="color: #666666">0</span>:
        <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Epoch </span><span style="color: #BB6688; font-weight: bold">{</span>epoch<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">/</span><span style="color: #BB6688; font-weight: bold">{</span>epochs<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">, Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss<span style="color: #666666">.</span>item()<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Training Details: We train for 50 epochs, printing the training loss
every 10 epochs. As training proceeds, the loss (MSE) typically
decreases, indicating the RNN is learning the sine-wave pattern &#65532;.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># 4. Evaluation on test set</span>
model<span style="color: #666666">.</span>eval()
<span style="color: #008000; font-weight: bold">with</span> torch<span style="color: #666666">.</span>no_grad():
    pred <span style="color: #666666">=</span> model(X_test)
    test_loss <span style="color: #666666">=</span> criterion(pred, y_test)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Test Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>test_loss<span style="color: #666666">.</span>item()<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)

<span style="color: #408080; font-style: italic"># (Optional) View a few actual vs. predicted values</span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Actual:&quot;</span>, y_test[:<span style="color: #666666">5</span>]<span style="color: #666666">.</span>flatten()<span style="color: #666666">.</span>numpy())
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Pred : &quot;</span>, pred[:<span style="color: #666666">5</span>]<span style="color: #666666">.</span>flatten()<span style="color: #666666">.</span>numpy())
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Evaluation: We switch to eval mode and compute loss on the test
set. The lower test loss indicates how well the model generalizes. The
code prints a few sample predictions against actual values for
qualitative assessment. 
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="tensorflow-keras-rnn-time-series-example">Tensorflow (Keras) RNN Time Series Example </h2>

<p>Next, we use TensorFlow/Keras to do the same task. We build a
tf.keras.Sequential model with a SimpleRNN layer (the most basic
recurrent layer) &#65532; followed by a Dense output. The workflow is
similar: create the same synthetic sine data and split it into
train/test sets; then define, train, and evaluate the model.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>

<span style="color: #408080; font-style: italic"># 1. Data preparation: same sine wave data and sequences as above</span>
time_steps <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>, <span style="color: #666666">100</span>, <span style="color: #666666">500</span>)
data <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sin(time_steps)                     <span style="color: #408080; font-style: italic"># (500,)</span>
seq_length <span style="color: #666666">=</span> <span style="color: #666666">20</span>
X, y <span style="color: #666666">=</span> [], []
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(data) <span style="color: #666666">-</span> seq_length):
    X<span style="color: #666666">.</span>append(data[i:i<span style="color: #666666">+</span>seq_length])
    y<span style="color: #666666">.</span>append(data[i<span style="color: #666666">+</span>seq_length])
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(X)                               <span style="color: #408080; font-style: italic"># (480, seq_length)</span>
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(y)                               <span style="color: #408080; font-style: italic"># (480,)</span>
<span style="color: #408080; font-style: italic"># reshape for RNN: (samples, timesteps, features)</span>
X <span style="color: #666666">=</span> X<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, seq_length, <span style="color: #666666">1</span>)             <span style="color: #408080; font-style: italic"># (480, 20, 1)</span>
y <span style="color: #666666">=</span> y<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)                         <span style="color: #408080; font-style: italic"># (480, 1)</span>

<span style="color: #408080; font-style: italic"># Split into train/test (80/20)</span>
split <span style="color: #666666">=</span> <span style="color: #008000">int</span>(<span style="color: #666666">0.8</span> <span style="color: #666666">*</span> <span style="color: #008000">len</span>(X))
X_train, X_test <span style="color: #666666">=</span> X[:split], X[split:]
y_train, y_test <span style="color: #666666">=</span> y[:split], y[split:]
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Data: We use the same sine-wave sequence and sliding-window split as
in the PyTorch example &#65532;. The arrays are reshaped to (batch,
timesteps, features) for Keras.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># 2. Model definition: Keras SimpleRNN and Dense</span>
model <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>Sequential([
    tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>layers<span style="color: #666666">.</span>SimpleRNN(<span style="color: #666666">16</span>, input_shape<span style="color: #666666">=</span>(seq_length, <span style="color: #666666">1</span>)),
    tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>layers<span style="color: #666666">.</span>Dense(<span style="color: #666666">1</span>)
])
model<span style="color: #666666">.</span>compile(optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&#39;adam&#39;</span>, loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;mse&#39;</span>)   <span style="color: #408080; font-style: italic"># MSE loss and Adam optimizer</span>
model<span style="color: #666666">.</span>summary()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Explanation: Here SimpleRNN(16) creates 16 recurrent units. The model
summary shows the shapes and number of parameters. (Keras handles the
sequence dimension internally.)
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># 3. Training</span>
history <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(
    X_train, y_train,
    epochs<span style="color: #666666">=50</span>,
    batch_size<span style="color: #666666">=32</span>,
    validation_split<span style="color: #666666">=0.2</span>,    <span style="color: #408080; font-style: italic"># use 20% of train data for validation</span>
    verbose<span style="color: #666666">=1</span>
)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Training: We train for 50 epochs. The fit call also reports validation
loss (using a 20$%$ split of the training data) to monitor
generalization.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># 4. Evaluation on test set</span>
test_loss <span style="color: #666666">=</span> model<span style="color: #666666">.</span>evaluate(X_test, y_test, verbose<span style="color: #666666">=0</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Test Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>test_loss<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)

<span style="color: #408080; font-style: italic"># (Optional) Predictions</span>
predictions <span style="color: #666666">=</span> model<span style="color: #666666">.</span>predict(X_test)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Actual:&quot;</span>, y_test<span style="color: #666666">.</span>flatten()[:<span style="color: #666666">5</span>])
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Pred : &quot;</span>, predictions<span style="color: #666666">.</span>flatten()[:<span style="color: #666666">5</span>])
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Evaluation: After training, we call model.evaluate on the test set. A
low test loss indicates good forecasting accuracy. We also predict and
compare a few samples of actual vs. predicted values. This completes
the simple RNN forecasting example in TensorFlow.
</p>

<p>Both examples use only basic RNN cells (no LSTM/GRU) and include data
preparation, model definition, training loop, and evaluation. The
PyTorch code uses nn.RNN as and the Keras
code uses SimpleRNN layer. Each code block above is self-contained
and can be run independently with standard libraries (NumPy, PyTorch
or TensorFlow).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-mathematics-of-rnns-the-basic-architecture">The mathematics of RNNs, the basic architecture  </h2>

<p>See notebook at <a href="https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week7/ipynb/rnnmath.ipynb" target="_blank"><tt>https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week7/ipynb/rnnmath.ipynb</tt></a></p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="gating-mechanism-long-short-term-memory-lstm">Gating mechanism: Long Short Term Memory (LSTM) </h2>

<p>Besides a simple recurrent neural network layer, as discussed above, there are two other
commonly used types of recurrent neural network layers: Long Short
Term Memory (LSTM) and Gated Recurrent Unit (GRU).  For a short
introduction to these layers see <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_blank"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>
and <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_blank"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>.
</p>

<p>LSTM uses a memory cell for 
modeling long-range dependencies and avoid vanishing gradient
 problems.
Capable of modeling longer term dependencies by having
memory cells and gates that controls the information flow along
with the memory cells.
</p>

<ol>
<li> Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).</li>
<li> They designed a memory cell using logistic and linear units with multiplicative interactions.</li>
<li> Information gets into the cell whenever its &#8220;write&#8221; gate is on.</li>
<li> The information stays in the cell so long as its <b>keep</b> gate is on.</li>
<li> Information can be read from the cell by turning on its <b>read</b> gate.</li> 
</ol>
<p>The LSTM were first introduced to overcome the vanishing gradient problem.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="implementing-a-memory-cell-in-a-neural-network">Implementing a memory cell in a neural network </h2>

<p>To preserve information for a long time in
the activities of an RNN, we use a circuit
that implements an analog memory cell.
</p>

<ol>
<li> A linear unit that has a self-link with a weight of 1 will maintain its state.</li>
<li> Information is stored in the cell by activating its write gate.</li>
<li> Information is retrieved by activating the read gate.</li>
<li> We can backpropagate through this circuit because logistics are have nice derivatives.</li> 
</ol>
<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-details">LSTM details </h2>

<p>The LSTM is a unit cell that is made of three gates:</p>
<ol>
<li> the input gate,</li>
<li> the forget gate,</li>
<li> and the output gate.</li>
</ol>
<p>It also introduces a cell state \( c \), which can be thought of as the
long-term memory, and a hidden state \( h \) which can be thought of as
the short-term memory.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="basic-layout-all-figures-from-raschka-et-al">Basic layout (All figures from Raschka <em>et al.,</em>) </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM1.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-details">LSTM details </h2>

<p>The first stage is called the forget gate, where we combine the input
at (say, time \( t \)), and the hidden cell state input at \( t-1 \), passing
it through the Sigmoid activation function and then performing an
element-wise multiplication, denoted by \( \odot \).
</p>

<p>Mathematically we have (see also figure below)</p>
$$
\mathbf{f}^{(t)} = \sigma(W_{fx}\mathbf{x}^{(t)} + W_{fh}\mathbf{h}^{(t-1)} + \mathbf{b}_f)
$$

<p>where the $W$s are the weights to be trained.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="comparing-with-a-standard-rnn">Comparing with a standard  RNN  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM2.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-details-i">LSTM details I </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM3.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-details-ii">LSTM details II  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM4.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-details-iii">LSTM details III  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM5.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="forget-gate">Forget gate  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM6.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-forget-gate">The forget gate </h2>

<p>The naming forget gate stems from the fact that  the Sigmoid activation function's
outputs are very close to \( 0 \) if the argument for the function is very
negative, and \( 1 \) if the argument is very positive. Hence we can
control the amount of information we want to take from the long-term
memory.
</p>
$$
\mathbf{f}^{(t)} = \sigma(W_{fx}\mathbf{x}^{(t)} + W_{fh}\mathbf{h}^{(t-1)} + \mathbf{b}_f)
$$

<p>where the $W$s are the weights to be trained.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="basic-layout">Basic layout </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM7.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="input-gate">Input gate </h2>

<p>The next stage is the input gate, which consists of both a Sigmoid
function (\( \sigma_i \)), which decide what percentage of the input will
be stored in the long-term memory, and the \( \tanh_i \) function, which
decide what is the full memory that can be stored in the long term
memory. When these results are calculated and multiplied together, it
is added to the cell state or stored in the long-term memory, denoted
as \( \oplus \). 
</p>

<p>We have</p>
$$
\mathbf{i}^{(t)} = \sigma_g(W_{ix}\mathbf{x}^{(t)} + W_{ih}\mathbf{h}^{(t-1)} + \mathbf{b}_i),
$$

<p>and</p>
$$
\mathbf{g}^{(t)} = \tanh(W_{gx}\mathbf{x}^{(t)} + W_{gh}\mathbf{h}^{(t-1)} + \mathbf{b}_g),
$$

<p>again the $W$s are the weights to train.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="short-summary">Short summary  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM8.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="forget-and-input">Forget and input </h2>

<p>The forget gate and the input gate together also update the cell state with the following equation, </p>
$$
\mathbf{c}^{(t)} = \mathbf{f}^{(t)} \otimes \mathbf{c}^{(t-1)} + \mathbf{i}^{(t)} \otimes \mathbf{g}^{(t)},
$$

<p>where \( f^{(t)} \) and \( i^{(t)} \) are the outputs of the forget gate and the input gate, respectively.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="basic-layout">Basic layout </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM9.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="output-gate">Output gate </h2>

<p>The final stage of the LSTM is the output gate, and its purpose is to
update the short-term memory.  To achieve this, we take the newly
generated long-term memory and process it through a hyperbolic tangent
(\( \tanh \)) function creating a potential new short-term memory. We then
multiply this potential memory by the output of the Sigmoid function
(\( \sigma_o \)). This multiplication generates the final output as well
as the input for the next hidden cell (\( h^{\langle t \rangle} \)) within
the LSTM cell.
</p>

<p>We have </p>
$$
\begin{aligned}
\mathbf{o}^{(t)} &= \sigma_g(W_o\mathbf{x}^{(t)} + U_o\mathbf{h}^{(t-1)} + \mathbf{b}_o), \\
\mathbf{h}^{(t)} &= \mathbf{o}^{(t)} \otimes \sigma_h(\mathbf{c}^{(t)}). \\
\end{aligned}
$$

<p>where \( \mathbf{W_o,U_o} \) are the weights of the output gate and \( \mathbf{b_o} \) is the bias of the output gate.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="summary-of-lstm">Summary of LSTM </h2>

<p>LSTMs provide a basic approach for modeling long-range dependencies in sequences.
If you wish to read more, see <b>An Empirical Exploration of Recurrent Network Architectures</b>, authored
by Rafal Jozefowicz <em>et al.,</em>  Proceedings of ICML, 2342-2350, 2015).
</p>

<p>An important recent development are the so-called <b>gated recurrent units (GRU)</b>, see for example the article
by Junyoung Chung <em>et al.,</em>, at URL:"https://arxiv.org/abs/1412.3555.
This article is an excellent read if you are interested in learning
more about these modern RNN architectures
</p>

<p>The GRUs have a simpler
architecture than LSTMs. This leads to computationally more efficient methods, while their
performance in some tasks, such as polyphonic music modeling, is comparable to LSTMs.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="lstm-implementation-using-tensorflow">LSTM implementation using TensorFlow </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">Key points:</span>
<span style="color: #BA2121; font-style: italic">1. The input images (28x28 pixels) are treated as sequences of 28 timesteps with 28 features each</span>
<span style="color: #BA2121; font-style: italic">2. The LSTM layer processes this sequential data</span>
<span style="color: #BA2121; font-style: italic">3. A final dense layer with softmax activation handles the classification</span>
<span style="color: #BA2121; font-style: italic">4. Typical accuracy ranges between 95-98% (lower than CNNs but reasonable for demonstration)</span>

<span style="color: #BA2121; font-style: italic">Note: LSTMs are not typically used for image classification (CNNs are more efficient), but this demonstrates how to adapt them for such tasks. Training might take longer compared to CNN architectures.</span>

<span style="color: #BA2121; font-style: italic">To improve performance, you could:</span>
<span style="color: #BA2121; font-style: italic">1. Add more LSTM layers</span>
<span style="color: #BA2121; font-style: italic">2. Use Bidirectional LSTMs</span>
<span style="color: #BA2121; font-style: italic">3. Increase the number of units</span>
<span style="color: #BA2121; font-style: italic">4. Add dropout for regularization</span>
<span style="color: #BA2121; font-style: italic">5. Use learning rate scheduling</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.models</span> <span style="color: #008000; font-weight: bold">import</span> Sequential
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.layers</span> <span style="color: #008000; font-weight: bold">import</span> LSTM, Dense
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.utils</span> <span style="color: #008000; font-weight: bold">import</span> to_categorical

<span style="color: #408080; font-style: italic"># Load and preprocess data</span>
(x_train, y_train), (x_test, y_test) <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>datasets<span style="color: #666666">.</span>mnist<span style="color: #666666">.</span>load_data()

<span style="color: #408080; font-style: italic"># Normalize pixel values to [0, 1]</span>
x_train <span style="color: #666666">=</span> x_train<span style="color: #666666">.</span>astype(<span style="color: #BA2121">&#39;float32&#39;</span>) <span style="color: #666666">/</span> <span style="color: #666666">255.0</span>
x_test <span style="color: #666666">=</span> x_test<span style="color: #666666">.</span>astype(<span style="color: #BA2121">&#39;float32&#39;</span>) <span style="color: #666666">/</span> <span style="color: #666666">255.0</span>

<span style="color: #408080; font-style: italic"># Reshape data for LSTM (samples, timesteps, features)</span>
<span style="color: #408080; font-style: italic"># MNIST images are 28x28, so we treat each image as 28 timesteps of 28 features</span>
x_train <span style="color: #666666">=</span> x_train<span style="color: #666666">.</span>reshape((<span style="color: #666666">-1</span>, <span style="color: #666666">28</span>, <span style="color: #666666">28</span>))
x_test <span style="color: #666666">=</span> x_test<span style="color: #666666">.</span>reshape((<span style="color: #666666">-1</span>, <span style="color: #666666">28</span>, <span style="color: #666666">28</span>))

<span style="color: #408080; font-style: italic"># Convert labels to one-hot encoding</span>
y_train <span style="color: #666666">=</span> to_categorical(y_train, <span style="color: #666666">10</span>)
y_test <span style="color: #666666">=</span> to_categorical(y_test, <span style="color: #666666">10</span>)

<span style="color: #408080; font-style: italic"># Build LSTM model</span>
model <span style="color: #666666">=</span> Sequential()
model<span style="color: #666666">.</span>add(LSTM(<span style="color: #666666">128</span>, input_shape<span style="color: #666666">=</span>(<span style="color: #666666">28</span>, <span style="color: #666666">28</span>)))  <span style="color: #408080; font-style: italic"># 128 LSTM units</span>
model<span style="color: #666666">.</span>add(Dense(<span style="color: #666666">10</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;softmax&#39;</span>))

<span style="color: #408080; font-style: italic"># Compile the model</span>
model<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;categorical_crossentropy&#39;</span>,
             optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&#39;adam&#39;</span>,
             metrics<span style="color: #666666">=</span>[<span style="color: #BA2121">&#39;accuracy&#39;</span>])

<span style="color: #408080; font-style: italic"># Display model summary</span>
model<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># Train the model</span>
history <span style="color: #666666">=</span> model<span style="color: #666666">.</span>fit(x_train, y_train,
                   batch_size<span style="color: #666666">=64</span>,
                   epochs<span style="color: #666666">=10</span>,
                   validation_split<span style="color: #666666">=0.2</span>)

<span style="color: #408080; font-style: italic"># Evaluate on test data</span>
test_loss, test_acc <span style="color: #666666">=</span> model<span style="color: #666666">.</span>evaluate(x_test, y_test, verbose<span style="color: #666666">=2</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">Test accuracy: </span><span style="color: #BB6688; font-weight: bold">{</span>test_acc<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="and-the-corresponding-one-with-pytorch">And the corresponding one with PyTorch </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">Key components:</span>
<span style="color: #BA2121; font-style: italic">1. **Data Handling**: Uses PyTorch DataLoader with MNIST dataset</span>
<span style="color: #BA2121; font-style: italic">2. **LSTM Architecture**:</span>
<span style="color: #BA2121; font-style: italic">  - Input sequence of 28 timesteps (image rows)</span>
<span style="color: #BA2121; font-style: italic">  - 128 hidden units in LSTM layer</span>
<span style="color: #BA2121; font-style: italic">  - Fully connected layer for classification</span>
<span style="color: #BA2121; font-style: italic">3. **Training**:</span>
<span style="color: #BA2121; font-style: italic">  - Cross-entropy loss</span>
<span style="color: #BA2121; font-style: italic">  - Adam optimizer</span>
<span style="color: #BA2121; font-style: italic">  - Automatic GPU utilization if available</span>

<span style="color: #BA2121; font-style: italic">This implementation typically achieves **97-98% accuracy** after 10 epochs. The main differences from the TensorFlow/Keras version:</span>
<span style="color: #BA2121; font-style: italic">- Explicit device management (CPU/GPU)</span>
<span style="color: #BA2121; font-style: italic">- Manual training loop</span>
<span style="color: #BA2121; font-style: italic">- Different data loading pipeline</span>
<span style="color: #BA2121; font-style: italic">- More explicit tensor reshaping</span>

<span style="color: #BA2121; font-style: italic">To improve performance, you could:</span>
<span style="color: #BA2121; font-style: italic">1. Add dropout regularization</span>
<span style="color: #BA2121; font-style: italic">2. Use bidirectional LSTM</span>
<span style="color: #BA2121; font-style: italic">3. Implement learning rate scheduling</span>
<span style="color: #BA2121; font-style: italic">4. Add batch normalization</span>
<span style="color: #BA2121; font-style: italic">5. Increase model capacity (more layers/units)</span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.nn</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">nn</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.optim</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">optim</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torchvision</span> <span style="color: #008000; font-weight: bold">import</span> datasets, transforms
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch.utils.data</span> <span style="color: #008000; font-weight: bold">import</span> DataLoader

<span style="color: #408080; font-style: italic"># Hyperparameters</span>
input_size <span style="color: #666666">=</span> <span style="color: #666666">28</span>     <span style="color: #408080; font-style: italic"># Number of features (pixels per row)</span>
hidden_size <span style="color: #666666">=</span> <span style="color: #666666">128</span>   <span style="color: #408080; font-style: italic"># LSTM hidden state size</span>
num_classes <span style="color: #666666">=</span> <span style="color: #666666">10</span>    <span style="color: #408080; font-style: italic"># Digits 0-9</span>
num_epochs <span style="color: #666666">=</span> <span style="color: #666666">10</span>     <span style="color: #408080; font-style: italic"># Training iterations</span>
batch_size <span style="color: #666666">=</span> <span style="color: #666666">64</span>     <span style="color: #408080; font-style: italic"># Batch size</span>
learning_rate <span style="color: #666666">=</span> <span style="color: #666666">0.001</span>

<span style="color: #408080; font-style: italic"># Device configuration</span>
device <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>device(<span style="color: #BA2121">&#39;cuda&#39;</span> <span style="color: #008000; font-weight: bold">if</span> torch<span style="color: #666666">.</span>cuda<span style="color: #666666">.</span>is_available() <span style="color: #008000; font-weight: bold">else</span> <span style="color: #BA2121">&#39;cpu&#39;</span>)

<span style="color: #408080; font-style: italic"># MNIST dataset</span>
transform <span style="color: #666666">=</span> transforms<span style="color: #666666">.</span>Compose([
   transforms<span style="color: #666666">.</span>ToTensor(),
   transforms<span style="color: #666666">.</span>Normalize((<span style="color: #666666">0.1307</span>,), (<span style="color: #666666">0.3081</span>,))  <span style="color: #408080; font-style: italic"># MNIST mean and std</span>
])

train_dataset <span style="color: #666666">=</span> datasets<span style="color: #666666">.</span>MNIST(root<span style="color: #666666">=</span><span style="color: #BA2121">&#39;./data&#39;</span>,
                              train<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,
                              transform<span style="color: #666666">=</span>transform,
                              download<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)

test_dataset <span style="color: #666666">=</span> datasets<span style="color: #666666">.</span>MNIST(root<span style="color: #666666">=</span><span style="color: #BA2121">&#39;./data&#39;</span>,
                             train<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>,
                             transform<span style="color: #666666">=</span>transform)

train_loader <span style="color: #666666">=</span> DataLoader(dataset<span style="color: #666666">=</span>train_dataset,
                         batch_size<span style="color: #666666">=</span>batch_size,
                         shuffle<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)

test_loader <span style="color: #666666">=</span> DataLoader(dataset<span style="color: #666666">=</span>test_dataset,
                        batch_size<span style="color: #666666">=</span>batch_size,
                        shuffle<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)

<span style="color: #408080; font-style: italic"># LSTM model</span>
<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">LSTMModel</span>(nn<span style="color: #666666">.</span>Module):
   <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, input_size, hidden_size, num_classes):
       <span style="color: #008000">super</span>(LSTMModel, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
       <span style="color: #008000">self</span><span style="color: #666666">.</span>hidden_size <span style="color: #666666">=</span> hidden_size
       <span style="color: #008000">self</span><span style="color: #666666">.</span>lstm <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>LSTM(input_size, hidden_size, batch_first<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
       <span style="color: #008000">self</span><span style="color: #666666">.</span>fc <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>Linear(hidden_size, num_classes)

   <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
       <span style="color: #408080; font-style: italic"># Reshape input to (batch_size, sequence_length, input_size)</span>
       x <span style="color: #666666">=</span> x<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">28</span>, <span style="color: #666666">28</span>)

       <span style="color: #408080; font-style: italic"># Forward propagate LSTM</span>
       out, _ <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>lstm(x)  <span style="color: #408080; font-style: italic"># out: (batch_size, seq_length, hidden_size)</span>

       <span style="color: #408080; font-style: italic"># Decode the hidden state of the last time step</span>
       out <span style="color: #666666">=</span> out[:, <span style="color: #666666">-1</span>, :]
       out <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>fc(out)
       <span style="color: #008000; font-weight: bold">return</span> out

<span style="color: #408080; font-style: italic"># Initialize model</span>
model <span style="color: #666666">=</span> LSTMModel(input_size, hidden_size, num_classes)<span style="color: #666666">.</span>to(device)

<span style="color: #408080; font-style: italic"># Loss and optimizer</span>
criterion <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>CrossEntropyLoss()
optimizer <span style="color: #666666">=</span> optim<span style="color: #666666">.</span>Adam(model<span style="color: #666666">.</span>parameters(), lr<span style="color: #666666">=</span>learning_rate)

<span style="color: #408080; font-style: italic"># Training loop</span>
total_step <span style="color: #666666">=</span> <span style="color: #008000">len</span>(train_loader)
<span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(num_epochs):
   model<span style="color: #666666">.</span>train()
   <span style="color: #008000; font-weight: bold">for</span> i, (images, labels) <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(train_loader):
       images <span style="color: #666666">=</span> images<span style="color: #666666">.</span>to(device)
       labels <span style="color: #666666">=</span> labels<span style="color: #666666">.</span>to(device)

       <span style="color: #408080; font-style: italic"># Forward pass</span>
       outputs <span style="color: #666666">=</span> model(images)
       loss <span style="color: #666666">=</span> criterion(outputs, labels)

       <span style="color: #408080; font-style: italic"># Backward and optimize</span>
       optimizer<span style="color: #666666">.</span>zero_grad()
       loss<span style="color: #666666">.</span>backward()
       optimizer<span style="color: #666666">.</span>step()

       <span style="color: #008000; font-weight: bold">if</span> (i<span style="color: #666666">+1</span>) <span style="color: #666666">%</span> <span style="color: #666666">100</span> <span style="color: #666666">==</span> <span style="color: #666666">0</span>:
           <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Epoch [</span><span style="color: #BB6688; font-weight: bold">{</span>epoch<span style="color: #666666">+1</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">/</span><span style="color: #BB6688; font-weight: bold">{</span>num_epochs<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">], Step [</span><span style="color: #BB6688; font-weight: bold">{</span>i<span style="color: #666666">+1</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">/</span><span style="color: #BB6688; font-weight: bold">{</span>total_step<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">], Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss<span style="color: #666666">.</span>item()<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)

   <span style="color: #408080; font-style: italic"># Test the model</span>
   model<span style="color: #666666">.</span>eval()
   <span style="color: #008000; font-weight: bold">with</span> torch<span style="color: #666666">.</span>no_grad():
       correct <span style="color: #666666">=</span> <span style="color: #666666">0</span>
       total <span style="color: #666666">=</span> <span style="color: #666666">0</span>
       <span style="color: #008000; font-weight: bold">for</span> images, labels <span style="color: #AA22FF; font-weight: bold">in</span> test_loader:
           images <span style="color: #666666">=</span> images<span style="color: #666666">.</span>to(device)
           labels <span style="color: #666666">=</span> labels<span style="color: #666666">.</span>to(device)
           outputs <span style="color: #666666">=</span> model(images)
           _, predicted <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>max(outputs<span style="color: #666666">.</span>data, <span style="color: #666666">1</span>)
           total <span style="color: #666666">+=</span> labels<span style="color: #666666">.</span>size(<span style="color: #666666">0</span>)
           correct <span style="color: #666666">+=</span> (predicted <span style="color: #666666">==</span> labels)<span style="color: #666666">.</span>sum()<span style="color: #666666">.</span>item()

       <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Test Accuracy: </span><span style="color: #BB6688; font-weight: bold">{</span><span style="color: #666666">100</span> <span style="color: #666666">*</span> correct <span style="color: #666666">/</span> total<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.2f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">%&#39;</span>)

<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Training finished.&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="dynamical-ordinary-differential-equation">Dynamical ordinary differential equation </h2>

<p>Let us illustrate how we could train an RNN using data from the
solution of a well-known differential equation, namely Newton's
equation for oscillatory motion for an object being forced into
harmonic oscillations by an applied external force.
</p>

<p>We will start with the basic algorithm for solving this type of
equations using the Runge-Kutta-4 approach. The first code example is
a standalone differential equation solver. It yields positions and
velocities as function of time, starting with an initial time \( t_0 \)
and ending with a final time.
</p>

<p>The data the program produces will in turn be used to train an RNN for
a selected number of training data. With a trained RNN, we will then
use the network to make predictions for data not included in the
training. That is, we will train a model which should be able to
reproduce velocities and positions not included in training data.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="the-runge-kutta-4-code">The Runge-Kutta-4 code </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">math</span> <span style="color: #008000; font-weight: bold">import</span> <span style="color: #666666">*</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>

<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">SpringForce</span>(v,x,t):
<span style="color: #408080; font-style: italic">#   note here that we have divided by mass and we return the acceleration</span>
    <span style="color: #008000; font-weight: bold">return</span>  <span style="color: #666666">-2*</span>gamma<span style="color: #666666">*</span>v<span style="color: #666666">-</span>x<span style="color: #666666">+</span>Ftilde<span style="color: #666666">*</span>cos(t<span style="color: #666666">*</span>Omegatilde)


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">RK4</span>(v,x,t,n,Force):
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n<span style="color: #666666">-1</span>):
<span style="color: #408080; font-style: italic"># Setting up k1</span>
        k1x <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>v[i]
        k1v <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>Force(v[i],x[i],t[i])
<span style="color: #408080; font-style: italic"># Setting up k2</span>
        vv <span style="color: #666666">=</span> v[i]<span style="color: #666666">+</span>k1v<span style="color: #666666">*0.5</span>
        xx <span style="color: #666666">=</span> x[i]<span style="color: #666666">+</span>k1x<span style="color: #666666">*0.5</span>
        k2x <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>vv
        k2v <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>Force(vv,xx,t[i]<span style="color: #666666">+</span>DeltaT<span style="color: #666666">*0.5</span>)
<span style="color: #408080; font-style: italic"># Setting up k3</span>
        vv <span style="color: #666666">=</span> v[i]<span style="color: #666666">+</span>k2v<span style="color: #666666">*0.5</span>
        xx <span style="color: #666666">=</span> x[i]<span style="color: #666666">+</span>k2x<span style="color: #666666">*0.5</span>
        k3x <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>vv
        k3v <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>Force(vv,xx,t[i]<span style="color: #666666">+</span>DeltaT<span style="color: #666666">*0.5</span>)
<span style="color: #408080; font-style: italic"># Setting up k4</span>
        vv <span style="color: #666666">=</span> v[i]<span style="color: #666666">+</span>k3v
        xx <span style="color: #666666">=</span> x[i]<span style="color: #666666">+</span>k3x
        k4x <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>vv
        k4v <span style="color: #666666">=</span> DeltaT<span style="color: #666666">*</span>Force(vv,xx,t[i]<span style="color: #666666">+</span>DeltaT)
<span style="color: #408080; font-style: italic"># Final result</span>
        x[i<span style="color: #666666">+1</span>] <span style="color: #666666">=</span> x[i]<span style="color: #666666">+</span>(k1x<span style="color: #666666">+2*</span>k2x<span style="color: #666666">+2*</span>k3x<span style="color: #666666">+</span>k4x)<span style="color: #666666">/6.</span>
        v[i<span style="color: #666666">+1</span>] <span style="color: #666666">=</span> v[i]<span style="color: #666666">+</span>(k1v<span style="color: #666666">+2*</span>k2v<span style="color: #666666">+2*</span>k3v<span style="color: #666666">+</span>k4v)<span style="color: #666666">/6.</span>
        t[i<span style="color: #666666">+1</span>] <span style="color: #666666">=</span> t[i] <span style="color: #666666">+</span> DeltaT


<span style="color: #408080; font-style: italic"># Main part begins here</span>

DeltaT <span style="color: #666666">=</span> <span style="color: #666666">0.001</span>
<span style="color: #408080; font-style: italic">#set up arrays </span>
tfinal <span style="color: #666666">=</span> <span style="color: #666666">20</span> <span style="color: #408080; font-style: italic"># in dimensionless time</span>
n <span style="color: #666666">=</span> ceil(tfinal<span style="color: #666666">/</span>DeltaT)
<span style="color: #408080; font-style: italic"># set up arrays for t, v, and x</span>
t <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n)
v <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n)
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(n)
<span style="color: #408080; font-style: italic"># Initial conditions (can change to more than one dim)</span>
x0 <span style="color: #666666">=</span>  <span style="color: #666666">1.0</span> 
v0 <span style="color: #666666">=</span> <span style="color: #666666">0.0</span>
x[<span style="color: #666666">0</span>] <span style="color: #666666">=</span> x0
v[<span style="color: #666666">0</span>] <span style="color: #666666">=</span> v0
gamma <span style="color: #666666">=</span> <span style="color: #666666">0.2</span>
Omegatilde <span style="color: #666666">=</span> <span style="color: #666666">0.5</span>
Ftilde <span style="color: #666666">=</span> <span style="color: #666666">1.0</span>
<span style="color: #408080; font-style: italic"># Start integrating using Euler&#39;s method</span>
<span style="color: #408080; font-style: italic"># Note that we define the force function as a SpringForce</span>
RK4(v,x,t,n,SpringForce)

<span style="color: #408080; font-style: italic"># Plot position as function of time    </span>
fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots()
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&#39;x[m]&#39;</span>)
ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&#39;t[s]&#39;</span>)
ax<span style="color: #666666">.</span>plot(t, x)
fig<span style="color: #666666">.</span>tight_layout()
save_fig(<span style="color: #BA2121">&quot;ForcedBlockRK4&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="using-the-above-data-to-train-an-rnn">Using the above data to train an RNN </h2>

<p>In the code here we have reworked the previous example in order to
generate data that can be handled by recurrent neural networks in
order to train our model. The first code is written using Tensorflow/keras while the second example uses PyTorch.
In both cases we use the Runge Kutta to fourth order as a way to generate the data. We have implemented a simple RNN only.
We leave it as an exercise (possible path in project 3) to implement LSTMs.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># train_rnn_from_rk4.py</span>

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">runpy</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>

<span style="color: #408080; font-style: italic"># ---------- Load RK4-generated data from your script ----------</span>
<span style="color: #408080; font-style: italic"># This runs rungekutta.py and collects its globals. It must populate &#39;t&#39; and &#39;x&#39; arrays.</span>
g <span style="color: #666666">=</span> runpy<span style="color: #666666">.</span>run_path(<span style="color: #BA2121">&#39;rungekutta.py&#39;</span>)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> <span style="color: #008000">all</span>(k <span style="color: #AA22FF; font-weight: bold">in</span> g <span style="color: #008000; font-weight: bold">for</span> k <span style="color: #AA22FF; font-weight: bold">in</span> (<span style="color: #BA2121">&#39;t&#39;</span>,<span style="color: #BA2121">&#39;x&#39;</span>,<span style="color: #BA2121">&#39;v&#39;</span>)):
    <span style="color: #008000; font-weight: bold">raise</span> <span style="color: #D2413A; font-weight: bold">RuntimeError</span>(<span style="color: #BA2121">&quot;rungekutta.py did not expose required variables &#39;t&#39;, &#39;x&#39;, &#39;v&#39; in its globals.&quot;</span>)

t <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(g[<span style="color: #BA2121">&#39;t&#39;</span>])<span style="color: #666666">.</span>ravel()
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(g[<span style="color: #BA2121">&#39;x&#39;</span>])<span style="color: #666666">.</span>ravel()
v <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(g[<span style="color: #BA2121">&#39;v&#39;</span>])<span style="color: #666666">.</span>ravel()

<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Loaded shapes:&quot;</span>, t<span style="color: #666666">.</span>shape, x<span style="color: #666666">.</span>shape, v<span style="color: #666666">.</span>shape)

<span style="color: #408080; font-style: italic"># Simple plot of the original trajectory</span>
plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">8</span>,<span style="color: #666666">3</span>))
plt<span style="color: #666666">.</span>plot(t, x)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;t&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;x&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&#39;True trajectory from RK4&#39;</span>)
plt<span style="color: #666666">.</span>tight_layout()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># ---------- Prepare datasets ----------</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">make_dataset</span>(series, input_len):
    X, y <span style="color: #666666">=</span> [], []
    N <span style="color: #666666">=</span> <span style="color: #008000">len</span>(series)
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(N <span style="color: #666666">-</span> input_len):
        X<span style="color: #666666">.</span>append(series[i:i<span style="color: #666666">+</span>input_len])
        y<span style="color: #666666">.</span>append(series[i<span style="color: #666666">+</span>input_len])
    X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(X)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, input_len, <span style="color: #666666">1</span>)  <span style="color: #408080; font-style: italic"># (samples, timesteps, 1)</span>
    y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(y)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)
    <span style="color: #008000; font-weight: bold">return</span> X, y

<span style="color: #408080; font-style: italic"># normalize using global mean/std</span>
mean_x, std_x <span style="color: #666666">=</span> x<span style="color: #666666">.</span>mean(), x<span style="color: #666666">.</span>std()
x_norm <span style="color: #666666">=</span> (x <span style="color: #666666">-</span> mean_x) <span style="color: #666666">/</span> std_x

<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Normalization: mean=</span><span style="color: #BB6688; font-weight: bold">{</span>mean_x<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.6f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">, std=</span><span style="color: #BB6688; font-weight: bold">{</span>std_x<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.6f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)

<span style="color: #408080; font-style: italic"># Model A: input_len = 1 (x_t -&gt; x_{t+1})</span>
input_len_A <span style="color: #666666">=</span> <span style="color: #666666">1</span>
X_A, y_A <span style="color: #666666">=</span> make_dataset(x_norm, input_len_A)

<span style="color: #408080; font-style: italic"># Model B: input_len = 10 (used for autoregressive generation)</span>
input_len_B <span style="color: #666666">=</span> <span style="color: #666666">10</span>
X_B, y_B <span style="color: #666666">=</span> make_dataset(x_norm, input_len_B)

<span style="color: #408080; font-style: italic"># train/test split</span>
test_size <span style="color: #666666">=</span> <span style="color: #666666">0.2</span>
random_seed <span style="color: #666666">=</span> <span style="color: #666666">42</span>
Xa_train, Xa_test, ya_train, ya_test <span style="color: #666666">=</span> train_test_split(X_A, y_A, test_size<span style="color: #666666">=</span>test_size, random_state<span style="color: #666666">=</span>random_seed)
Xb_train, Xb_test, yb_train, yb_test <span style="color: #666666">=</span> train_test_split(X_B, y_B, test_size<span style="color: #666666">=</span>test_size, random_state<span style="color: #666666">=</span>random_seed)

<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Model A shapes:&quot;</span>, Xa_train<span style="color: #666666">.</span>shape, ya_train<span style="color: #666666">.</span>shape, <span style="color: #BA2121">&quot;Model B shapes:&quot;</span>, Xb_train<span style="color: #666666">.</span>shape, yb_train<span style="color: #666666">.</span>shape)

<span style="color: #408080; font-style: italic"># ---------- Build models ----------</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">build_simple_rnn</span>(input_len, hidden_size<span style="color: #666666">=32</span>):
    model <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>Sequential([
        tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>Input(shape<span style="color: #666666">=</span>(input_len,<span style="color: #666666">1</span>)),
        tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>layers<span style="color: #666666">.</span>SimpleRNN(hidden_size, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;tanh&#39;</span>),
        tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>layers<span style="color: #666666">.</span>Dense(<span style="color: #666666">1</span>)
    ])
    model<span style="color: #666666">.</span>compile(optimizer<span style="color: #666666">=</span>tf<span style="color: #666666">.</span>keras<span style="color: #666666">.</span>optimizers<span style="color: #666666">.</span>Adam(<span style="color: #666666">1e-3</span>),
                  loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;mse&#39;</span>,
                  metrics<span style="color: #666666">=</span>[<span style="color: #BA2121">&#39;mse&#39;</span>])
    <span style="color: #008000; font-weight: bold">return</span> model

model_A <span style="color: #666666">=</span> build_simple_rnn(input_len_A, hidden_size<span style="color: #666666">=32</span>)
model_B <span style="color: #666666">=</span> build_simple_rnn(input_len_B, hidden_size<span style="color: #666666">=64</span>)

<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Model A summary:&quot;</span>)
model_A<span style="color: #666666">.</span>summary()
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;</span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">Model B summary:&quot;</span>)
model_B<span style="color: #666666">.</span>summary()

<span style="color: #408080; font-style: italic"># ---------- Train ----------</span>
epochs_A <span style="color: #666666">=</span> <span style="color: #666666">30</span>
epochs_B <span style="color: #666666">=</span> <span style="color: #666666">40</span>

hist_A <span style="color: #666666">=</span> model_A<span style="color: #666666">.</span>fit(Xa_train, ya_train, validation_data<span style="color: #666666">=</span>(Xa_test, ya_test),
                     epochs<span style="color: #666666">=</span>epochs_A, batch_size<span style="color: #666666">=32</span>, verbose<span style="color: #666666">=1</span>)

hist_B <span style="color: #666666">=</span> model_B<span style="color: #666666">.</span>fit(Xb_train, yb_train, validation_data<span style="color: #666666">=</span>(Xb_test, yb_test),
                     epochs<span style="color: #666666">=</span>epochs_B, batch_size<span style="color: #666666">=32</span>, verbose<span style="color: #666666">=1</span>)

<span style="color: #408080; font-style: italic"># ---------- Plot training curves ----------</span>
plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">3</span>))
plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">1</span>,<span style="color: #666666">2</span>,<span style="color: #666666">1</span>)
plt<span style="color: #666666">.</span>plot(hist_A<span style="color: #666666">.</span>history[<span style="color: #BA2121">&#39;loss&#39;</span>], label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;train&#39;</span>)
plt<span style="color: #666666">.</span>plot(hist_A<span style="color: #666666">.</span>history[<span style="color: #BA2121">&#39;val_loss&#39;</span>], label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;val&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&#39;Model A loss&#39;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;epoch&#39;</span>); plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;mse&#39;</span>); plt<span style="color: #666666">.</span>legend()

plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">1</span>,<span style="color: #666666">2</span>,<span style="color: #666666">2</span>)
plt<span style="color: #666666">.</span>plot(hist_B<span style="color: #666666">.</span>history[<span style="color: #BA2121">&#39;loss&#39;</span>], label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;train&#39;</span>)
plt<span style="color: #666666">.</span>plot(hist_B<span style="color: #666666">.</span>history[<span style="color: #BA2121">&#39;val_loss&#39;</span>], label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;val&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&#39;Model B loss&#39;</span>)
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&#39;epoch&#39;</span>); plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&#39;mse&#39;</span>); plt<span style="color: #666666">.</span>legend()

plt<span style="color: #666666">.</span>tight_layout()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># ---------- Evaluate one-step predictions ----------</span>
preds_A <span style="color: #666666">=</span> model_A<span style="color: #666666">.</span>predict(Xa_test)
preds_A_un <span style="color: #666666">=</span> preds_A<span style="color: #666666">.</span>flatten() <span style="color: #666666">*</span> std_x <span style="color: #666666">+</span> mean_x
ya_test_un <span style="color: #666666">=</span> ya_test<span style="color: #666666">.</span>flatten() <span style="color: #666666">*</span> std_x <span style="color: #666666">+</span> mean_x

<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Model A one-step MSE (unnormalized):&quot;</span>, np<span style="color: #666666">.</span>mean((preds_A_un <span style="color: #666666">-</span> ya_test_un)<span style="color: #666666">**2</span>))

plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">8</span>,<span style="color: #666666">3</span>))
nplot <span style="color: #666666">=</span> <span style="color: #008000">min</span>(<span style="color: #666666">100</span>, <span style="color: #008000">len</span>(ya_test_un))
plt<span style="color: #666666">.</span>plot(ya_test_un[:nplot], label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;true next x&#39;</span>)
plt<span style="color: #666666">.</span>plot(preds_A_un[:nplot], label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;predicted next x (Model A)&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Model A: one-step predictions (segment)&quot;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># ---------- Autoregressive generation using Model B ----------</span>
<span style="color: #408080; font-style: italic"># Start from the first input_len_B true values, then generate the remainder autoregressively</span>
initial_window <span style="color: #666666">=</span> x_norm[:input_len_B]<span style="color: #666666">.</span>reshape(<span style="color: #666666">1</span>,input_len_B,<span style="color: #666666">1</span>)
gen_steps <span style="color: #666666">=</span> <span style="color: #008000">len</span>(x_norm) <span style="color: #666666">-</span> input_len_B
generated <span style="color: #666666">=</span> []
current_window <span style="color: #666666">=</span> initial_window<span style="color: #666666">.</span>copy()

<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(gen_steps):
    pred_norm <span style="color: #666666">=</span> model_B<span style="color: #666666">.</span>predict(current_window, verbose<span style="color: #666666">=0</span>)  <span style="color: #408080; font-style: italic"># shape (1,1)</span>
    generated<span style="color: #666666">.</span>append(pred_norm<span style="color: #666666">.</span>flatten()[<span style="color: #666666">0</span>])
    <span style="color: #408080; font-style: italic"># roll the window and append prediction</span>
    current_window <span style="color: #666666">=</span> np<span style="color: #666666">.</span>concatenate([current_window[:,<span style="color: #666666">1</span>:,:], pred_norm<span style="color: #666666">.</span>reshape(<span style="color: #666666">1</span>,<span style="color: #666666">1</span>,<span style="color: #666666">1</span>)], axis<span style="color: #666666">=1</span>)

generated_un <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(generated) <span style="color: #666666">*</span> std_x <span style="color: #666666">+</span> mean_x
true_remainder <span style="color: #666666">=</span> x[input_len_B:]

plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">8</span>,<span style="color: #666666">3</span>))
plt<span style="color: #666666">.</span>plot(true_remainder, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;true remainder&#39;</span>)
plt<span style="color: #666666">.</span>plot(generated_un, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;generated (Model B)&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&#39;Model B autoregressive generation&#39;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #408080; font-style: italic"># ---------- Save models ----------</span>
os<span style="color: #666666">.</span>makedirs(<span style="color: #BA2121">&#39;saved_models&#39;</span>, exist_ok<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
path_A <span style="color: #666666">=</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(<span style="color: #BA2121">&#39;saved_models&#39;</span>,<span style="color: #BA2121">&#39;model_A_rnn.h5&#39;</span>)
path_B <span style="color: #666666">=</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(<span style="color: #BA2121">&#39;saved_models&#39;</span>,<span style="color: #BA2121">&#39;model_B_rnn.h5&#39;</span>)
model_A<span style="color: #666666">.</span>save(path_A)
model_B<span style="color: #666666">.</span>save(path_B)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Saved models to:&quot;</span>, path_A, path_B)

<span style="color: #408080; font-style: italic"># ---------- Final numeric summaries ----------</span>
preds_B <span style="color: #666666">=</span> model_B<span style="color: #666666">.</span>predict(Xb_test)
preds_B_un <span style="color: #666666">=</span> preds_B<span style="color: #666666">.</span>flatten() <span style="color: #666666">*</span> std_x <span style="color: #666666">+</span> mean_x
yb_test_un <span style="color: #666666">=</span> yb_test<span style="color: #666666">.</span>flatten() <span style="color: #666666">*</span> std_x <span style="color: #666666">+</span> mean_x
mse_A <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean((preds_A_un <span style="color: #666666">-</span> ya_test_un)<span style="color: #666666">**2</span>)
mse_B <span style="color: #666666">=</span> np<span style="color: #666666">.</span>mean((preds_B_un <span style="color: #666666">-</span> yb_test_un)<span style="color: #666666">**2</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;One-step MSE (Model A): </span><span style="color: #BB6688; font-weight: bold">{</span>mse_A<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.6e</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
<span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;One-step MSE (Model B): </span><span style="color: #BB6688; font-weight: bold">{</span>mse_B<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.6e</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="similar-code-using-pytorch">Similar code using PyTorch </h2>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.nn</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">nn</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.optim</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">optim</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">runpy</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>

<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>
<span style="color: #408080; font-style: italic"># 1. Load your RK4 integrator and generate the dataset</span>
<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>
data <span style="color: #666666">=</span> runpy<span style="color: #666666">.</span>run_path(<span style="color: #BA2121">&quot;rungekutta.py&quot;</span>)

t <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(data[<span style="color: #BA2121">&quot;t&quot;</span>])
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(data[<span style="color: #BA2121">&quot;x&quot;</span>])

x <span style="color: #666666">=</span> x<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>)          <span style="color: #408080; font-style: italic"># shape: (T, 1)</span>
T <span style="color: #666666">=</span> <span style="color: #008000">len</span>(x)

<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>
<span style="color: #408080; font-style: italic"># 2. Build supervised learning dataset</span>
<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>

<span style="color: #408080; font-style: italic"># ---------- Task 1: one-step predictor x_t  x_{t+1} ----------</span>
X1 <span style="color: #666666">=</span> x[:<span style="color: #666666">-1</span>]
Y1 <span style="color: #666666">=</span> x[<span style="color: #666666">1</span>:]

X1_torch <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(X1, dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)
Y1_torch <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(Y1, dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)

<span style="color: #408080; font-style: italic"># ---------- Task 2: sequence predictor ----------</span>
seq_len <span style="color: #666666">=</span> <span style="color: #666666">20</span>        <span style="color: #408080; font-style: italic"># length of input window</span>
pred_len <span style="color: #666666">=</span> <span style="color: #666666">20</span>       <span style="color: #408080; font-style: italic"># number of future steps to predict</span>

X2 <span style="color: #666666">=</span> []
Y2 <span style="color: #666666">=</span> []

<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(T <span style="color: #666666">-</span> seq_len <span style="color: #666666">-</span> pred_len):
    X2<span style="color: #666666">.</span>append(x[i : i <span style="color: #666666">+</span> seq_len])
    Y2<span style="color: #666666">.</span>append(x[i <span style="color: #666666">+</span> seq_len : i <span style="color: #666666">+</span> seq_len <span style="color: #666666">+</span> pred_len])

X2 <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(X2)     <span style="color: #408080; font-style: italic"># (N, seq_len, 1)</span>
Y2 <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array(Y2)     <span style="color: #408080; font-style: italic"># (N, pred_len, 1)</span>

X2_torch <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(X2, dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)
Y2_torch <span style="color: #666666">=</span> torch<span style="color: #666666">.</span>tensor(Y2, dtype<span style="color: #666666">=</span>torch<span style="color: #666666">.</span>float32)

<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>
<span style="color: #408080; font-style: italic"># 3. Define RNN models</span>
<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>

<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">RNNOneStep</span>(nn<span style="color: #666666">.</span>Module):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;Model 1: x_t  x_{t+1}&quot;&quot;&quot;</span>
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, hidden<span style="color: #666666">=32</span>):
        <span style="color: #008000">super</span>()<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #008000">self</span><span style="color: #666666">.</span>rnn <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>RNN(<span style="color: #666666">1</span>, hidden, batch_first<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>fc <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>Linear(hidden, <span style="color: #666666">1</span>)

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        out, _ <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>rnn(x<span style="color: #666666">.</span>unsqueeze(<span style="color: #666666">1</span>))   <span style="color: #408080; font-style: italic"># shape (batch, 1, hidden)</span>
        out <span style="color: #666666">=</span> out[:, <span style="color: #666666">-1</span>, :]                 <span style="color: #408080; font-style: italic"># last time step</span>
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>fc(out)


<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">RNNSequence</span>(nn<span style="color: #666666">.</span>Module):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;Model 2: Predict multiple future steps&quot;&quot;&quot;</span>
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, hidden<span style="color: #666666">=64</span>):
        <span style="color: #008000">super</span>()<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #008000">self</span><span style="color: #666666">.</span>rnn <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>RNN(<span style="color: #666666">1</span>, hidden, batch_first<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>fc <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>Linear(hidden, <span style="color: #666666">1</span>)

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        out, _ <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>rnn(x)           <span style="color: #408080; font-style: italic"># out: (batch, seq_len, hidden)</span>
        out <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>fc(out)             <span style="color: #408080; font-style: italic"># (batch, seq_len, 1)</span>
        <span style="color: #008000; font-weight: bold">return</span> out


<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>
<span style="color: #408080; font-style: italic"># 4. Train Model 1 (single-step predictor)</span>
<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>

model1 <span style="color: #666666">=</span> RNNOneStep()
criterion <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>MSELoss()
optimizer <span style="color: #666666">=</span> optim<span style="color: #666666">.</span>Adam(model1<span style="color: #666666">.</span>parameters(), lr<span style="color: #666666">=1e-3</span>)

<span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">200</span>):
    optimizer<span style="color: #666666">.</span>zero_grad()
    pred <span style="color: #666666">=</span> model1(X1_torch)
    loss <span style="color: #666666">=</span> criterion(pred, Y1_torch)
    loss<span style="color: #666666">.</span>backward()
    optimizer<span style="color: #666666">.</span>step()
    <span style="color: #008000; font-weight: bold">if</span> epoch <span style="color: #666666">%</span> <span style="color: #666666">50</span> <span style="color: #666666">==</span> <span style="color: #666666">0</span>:
        <span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;One-step Epoch </span><span style="color: #BB6688; font-weight: bold">{</span>epoch<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">, Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss<span style="color: #666666">.</span>item()<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.6f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)

<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>
<span style="color: #408080; font-style: italic"># 5. Train Model 2 (sequence predictor)</span>
<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>

model2 <span style="color: #666666">=</span> RNNSequence()
optimizer <span style="color: #666666">=</span> optim<span style="color: #666666">.</span>Adam(model4<span style="color: #666666">.</span>parameters(), lr<span style="color: #666666">=1e-3</span>)

<span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">200</span>):
    optimizer<span style="color: #666666">.</span>zero_grad()
    pred <span style="color: #666666">=</span> model2(X2_torch)
    loss <span style="color: #666666">=</span> criterion(pred, Y2_torch)
    loss<span style="color: #666666">.</span>backward()
    optimizer<span style="color: #666666">.</span>step()
    <span style="color: #008000; font-weight: bold">if</span> epoch <span style="color: #666666">%</span> <span style="color: #666666">50</span> <span style="color: #666666">==</span> <span style="color: #666666">0</span>:
        <span style="color: #008000">print</span>(<span style="color: #BA2121">f&quot;Sequence Epoch </span><span style="color: #BB6688; font-weight: bold">{</span>epoch<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">, Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss<span style="color: #666666">.</span>item()<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.6f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&quot;</span>)

<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>
<span style="color: #408080; font-style: italic"># 6. Evaluate: multi-step prediction</span>
<span style="color: #408080; font-style: italic"># -------------------------------------------------------</span>

<span style="color: #008000; font-weight: bold">with</span> torch<span style="color: #666666">.</span>no_grad():
    sample_input <span style="color: #666666">=</span> X2_torch[<span style="color: #666666">10</span>:<span style="color: #666666">11</span>]      <span style="color: #408080; font-style: italic"># shape (1, seq_len, 1)</span>
    predicted_seq <span style="color: #666666">=</span> model4(sample_input)<span style="color: #666666">.</span>numpy()<span style="color: #666666">.</span>squeeze()
    true_seq <span style="color: #666666">=</span> Y2[<span style="color: #666666">10</span>]<span style="color: #666666">.</span>squeeze()

plt<span style="color: #666666">.</span>plot(true_seq, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;True&quot;</span>)
plt<span style="color: #666666">.</span>plot(predicted_seq, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Predicted&quot;</span>, linestyle<span style="color: #666666">=</span><span style="color: #BA2121">&quot;--&quot;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Sequence prediction (20 steps ahead)&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="autoencoders-overarching-view">Autoencoders: Overarching view </h2>

<p>Autoencoders are artificial neural networks capable of learning
efficient representations of the input data (these representations are called codings)  without
any supervision (i.e., the training set is unlabeled). These codings
typically have a much lower dimensionality than the input data, making
autoencoders useful for dimensionality reduction. 
</p>

<p>Autoencoders learn to encode the
input data into a lower-dimensional representation, and then decode it
back to the original data. The goal of autoencoders is to minimize the
reconstruction error, which measures how well the output matches the
input. Autoencoders can be seen as a way of learning the latent
features or hidden structure of the data, and they can be used for
data compression, denoising, anomaly detection, and generative
modeling.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="powerful-detectors">Powerful detectors </h2>

<p>More importantly, autoencoders act as powerful feature detectors, and
they can be used for unsupervised pretraining of deep neural networks.
</p>

<p>Lastly, they are capable of randomly generating new data that looks
very similar to the training data; this is called a generative
model. For example, you could train an autoencoder on pictures of
faces, and it would then be able to generate new faces.  Surprisingly,
autoencoders work by simply learning to copy their inputs to their
outputs. This may sound like a trivial task, but we will see that
constraining the network in various ways can make it rather
difficult. For example, you can limit the size of the internal
representation, or you can add noise to the inputs and train the
network to recover the original inputs. These constraints prevent the
autoencoder from trivially copying the inputs directly to the outputs,
which forces it to learn efficient ways of representing the data. In
short, the codings are byproducts of the autoencoder&#8217;s attempt to
learn the identity function under some constraints.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="first-introduction-of-aes">First introduction of AEs </h2>

<p>Autoencoders were first introduced by Rumelhart, Hinton, and Williams
in 1986 with the goal of learning to reconstruct the input
observations with the lowest error possible.
</p>

<p>Why would one want to learn to reconstruct the input observations? If
you have problems imagining what that means, think of having a dataset
made of images. An autoencoder would be an algorithm that can give as
output an image that is as similar as possible to the input one. You
may be confused, as there is no apparent reason of doing so. To better
understand why autoencoders are useful we need a more informative
(although not yet unambiguous) definition.
</p>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>An autoencoder is a type of algorithm with the primary purpose of learning an "informative" representation of the data that can be used for different applications (<a href="https://arxiv.org/abs/2003.05991" target="_blank">see Bank, D., Koenigstein, N., and Giryes, R., Autoencoders</a>) by learning to reconstruct a set of input observations well enough.</p>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="autoencoder-structure">Autoencoder structure </h2>

<p>Autoencoders are neural networks where the outputs are its own
inputs. They are split into an <b>encoder part</b>
which maps the input \( \boldsymbol{x} \) via a function \( f(\boldsymbol{x},\boldsymbol{W}) \) (this
is the encoder part) to a <b>so-called code part</b> (or intermediate part)
with the result \( \boldsymbol{h} \)
</p>

$$
\boldsymbol{h} = f(\boldsymbol{x},\boldsymbol{W})),
$$

<p>where \( \boldsymbol{W} \) are the weights to be determined.  The <b>decoder</b> parts maps, via its own parameters (weights given by the matrix \( \boldsymbol{V} \) and its own biases) to 
the final ouput
</p>
$$
\tilde{\boldsymbol{x}} = g(\boldsymbol{h},\boldsymbol{V})).
$$

<p>The goal is to minimize the construction error.</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="schematic-image-of-an-autoencoder">Schematic image of an Autoencoder </h2>

<br/><br/>
<center>
<p><img src="figures/ae1.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="more-on-the-structure">More on the structure </h2>

<p>In most typical architectures, the encoder and the decoder are neural networks
since they can be easily trained with existing software libraries such as TensorFlow or PyTorch with back propagation.
</p>

<p>In general, the encoder can be written as a function \( g \) that will depend on some parameters</p>
$$
\mathbf{h}_{i} = g(\mathbf{x}_{i}),
$$

<p>where \( \mathbf{h}_{i}\in\mathbb{R}^{q} \)  (the latent feature representation) is the output of the encoder block where we evaluate
it using the input \( \mathbf{x}_{i} \).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="decoder-part">Decoder part </h2>

<p>Note that we have \( g:\mathbb{R}^{n}\rightarrow\mathbb{R}^{q} \)
The decoder and the output of the network \( \tilde{\mathbf{x}}_{i} \) can be written then as a second generic function
of the latent features
</p>
$$
\tilde{\mathbf{x}}_{i} = f\left(\mathbf{h}_{i}\right) = f\left(g\left(\mathbf{x}_{i}\right)\right),
$$

<p>where \( \tilde{\mathbf{x}}_{i}\mathbf{\in }\mathbb{R}^{n} \).</p>

<p>Training an autoencoder simply means finding the functions \( g(\cdot) \) and \( f(\cdot) \)
that satisfy
</p>
$$
\textrm{arg}\min_{f,g}< \left[\Delta (\mathbf{x}_{i}, f(g\left(\mathbf{x}_{i}\right))\right]>.
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="typical-aes">Typical AEs </h2>

<p>The standard setup is done via a standard feed forward neural network (FFNN), or what is called a Feed Forward Autoencoder.</p>

<p>A typical FFNN architecture has a given  number of layers and is symmetrical with respect to the middle layer.</p>

<p>Typically, the first layer has a number of neurons \( n_{1} = n \) which equals the size of the input observation \( \mathbf{x}_{\mathbf{i}} \).</p>

<p>As we move toward the center of the network, the number of neurons in each layer drops in some measure.
The middle layer usually has the smallest number of neurons.
The fact that the number of neurons in this layer is smaller than the size of the input, is often called the <b>bottleneck</b>.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="feed-forward-autoencoder">Feed Forward Autoencoder </h2>

<br/><br/>
<center>
<p><img src="figures/ae2.png" width="700" align="bottom"></p>
</center>
<br/><br/>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="mirroring">Mirroring </h2>

<p>In almost all practical applications,
the layers after the middle one are a mirrored version of the layers before the middle one.
For example, an autoencoder with three layers could have the following numbers of neurons:
</p>

<p>\( n_{1} = 10 \), \( n_{2} = 5 \) and then \( n_{3} = n_{1} = 10 \) where the input dimension is equal to ten.</p>

<p>All the layers up to and including the middle one, make what is called the encoder, and all the layers from and including
the middle one (up to the output) make what is called the decoder.
</p>

<p>If the FFNN training is successful, the result will
be a good approximation of the input \( \tilde{\mathbf{x}}_{i}\approx\mathbf{x}_{i} \).
</p>

<p>What is essential to notice is that the decoder can reconstruct the
input by using only a much smaller number of features than the input
observations initially have.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="output-of-middle-layer">Output of middle layer </h2>

<p>The output of the middle layer
\( \mathbf{h}_{\mathbf{i}} \) are also called a <b>learned representation</b> of the input observation \( \mathbf{x}_{i} \).
</p>

<p>The encoder can reduce the number of dimensions of the input
observation and create a learned representation
\( \mathbf{h}_{\mathbf{i}}\mathbf{) } \) of the input that has a smaller
dimension \( q < n \).
</p>

<p>This learned representation is enough for the decoder to reconstruct
the input accurately (if the autoencoder training was successful as
intended).
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="activation-function-of-the-output-layer">Activation Function of the Output Layer </h2>

<p>In autoencoders based on neural networks, the output layer's
activation function plays a particularly important role.  The most
used functions are ReLU and Sigmoid. 
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="relu">ReLU </h2>

<p>The  ReLU activation function can assume all values in the range \( \left[0,\infty\right] \). As a remainder, its formula is</p>
$$
\textrm{ReLU}\left(x\right) = \max\left(0,x\right).
$$

<p>This choice is good when the input observations \(\mathbf{x}_{i}\) assume a wide range of positive values.
If the input \( \mathbf{x}_{i} \) can assume negative values, the ReLU is, of course, a terrible choice, and the identity function is a much better choice. It is then common to replace to the ReLU with the so-called <b>Leaky ReLu</b> or just modified ReLU.
</p>

<p>The ReLU activation function for the output layer is well suited for cases when the input observations \(\mathbf{x}_{i}\) assume a wide range of positive real values.  </p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="sigmoid">Sigmoid </h2>

<p>The sigmoid function \( \sigma \) can assume all values in the range \( [0,1] \),</p>
$$
\sigma\left(x\right) =\frac{1}{1+e^{-x}}.
$$

<p>This activation function can only be used if the input observations
\( \mathbf{x}_{i} \) are all in the range \( [0,1] \)  or if you have
normalized them to be in that range. Consider as an example the MNIST
dataset. Each value of the input observation \( \mathbf{x}_{i} \) (one
image) is the gray values of the pixels that can assume any value from
0 to 255. Normalizing the data by dividing the pixel values by 255
would make each observation (each image) have only pixel values
between 0 and 1. In this case, the sigmoid would be a good choice for
the output layer's activation function.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="cost-loss-function">Cost/Loss Function </h2>

<p>If an autoencoder is trying to solve a regression problem, the most
common choice as a loss function is the Mean Square Error
</p>

$$
L_{\textrm{MSE}} = \textrm{MSE} = \frac{1}{n}\sum_{i = 1}^{n}\left\vert\vert\mathbf{x}_{i}-\tilde{\mathbf{x}}_{i}\right\vert\vert^{2}_2.
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="binary-cross-entropy">Binary Cross-Entropy </h2>

<p>If the activation function of the output layer of the AE is a sigmoid
function, thus limiting neuron outputs to be between 0 and 1, and the
input features are normalized to be between 0 and 1 we can use as loss
function the binary cross-entropy. This cots/loss function is
typically used in classification problems, but it works well for
autoencoders. The formula for it is
</p>

$$
L_{\textrm{CE}} = -\frac{1}{n}\sum_{i = 1}^{n}\sum_{j = 1}^{p}[x_{j,i} \log\tilde{x}_{j,i}+\left(1-x_{j,i}\right)\log (1-\tilde{x}_{j,i})].
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="reconstruction-error">Reconstruction Error </h2>

<p>The reconstruction error (RE) is a metric that gives you an indication of how good (or bad) the autoencoder was able to reconstruct
the input observation \( \mathbf{x}_{i} \). The most typical RE used is the MSE
</p>

$$
\textrm{RE}\equiv \textrm{MSE} = \frac{1}{n}\sum_{i = 1}^{n}\left\vert\vert\mathbf{x}_{i}-\tilde{\mathbf{x}}_{i}\right\vert\vert^{2}_2.
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="implementation-using-tensorflow">Implementation using TensorFlow </h2>

<p>The code here has the following structure</p>
<ol>
<li> Data Loading: The MNIST dataset is loaded and normalized to a range of \( [0, 1] \). Each image is reshaped into a flat vector.</li>
<li> Model Definition: An autoencoder architecture is defined with an encoder that compresses the input and a decoder that reconstructs it back to its original form.</li>
<li> Training: The model is trained using binary crossentropy as the loss function over several epochs.</li>
<li> Visualization: After training completes, it visualizes original images alongside their reconstructions.</li>
</ol>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic">### Autoencoder Implementation in TensorFlow/Keras</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">import</span> keras
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras</span> <span style="color: #008000; font-weight: bold">import</span> layers
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">tensorflow.keras.datasets</span> <span style="color: #008000; font-weight: bold">import</span> mnist

<span style="color: #408080; font-style: italic"># Load MNIST dataset</span>
(x_train, _), (x_test, _) <span style="color: #666666">=</span> mnist<span style="color: #666666">.</span>load_data()

<span style="color: #408080; font-style: italic"># Normalize the images to [0, 1] range and reshape them to (num_samples, 28*28)</span>
x_train <span style="color: #666666">=</span> x_train<span style="color: #666666">.</span>astype(<span style="color: #BA2121">&#39;float32&#39;</span>) <span style="color: #666666">/</span> <span style="color: #666666">255.</span>
x_test <span style="color: #666666">=</span> x_test<span style="color: #666666">.</span>astype(<span style="color: #BA2121">&#39;float32&#39;</span>) <span style="color: #666666">/</span> <span style="color: #666666">255.</span>
x_train <span style="color: #666666">=</span> x_train<span style="color: #666666">.</span>reshape((<span style="color: #008000">len</span>(x_train), <span style="color: #666666">-1</span>))
x_test <span style="color: #666666">=</span> x_test<span style="color: #666666">.</span>reshape((<span style="color: #008000">len</span>(x_test), <span style="color: #666666">-1</span>))

<span style="color: #408080; font-style: italic"># Define the Autoencoder Model</span>
input_dim <span style="color: #666666">=</span> x_train<span style="color: #666666">.</span>shape[<span style="color: #666666">1</span>]
encoding_dim <span style="color: #666666">=</span> <span style="color: #666666">64</span>  <span style="color: #408080; font-style: italic"># Dimension of the encoding layer</span>

<span style="color: #408080; font-style: italic"># Encoder</span>
input_img <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Input(shape<span style="color: #666666">=</span>(input_dim,))
encoded <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(<span style="color: #666666">256</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;relu&#39;</span>)(input_img)
encoded <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(encoding_dim, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;relu&#39;</span>)(encoded)

<span style="color: #408080; font-style: italic"># Decoder</span>
decoded <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(<span style="color: #666666">256</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;relu&#39;</span>)(encoded)
decoded <span style="color: #666666">=</span> layers<span style="color: #666666">.</span>Dense(input_dim, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;sigmoid&#39;</span>)(decoded)  <span style="color: #408080; font-style: italic"># Use sigmoid since we normalized input between 0 and 1.</span>

<span style="color: #408080; font-style: italic"># Autoencoder Model</span>
autoencoder <span style="color: #666666">=</span> keras<span style="color: #666666">.</span>Model(input_img, decoded)

<span style="color: #408080; font-style: italic"># Compile the model</span>
autoencoder<span style="color: #666666">.</span>compile(optimizer<span style="color: #666666">=</span><span style="color: #BA2121">&#39;adam&#39;</span>, loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;binary_crossentropy&#39;</span>)

<span style="color: #408080; font-style: italic"># Train the model</span>
autoencoder<span style="color: #666666">.</span>fit(x_train, x_train,
                epochs<span style="color: #666666">=10</span>,
                batch_size<span style="color: #666666">=128</span>,
                shuffle<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>,
                validation_data<span style="color: #666666">=</span>(x_test, x_test))

<span style="color: #408080; font-style: italic"># Visualize some results after training</span>
decoded_imgs <span style="color: #666666">=</span> autoencoder<span style="color: #666666">.</span>predict(x_test)

n <span style="color: #666666">=</span> <span style="color: #666666">8</span>  <span style="color: #408080; font-style: italic"># Number of digits to display</span>
plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">9</span>,<span style="color: #666666">4</span>))
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(n):
    <span style="color: #408080; font-style: italic"># Display original images on top row </span>
    ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">2</span>,n,i<span style="color: #666666">+1</span>)
    plt<span style="color: #666666">.</span>imshow(x_test[i]<span style="color: #666666">.</span>reshape(<span style="color: #666666">28</span>, <span style="color: #666666">28</span>), cmap<span style="color: #666666">=</span><span style="color: #BA2121">&#39;gray&#39;</span>)
    ax<span style="color: #666666">.</span>axis(<span style="color: #BA2121">&#39;off&#39;</span>)

    <span style="color: #408080; font-style: italic"># Display reconstructed images on bottom row </span>
    ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">2</span>,n,i<span style="color: #666666">+</span>n<span style="color: #666666">+1</span>)
    plt<span style="color: #666666">.</span>imshow(decoded_imgs[i]<span style="color: #666666">.</span>reshape(<span style="color: #666666">28</span>, <span style="color: #666666">28</span>), cmap<span style="color: #666666">=</span><span style="color: #BA2121">&#39;gray&#39;</span>)
    ax<span style="color: #666666">.</span>axis(<span style="color: #BA2121">&#39;off&#39;</span>)

plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="implementation-using-pytorch">Implementation using PyTorch </h2>

<p>The code here as the same structure as the previous one which uses TensorFlow.</p>
<ol>
<li> Data Loading: The MNIST dataset is loaded with normalization applied.</li>
<li> Model Definition: An <em>Autoencoder</em> class defines both encoder and decoder networks.</li>
<li> Training part: The network is trained over several epochs using Mean Squared Error (MSE) as the loss function.</li>
<li> Visualization: After training completes, it visualizes original images alongside their reconstructions.</li>
</ol>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.nn</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">nn</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">torch.optim</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">optim</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torchvision</span> <span style="color: #008000; font-weight: bold">import</span> datasets, transforms
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">torch.utils.data</span> <span style="color: #008000; font-weight: bold">import</span> DataLoader
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>

<span style="color: #408080; font-style: italic"># Hyperparameters</span>
batch_size <span style="color: #666666">=</span> <span style="color: #666666">128</span>
learning_rate <span style="color: #666666">=</span> <span style="color: #666666">0.001</span>
num_epochs <span style="color: #666666">=</span> <span style="color: #666666">10</span>

<span style="color: #408080; font-style: italic"># Transform to normalize the data</span>
transform <span style="color: #666666">=</span> transforms<span style="color: #666666">.</span>Compose([
    transforms<span style="color: #666666">.</span>ToTensor(),
    transforms<span style="color: #666666">.</span>Normalize((<span style="color: #666666">0.5</span>,), (<span style="color: #666666">0.5</span>,))
])

<span style="color: #408080; font-style: italic"># Load MNIST dataset</span>
train_dataset <span style="color: #666666">=</span> datasets<span style="color: #666666">.</span>MNIST(root<span style="color: #666666">=</span><span style="color: #BA2121">&#39;./data&#39;</span>, train<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, transform<span style="color: #666666">=</span>transform, download<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)
train_loader <span style="color: #666666">=</span> DataLoader(dataset<span style="color: #666666">=</span>train_dataset, batch_size<span style="color: #666666">=</span>batch_size, shuffle<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>)

<span style="color: #408080; font-style: italic"># Define the Autoencoder Model</span>
<span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">Autoencoder</span>(nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>):
        <span style="color: #008000">super</span>(Autoencoder, <span style="color: #008000">self</span>)<span style="color: #666666">.</span><span style="color: #0000FF">__init__</span>()
        <span style="color: #408080; font-style: italic"># Encoder layers</span>
        <span style="color: #008000">self</span><span style="color: #666666">.</span>encoder <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>Sequential(
            nn<span style="color: #666666">.</span>Linear(<span style="color: #666666">28</span> <span style="color: #666666">*</span> <span style="color: #666666">28</span>, <span style="color: #666666">256</span>),
            nn<span style="color: #666666">.</span>ReLU(<span style="color: #008000; font-weight: bold">True</span>),
            nn<span style="color: #666666">.</span>Linear(<span style="color: #666666">256</span>, <span style="color: #666666">64</span>),
            nn<span style="color: #666666">.</span>ReLU(<span style="color: #008000; font-weight: bold">True</span>)
        )
        <span style="color: #408080; font-style: italic"># Decoder layers</span>
        <span style="color: #008000">self</span><span style="color: #666666">.</span>decoder <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>Sequential(
            nn<span style="color: #666666">.</span>Linear(<span style="color: #666666">64</span>, <span style="color: #666666">256</span>),
            nn<span style="color: #666666">.</span>ReLU(<span style="color: #008000; font-weight: bold">True</span>),
            nn<span style="color: #666666">.</span>Linear(<span style="color: #666666">256</span>, <span style="color: #666666">28</span> <span style="color: #666666">*</span> <span style="color: #666666">28</span>),
            nn<span style="color: #666666">.</span>Tanh()   <span style="color: #408080; font-style: italic"># Use Tanh since we normalized input between -1 and 1.</span>
        )

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        x <span style="color: #666666">=</span> x<span style="color: #666666">.</span>view(<span style="color: #666666">-1</span>, <span style="color: #666666">28</span> <span style="color: #666666">*</span> <span style="color: #666666">28</span>)  <span style="color: #408080; font-style: italic"># Flatten the image tensor into vectors.</span>
        encoded <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>encoder(x)
        decoded <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>decoder(encoded)
        <span style="color: #008000; font-weight: bold">return</span> decoded<span style="color: #666666">.</span>view(<span style="color: #666666">-1</span>, <span style="color: #666666">1</span>, <span style="color: #666666">28</span>, <span style="color: #666666">28</span>)   <span style="color: #408080; font-style: italic"># Reshape back to original image dimensions.</span>

<span style="color: #408080; font-style: italic"># Initialize model, loss function and optimizer</span>
model <span style="color: #666666">=</span> Autoencoder()
criterion <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>MSELoss()
optimizer <span style="color: #666666">=</span> optim<span style="color: #666666">.</span>Adam(model<span style="color: #666666">.</span>parameters(), lr<span style="color: #666666">=</span>learning_rate)

<span style="color: #408080; font-style: italic"># Training Loop</span>
<span style="color: #008000; font-weight: bold">for</span> epoch <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(num_epochs):
    <span style="color: #008000; font-weight: bold">for</span> data <span style="color: #AA22FF; font-weight: bold">in</span> train_loader:
        img, _ <span style="color: #666666">=</span> data
        
        <span style="color: #408080; font-style: italic"># Forward pass </span>
        output <span style="color: #666666">=</span> model(img)
        
        <span style="color: #408080; font-style: italic"># Compute loss </span>
        loss <span style="color: #666666">=</span> criterion(output, img)
        
        <span style="color: #408080; font-style: italic"># Backward pass and optimization </span>
        optimizer<span style="color: #666666">.</span>zero_grad()
        loss<span style="color: #666666">.</span>backward()
        optimizer<span style="color: #666666">.</span>step()

    <span style="color: #008000">print</span>(<span style="color: #BA2121">f&#39;Epoch [</span><span style="color: #BB6688; font-weight: bold">{</span>epoch <span style="color: #666666">+</span> <span style="color: #666666">1</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">/</span><span style="color: #BB6688; font-weight: bold">{</span>num_epochs<span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">], Loss: </span><span style="color: #BB6688; font-weight: bold">{</span>loss<span style="color: #666666">.</span>item()<span style="color: #BB6688; font-weight: bold">:</span><span style="color: #BA2121">.4f</span><span style="color: #BB6688; font-weight: bold">}</span><span style="color: #BA2121">&#39;</span>)

<span style="color: #408080; font-style: italic"># Visualize some results after training</span>
<span style="color: #008000; font-weight: bold">with</span> torch<span style="color: #666666">.</span>no_grad():
    sample_data <span style="color: #666666">=</span> <span style="color: #008000">next</span>(<span style="color: #008000">iter</span>(train_loader))[<span style="color: #666666">0</span>]
    reconstructed_data <span style="color: #666666">=</span> model(sample_data)

plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">9</span>,<span style="color: #666666">4</span>))
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">8</span>):
    ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">2</span>,<span style="color: #666666">8</span>,i<span style="color: #666666">+1</span>)
    plt<span style="color: #666666">.</span>imshow(sample_data[i][<span style="color: #666666">0</span>], cmap<span style="color: #666666">=</span><span style="color: #BA2121">&#39;gray&#39;</span>)
    ax<span style="color: #666666">.</span>axis(<span style="color: #BA2121">&#39;off&#39;</span>)

    ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplot(<span style="color: #666666">2</span>,<span style="color: #666666">8</span>,i<span style="color: #666666">+9</span>)
    plt<span style="color: #666666">.</span>imshow(reconstructed_data[i][<span style="color: #666666">0</span>], cmap<span style="color: #666666">=</span><span style="color: #BA2121">&#39;gray&#39;</span>)
    ax<span style="color: #666666">.</span>axis(<span style="color: #BA2121">&#39;off&#39;</span>)

plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="dimensionality-reduction-and-links-with-principal-component-analysis">Dimensionality reduction and links with Principal component analysis </h2>

<p>The hope is that the training of the autoencoder can unravel some
useful properties of the function \( f \). They are often trained with
only single-layer neural networks (although deep networks can improve
the training) and are essentially given by feed forward neural
networks.
</p>

<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="linear-functions">Linear functions </h2>

<p>If the function \( f \) and \( g \) are given by a linear dependence on the
weight matrices \( \boldsymbol{W} \) and \( \boldsymbol{V} \), we can show that for a
regression case, by miminizing the mean squared error between \( \boldsymbol{x} \)
and \( \tilde{\boldsymbol{x}} \), the autoencoder learns the same subspace as the
standard principal component analysis (PCA).
</p>

<p>In order to see this, we define then</p>
$$
\boldsymbol{h} = f(\boldsymbol{x},\boldsymbol{W}))=\boldsymbol{W}\boldsymbol{x},
$$

<p>and</p>
$$
\tilde{\boldsymbol{x}} = g(\boldsymbol{h},\boldsymbol{V}))=\boldsymbol{V}\boldsymbol{h}=\boldsymbol{V}\boldsymbol{W}\boldsymbol{x}.
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="ae-mean-squared-error">AE mean-squared error </h2>

<p>With the above linear dependence we can in turn define our
optimization problem in terms of the optimization of the mean-squared
error, that is we wish to optimize
</p>

$$
\min_{\boldsymbol{W},\boldsymbol{V}\in {\mathbb{R}}}\frac{1}{n}\sum_{i=0}^{n-1}\left(x_i-\tilde{x}_i\right)^2=\frac{1}{n}\vert\vert \boldsymbol{x}-\boldsymbol{V}\boldsymbol{W}\boldsymbol{x}\vert\vert_2^2,
$$

<p>where we have used the definition of  a norm-2 vector, that is</p>
$$
\vert\vert \boldsymbol{x}\vert\vert_2 = \sqrt{\sum_i x_i^2}. 
$$


<!-- !split --><br><br><br><br><br><br><br><br><br><br>
<h2 id="dimensionality-reduction">Dimensionality reduction </h2>

<p>This is equivalent to our functions learning the same subspace as
the PCA method. This means that we can interpret AEs as a
dimensionality reduction method.  To see this, we need to remind
ourselves about the PCA method. This will be the topic of the last lecture, on Monday November 24. We will use this lecture (second lecture) to summarize the course as well. Stay tuned.
</p>
<!-- ------------------- end of main content --------------- -->
<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2025, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</body>
</html>

