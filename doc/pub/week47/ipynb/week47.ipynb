{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83498c9",
   "metadata": {},
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html week47.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Week 47: Unsupervised learning (PCA and Clustering)  and Summary of Course -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f0819",
   "metadata": {},
   "source": [
    "# Week 47: Unsupervised learning (PCA and Clustering)  and Summary of Course\n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University\n",
    "\n",
    "Date: **Nov 22, 2022**\n",
    "\n",
    "Copyright 1999-2022, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb854d",
   "metadata": {},
   "source": [
    "## Overview of week 47\n",
    "\n",
    "* **Thursday**: Dimensionality reduction and unsupervised learning: Principal Component analysis (PCA) and clustering\n",
    "\n",
    "* **Friday**: PCA and clustering  and Summary of Course\n",
    "\n",
    "1. We recommend highly the video on PCA by [Brunton and Kutz](http://www.databookuw.com/page-2/page-4/), see in particular the video of section 1.5. Repeating about the singular value discussion is also very useful as we will use this material as background.\n",
    "\n",
    "2. [And another good video on PCA](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n",
    "\n",
    "3. [k-means clustering video](https://www.youtube.com/watch?v=4b5d3muPQmA)\n",
    "\n",
    "**Reading recommendations:**\n",
    "\n",
    "1. Geron's chapter 9 on PCA\n",
    "\n",
    "2. Hastie et al Chapter 13 (sections 13.1-13.2 are the most relevant ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824691f",
   "metadata": {},
   "source": [
    "## Basic ideas of the Principal Component Analysis (PCA)\n",
    "\n",
    "The principal component analysis deals with the problem of fitting a\n",
    "low-dimensional affine subspace $S$ of dimension $d$ much smaller than\n",
    "the total dimension $D$ of the problem at hand (our data\n",
    "set). Mathematically it can be formulated as a statistical problem or\n",
    "a geometric problem.  In our discussion of the theorem for the\n",
    "classical PCA, we will stay with a statistical approach. \n",
    "Historically, the PCA was first formulated in a statistical setting in order to estimate the principal component of a multivariate random variable.\n",
    "\n",
    "We have a data set defined by a design/feature matrix $\\boldsymbol{X}$ (see below for its definition) \n",
    "* Each data point is determined by $p$ extrinsic (measurement) variables\n",
    "\n",
    "* We may want to ask the following question: Are there fewer intrinsic variables (say $d << p$) that still approximately describe the data?\n",
    "\n",
    "* If so, these intrinsic variables may tell us something important and finding these intrinsic variables is what dimension reduction methods do. \n",
    "\n",
    "A good read is for example [Vidal, Ma and Sastry](https://www.springer.com/gp/book/9780387878102)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905a4a02",
   "metadata": {},
   "source": [
    "## Introducing the Covariance and Correlation functions\n",
    "\n",
    "Before we discuss the PCA theorem, we need to remind ourselves about\n",
    "the definition of the covariance and the correlation function. These are quantities \n",
    "\n",
    "Suppose we have defined two vectors\n",
    "$\\hat{x}$ and $\\hat{y}$ with $n$ elements each. The covariance matrix $\\boldsymbol{C}$ is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4783f702",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x},\\boldsymbol{y}] = \\begin{bmatrix} \\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{x}] & \\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{y}] \\\\\n",
    "                              \\mathrm{cov}[\\boldsymbol{y},\\boldsymbol{x}] & \\mathrm{cov}[\\boldsymbol{y},\\boldsymbol{y}] \\\\\n",
    "             \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f14c5",
   "metadata": {},
   "source": [
    "where for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9fe87",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{y}] =\\frac{1}{n} \\sum_{i=0}^{n-1}(x_i- \\overline{x})(y_i- \\overline{y}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3e4de",
   "metadata": {},
   "source": [
    "With this definition and recalling that the variance is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e2869",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{var}[\\boldsymbol{x}]=\\frac{1}{n} \\sum_{i=0}^{n-1}(x_i- \\overline{x})^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f9a40",
   "metadata": {},
   "source": [
    "we can rewrite the covariance matrix as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dbc1a5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x},\\boldsymbol{y}] = \\begin{bmatrix} \\mathrm{var}[\\boldsymbol{x}] & \\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{y}] \\\\\n",
    "                              \\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{y}] & \\mathrm{var}[\\boldsymbol{y}] \\\\\n",
    "             \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583861a5",
   "metadata": {},
   "source": [
    "## More on the covariance\n",
    "The covariance takes values between zero and infinity and may thus\n",
    "lead to problems with loss of numerical precision for particularly\n",
    "large values. It is common to scale the covariance matrix by\n",
    "introducing instead the correlation matrix defined via the so-called\n",
    "correlation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6523a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{corr}[\\boldsymbol{x},\\boldsymbol{y}]=\\frac{\\mathrm{cov}[\\boldsymbol{x},\\boldsymbol{y}]}{\\sqrt{\\mathrm{var}[\\boldsymbol{x}] \\mathrm{var}[\\boldsymbol{y}]}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c685ab",
   "metadata": {},
   "source": [
    "The correlation function is then given by values $\\mathrm{corr}[\\boldsymbol{x},\\boldsymbol{y}]\n",
    "\\in [-1,1]$. This avoids eventual problems with too large values. We\n",
    "can then define the correlation matrix for the two vectors $\\boldsymbol{x}$\n",
    "and $\\boldsymbol{y}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe3f5f7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{K}[\\boldsymbol{x},\\boldsymbol{y}] = \\begin{bmatrix} 1 & \\mathrm{corr}[\\boldsymbol{x},\\boldsymbol{y}] \\\\\n",
    "                              \\mathrm{corr}[\\boldsymbol{y},\\boldsymbol{x}] & 1 \\\\\n",
    "             \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219eba8c",
   "metadata": {},
   "source": [
    "In the above example this is the function we constructed using **pandas**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a61e3",
   "metadata": {},
   "source": [
    "## Reminding ourselves about Linear Regression\n",
    "In our derivation of the various regression algorithms like **Ordinary Least Squares** or **Ridge regression**\n",
    "we defined the design/feature matrix $\\boldsymbol{X}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9acdb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\begin{bmatrix}\n",
    "x_{0,0} & x_{0,1} & x_{0,2}& \\dots & \\dots x_{0,p-1}\\\\\n",
    "x_{1,0} & x_{1,1} & x_{1,2}& \\dots & \\dots x_{1,p-1}\\\\\n",
    "x_{2,0} & x_{2,1} & x_{2,2}& \\dots & \\dots x_{2,p-1}\\\\\n",
    "\\dots & \\dots & \\dots & \\dots \\dots & \\dots \\\\\n",
    "x_{n-2,0} & x_{n-2,1} & x_{n-2,2}& \\dots & \\dots x_{n-2,p-1}\\\\\n",
    "x_{n-1,0} & x_{n-1,1} & x_{n-1,2}& \\dots & \\dots x_{n-1,p-1}\\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd640d7e",
   "metadata": {},
   "source": [
    "with $\\boldsymbol{X}\\in {\\mathbb{R}}^{n\\times p}$, with the predictors/features $p$  refering to the column numbers and the\n",
    "entries $n$ being the row elements.\n",
    "We can rewrite the design/feature matrix in terms of its column vectors as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307231e1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\begin{bmatrix} \\boldsymbol{x}_0 & \\boldsymbol{x}_1 & \\boldsymbol{x}_2 & \\dots & \\dots & \\boldsymbol{x}_{p-1}\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165621f",
   "metadata": {},
   "source": [
    "with a given vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569eb876",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{x}_i^T = \\begin{bmatrix}x_{0,i} & x_{1,i} & x_{2,i}& \\dots & \\dots x_{n-1,i}\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f53560e",
   "metadata": {},
   "source": [
    "## Simple Example\n",
    "With these definitions, we can now rewrite our $2\\times 2$\n",
    "correlation/covariance matrix in terms of a moe general design/feature\n",
    "matrix $\\boldsymbol{X}\\in {\\mathbb{R}}^{n\\times p}$. This leads to a $p\\times p$\n",
    "covariance matrix for the vectors $\\boldsymbol{x}_i$ with $i=0,1,\\dots,p-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011f58c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x}] = \\begin{bmatrix}\n",
    "\\mathrm{var}[\\boldsymbol{x}_0] & \\mathrm{cov}[\\boldsymbol{x}_0,\\boldsymbol{x}_1]  & \\mathrm{cov}[\\boldsymbol{x}_0,\\boldsymbol{x}_2] & \\dots & \\dots & \\mathrm{cov}[\\boldsymbol{x}_0,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\mathrm{cov}[\\boldsymbol{x}_1,\\boldsymbol{x}_0] & \\mathrm{var}[\\boldsymbol{x}_1]  & \\mathrm{cov}[\\boldsymbol{x}_1,\\boldsymbol{x}_2] & \\dots & \\dots & \\mathrm{cov}[\\boldsymbol{x}_1,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\mathrm{cov}[\\boldsymbol{x}_2,\\boldsymbol{x}_0]   & \\mathrm{cov}[\\boldsymbol{x}_2,\\boldsymbol{x}_1] & \\mathrm{var}[\\boldsymbol{x}_2] & \\dots & \\dots & \\mathrm{cov}[\\boldsymbol{x}_2,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "\\mathrm{cov}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_0]   & \\mathrm{cov}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_1] & \\mathrm{cov}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_{2}]  & \\dots & \\dots  & \\mathrm{var}[\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c7d25",
   "metadata": {},
   "source": [
    "## The Correlation Matrix\n",
    "\n",
    "and the correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47cfea6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{K}[\\boldsymbol{x}] = \\begin{bmatrix}\n",
    "1 & \\mathrm{corr}[\\boldsymbol{x}_0,\\boldsymbol{x}_1]  & \\mathrm{corr}[\\boldsymbol{x}_0,\\boldsymbol{x}_2] & \\dots & \\dots & \\mathrm{corr}[\\boldsymbol{x}_0,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\mathrm{corr}[\\boldsymbol{x}_1,\\boldsymbol{x}_0] & 1  & \\mathrm{corr}[\\boldsymbol{x}_1,\\boldsymbol{x}_2] & \\dots & \\dots & \\mathrm{corr}[\\boldsymbol{x}_1,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\mathrm{corr}[\\boldsymbol{x}_2,\\boldsymbol{x}_0]   & \\mathrm{corr}[\\boldsymbol{x}_2,\\boldsymbol{x}_1] & 1 & \\dots & \\dots & \\mathrm{corr}[\\boldsymbol{x}_2,\\boldsymbol{x}_{p-1}]\\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "\\mathrm{corr}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_0]   & \\mathrm{corr}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_1] & \\mathrm{corr}[\\boldsymbol{x}_{p-1},\\boldsymbol{x}_{2}]  & \\dots & \\dots  & 1\\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776b677",
   "metadata": {},
   "source": [
    "## Numpy Functionality\n",
    "\n",
    "The Numpy function **np.cov** calculates the covariance elements using\n",
    "the factor $1/(n-1)$ instead of $1/n$ since it assumes we do not have\n",
    "the exact mean values.  The following simple function uses the\n",
    "**np.vstack** function which takes each vector of dimension $1\\times n$\n",
    "and produces a $2\\times n$ matrix $\\boldsymbol{W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8b835",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{W}^T = \\begin{bmatrix} x_0 & y_0 \\\\\n",
    "                          x_1 & y_1 \\\\\n",
    "                          x_2 & y_2\\\\\n",
    "                          \\dots & \\dots \\\\\n",
    "                          x_{n-2} & y_{n-2}\\\\\n",
    "                          x_{n-1} & y_{n-1} & \n",
    "             \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385fe3d",
   "metadata": {},
   "source": [
    "which in turn is converted into into the $2\\times 2$ covariance matrix\n",
    "$\\boldsymbol{C}$ via the Numpy function **np.cov()**. We note that we can also calculate\n",
    "the mean value of each set of samples $\\boldsymbol{x}$ etc using the Numpy\n",
    "function **np.mean(x)**. We can also extract the eigenvalues of the\n",
    "covariance matrix through the **np.linalg.eig()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43385082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "import numpy as np\n",
    "n = 100\n",
    "x = np.random.normal(size=n)\n",
    "print(np.mean(x))\n",
    "y = 4+3*x+np.random.normal(size=n)\n",
    "print(np.mean(y))\n",
    "W = np.vstack((x, y))\n",
    "C = np.cov(W)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ebeb2",
   "metadata": {},
   "source": [
    "## Correlation Matrix again\n",
    "\n",
    "The previous example can be converted into the correlation matrix by\n",
    "simply scaling the matrix elements with the variances.  We should also\n",
    "subtract the mean values for each column. This leads to the following\n",
    "code which sets up the correlations matrix for the previous example in\n",
    "a more brute force way. Here we scale the mean values for each column of the design matrix, calculate the relevant mean values and variances and then finally set up the $2\\times 2$ correlation matrix (since we have only two vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1df3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "# define two vectors                                                                                           \n",
    "x = np.random.random(size=n)\n",
    "y = 4+3*x+np.random.normal(size=n)\n",
    "#scaling the x and y vectors                                                                                   \n",
    "x = x - np.mean(x)\n",
    "y = y - np.mean(y)\n",
    "variance_x = np.sum(x@x)/n\n",
    "variance_y = np.sum(y@y)/n\n",
    "print(variance_x)\n",
    "print(variance_y)\n",
    "cov_xy = np.sum(x@y)/n\n",
    "cov_xx = np.sum(x@x)/n\n",
    "cov_yy = np.sum(y@y)/n\n",
    "C = np.zeros((2,2))\n",
    "C[0,0]= cov_xx/variance_x\n",
    "C[1,1]= cov_yy/variance_y\n",
    "C[0,1]= cov_xy/np.sqrt(variance_y*variance_x)\n",
    "C[1,0]= C[0,1]\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e3bc7",
   "metadata": {},
   "source": [
    "We see that the matrix elements along the diagonal are one as they\n",
    "should be and that the matrix is symmetric. Furthermore, diagonalizing\n",
    "this matrix we easily see that it is a positive definite matrix.\n",
    "\n",
    "The above procedure with **numpy** can be made more compact if we use **pandas**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd243b",
   "metadata": {},
   "source": [
    "## Using Pandas\n",
    "\n",
    "We whow here how we can set up the correlation matrix using **pandas**, as done in this simple code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5840dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "n = 10\n",
    "x = np.random.normal(size=n)\n",
    "x = x - np.mean(x)\n",
    "y = 4+3*x+np.random.normal(size=n)\n",
    "y = y - np.mean(y)\n",
    "X = (np.vstack((x, y))).T\n",
    "print(X)\n",
    "Xpd = pd.DataFrame(X)\n",
    "print(Xpd)\n",
    "correlation_matrix = Xpd.corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d4eab",
   "metadata": {},
   "source": [
    "## And then the Franke Function\n",
    "\n",
    "We expand this model to the Franke function discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c80e7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def FrankeFunction(x,y):\n",
    "\tterm1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "\tterm2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "\tterm3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "\tterm4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "\treturn term1 + term2 + term3 + term4\n",
    "\n",
    "\n",
    "def create_X(x, y, n ):\n",
    "\tif len(x.shape) > 1:\n",
    "\t\tx = np.ravel(x)\n",
    "\t\ty = np.ravel(y)\n",
    "\n",
    "\tN = len(x)\n",
    "\tl = int((n+1)*(n+2)/2)\t\t# Number of elements in beta\n",
    "\tX = np.ones((N,l))\n",
    "\n",
    "\tfor i in range(1,n+1):\n",
    "\t\tq = int((i)*(i+1)/2)\n",
    "\t\tfor k in range(i+1):\n",
    "\t\t\tX[:,q+k] = (x**(i-k))*(y**k)\n",
    "\n",
    "\treturn X\n",
    "\n",
    "\n",
    "# Making meshgrid of datapoints and compute Franke's function\n",
    "n = 4\n",
    "N = 100\n",
    "x = np.sort(np.random.uniform(0, 1, N))\n",
    "y = np.sort(np.random.uniform(0, 1, N))\n",
    "z = FrankeFunction(x, y)\n",
    "X = create_X(x, y, n=n)    \n",
    "\n",
    "Xpd = pd.DataFrame(X)\n",
    "# subtract the mean values and set up the covariance matrix\n",
    "Xpd = Xpd - Xpd.mean()\n",
    "covariance_matrix = Xpd.cov()\n",
    "print(covariance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161b5f6",
   "metadata": {},
   "source": [
    "We note here that the covariance is zero for the first rows and\n",
    "columns since all matrix elements in the design matrix were set to one\n",
    "(we are fitting the function in terms of a polynomial of degree $n$). We would however not include the intercept\n",
    "and wee can simply\n",
    "drop these elements and construct a correlation\n",
    "matrix without them by centering our matrix elements by subtracting the mean of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd30963b",
   "metadata": {},
   "source": [
    "## Links with the Design Matrix\n",
    "\n",
    "We can rewrite the covariance matrix in a more compact form in terms of the design/feature matrix $\\boldsymbol{X}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7863a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x}] = \\frac{1}{n}\\boldsymbol{X}^T\\boldsymbol{X}= \\mathbb{E}[\\boldsymbol{X}^T\\boldsymbol{X}].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65099d0c",
   "metadata": {},
   "source": [
    "To see this let us simply look at a design matrix $\\boldsymbol{X}\\in {\\mathbb{R}}^{2\\times 2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c1b67",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\begin{bmatrix}\n",
    "x_{00} & x_{01}\\\\\n",
    "x_{10} & x_{11}\\\\\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "\\boldsymbol{x}_{0} & \\boldsymbol{x}_{1}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a9d26",
   "metadata": {},
   "source": [
    "## Computing the Expectation Values\n",
    "\n",
    "If we then compute the expectation value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886e7a9",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}[\\boldsymbol{X}^T\\boldsymbol{X}] = \\frac{1}{n}\\boldsymbol{X}^T\\boldsymbol{X}=\\begin{bmatrix}\n",
    "x_{00}^2+x_{01}^2 & x_{00}x_{10}+x_{01}x_{11}\\\\\n",
    "x_{10}x_{00}+x_{11}x_{01} & x_{10}^2+x_{11}^2\\\\\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ee22ef",
   "metadata": {},
   "source": [
    "which is just"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d72aa6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x}_0,\\boldsymbol{x}_1] = \\boldsymbol{C}[\\boldsymbol{x}]=\\begin{bmatrix} \\mathrm{var}[\\boldsymbol{x}_0] & \\mathrm{cov}[\\boldsymbol{x}_0,\\boldsymbol{x}_1] \\\\\n",
    "                              \\mathrm{cov}[\\boldsymbol{x}_1,\\boldsymbol{x}_0] & \\mathrm{var}[\\boldsymbol{x}_1] \\\\\n",
    "             \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329fbab3",
   "metadata": {},
   "source": [
    "where we wrote $$\\boldsymbol{C}[\\boldsymbol{x}_0,\\boldsymbol{x}_1] = \\boldsymbol{C}[\\boldsymbol{x}]$$ to indicate that this the covariance of the vectors $\\boldsymbol{x}$ of the design/feature matrix $\\boldsymbol{X}$.\n",
    "\n",
    "It is easy to generalize this to a matrix $\\boldsymbol{X}\\in {\\mathbb{R}}^{n\\times p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0eee8",
   "metadata": {},
   "source": [
    "## Towards the PCA theorem\n",
    "\n",
    "We have that the covariance matrix (the correlation matrix involves a simple rescaling) is given as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77dcd3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x}] = \\frac{1}{n}\\boldsymbol{X}^T\\boldsymbol{X}= \\mathbb{E}[\\boldsymbol{X}^T\\boldsymbol{X}].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb1305",
   "metadata": {},
   "source": [
    "Let us now assume that we can perform a series of orthogonal transformations where we employ some orthogonal matrices $\\boldsymbol{S}$.\n",
    "These matrices are defined as $\\boldsymbol{S}\\in {\\mathbb{R}}^{p\\times p}$ and obey the orthogonality requirements $\\boldsymbol{S}\\boldsymbol{S}^T=\\boldsymbol{S}^T\\boldsymbol{S}=\\boldsymbol{I}$. The matrix can be written out in terms of the column vectors $\\boldsymbol{s}_i$ as $\\boldsymbol{S}=[\\boldsymbol{s}_0,\\boldsymbol{s}_1,\\dots,\\boldsymbol{s}_{p-1}]$ and $\\boldsymbol{s}_i \\in {\\mathbb{R}}^{p}$.\n",
    "\n",
    "Assume also that there is a transformation $\\boldsymbol{S}^T\\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{S}=\\boldsymbol{C}[\\boldsymbol{y}]$ such that the new matrix $\\boldsymbol{C}[\\boldsymbol{y}]$ is diagonal with elements $[\\lambda_0,\\lambda_1,\\lambda_2,\\dots,\\lambda_{p-1}]$.  \n",
    "\n",
    "That is we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eededfc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{y}] = \\mathbb{E}[\\boldsymbol{S}^T\\boldsymbol{X}^T\\boldsymbol{X}T\\boldsymbol{S}]=\\boldsymbol{S}^T\\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{S},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13cd39",
   "metadata": {},
   "source": [
    "since the matrix $\\boldsymbol{S}$ is not a data dependent matrix.   Multiplying with $\\boldsymbol{S}$ from the left we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079510b9",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{S}\\boldsymbol{C}[\\boldsymbol{y}] = \\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{S},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5f232",
   "metadata": {},
   "source": [
    "and since $\\boldsymbol{C}[\\boldsymbol{y}]$ is diagonal we have for a given eigenvalue $i$ of the covariance matrix that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83004d5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{S}_i\\lambda_i = \\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{S}_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301a0b5",
   "metadata": {},
   "source": [
    "## More on the PCA Theorem\n",
    "\n",
    "In the derivation of the PCA theorem we will assume that the\n",
    "eigenvalues are ordered in descending order, that is $\\lambda_0 > \\lambda_1 > \\dots > \\lambda_{p-1}$.\n",
    "\n",
    "The eigenvalues tell us then how much we need to stretch the\n",
    "corresponding eigenvectors. Dimensions with large eigenvalues have\n",
    "thus large variations (large variance) and define therefore useful\n",
    "dimensions. The data points are more spread out in the direction of\n",
    "these eigenvectors.  Smaller eigenvalues mean on the other hand that\n",
    "the corresponding eigenvectors are shrunk accordingly and the data\n",
    "points are tightly bunched together and there is not much variation in\n",
    "these specific directions. Hopefully then we could leave it out\n",
    "dimensions where the eigenvalues are very small. If $p$ is very large,\n",
    "we could then aim at reducing $p$ to $l << p$ and handle only $l$\n",
    "features/predictors.\n",
    "\n",
    "Here is how we would proceed in setting up the algorithm for the PCA, see also discussion below here. \n",
    "* Set up the datapoints for the design/feature matrix with the predictors/features $p$  referring to the column numbers and the entries $n$ being the row elements.\n",
    "\n",
    "* Center the data by subtracting the mean value for each column. \n",
    "\n",
    "* Compute then the covariance/correlation matrix.\n",
    "\n",
    "* Find the eigenpairs of the covariance matrix with eigenvalues $[\\lambda_0,\\lambda_1,\\dots,\\lambda_{p-1}]$ and eigenvectors $[\\boldsymbol{s}_0,\\boldsymbol{s}_1,\\dots,\\boldsymbol{s}_{p-1}]$.\n",
    "\n",
    "* Order the eigenvalue (and the eigenvectors accordingly) in order of decreasing eigenvalues.\n",
    "\n",
    "* Keep only those $l$ eigenvalues larger than a selected threshold value, discarding thus $p-l$ features since we expect small variations in the data here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8a7ed",
   "metadata": {},
   "source": [
    "## A kind of Bird's view  on PCA\n",
    "\n",
    "**Why do we maximize variance during Principal Component Analysis?**\n",
    "\n",
    "Variance is a measure of the *variability* of the data you\n",
    "have. Potentially the number of components is infinite, so you want to \"squeeze\" the most\n",
    "information in each component of the finite set you build.\n",
    "\n",
    "If, to exaggerate, you were to select a single principal component,\n",
    "you would want it to account for the most variability possible: hence\n",
    "the search for maximum variance, so that the one component collects\n",
    "the most \"uniqueness\" from the data set.\n",
    "\n",
    "Maximizing the component vector variances is the same as maximizing\n",
    "the 'uniqueness' of those vectors. The vectors are as distant\n",
    "from each other as possible (orthogonal to each other).\n",
    "\n",
    "Take for example a situation where you have 2 lines that are\n",
    "orthogonal in a 3D space. You can capture the environment much more\n",
    "completely with those orthogonal lines than 2 lines that are parallel\n",
    "(or nearly parallel). When applied to very high dimensional states\n",
    "using very few vectors, this becomes a much more important\n",
    "relationship among the vectors to maintain. In a linear algebra sense\n",
    "you want independent rows to be produced by PCA, otherwise some of\n",
    "those rows will be redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48271a8",
   "metadata": {},
   "source": [
    "## Writing our own PCA code\n",
    "\n",
    "We will use a simple example first with two-dimensional data\n",
    "drawn from a multivariate normal distribution with the following mean and covariance matrix (we have fixed these quantities but will play around with them below):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d6031",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu = (-1,2) \\qquad \\Sigma = \\begin{bmatrix} 4 & 2 \\\\\n",
    "2 & 2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def29dc6",
   "metadata": {},
   "source": [
    "Note that the mean refers to each column of data. \n",
    "We will generate $n = 10000$ points $X = \\{ x_1, \\ldots, x_N \\}$ from\n",
    "this distribution, and store them in the $1000 \\times 2$ matrix $\\boldsymbol{X}$. This is our design matrix where we have forced the covariance and mean values to take specific values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40064229",
   "metadata": {},
   "source": [
    "## Implementing it\n",
    "The following Python code aids in setting up the data and writing out the design matrix.\n",
    "Note that the function **multivariate** returns also the covariance discussed above and that it is defined by dividing by $n-1$ instead of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c97695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "n = 10000\n",
    "mean = (-1, 2)\n",
    "cov = [[20, 0.2], [0.2, 0.2]]\n",
    "X = np.random.multivariate_normal(mean, cov, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2785547a",
   "metadata": {},
   "source": [
    "Now we are going to implement the PCA algorithm. We will break it down into various substeps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a2380",
   "metadata": {},
   "source": [
    "## First Step\n",
    "\n",
    "The first step of PCA is to compute the sample mean of the data and use it to center the data. Recall that the sample mean is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44ee8c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu_n = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02d9a1",
   "metadata": {},
   "source": [
    "and the mean-centered data $\\bar{X} = \\{ \\bar{x}_1, \\ldots, \\bar{x}_n \\}$ takes the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86add3a3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{x}_i = x_i - \\mu_n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83136dc",
   "metadata": {},
   "source": [
    "When you are done with these steps, print out $\\mu_n$ to verify it is\n",
    "close to $\\mu$ and plot your mean centered data to verify it is\n",
    "centered at the origin! \n",
    "The following code elements perform these operations using **pandas** or using our own functionality for doing so. The latter, using **numpy** is rather simple through the **mean()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "211651ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)\n",
    "# Pandas does the centering for us\n",
    "df = df -df.mean()\n",
    "# we center it ourselves\n",
    "X_centered = X - X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4dcc5",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "Alternatively, we could use the functions we discussed\n",
    "earlier for scaling the data set.  That is, we could have used the\n",
    "**StandardScaler** function in **Scikit-Learn**, a function which ensures\n",
    "that for each feature/predictor we study the mean value is zero and\n",
    "the variance is one (every column in the design/feature matrix).  You\n",
    "would then not get the same results, since we divide by the\n",
    "variance. The diagonal covariance matrix elements will then be one,\n",
    "while the non-diagonal ones need to be divided by $2\\sqrt{2}$ for our\n",
    "specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a3192e",
   "metadata": {},
   "source": [
    "## Centered Data\n",
    "\n",
    "Now we are going to use the mean centered data to compute the sample covariance of the data by using the following equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ffd716",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Sigma_n = \\frac{1}{n-1} \\sum_{i=1}^n \\bar{x}_i^T \\bar{x}_i = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\mu_n)^T (x_i - \\mu_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b60d5",
   "metadata": {},
   "source": [
    "where the data points $x_i \\in \\mathbb{R}^p$ (here in this example $p = 2$) are column vectors and $x^T$ is the transpose of $x$.\n",
    "We can write our own code or simply use either the functionaly of **numpy** or that of **pandas**, as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "472fab3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1\n",
      "0  19.993702  0.184113\n",
      "1   0.184113  0.198691\n",
      "[[19.99370195  0.18411281]\n",
      " [ 0.18411281  0.19869054]]\n"
     ]
    }
   ],
   "source": [
    "print(df.cov())\n",
    "print(np.cov(X_centered.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ca582",
   "metadata": {},
   "source": [
    "Note that the way we define the covariance matrix here has a factor $n-1$ instead of $n$. This is included in the **cov()** function by **numpy** and **pandas**. \n",
    "Our own code here is not very elegant and asks for obvious improvements. It is tailored to this specific $2\\times 2$ covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a380846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centered covariance using own code\n",
      "[[19.99370195  0.18411281]\n",
      " [ 0.18411281  0.19869054]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract the relevant columns from the centered design matrix of dim n x 2\n",
    "x = X_centered[:,0]\n",
    "y = X_centered[:,1]\n",
    "Cov = np.zeros((2,2))\n",
    "Cov[0,1] = np.sum(x.T@y)/(n-1.0)\n",
    "Cov[0,0] = np.sum(x.T@x)/(n-1.0)\n",
    "Cov[1,1] = np.sum(y.T@y)/(n-1.0)\n",
    "Cov[1,0]= Cov[0,1]\n",
    "print(\"Centered covariance using own code\")\n",
    "print(Cov)\n",
    "plt.plot(x, y, 'x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abed26",
   "metadata": {},
   "source": [
    "## Exploring\n",
    "\n",
    "Depending on the number of points $n$, we will get results that are close to the covariance values defined above.\n",
    "The plot shows how the data are clustered around a line with slope close to one. Is this expected?  Try to change the covariance and the mean values. For example, try to make the variance of the first element much larger than that of the second diagonal element. Try also to shrink the covariance  (the non-diagonal elements) and see how the data points are distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99342eb",
   "metadata": {},
   "source": [
    "## Diagonalize the sample covariance matrix to obtain the principal components\n",
    "\n",
    "Now we are ready to solve for the principal components! To do so we\n",
    "diagonalize the sample covariance matrix $\\Sigma$. We can use the\n",
    "function **np.linalg.eig** to do so. It will return the eigenvalues and\n",
    "eigenvectors of $\\Sigma$. Once we have these we can perform the \n",
    "following tasks:\n",
    "\n",
    "* We compute the percentage of the total variance captured by the first principal component\n",
    "\n",
    "* We plot the mean centered data and lines along the first and second principal components\n",
    "\n",
    "* Then we project the mean centered data onto the first and second principal components, and plot the projected data. \n",
    "\n",
    "* Finally, we approximate the data as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f713cbc",
   "metadata": {},
   "source": [
    "$$\n",
    "x_i \\approx \\tilde{x}_i = \\mu_n + \\langle x_i, v_0 \\rangle v_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835e685",
   "metadata": {},
   "source": [
    "where $v_0$ is the first principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80188dc",
   "metadata": {},
   "source": [
    "## Collecting all Steps\n",
    "\n",
    "Collecting all these steps we can write our own PCA function and\n",
    "compare this with the functionality included in **Scikit-Learn**.  \n",
    "\n",
    "The code here outlines some of the elements we could include in the\n",
    "analysis. Feel free to extend upon this in order to address the above\n",
    "questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdfa7196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagonalize and obtain eigenvalues, not necessarily sorted\n",
    "EigValues, EigVectors = np.linalg.eig(Cov)\n",
    "# sort eigenvectors and eigenvalues\n",
    "#permute = EigValues.argsort()\n",
    "#EigValues = EigValues[permute]\n",
    "#EigVectors = EigVectors[:,permute]\n",
    "print(\"Eigenvalues of Covariance matrix\")\n",
    "for i in range(2):\n",
    "    print(EigValues[i])\n",
    "FirstEigvector = EigVectors[:,0]\n",
    "SecondEigvector = EigVectors[:,1]\n",
    "print(\"First eigenvector\")\n",
    "print(FirstEigvector)\n",
    "print(\"Second eigenvector\")\n",
    "print(SecondEigvector)\n",
    "#thereafter we do a PCA with Scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X2Dsl = pca.fit_transform(X)\n",
    "print(\"Eigenvector of largest eigenvalue\")\n",
    "print(pca.components_.T[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759948ad",
   "metadata": {},
   "source": [
    "This code does not contain all the above elements, but it shows how we can use **Scikit-Learn** to extract the eigenvector which corresponds to the largest eigenvalue. Try to address the questions we pose before the above code.  Try also to change the values of the covariance matrix by making one of the diagonal elements much larger than the other. What do you observe then?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd62e2",
   "metadata": {},
   "source": [
    "## Classical PCA Theorem\n",
    "\n",
    "We assume now that we have a design matrix $\\boldsymbol{X}$ which has been\n",
    "centered as discussed above. For the sake of simplicity we skip the\n",
    "overline symbol. The matrix is defined in terms of the various column\n",
    "vectors $[\\boldsymbol{x}_0,\\boldsymbol{x}_1,\\dots, \\boldsymbol{x}_{p-1}]$ each with dimension\n",
    "$\\boldsymbol{x}\\in {\\mathbb{R}}^{n}$.\n",
    "\n",
    "The PCA theorem states that minimizing the above reconstruction error\n",
    "corresponds to setting $\\boldsymbol{W}=\\boldsymbol{S}$, the orthogonal matrix which\n",
    "diagonalizes the empirical covariance(correlation) matrix. The optimal\n",
    "low-dimensional encoding of the data is then given by a set of vectors\n",
    "$\\boldsymbol{z}_i$ with at most $l$ vectors, with $l << p$, defined by the\n",
    "orthogonal projection of the data onto the columns spanned by the\n",
    "eigenvectors of the covariance(correlations matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df57ba",
   "metadata": {},
   "source": [
    "## The PCA Theorem\n",
    "\n",
    "To show the PCA theorem let us start with the assumption that there is one vector $\\boldsymbol{s}_0$ which corresponds to a solution which minimized the reconstruction error $J$. This is an orthogonal vector. It means that we now approximate the reconstruction error in terms of $\\boldsymbol{w}_0$ and $\\boldsymbol{z}_0$ as\n",
    "\n",
    "We are almost there, we have obtained a relation between minimizing\n",
    "the reconstruction error and the variance and the covariance\n",
    "matrix. Minimizing the error is equivalent to maximizing the variance\n",
    "of the projected data.\n",
    "\n",
    "We could trivially maximize the variance of the projection (and\n",
    "thereby minimize the error in the reconstruction function) by letting\n",
    "the norm-2 of $\\boldsymbol{w}_0$ go to infinity. However, this norm since we\n",
    "want the matrix $\\boldsymbol{W}$ to be an orthogonal matrix, is constrained by\n",
    "$\\vert\\vert \\boldsymbol{w}_0 \\vert\\vert_2^2=1$. Imposing this condition via a\n",
    "Lagrange multiplier we can then in turn maximize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e8ad2",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\boldsymbol{w}_0)= \\boldsymbol{w}_0^T\\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{w}_0+\\lambda_0(1-\\boldsymbol{w}_0^T\\boldsymbol{w}_0).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ddd4a7",
   "metadata": {},
   "source": [
    "Taking the derivative with respect to $\\boldsymbol{w}_0$ we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a5a03",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J(\\boldsymbol{w}_0)}{\\partial \\boldsymbol{w}_0}= 2\\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{w}_0-2\\lambda_0\\boldsymbol{w}_0=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844b665",
   "metadata": {},
   "source": [
    "meaning that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aea501",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{w}_0=\\lambda_0\\boldsymbol{w}_0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120dd213",
   "metadata": {},
   "source": [
    "**The direction that maximizes the variance (or minimizes the construction error) is an eigenvector of the covariance matrix**! If we left multiply with $\\boldsymbol{w}_0^T$ we have the variance of the projected data is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc8b29",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{w}_0^T\\boldsymbol{C}[\\boldsymbol{x}]\\boldsymbol{w}_0=\\lambda_0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127aa4c0",
   "metadata": {},
   "source": [
    "If we want to maximize the variance (minimize the construction error)\n",
    "we simply pick the eigenvector of the covariance matrix with the\n",
    "largest eigenvalue. This establishes the link between the minimization\n",
    "of the reconstruction function $J$ in terms of an orthogonal matrix\n",
    "and the maximization of the variance and thereby the covariance of our\n",
    "observations encoded in the design/feature matrix $\\boldsymbol{X}$.\n",
    "\n",
    "The proof\n",
    "for the other eigenvectors $\\boldsymbol{w}_1,\\boldsymbol{w}_2,\\dots$ can be\n",
    "established by applying the above arguments and using the fact that\n",
    "our basis of eigenvectors is orthogonal, see [Murphy chapter\n",
    "12.2](https://mitpress.mit.edu/books/machine-learning-1).  The\n",
    "discussion in chapter 12.2 of Murphy's text has also a nice link with\n",
    "the Singular Value Decomposition theorem. For categorical data, see\n",
    "chapter 12.4 and discussion therein.\n",
    "\n",
    "For more details, see for example [Vidal, Ma and Sastry, chapter 2](https://www.springer.com/gp/book/9780387878102)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c67d1b",
   "metadata": {},
   "source": [
    "## Geometric Interpretation and link with Singular Value Decomposition\n",
    "\n",
    "For a detailed demonstration of the geometric interpretation, see [Vidal, Ma and Sastry, section 2.1.2](https://www.springer.com/gp/book/9780387878102).\n",
    "\n",
    "Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm.\n",
    "First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n",
    "\n",
    "The following Python code uses NumPys **svd()** function to obtain all the principal components of the\n",
    "training set, then extracts the first two principal components. First we center the data using either **pandas** or our own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f2885aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "np.random.seed(100)\n",
    "# setting up a 10 x 5 vanilla matrix \n",
    "rows = 10\n",
    "cols = 5\n",
    "X = np.random.randn(rows,cols)\n",
    "df = pd.DataFrame(X)\n",
    "# Pandas does the centering for us\n",
    "df = df -df.mean()\n",
    "display(df)\n",
    "\n",
    "# we center it ourselves\n",
    "X_centered = X - X.mean(axis=0)\n",
    "# Then check the difference between pandas and our own set up\n",
    "print(X_centered-df)\n",
    "#Now we do an SVD\n",
    "U, s, V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:, 0]\n",
    "c2 = V.T[:, 1]\n",
    "W2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(W2)\n",
    "print(X2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a710c59",
   "metadata": {},
   "source": [
    "PCA assumes that the dataset is centered around the origin. Scikit-Learns PCA classes take care of centering\n",
    "the data for you. However, if you implement PCA yourself (as in the preceding example), or if you use other libraries, dont\n",
    "forget to center the data first.\n",
    "\n",
    "Once you have identified all the principal components, you can reduce the dimensionality of the dataset\n",
    "down to $d$ dimensions by projecting it onto the hyperplane defined by the first $d$ principal components.\n",
    "Selecting this hyperplane ensures that the projection will preserve as much variance as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9795e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e67d3",
   "metadata": {},
   "source": [
    "## PCA and scikit-learn\n",
    "\n",
    "Scikit-Learns PCA class implements PCA using SVD decomposition just like we did before. The\n",
    "following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note\n",
    "that it automatically takes care of centering the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7b6979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thereafter we do a PCA with Scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)\n",
    "print(X2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583397ec",
   "metadata": {},
   "source": [
    "After fitting the PCA transformer to the dataset, you can access the principal components using the\n",
    "components variable (note that it contains the PCs as horizontal vectors, so, for example, the first\n",
    "principal component is equal to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "210037e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.T[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577bb61b",
   "metadata": {},
   "source": [
    "Another very useful piece of information is the explained variance ratio of each principal component,\n",
    "available via the $explained\\_variance\\_ratio$ variable. It indicates the proportion of the datasets\n",
    "variance that lies along the axis of each principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d3db5",
   "metadata": {},
   "source": [
    "## Back to the Cancer Data\n",
    "We can now repeat the above but applied to real data, in this case our breast cancer data.\n",
    "Here we compute performance scores on the training data using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8827956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import  train_test_split \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data,cancer.target,random_state=0)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "print(\"Train set accuracy from Logistic Regression: {:.2f}\".format(logreg.score(X_train,y_train)))\n",
    "# We scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Then perform again a log reg fit\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print(\"Train set accuracy scaled data: {:.2f}\".format(logreg.score(X_train_scaled,y_train)))\n",
    "#thereafter we do a PCA with Scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X2D_train = pca.fit_transform(X_train_scaled)\n",
    "# and finally compute the log reg fit and the score on the training data\t\n",
    "logreg.fit(X2D_train,y_train)\n",
    "print(\"Train set accuracy scaled and PCA data: {:.2f}\".format(logreg.score(X2D_train,y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a1a5b",
   "metadata": {},
   "source": [
    "We see that our training data after the PCA decomposition has a performance similar to the non-scaled data. \n",
    "\n",
    "Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to\n",
    "choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%).\n",
    "Unless, of course, you are reducing dimensionality for data visualization  in that case you will\n",
    "generally want to reduce the dimensionality down to 2 or 3.\n",
    "The following code computes PCA without reducing dimensionality, then computes the minimum number\n",
    "of dimensions required to preserve 95% of the training sets variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caba8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227518c",
   "metadata": {},
   "source": [
    "You could then set $n\\_components=d$ and run PCA again. However, there is a much better option: instead\n",
    "of specifying the number of principal components you want to preserve, you can set $n\\_components$ to be\n",
    "a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1e4e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36612657",
   "metadata": {},
   "source": [
    "## Incremental PCA\n",
    "\n",
    "One problem with the preceding implementation of PCA is that it requires the whole training set to fit in\n",
    "memory in order for the SVD algorithm to run. Fortunately, Incremental PCA (IPCA) algorithms have\n",
    "been developed: you can split the training set into mini-batches and feed an IPCA algorithm one minibatch\n",
    "at a time. This is useful for large training sets, and also to apply PCA online (i.e., on the fly, as new\n",
    "instances arrive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae2de1",
   "metadata": {},
   "source": [
    "### Randomized PCA\n",
    "\n",
    "Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This is a stochastic\n",
    "algorithm that quickly finds an approximation of the first d principal components. Its computational\n",
    "complexity is $O(m \\times d^2)+O(d^3)$, instead of $O(m \\times n^2) + O(n^3)$, so it is dramatically faster than the\n",
    "previous algorithms when $d$ is much smaller than $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ddfdb",
   "metadata": {},
   "source": [
    "### Kernel PCA\n",
    "\n",
    "The kernel trick is a mathematical technique that implicitly maps instances into a\n",
    "very high-dimensional space (called the feature space), enabling nonlinear classification and regression\n",
    "with Support Vector Machines. Recall that a linear decision boundary in the high-dimensional feature\n",
    "space corresponds to a complex nonlinear decision boundary in the original space.\n",
    "It turns out that the same trick can be applied to PCA, making it possible to perform complex nonlinear\n",
    "projections for dimensionality reduction. This is called Kernel PCA (kPCA). It is often good at\n",
    "preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a\n",
    "twisted manifold.\n",
    "For example, the following code uses Scikit-Learns KernelPCA class to perform kPCA with an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a0eff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba826a2",
   "metadata": {},
   "source": [
    "## Other techniques\n",
    "\n",
    "There are many other dimensionality reduction techniques, several of which are available in Scikit-Learn.\n",
    "\n",
    "Here are some of the most popular:\n",
    "* **Multidimensional Scaling (MDS)** reduces dimensionality while trying to preserve the distances between the instances.\n",
    "\n",
    "* **Isomap** creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the geodesic distances between the instances.\n",
    "\n",
    "* **t-Distributed Stochastic Neighbor Embedding** (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space (e.g., to visualize the MNIST images in 2D).\n",
    "\n",
    "* Linear Discriminant Analysis (LDA) is actually a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as a Support Vector Machine (SVM) classifier discussed in the SVM lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c1fea",
   "metadata": {},
   "source": [
    "## Clustering and Unsupervised Learning\n",
    "\n",
    "In general terms cluster analysis, or clustering, is the task of grouping a\n",
    "data-set into different distinct categories based on some measure of equality of\n",
    "the data. This measure is often referred to as a **metric** or **similarity\n",
    "measure** in the literature (note: sometimes we deal with a **dissimilarity\n",
    "measure** instead). Usually, these metrics are formulated as some kind of\n",
    "distance function between points in a high-dimensional space.\n",
    "\n",
    "The simplest, and also the most\n",
    "common is the **Euclidean distance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51491a2a",
   "metadata": {},
   "source": [
    "## Basic Idea of the $k$-means Clustering Algorithm\n",
    "\n",
    "The simplest of all clustering algorithms is the  **k-means algorithm**\n",
    ", sometimes also referred to as *Lloyds algorithm*. It is the simplest and also\n",
    "the most common. From its simplicity it obtains both strengths and weaknesses.\n",
    "These will be discussed in more detail later. The $k$-means algorithm is a\n",
    "**centroid based** clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ddabda",
   "metadata": {},
   "source": [
    "## The $k$-means Algorithm\n",
    "\n",
    "Assume, we are given $n$ data points and we wish to split the data into $K < n$\n",
    "different categories, or clusters. We label each cluster by an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d59585",
   "metadata": {},
   "source": [
    "$$\n",
    "k\\in\\{1, \\cdots, K \\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c822f",
   "metadata": {},
   "source": [
    "In the basic k-means algorithm each point is assigned to only\n",
    "one cluster $k$, and these assignments are *non-injective* i.e. many-to-one. We\n",
    "can think of these mappings as an encoder $k = C(i)$, which assigns the $i$-th\n",
    "data-point $\\bf x_i$ to the $k$-th cluster.\n",
    "\n",
    "$k$-means algorithm in words:\n",
    "1. We start with guesses / random initializations of our $k$ cluster centers/centroids\n",
    "\n",
    "2. For each centroid the points that are most similar are identified\n",
    "\n",
    "3. Then we move / replace each centroid with a coordinate average of all the points that were assigned to that centroid.\n",
    "\n",
    "4. Iterate 2-3 until the centroids no longer move (to some tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcec766",
   "metadata": {},
   "source": [
    "## Basic Math of the $k$-means Algorithm\n",
    "\n",
    "We assume we have $n$ data-points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3123956",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:kmeanspoints\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:kmeanspoints} \\tag{1}\n",
    "  \\boldsymbol{x_i}  = \\{x_{i, 1}, \\cdots, x_{i, p}\\}\\in\\mathbb{R}^p.\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd990c92",
   "metadata": {},
   "source": [
    "which we wish to group into $K < n$ clusters. For our dissimilarity measure we\n",
    "use the *squared Euclidean distance*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc5334",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:squaredeuclidean\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:squaredeuclidean} \\tag{2}\n",
    "  d(\\boldsymbol{x_i}, \\boldsymbol{x_i'}) = \\sum_{j=1}^p(x_{ij} - x_{i'j})^2\n",
    "                         = ||\\boldsymbol{x_i} - \\boldsymbol{x_{i'}}||^2\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd93eae",
   "metadata": {},
   "source": [
    "## Within Cluster Point Scatter\n",
    "\n",
    "We define the so called *within-cluster point scatter* which gives us a\n",
    "measure of how close each data point assigned to the same cluster tends to be to\n",
    "the all the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10772d66",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:withincluster\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:withincluster} \\tag{3}\n",
    "  W(C) = \\frac{1}{2}\\sum_{k=1}^K\\sum_{C(i)=k}\n",
    "          \\sum_{C(i')=k}d(\\boldsymbol{x_i}, \\boldsymbol{x_{i'}}) =\n",
    "          \\sum_{k=1}^KN_k\\sum_{C(i)=k}||\\boldsymbol{x_i} - \\boldsymbol{\\overline{x_k}}||^2\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6352eb",
   "metadata": {},
   "source": [
    "where $\\boldsymbol{\\overline{x_k}}$ is the mean vector associated with the $k$-th\n",
    "cluster, and $N_k = \\sum_{i=1}^nI(C(i) = k)$, where the $I()$ notation is\n",
    "similar to the Kronecker delta (*Commonly used in statistics, it just means that\n",
    "when $i = k$ we have the encoder $C(i)$*). In other words,  the within-cluster\n",
    "scatter measures the compactness of each cluster with respect to the data points\n",
    "assigned to each cluster. This is the quantity that the $k$-means algorithm aims\n",
    "to minimize. We refer to this quantity $W(C)$ as the within cluster scatter\n",
    "because of its relation to the *total scatter*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b02015",
   "metadata": {},
   "source": [
    "## More Details\n",
    "\n",
    "We have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e33ce2",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:totalscatter\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:totalscatter} \\tag{4}\n",
    "  T = W(C) + B(C) = \\frac{1}{2}\\sum_{i=1}^n\n",
    "                    \\sum_{i'=1}^nd(\\boldsymbol{x_i}, \\boldsymbol{x_{i'}})\n",
    "                  = \\frac{1}{2}\\sum_{k=1}^K\\sum_{C(i)=k}\n",
    "                    \\Big(\\sum_{C(i') = k}d(\\boldsymbol{x_i}, \\boldsymbol{x_{i'}})\n",
    "                  + \\sum_{C(i')\\neq k}d(\\boldsymbol{x_i}, \\boldsymbol{x_{i'}})\\Big).\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56cc352",
   "metadata": {},
   "source": [
    "This is a quantity that is conserved throughout the $k$-means algorithm. It can\n",
    "be thought of as the total amount of information in the data, and it is composed\n",
    "of the aforementioned within-cluster scatter and the *between-cluster scatter*\n",
    "$B(C)$. In methods such as principle component analysis the total scatter is not\n",
    "conserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02bc219",
   "metadata": {},
   "source": [
    "## Total Cluster Variance\n",
    "Given a cluster mean $\\boldsymbol{m_k}$ we define the **total cluster variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81800ba9",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:totalclustervariance\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:totalclustervariance} \\tag{5}\n",
    "  \\min_{C, \\{\\boldsymbol{m_k}\\}_1^K}\\sum_{k=1}^KN_k\\sum||\\boldsymbol{x_i} - \\boldsymbol{m_k}||^2\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dfc235",
   "metadata": {},
   "source": [
    "Now we have all the pieces necessary to formally revisit the $k$-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72426487",
   "metadata": {},
   "source": [
    "## The $k$-means Clustering Algorithm\n",
    "\n",
    "The $k$-means clustering algorithm goes as follows \n",
    "\n",
    "1. For a given cluster assignment $C$, and $k$ cluster means $\\left\\{m_1, \\cdots, m_k\\right\\}$. We minimize the total cluster variance with respect to the cluster means $\\{m_k\\}$ yielding the means of the currently assigned clusters.\n",
    "\n",
    "2. Given a current set of $k$ means $\\{m_k\\}$ the total cluster variance is minimized by assigning each observation to the closest (current) cluster mean. That is $$C(i) = \\underset{1\\leq k\\leq K}{\\mathrm{argmin}} ||\\boldsymbol{x_i} - \\boldsymbol{m_k}||^2$$\n",
    "\n",
    "3. Steps 1 and 2 are repeated until the assignments do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2fa15d",
   "metadata": {},
   "source": [
    "## Summarizing\n",
    "\n",
    "1. Before we start we specify a number $k$ which is the number of clusters we want to try to separate our data into.\n",
    "\n",
    "2. We initially choose $k$ random data points in our data as our initial centroids, *or means* (this is where the name comes from).\n",
    "\n",
    "3. Assign each data point to their closest centroid, based on the squared Euclidean distance.\n",
    "\n",
    "4. For each of the $k$ cluster we update the centroid by calculating new mean values for all the data points in the cluster.\n",
    "\n",
    "5. Iteratively minimize the within cluster scatter by performing steps (3, 4) until the new assignments stop changing (can be to some tolerance) or until a maximum number of iterations have passed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a45ff0",
   "metadata": {},
   "source": [
    "## Writing our own Code, the Data Set\n",
    "\n",
    "Let us now program the most basic version of the algorithm using nothing but\n",
    "Python with numpy arrays. This code is kept intentionally simple to gradually\n",
    "progress our understanding. There is no vectorization of any kind, and even most\n",
    "helper functions are not utilized.\n",
    "\n",
    "We need first a dataset to do our cluster analysis on. In our case\n",
    "this is a plain *vanilla* data set using random numbers using a\n",
    "Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0391ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import display\n",
    "\n",
    "np.random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f9c5b",
   "metadata": {},
   "source": [
    "Next we define functions, for ease of use later, to generate Gaussians and to\n",
    "set up our toy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38163479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_points(dim=2, n_points=1000, mean_vector=np.array([0, 0]),\n",
    "                    sample_variance=1):\n",
    "    \"\"\"\n",
    "    Very simple custom function to generate gaussian distributed point clusters\n",
    "    with variable dimension, number of points, means in each direction\n",
    "    (must match dim) and sample variance.\n",
    "\n",
    "    Inputs:\n",
    "        dim (int)\n",
    "        n_points (int)\n",
    "        mean_vector (np.array) (where index 0 is x, index 1 is y etc.)\n",
    "        sample_variance (float)\n",
    "\n",
    "    Returns:\n",
    "        data (np.array): with dimensions (dim x n_points)\n",
    "    \"\"\"\n",
    "\n",
    "    mean_matrix = np.zeros(dim) + mean_vector\n",
    "    covariance_matrix = np.eye(dim) * sample_variance\n",
    "    data = np.random.multivariate_normal(mean_matrix, covariance_matrix,\n",
    "                                    n_points)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def generate_simple_clustering_dataset(dim=2, n_points=1000, plotting=True,\n",
    "                                    return_data=True):\n",
    "    \"\"\"\n",
    "    Toy model to illustrate k-means clustering\n",
    "    \"\"\"\n",
    "\n",
    "    data1 = gaussian_points(mean_vector=np.array([5, 5]))\n",
    "    data2 = gaussian_points()\n",
    "    data3 = gaussian_points(mean_vector=np.array([1, 4.5]))\n",
    "    data4 = gaussian_points(mean_vector=np.array([5, 1]))\n",
    "    data = np.concatenate((data1, data2, data3, data4), axis=0)\n",
    "\n",
    "    if plotting:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(data[:, 0], data[:, 1], alpha=0.2)\n",
    "        ax.set_title('Toy Model Dataset')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    if return_data:\n",
    "        return data\n",
    "\n",
    "\n",
    "data = generate_simple_clustering_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a397151",
   "metadata": {},
   "source": [
    "## Implementing the $k$-means Algorithm\n",
    "\n",
    "With the above dataset we start\n",
    "implementing the $k$-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46e120cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_samples, dimensions = data.shape\n",
    "n_clusters = 4\n",
    "\n",
    "# we randomly initialize our centroids\n",
    "np.random.seed(2021)\n",
    "centroids = data[np.random.choice(n_samples, n_clusters, replace=False), :]\n",
    "distances = np.zeros((n_samples, n_clusters))\n",
    "\n",
    "# first we need to calculate the distance to each centroid from our data\n",
    "for k in range(n_clusters):\n",
    "    for n in range(n_samples):\n",
    "        dist = 0\n",
    "        for d in range(dimensions):\n",
    "            dist += np.abs(data[n, d] - centroids[k, d])**2\n",
    "            distances[n, k] = dist\n",
    "\n",
    "# we initialize an array to keep track of to which cluster each point belongs\n",
    "# the way we set it up here the index tracks which point and the value which\n",
    "# cluster the point belongs to\n",
    "cluster_labels = np.zeros(n_samples, dtype='int')\n",
    "\n",
    "# next we loop through our samples and for every point assign it to the cluster\n",
    "# to which it has the smallest distance to\n",
    "for n in range(n_samples):\n",
    "    # tracking variables (all of this is basically just an argmin)\n",
    "    smallest = 1e10\n",
    "    smallest_row_index = 1e10\n",
    "    for k in range(n_clusters):\n",
    "        if distances[n, k] < smallest:\n",
    "            smallest = distances[n, k]\n",
    "            smallest_row_index = k\n",
    "\n",
    "    cluster_labels[n] = smallest_row_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf2219",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "230ccf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "unique_cluster_labels = np.unique(cluster_labels)\n",
    "for i in unique_cluster_labels:\n",
    "    ax.scatter(data[cluster_labels == i, 0],\n",
    "               data[cluster_labels == i, 1],\n",
    "               label = i,\n",
    "               alpha = 0.2)\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], c='black')\n",
    "\n",
    "ax.set_title(\"First Grouping of Points to Centroids\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc77504",
   "metadata": {},
   "source": [
    "So what do we have so far? We have 'picked' $k$ centroids at random from our\n",
    "data points. There are other ways of more intelligently choosing their\n",
    "initializations, however for our purposes randomly is fine. Then we have\n",
    "initialized an array 'distances' which holds the information of the distance,\n",
    "*or dissimilarity*, of every point to of our centroids. Finally, we have\n",
    "initialized an array 'cluster_labels' which according to our distances array\n",
    "holds the information of to which centroid every point is assigned. This was the\n",
    "first pass of our algorithm. Essentially, all we need to do now is repeat the\n",
    "distance and assignment steps above until we have reached a desired convergence\n",
    "or a maximum amount of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb7ce3",
   "metadata": {},
   "source": [
    "## Continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0c6583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_iterations = 100\n",
    "tolerance = 1e-8\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    prev_centroids = centroids.copy()\n",
    "    for k in range(n_clusters):\n",
    "        # this array will be used to update our centroid positions\n",
    "        vector_mean = np.zeros(dimensions)\n",
    "        mean_divisor = 0\n",
    "        for n in range(n_samples):\n",
    "            if cluster_labels[n] == k:\n",
    "                vector_mean += data[n, :]\n",
    "                mean_divisor += 1\n",
    "\n",
    "        # update according to the k means\n",
    "        centroids[k, :] = vector_mean / mean_divisor\n",
    "\n",
    "    # we find the dissimilarity\n",
    "    for k in range(n_clusters):\n",
    "        for n in range(n_samples):\n",
    "            dist = 0\n",
    "            for d in range(dimensions):\n",
    "                dist += np.abs(data[n, d] - centroids[k, d])**2\n",
    "                distances[n, k] = dist\n",
    "\n",
    "    # assign each point\n",
    "    for n in range(n_samples):\n",
    "        smallest = 1e10\n",
    "        smallest_row_index = 1e10\n",
    "        for k in range(n_clusters):\n",
    "            if distances[n, k] < smallest:\n",
    "                smallest = distances[n, k]\n",
    "                smallest_row_index = k\n",
    "\n",
    "        cluster_labels[n] = smallest_row_index\n",
    "\n",
    "    # convergence criteria\n",
    "    centroid_difference = np.sum(np.abs(centroids - prev_centroids))\n",
    "    if centroid_difference < tolerance:\n",
    "        print(f'Converged at iteration {iteration}')\n",
    "        break\n",
    "\n",
    "    elif iteration == max_iterations:\n",
    "        print(f'Did not converge in {max_iterations} iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad94a0",
   "metadata": {},
   "source": [
    "## Wrapping it up\n",
    "We now have a simple , un-optimized $k$-means\n",
    "clustering implementation. Lets plot the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f43fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "unique_cluster_labels = np.unique(cluster_labels)\n",
    "for i in unique_cluster_labels:\n",
    "    ax.scatter(data[cluster_labels == i, 0],\n",
    "               data[cluster_labels == i, 1],\n",
    "               label = i,\n",
    "               alpha = 0.2)\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], c='black')\n",
    "\n",
    "ax.set_title(\"Final Result of K-means Clustering\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13d966fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_kmeans(data, n_clusters=4, max_iterations=100, tolerance=1e-8):\n",
    "    start_time = time.time()\n",
    "\n",
    "    n_samples, dimensions = data.shape\n",
    "    n_clusters = 4\n",
    "    #np.random.seed(2021)\n",
    "    centroids = data[np.random.choice(n_samples, n_clusters, replace=False), :]\n",
    "    distances = np.zeros((n_samples, n_clusters))\n",
    "\n",
    "    for k in range(n_clusters):\n",
    "        for n in range(n_samples):\n",
    "            dist = 0\n",
    "            for d in range(dimensions):\n",
    "                dist += np.abs(data[n, d] - centroids[k, d])**2\n",
    "                distances[n, k] = dist\n",
    "\n",
    "    cluster_labels = np.zeros(n_samples, dtype='int')\n",
    "\n",
    "    for n in range(n_samples):\n",
    "        smallest = 1e10\n",
    "        smallest_row_index = 1e10\n",
    "        for k in range(n_clusters):\n",
    "            if distances[n, k] < smallest:\n",
    "                smallest = distances[n, k]\n",
    "                smallest_row_index = k\n",
    "\n",
    "        cluster_labels[n] = smallest_row_index\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        prev_centroids = centroids.copy()\n",
    "        for k in range(n_clusters):\n",
    "            vector_mean = np.zeros(dimensions)\n",
    "            mean_divisor = 0\n",
    "            for n in range(n_samples):\n",
    "                if cluster_labels[n] == k:\n",
    "                    vector_mean += data[n, :]\n",
    "                    mean_divisor += 1\n",
    "\n",
    "            centroids[k, :] = vector_mean / mean_divisor\n",
    "\n",
    "        for k in range(n_clusters):\n",
    "            for n in range(n_samples):\n",
    "                dist = 0\n",
    "                for d in range(dimensions):\n",
    "                    dist += np.abs(data[n, d] - centroids[k, d])**2\n",
    "                    distances[n, k] = dist\n",
    "\n",
    "        for n in range(n_samples):\n",
    "            smallest = 1e10\n",
    "            smallest_row_index = 1e10\n",
    "            for k in range(n_clusters):\n",
    "                if distances[n, k] < smallest:\n",
    "                    smallest = distances[n, k]\n",
    "                    smallest_row_index = k\n",
    "\n",
    "            cluster_labels[n] = smallest_row_index\n",
    "\n",
    "        centroid_difference = np.sum(np.abs(centroids - prev_centroids))\n",
    "        if centroid_difference < tolerance:\n",
    "            print(f'Converged at iteration {iteration}')\n",
    "            print(f'Runtime: {time.time() - start_time} seconds')\n",
    "\n",
    "            return cluster_labels, centroids\n",
    "\n",
    "    print(f'Did not converge in {max_iterations} iterations')\n",
    "    print(f'Runtime: {time.time() - start_time} seconds')\n",
    "\n",
    "    return cluster_labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69426baf",
   "metadata": {},
   "source": [
    "## Summary of course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d1af84",
   "metadata": {},
   "source": [
    "## What? Me worry? No final exam in this course!\n",
    "<!-- dom:FIGURE: [figures/exam1.jpeg, width=500 frac=0.6] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/exam1.jpeg\" width=\"500\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ad2bf",
   "metadata": {},
   "source": [
    "## What is the link between Artificial Intelligence and Machine Learning and some general Remarks\n",
    "\n",
    "Artificial intelligence is built upon integrated machine learning\n",
    "algorithms as discussed in this course, which in turn are fundamentally rooted in optimization and\n",
    "statistical learning.\n",
    "\n",
    "Can we have Artificial Intelligence without Machine Learning? See [this post for inspiration](https://www.linkedin.com/pulse/what-artificial-intelligence-without-machine-learning-claudia-pohlink)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9146fc4",
   "metadata": {},
   "source": [
    "## Going back to the beginning of the semester\n",
    "\n",
    "Traditionally the field of machine learning has had its main focus on\n",
    "predictions and correlations.  These concepts outline in some sense\n",
    "the difference between machine learning and what is normally called\n",
    "Bayesian statistics or Bayesian inference.\n",
    "\n",
    "In machine learning and prediction based tasks, we are often\n",
    "interested in developing algorithms that are capable of learning\n",
    "patterns from given data in an automated fashion, and then using these\n",
    "learned patterns to make predictions or assessments of newly given\n",
    "data. In many cases, our primary concern is the quality of the\n",
    "predictions or assessments, and we are less concerned with the\n",
    "underlying patterns that were learned in order to make these\n",
    "predictions.  This leads to what normally has been labeled as a\n",
    "frequentist approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a4a39",
   "metadata": {},
   "source": [
    "## Not so sharp distinctions\n",
    "\n",
    "You should keep in mind that the division between a traditional\n",
    "frequentist approach with focus on predictions and correlations only\n",
    "and a Bayesian approach with an emphasis on estimations and\n",
    "causations, is not that sharp. Machine learning can be frequentist\n",
    "with ensemble methods (EMB) as examples and Bayesian with Gaussian\n",
    "Processes as examples.\n",
    "\n",
    "If one views ML from a statistical learning\n",
    "perspective, one is then equally interested in estimating errors as\n",
    "one is in finding correlations and making predictions. It is important\n",
    "to keep in mind that the frequentist and Bayesian approaches differ\n",
    "mainly in their interpretations of probability. In the frequentist\n",
    "world, we can only assign probabilities to repeated random\n",
    "phenomena. From the observations of these phenomena, we can infer the\n",
    "probability of occurrence of a specific event.  In Bayesian\n",
    "statistics, we assign probabilities to specific events and the\n",
    "probability represents the measure of belief/confidence for that\n",
    "event. The belief can be updated in the light of new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947159c1",
   "metadata": {},
   "source": [
    "## Topics we have covered this year\n",
    "\n",
    "The course has two central parts\n",
    "\n",
    "1. Statistical analysis and optimization of data\n",
    "\n",
    "2. Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11bb61",
   "metadata": {},
   "source": [
    "## Statistical analysis and optimization of data\n",
    "\n",
    "The following topics have been discussed:\n",
    "1. Basic concepts, expectation values, variance, covariance, correlation functions and errors;\n",
    "\n",
    "2. Simpler models, binomial distribution, the Poisson distribution, simple and multivariate normal distributions;\n",
    "\n",
    "3. Central elements from linear algebra, matrix inversion and SVD\n",
    "\n",
    "4. Gradient methods for data optimization\n",
    "\n",
    "5. Estimation of errors using cross-validation, bootstrapping and jackknife methods;\n",
    "\n",
    "6. Practical optimization using Singular-value decomposition and least squares for parameterizing data.\n",
    "\n",
    "7. Principal Component Analysis to reduce the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609dbcc",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "\n",
    "The following topics will be covered\n",
    "1. Linear methods for regression and classification:\n",
    "\n",
    "a. Ordinary Least Squares\n",
    "\n",
    "b. Ridge regression\n",
    "\n",
    "c. Lasso regression\n",
    "\n",
    "d. Logistic regression\n",
    "\n",
    "5. Neural networks and deep learning:\n",
    "\n",
    "a. Feed Forward Neural Networks\n",
    "\n",
    "b. Convolutional Neural Networks\n",
    "\n",
    "c. Recurrent Neural Networks\n",
    "\n",
    "4. Decisions trees and ensemble methods:\n",
    "\n",
    "a. Decision trees\n",
    "\n",
    "b. Bagging and voting\n",
    "\n",
    "c. Random forests\n",
    "\n",
    "d. Boosting and gradient boosting\n",
    "\n",
    "5. Support vector machines\n",
    "\n",
    "a. Binary classification and multiclass classification\n",
    "\n",
    "b. Kernel methods\n",
    "\n",
    "c. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723bf9f",
   "metadata": {},
   "source": [
    "## Learning outcomes and overarching aims of this course\n",
    "\n",
    "The course introduces a variety of central algorithms and methods\n",
    "essential for studies of data analysis and machine learning. The\n",
    "course is project based and through the various projects, normally\n",
    "three, you will be exposed to fundamental research problems\n",
    "in these fields, with the aim to reproduce state of the art scientific\n",
    "results. The students will learn to develop and structure large codes\n",
    "for studying these systems, get acquainted with computing facilities\n",
    "and learn to handle large scientific projects. A good scientific and\n",
    "ethical conduct is emphasized throughout the course. \n",
    "\n",
    "* Understand linear methods for regression and classification;\n",
    "\n",
    "* Learn about neural network;\n",
    "\n",
    "* Learn about bagging, boosting and trees\n",
    "\n",
    "* Support vector machines\n",
    "\n",
    "* Learn about basic data analysis;\n",
    "\n",
    "* Be capable of extending the acquired knowledge to other systems and cases;\n",
    "\n",
    "* Have an understanding of central algorithms used in data analysis and machine learning;\n",
    "\n",
    "* Work on numerical projects to illustrate the theory. The projects play a central role and you are expected to know modern programming languages like Python or C++."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34c3e4",
   "metadata": {},
   "source": [
    "## Perspective on Machine Learning\n",
    "\n",
    "1. Rapidly emerging application area\n",
    "\n",
    "2. Experiment AND theory are evolving in many many fields. Still many low-hanging fruits.\n",
    "\n",
    "3. Requires education/retraining for more widespread adoption\n",
    "\n",
    "4. A lot of word-of-mouth development methods\n",
    "\n",
    "Huge amounts of data sets require automation, classical analysis tools often inadequate. \n",
    "High energy physics hit this wall in the 90s.\n",
    "In 2009 single top quark production was determined via [Boosted decision trees, Bayesian\n",
    "Neural Networks, etc.](https://arxiv.org/pdf/0903.0850.pdf). Similarly, the search for Higgs was a statistical learning tour de force. See this link on [Kaggle.com](https://www.kaggle.com/c/higgs-boson)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb6872",
   "metadata": {},
   "source": [
    "## Machine Learning Research\n",
    "\n",
    "Where to find recent results:\n",
    "1. Conference proceedings, arXiv and blog posts!\n",
    "\n",
    "2. **NIPS**: [Neural Information Processing Systems](https://papers.nips.cc)\n",
    "\n",
    "3. **ICLR**: [International Conference on Learning Representations](https://openreview.net/group?id=ICLR.cc/2018/Conference#accepted-oral-papers)\n",
    "\n",
    "4. **ICML**: International Conference on Machine Learning\n",
    "\n",
    "5. [Journal of Machine Learning Research](http://www.jmlr.org/papers/v19/) \n",
    "\n",
    "6. [Follow ML on ArXiv](https://arxiv.org/list/cs.LG/recent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e69fc7",
   "metadata": {},
   "source": [
    "## Starting your Machine Learning Project\n",
    "\n",
    "1. Identify problem type: classification, regression\n",
    "\n",
    "2. Consider your data carefully\n",
    "\n",
    "3. Choose a simple model that fits 1. and 2.\n",
    "\n",
    "4. Consider your data carefully again! Think of data representation more carefully.\n",
    "\n",
    "5. Based on your results, feedback loop to earliest possible point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460e395",
   "metadata": {},
   "source": [
    "## Choose a Model and Algorithm\n",
    "\n",
    "1. Supervised?\n",
    "\n",
    "2. Start with the simplest model that fits your problem\n",
    "\n",
    "3. Start with minimal processing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d474778",
   "metadata": {},
   "source": [
    "## Preparing Your Data\n",
    "\n",
    "1. Shuffle your data\n",
    "\n",
    "2. Mean center your data\n",
    "\n",
    "  * Why?\n",
    "\n",
    "3. Normalize the variance\n",
    "\n",
    "  * Why?\n",
    "\n",
    "4. [Whitening](https://multivariatestatsjl.readthedocs.io/en/latest/whiten.html)\n",
    "\n",
    "  * Decorrelates data\n",
    "\n",
    "  * Can be hit or miss\n",
    "\n",
    "5. When to do train/test split?\n",
    "\n",
    "Whitening is a decorrelation transformation that transforms a set of\n",
    "random variables into a set of new random variables with identity\n",
    "covariance (uncorrelated with unit variances)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1212230",
   "metadata": {},
   "source": [
    "## Which Activation and Weights to Choose in Neural Networks\n",
    "\n",
    "1. RELU? ELU?\n",
    "\n",
    "2. Sigmoid or Tanh?\n",
    "\n",
    "3. Set all weights to 0?\n",
    "\n",
    "  * Terrible idea\n",
    "\n",
    "4. Set all weights to random values?\n",
    "\n",
    "  * Small random values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263ca04a",
   "metadata": {},
   "source": [
    "## Optimization Methods and Hyperparameters\n",
    "1. Stochastic gradient descent\n",
    "\n",
    "a. Stochastic gradient descent + momentum\n",
    "\n",
    "2. State-of-the-art approaches:\n",
    "\n",
    "  * RMSProp\n",
    "\n",
    "  * Adam\n",
    "\n",
    "  * and more\n",
    "\n",
    "Which regularization and hyperparameters? $L_1$ or $L_2$, soft\n",
    "classifiers, depths of trees and many other. Need to explore a large\n",
    "set of hyperparameters and regularization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc713d6f",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "When do we resample?\n",
    "\n",
    "1. [Bootstrap](https://www.cambridge.org/core/books/bootstrap-methods-and-their-application/ED2FD043579F27952363566DC09CBD6A)\n",
    "\n",
    "2. [Cross-validation](https://www.youtube.com/watch?v=fSytzGwwBVw&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "3. Jackknife and many other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1064c4",
   "metadata": {},
   "source": [
    "## Other courses on Data science and Machine Learning  at UiO\n",
    "\n",
    "The link here <https://www.mn.uio.no/english/research/about/centre-focus/innovation/data-science/studies/>  gives an excellent overview of courses on Machine learning at UiO.\n",
    "\n",
    "1. [STK2100 Machine learning and statistical methods for prediction and classification](http://www.uio.no/studier/emner/matnat/math/STK2100/index-eng.html). \n",
    "\n",
    "2. [IN3050/IN4050 Introduction to Artificial Intelligence and Machine Learning](https://www.uio.no/studier/emner/matnat/ifi/IN3050/index-eng.html). Introductory course in machine learning and AI with an algorithmic approach. \n",
    "\n",
    "3. [STK-INF3000/4000 Selected Topics in Data Science](http://www.uio.no/studier/emner/matnat/math/STK-INF3000/index-eng.html). The course provides insight into selected contemporary relevant topics within Data Science. \n",
    "\n",
    "4. [IN4080 Natural Language Processing](https://www.uio.no/studier/emner/matnat/ifi/IN4080/index.html). Probabilistic and machine learning techniques applied to natural language processing. \n",
    "\n",
    "5. [STK-IN4300  Statistical learning methods in Data Science](https://www.uio.no/studier/emner/matnat/math/STK-IN4300/index-eng.html). An advanced introduction to statistical and machine learning. For students with a good mathematics and statistics background.\n",
    "\n",
    "6. [IN-STK5000  Adaptive Methods for Data-Based Decision Making](https://www.uio.no/studier/emner/matnat/ifi/IN-STK5000/index-eng.html). Methods for adaptive collection and processing of data based on machine learning techniques. \n",
    "\n",
    "7. [IN5400/INF5860  Machine Learning for Image Analysis](https://www.uio.no/studier/emner/matnat/ifi/IN5400/). An introduction to deep learning with particular emphasis on applications within Image analysis, but useful for other application areas too.\n",
    "\n",
    "8. [TEK5040  Dyp lring for autonome systemer](https://www.uio.no/studier/emner/matnat/its/TEK5040/). The course addresses advanced algorithms and architectures for deep learning with neural networks. The course provides an introduction to how deep-learning techniques can be used in the construction of key parts of advanced autonomous systems that exist in physical environments and cyber environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ca1f2",
   "metadata": {},
   "source": [
    "## Additional courses of interest\n",
    "\n",
    "1. [STK4051 Computational Statistics](https://www.uio.no/studier/emner/matnat/math/STK4051/index-eng.html)\n",
    "\n",
    "2. [STK4021 Applied Bayesian Analysis and Numerical Methods](https://www.uio.no/studier/emner/matnat/math/STK4021/index-eng.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c92cc",
   "metadata": {},
   "source": [
    "## What's the future like?\n",
    "\n",
    "Based on multi-layer nonlinear neural networks, deep learning can\n",
    "learn directly from raw data, automatically extract and abstract\n",
    "features from layer to layer, and then achieve the goal of regression,\n",
    "classification, or ranking. Deep learning has made breakthroughs in\n",
    "computer vision, speech processing and natural language, and reached\n",
    "or even surpassed human level. The success of deep learning is mainly\n",
    "due to the three factors: big data, big model, and big computing.\n",
    "\n",
    "In the past few decades, many different architectures of deep neural\n",
    "networks have been proposed, such as\n",
    "1. Convolutional neural networks, which are mostly used in image and video data processing, and have also been applied to sequential data such as text processing;\n",
    "\n",
    "2. Recurrent neural networks, which can process sequential data of variable length and have been widely used in natural language understanding and speech processing;\n",
    "\n",
    "3. Encoder-decoder framework, which is mostly used for image or sequence generation, such as machine translation, text summarization, and image captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b549993",
   "metadata": {},
   "source": [
    "## Types of Machine Learning, a repetition\n",
    "\n",
    "The approaches to machine learning are many, but are often split into two main categories. \n",
    "In *supervised learning* we know the answer to a problem,\n",
    "and let the computer deduce the logic behind it. On the other hand, *unsupervised learning*\n",
    "is a method for finding patterns and relationship in data sets without any prior knowledge of the system.\n",
    "Some authours also operate with a third category, namely *reinforcement learning*. This is a paradigm \n",
    "of learning inspired by behavioural psychology, where learning is achieved by trial-and-error, \n",
    "solely from rewards and punishment.\n",
    "\n",
    "Another way to categorize machine learning tasks is to consider the desired output of a system.\n",
    "Some of the most common tasks are:\n",
    "\n",
    "  * Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.\n",
    "\n",
    "  * Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.\n",
    "\n",
    "  * Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.\n",
    "\n",
    "  * Other unsupervised learning algortihms like **Boltzmann machines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0703acd",
   "metadata": {},
   "source": [
    "## Why Boltzmann machines?\n",
    "\n",
    "What is known as restricted Boltzmann Machines (RMB) have received a lot of attention lately. \n",
    "One of the major reasons is that they can be stacked layer-wise to build deep neural networks that capture complicated statistics.\n",
    "\n",
    "The original RBMs had just one visible layer and a hidden layer, but recently so-called Gaussian-binary RBMs have gained quite some popularity in imaging since they are capable of modeling continuous data that are common to natural images. \n",
    "\n",
    "Furthermore, they have been used to solve complicated [quantum mechanical many-particle problems or classical statistical physics problems like the Ising and Potts classes of models](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.91.045002)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e1f12",
   "metadata": {},
   "source": [
    "## Boltzmann Machines\n",
    "\n",
    "Why use a generative model rather than the more well known discriminative deep neural networks (DNN)? \n",
    "\n",
    "* Discriminitave methods have several limitations: They are mainly supervised learning methods, thus requiring labeled data. And there are tasks they cannot accomplish, like drawing new examples from an unknown probability distribution.\n",
    "\n",
    "* A generative model can learn to represent and sample from a probability distribution. The core idea is to learn a parametric model of the probability distribution from which the training data was drawn. As an example\n",
    "\n",
    "a. A model for images could learn to draw new examples of cats and dogs, given a training dataset of images of cats and dogs.\n",
    "\n",
    "b. Generate a sample of an ordered or disordered phase, having been given samples of such phases.\n",
    "\n",
    "c. Model the trial function for [Monte Carlo calculations](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.91.045002)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ba5d5",
   "metadata": {},
   "source": [
    "## Some similarities and differences from DNNs\n",
    "\n",
    "1. Both use gradient-descent based learning procedures for minimizing cost functions\n",
    "\n",
    "2. Energy based models don't use backpropagation and automatic differentiation for computing gradients, instead turning to Markov Chain Monte Carlo methods.\n",
    "\n",
    "3. DNNs often have several hidden layers. A restricted Boltzmann machine has only one hidden layer, however several RBMs can be stacked to make up Deep Belief Networks, of which they constitute the building blocks.\n",
    "\n",
    "History: The RBM was developed by amongst others [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton), called by some the \"Godfather of Deep Learning\", working with the University of Toronto and Google."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572964e",
   "metadata": {},
   "source": [
    "## Boltzmann machines (BM)\n",
    "\n",
    "A BM is what we would call an undirected probabilistic graphical model\n",
    "with stochastic continuous or discrete units.\n",
    "\n",
    "It is interpreted as a stochastic recurrent neural network where the\n",
    "state of each unit(neurons/nodes) depends on the units it is connected\n",
    "to. The weights in the network represent thus the strength of the\n",
    "interaction between various units/nodes.\n",
    "\n",
    "It turns into a Hopfield network if we choose deterministic rather\n",
    "than stochastic units. In contrast to a Hopfield network, a BM is a\n",
    "so-called generative model. It allows us to generate new samples from\n",
    "the learned distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39b364",
   "metadata": {},
   "source": [
    "## A standard BM setup\n",
    "\n",
    "A standard BM network is divided into a set of observable and visible units $\\hat{x}$ and a set of unknown hidden units/nodes $\\hat{h}$.\n",
    "\n",
    "Additionally there can be bias nodes for the hidden and visible layers. These biases are normally set to $1$.\n",
    "\n",
    "BMs are stackable, meaning they cwe can train a BM which serves as input to another BM. We can construct deep networks for learning complex PDFs. The layers can be trained one after another, a feature which makes them popular in deep learning\n",
    "\n",
    "However, they are often hard to train. This leads to the introduction of so-called restricted BMs, or RBMS.\n",
    "Here we take away all lateral connections between nodes in the visible layer as well as connections between nodes in the hidden layer. The network is illustrated in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad73f3",
   "metadata": {},
   "source": [
    "## The structure of the RBM network\n",
    "\n",
    "<!-- dom:FIGURE: [figures/RBM.png, width=800 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/RBM.png\" width=\"800\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe92ba",
   "metadata": {},
   "source": [
    "## The network\n",
    "\n",
    "**The network layers**:\n",
    "1. A function $\\mathbf{x}$ that represents the visible layer, a vector of $M$ elements (nodes). This layer represents both what the RBM might be given as training input, and what we want it to be able to reconstruct. This might for example be given by the pixels of an image or coefficients representing speech, or the coordinates of a quantum mechanical state function.\n",
    "\n",
    "2. The function $\\mathbf{h}$ represents the hidden, or latent, layer. A vector of $N$ elements (nodes). Also called \"feature detectors\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61992d",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "The goal of the hidden layer is to increase the model's expressive\n",
    "power. We encode complex interactions between visible variables by\n",
    "introducing additional, hidden variables that interact with visible\n",
    "degrees of freedom in a simple manner, yet still reproduce the complex\n",
    "correlations between visible degrees in the data once marginalized\n",
    "over (integrated out).\n",
    "\n",
    "**The network parameters, to be optimized/learned**:\n",
    "1. $\\mathbf{a}$ represents the visible bias, a vector of same length as $\\mathbf{x}$.\n",
    "\n",
    "2. $\\mathbf{b}$ represents the hidden bias, a vector of same lenght as $\\mathbf{h}$.\n",
    "\n",
    "3. $W$ represents the interaction weights, a matrix of size $M\\times N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0baf07",
   "metadata": {},
   "source": [
    "## Joint distribution\n",
    "\n",
    "The restricted Boltzmann machine is described by a Boltzmann distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5131be",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tP_{rbm}(\\mathbf{x},\\mathbf{h}) = \\frac{1}{Z} e^{-\\frac{1}{T_0}E(\\mathbf{x},\\mathbf{h})},\n",
    "\\label{_auto1} \\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c20e2",
   "metadata": {},
   "source": [
    "where $Z$ is the normalization constant or partition function, defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56a663",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tZ = \\int \\int e^{-\\frac{1}{T_0}E(\\mathbf{x},\\mathbf{h})} d\\mathbf{x} d\\mathbf{h}.\n",
    "\\label{_auto2} \\tag{7}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad45e0",
   "metadata": {},
   "source": [
    "It is common to ignore $T_0$ by setting it to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e4545f",
   "metadata": {},
   "source": [
    "## Network Elements, the energy function\n",
    "\n",
    "The function $E(\\mathbf{x},\\mathbf{h})$ gives the **energy** of a\n",
    "configuration (pair of vectors) $(\\mathbf{x}, \\mathbf{h})$. The lower\n",
    "the energy of a configuration, the higher the probability of it. This\n",
    "function also depends on the parameters $\\mathbf{a}$, $\\mathbf{b}$ and\n",
    "$W$. Thus, when we adjust them during the learning procedure, we are\n",
    "adjusting the energy function to best fit our problem.\n",
    "\n",
    "An expression for the energy function is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a927f",
   "metadata": {},
   "source": [
    "$$\n",
    "E(\\hat{x},\\hat{h}) = -\\sum_{ia}^{NA}b_i^a \\alpha_i^a(x_i)-\\sum_{jd}^{MD}c_j^d \\beta_j^d(h_j)-\\sum_{ijad}^{NAMD}b_i^a \\alpha_i^a(x_i)c_j^d \\beta_j^d(h_j)w_{ij}^{ad}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243fba20",
   "metadata": {},
   "source": [
    "Here $\\beta_j^d(h_j)$ and $\\alpha_i^a(x_j)$ are so-called transfer functions that map a given input value to a desired feature value. The labels $a$ and $d$ denote that there can be multiple transfer functions per variable. The first sum depends only on the visible units. The second on the hidden ones. **Note** that there is no connection between nodes in a layer.\n",
    "\n",
    "The quantities $b$ and $c$ can be interpreted as the visible and hidden biases, respectively.\n",
    "\n",
    "The connection between the nodes in the two layers is given by the weights $w_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47aa4b1",
   "metadata": {},
   "source": [
    "## Defining different types of RBMs\n",
    "There are different variants of RBMs, and the differences lie in the types of visible and hidden units we choose as well as in the implementation of the energy function $E(\\mathbf{x},\\mathbf{h})$. \n",
    "\n",
    "**Binary-Binary RBM:**\n",
    "\n",
    "RBMs were first developed using binary units in both the visible and hidden layer. The corresponding energy function is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41f814",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tE(\\mathbf{x}, \\mathbf{h}) = - \\sum_i^M x_i a_i- \\sum_j^N b_j h_j - \\sum_{i,j}^{M,N} x_i w_{ij} h_j,\n",
    "\\label{_auto3} \\tag{8}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2e905",
   "metadata": {},
   "source": [
    "where the binary values taken on by the nodes are most commonly 0 and 1.\n",
    "\n",
    "**Gaussian-Binary RBM:**\n",
    "\n",
    "Another varient is the RBM where the visible units are Gaussian while the hidden units remain binary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03754b04",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tE(\\mathbf{x}, \\mathbf{h}) = \\sum_i^M \\frac{(x_i - a_i)^2}{2\\sigma_i^2} - \\sum_j^N b_j h_j - \\sum_{i,j}^{M,N} \\frac{x_i w_{ij} h_j}{\\sigma_i^2}. \n",
    "\\label{_auto4} \\tag{9}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5a9d7",
   "metadata": {},
   "source": [
    "## More about RBMs\n",
    "1. Useful when we model continuous data (i.e., we wish $\\mathbf{x}$ to be continuous)\n",
    "\n",
    "2. Requires a smaller learning rate, since there's no upper bound to the value a component might take in the reconstruction\n",
    "\n",
    "Other types of units include:\n",
    "1. Softmax and multinomial units\n",
    "\n",
    "2. Gaussian visible and hidden units\n",
    "\n",
    "3. Binomial units\n",
    "\n",
    "4. Rectified linear units\n",
    "\n",
    "To read more, see [Lectures on Boltzmann machines in Physics](https://github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/pub/notebook2/ipynb/notebook2.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a17d3",
   "metadata": {},
   "source": [
    "## Autoencoders: Overarching view\n",
    "\n",
    "Autoencoders are artificial neural networks capable of learning\n",
    "efficient representations of the input data (these representations are called codings)  without\n",
    "any supervision (i.e., the training set is unlabeled). These codings\n",
    "typically have a much lower dimensionality than the input data, making\n",
    "autoencoders useful for dimensionality reduction. \n",
    "\n",
    "More importantly, autoencoders act as powerful feature detectors, and\n",
    "they can be used for unsupervised pretraining of deep neural networks.\n",
    "\n",
    "Lastly, they are capable of randomly generating new data that looks\n",
    "very similar to the training data; this is called a generative\n",
    "model. For example, you could train an autoencoder on pictures of\n",
    "faces, and it would then be able to generate new faces.  Surprisingly,\n",
    "autoencoders work by simply learning to copy their inputs to their\n",
    "outputs. This may sound like a trivial task, but we will see that\n",
    "constraining the network in various ways can make it rather\n",
    "difficult. For example, you can limit the size of the internal\n",
    "representation, or you can add noise to the inputs and train the\n",
    "network to recover the original inputs. These constraints prevent the\n",
    "autoencoder from trivially copying the inputs directly to the outputs,\n",
    "which forces it to learn efficient ways of representing the data. In\n",
    "short, the codings are byproducts of the autoencoders attempt to\n",
    "learn the identity function under some constraints.\n",
    "\n",
    "[Video on autoencoders](https://www.coursera.org/lecture/building-deep-learning-models-with-tensorflow/autoencoders-1U4L3)\n",
    "\n",
    "See also A. Geron's textbook, chapter 15."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a974e18",
   "metadata": {},
   "source": [
    "## Bayesian Machine Learning\n",
    "\n",
    "This is an important topic if we aim at extracting a probability\n",
    "distribution. This gives us also a confidence interval and error\n",
    "estimates.\n",
    "\n",
    "Bayesian machine learning allows us to encode our prior beliefs about\n",
    "what those models should look like, independent of what the data tells\n",
    "us. This is especially useful when we dont have a ton of data to\n",
    "confidently learn our model.\n",
    "\n",
    "[Video on Bayesian deep learning](https://www.youtube.com/watch?v=E1qhGw8QxqY&ab_channel=AndrewGordonWilson)\n",
    "\n",
    "See also the [slides here](https://github.com/CompPhysics/MachineLearning/blob/master/doc/Articles/lec03.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6375edf",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning (RL) is one of the most exciting fields of\n",
    "Machine Learning today, and also one of the oldest. It has been around\n",
    "since the 1950s, producing many interesting applications over the\n",
    "years.\n",
    "\n",
    "It studies\n",
    "how agents take actions based on trial and error, so as to maximize\n",
    "some notion of cumulative reward in a dynamic system or\n",
    "environment. Due to its generality, the problem has also been studied\n",
    "in many other disciplines, such as game theory, control theory,\n",
    "operations research, information theory, multi-agent systems, swarm\n",
    "intelligence, statistics, and genetic algorithms.\n",
    "\n",
    "In March 2016, AlphaGo, a computer program that plays the board game\n",
    "Go, beat Lee Sedol in a five-game match. This was the first time a\n",
    "computer Go program had beaten a 9-dan (highest rank) professional\n",
    "without handicaps. AlphaGo is based on deep convolutional neural\n",
    "networks and reinforcement learning. AlphaGos victory was a major\n",
    "milestone in artificial intelligence and it has also made\n",
    "reinforcement learning a hot research area in the field of machine\n",
    "learning.\n",
    "\n",
    "[Lecture on Reinforcement Learning](https://www.youtube.com/watch?v=FgzM3zpZ55o&ab_channel=stanfordonline).\n",
    "\n",
    "See also A. Geron's textbook, chapter 16."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b36c2",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "The goal of transfer learning is to transfer the model or knowledge\n",
    "obtained from a source task to the target task, in order to resolve\n",
    "the issues of insufficient training data in the target task. The\n",
    "rationality of doing so lies in that usually the source and target\n",
    "tasks have inter-correlations, and therefore either the features,\n",
    "samples, or models in the source task might provide useful information\n",
    "for us to better solve the target task. Transfer learning is a hot\n",
    "research topic in recent years, with many problems still waiting to be studied.\n",
    "\n",
    "[Lecture on transfer learning](https://www.ias.edu/video/machinelearning/2020/0331-SamoryKpotufe)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54342d9",
   "metadata": {},
   "source": [
    "## Adversarial learning\n",
    "\n",
    "The conventional deep generative model has a potential problem: the\n",
    "model tends to generate extreme instances to maximize the\n",
    "probabilistic likelihood, which will hurt its performance. Adversarial\n",
    "learning utilizes the adversarial behaviors (e.g., generating\n",
    "adversarial instances or training an adversarial model) to enhance the\n",
    "robustness of the model and improve the quality of the generated\n",
    "data. In recent years, one of the most promising unsupervised learning\n",
    "technologies, generative adversarial networks (GAN), has already been\n",
    "successfully applied to image, speech, and text.\n",
    "\n",
    "[Lecture on adversial learning](https://www.youtube.com/watch?v=CIfsB_EYsVI&ab_channel=StanfordUniversitySchoolofEngineering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360b9e6",
   "metadata": {},
   "source": [
    "## Dual learning\n",
    "\n",
    "Dual learning is a new learning paradigm, the basic idea of which is\n",
    "to use the primal-dual structure between machine learning tasks to\n",
    "obtain effective feedback/regularization, and guide and strengthen the\n",
    "learning process, thus reducing the requirement of large-scale labeled\n",
    "data for deep learning. The idea of dual learning has been applied to\n",
    "many problems in machine learning, including machine translation,\n",
    "image style conversion, question answering and generation, image\n",
    "classification and generation, text classification and generation,\n",
    "image-to-text, and text-to-image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad35e23",
   "metadata": {},
   "source": [
    "## Distributed machine learning\n",
    "\n",
    "Distributed computation will speed up machine learning algorithms,\n",
    "significantly improve their efficiency, and thus enlarge their\n",
    "application. When distributed meets machine learning, more than just\n",
    "implementing the machine learning algorithms in parallel is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48da1d7",
   "metadata": {},
   "source": [
    "## Meta learning\n",
    "\n",
    "Meta learning is an emerging research direction in machine\n",
    "learning. Roughly speaking, meta learning concerns learning how to\n",
    "learn, and focuses on the understanding and adaptation of the learning\n",
    "itself, instead of just completing a specific learning task. That is,\n",
    "a meta learner needs to be able to evaluate its own learning methods\n",
    "and adjust its own learning methods according to specific learning\n",
    "tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c0717",
   "metadata": {},
   "source": [
    "## The Challenges Facing Machine Learning\n",
    "\n",
    "While there has been much progress in machine learning, there are also challenges.\n",
    "\n",
    "For example, the mainstream machine learning technologies are\n",
    "black-box approaches, making us concerned about their potential\n",
    "risks. To tackle this challenge, we may want to make machine learning\n",
    "more explainable and controllable. As another example, the\n",
    "computational complexity of machine learning algorithms is usually\n",
    "very high and we may want to invent lightweight algorithms or\n",
    "implementations. Furthermore, in many domains such as physics,\n",
    "chemistry, biology, and social sciences, people usually seek elegantly\n",
    "simple equations (e.g., the Schrdinger equation) to uncover the\n",
    "underlying laws behind various phenomena. In the field of machine\n",
    "learning, can we reveal simple laws instead of designing more complex\n",
    "models for data fitting? Although there are many challenges, we are\n",
    "still very optimistic about the future of machine learning. As we look\n",
    "forward to the future, here are what we think the research hotspots in\n",
    "the next ten years will be.\n",
    "\n",
    "See the article on [Discovery of Physics From Data: Universal Laws and Discrepancies](https://www.frontiersin.org/articles/10.3389/frai.2020.00025/full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f136fbe",
   "metadata": {},
   "source": [
    "## Explainable machine learning\n",
    "\n",
    "Machine learning, especially deep learning, evolves rapidly. The\n",
    "ability gap between machine and human on many complex cognitive tasks\n",
    "becomes narrower and narrower. However, we are still in the very early\n",
    "stage in terms of explaining why those effective models work and how\n",
    "they work.\n",
    "\n",
    "**What is missing: the gap between correlation and causation**. Standard Machine Learning is based on what e have called a frequentist approach. \n",
    "\n",
    "Most\n",
    "machine learning techniques, especially the statistical ones, depend\n",
    "highly on correlations in data sets to make predictions and analyses. In\n",
    "contrast, rational humans tend to reply on clear and trustworthy\n",
    "causality relations obtained via logical reasoning on real and clear\n",
    "facts. It is one of the core goals of explainable machine learning to\n",
    "transition from solving problems by data correlation to solving\n",
    "problems by logical reasoning.\n",
    "\n",
    "**Bayesian Machine Learning is one of the exciting research directions in this field**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ac0b3",
   "metadata": {},
   "source": [
    "## Quantum machine learning\n",
    "\n",
    "Quantum machine learning is an emerging interdisciplinary research\n",
    "area at the intersection of quantum computing and machine learning.\n",
    "\n",
    "Quantum computers use effects such as quantum coherence and quantum\n",
    "entanglement to process information, which is fundamentally different\n",
    "from classical computers. Quantum algorithms have surpassed the best\n",
    "classical algorithms in several problems (e.g., searching for an\n",
    "unsorted database, inverting a sparse matrix), which we call quantum\n",
    "acceleration.\n",
    "\n",
    "When quantum computing meets machine learning, it can be a mutually\n",
    "beneficial and reinforcing process, as it allows us to take advantage\n",
    "of quantum computing to improve the performance of classical machine\n",
    "learning algorithms. In addition, we can also use the machine learning\n",
    "algorithms (on classic computers) to analyze and improve quantum\n",
    "computing systems.\n",
    "\n",
    "[Lecture on Quantum ML](https://www.youtube.com/watch?v=Xh9pUu3-WxM&ab_channel=InstituteforPure%26AppliedMathematics%28IPAM%29).\n",
    "\n",
    "[Read interview with Maria Schuld on her work on Quantum Machine Learning](https://physics.aps.org/articles/v13/179?utm_campaign=weekly&utm_medium=email&utm_source=emailalert). See also [her recent textbook](https://www.springer.com/gp/book/9783319964232)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86184acf",
   "metadata": {},
   "source": [
    "## Quantum machine learning algorithms based on linear algebra\n",
    "\n",
    "Many quantum machine learning algorithms are based on variants of\n",
    "quantum algorithms for solving linear equations, which can efficiently\n",
    "solve N-variable linear equations with complexity of O(log2 N) under\n",
    "certain conditions. The quantum matrix inversion algorithm can\n",
    "accelerate many machine learning methods, such as least square linear\n",
    "regression, least square version of support vector machine, Gaussian\n",
    "process, and more. The training of these algorithms can be simplified\n",
    "to solve linear equations. The key bottleneck of this type of quantum\n",
    "machine learning algorithms is data inputthat is, how to initialize\n",
    "the quantum system with the entire data set. Although efficient\n",
    "data-input algorithms exist for certain situations, how to efficiently\n",
    "input data into a quantum system is as yet unknown for most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1541609",
   "metadata": {},
   "source": [
    "## Quantum reinforcement learning\n",
    "\n",
    "In quantum reinforcement learning, a quantum agent interacts with the\n",
    "classical environment to obtain rewards from the environment, so as to\n",
    "adjust and improve its behavioral strategies. In some cases, it\n",
    "achieves quantum acceleration by the quantum processing capabilities\n",
    "of the agent or the possibility of exploring the environment through\n",
    "quantum superposition. Such algorithms have been proposed in\n",
    "superconducting circuits and systems of trapped ions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83f4f8",
   "metadata": {},
   "source": [
    "## Quantum deep learning\n",
    "\n",
    "Dedicated quantum information processors, such as quantum annealers\n",
    "and programmable photonic circuits, are well suited for building deep\n",
    "quantum networks. The simplest deep quantum network is the Boltzmann\n",
    "machine. The classical Boltzmann machine consists of bits with tunable\n",
    "interactions and is trained by adjusting the interaction of these bits\n",
    "so that the distribution of its expression conforms to the statistics\n",
    "of the data. To quantize the Boltzmann machine, the neural network can\n",
    "simply be represented as a set of interacting quantum spins that\n",
    "correspond to an adjustable Ising model. Then, by initializing the\n",
    "input neurons in the Boltzmann machine to a fixed state and allowing\n",
    "the system to heat up, we can read out the output qubits to get the\n",
    "result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de4ba3",
   "metadata": {},
   "source": [
    "## Social machine learning\n",
    "\n",
    "Machine learning aims to imitate how humans\n",
    "learn. While we have developed successful machine learning algorithms,\n",
    "until now we have ignored one important fact: humans are social. Each\n",
    "of us is one part of the total society and it is difficult for us to\n",
    "live, learn, and improve ourselves, alone and isolated. Therefore, we\n",
    "should design machines with social properties. Can we let machines\n",
    "evolve by imitating human society so as to achieve more effective,\n",
    "intelligent, interpretable social machine learning?\n",
    "\n",
    "And much more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72764dfe",
   "metadata": {},
   "source": [
    "## The last words?\n",
    "\n",
    "Early computer scientist Alan Kay said, **The best way to predict the\n",
    "future is to create it**. Therefore, all machine learning\n",
    "practitioners, whether scholars or engineers, professors or students,\n",
    "need to work together to advance these important research\n",
    "topics. Together, we will not just predict the future, but create it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c065f1a5",
   "metadata": {},
   "source": [
    "## Best wishes to you all and thanks so much for your heroic efforts this semester\n",
    "\n",
    "<!-- dom:FIGURE: [figures/Nebbdyr2.png, width=500 frac=0.6] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<img src=\"figures/Nebbdyr2.png\" width=\"500\"><p style=\"font-size: 0.9em\"><i>Figure 1: </i></p>\n",
    "<!-- end figure -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
