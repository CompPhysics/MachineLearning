<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week34.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week34-bs --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 34: Introduction to the course, Logistics and Practicalities">
<title>Week 34: Introduction to the course, Logistics and Practicalities</title>
<!-- Bootstrap style: bootstrap -->
<!-- doconce format html week34.do.txt --html_style=bootstrap --pygments_html_style=default --html_admon=bootstrap_panel --html_output=week34-bs --no_mako -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->
<style type="text/css">
/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}
/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>
</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Overview of first week', 2, None, 'overview-of-first-week'),
              ('AI/ML and some statements you may have heard (and what do they '
               'mean?)',
               2,
               None,
               'ai-ml-and-some-statements-you-may-have-heard-and-what-do-they-mean'),
              ('Schedule first week', 2, None, 'schedule-first-week'),
              ('Reading Recommendations', 2, None, 'reading-recommendations'),
              ('Lectures and ComputerLab', 2, None, 'lectures-and-computerlab'),
              ('Communication channels', 2, None, 'communication-channels'),
              ('Course Format', 2, None, 'course-format'),
              ('Teachers', 2, None, 'teachers'),
              ('Deadlines for projects (tentative)',
               2,
               None,
               'deadlines-for-projects-tentative'),
              ('Recommended textbooks', 2, None, 'recommended-textbooks'),
              ('Prerequisites', 2, None, 'prerequisites'),
              ('Learning outcomes', 2, None, 'learning-outcomes'),
              ('Topics covered in this course: Statistical analysis and '
               'optimization of data',
               2,
               None,
               'topics-covered-in-this-course-statistical-analysis-and-optimization-of-data'),
              ('Topics covered in this course',
               2,
               None,
               'topics-covered-in-this-course'),
              ('Extremely useful tools, strongly recommended',
               2,
               None,
               'extremely-useful-tools-strongly-recommended'),
              ('Other courses on Data science and Machine Learning  at UiO',
               2,
               None,
               'other-courses-on-data-science-and-machine-learning-at-uio'),
              ('Introduction', 2, None, 'introduction'),
              ('What is Machine Learning?',
               2,
               None,
               'what-is-machine-learning'),
              ('Types of Machine Learning',
               2,
               None,
               'types-of-machine-learning'),
              ('Essential elements of ML', 2, None, 'essential-elements-of-ml'),
              ('An optimization/minimization problem',
               2,
               None,
               'an-optimization-minimization-problem'),
              ('A Frequentist approach to data analysis',
               2,
               None,
               'a-frequentist-approach-to-data-analysis'),
              ('What is a good model?', 2, None, 'what-is-a-good-model'),
              ('What is a good model? Can we define it?',
               2,
               None,
               'what-is-a-good-model-can-we-define-it'),
              ('Software and needed installations',
               2,
               None,
               'software-and-needed-installations'),
              ('Python installers', 2, None, 'python-installers'),
              ('Useful Python libraries', 2, None, 'useful-python-libraries'),
              ('Installing R, C++, cython or Julia',
               2,
               None,
               'installing-r-c-cython-or-julia'),
              ('Installing R, C++, cython, Numba etc',
               2,
               None,
               'installing-r-c-cython-numba-etc'),
              ('Numpy examples and Important Matrix and vector handling '
               'packages',
               2,
               None,
               'numpy-examples-and-important-matrix-and-vector-handling-packages'),
              ('Numpy and arrays', 2, None, 'numpy-and-arrays'),
              ('Matrices in Python', 2, None, 'matrices-in-python'),
              ('Meet the Pandas', 2, None, 'meet-the-pandas'),
              ('Simple linear regression model using _scikit-learn_',
               3,
               None,
               'simple-linear-regression-model-using-scikit-learn'),
              ('To our real data: nuclear binding energies. Brief reminder on '
               'masses and binding energies',
               3,
               None,
               'to-our-real-data-nuclear-binding-energies-brief-reminder-on-masses-and-binding-energies'),
              ('Organizing our data', 3, None, 'organizing-our-data'),
              ('And what about using neural networks?',
               3,
               None,
               'and-what-about-using-neural-networks'),
              ('A first summary', 2, None, 'a-first-summary'),
              ('Why Linear Regression (aka Ordinary Least Squares and family)',
               2,
               None,
               'why-linear-regression-aka-ordinary-least-squares-and-family'),
              ('Regression analysis, overarching aims',
               2,
               None,
               'regression-analysis-overarching-aims'),
              ('Regression analysis, overarching aims II',
               2,
               None,
               'regression-analysis-overarching-aims-ii'),
              ('Examples', 2, None, 'examples'),
              ('General linear models', 2, None, 'general-linear-models'),
              ('Rewriting the fitting procedure as a linear algebra problem',
               2,
               None,
               'rewriting-the-fitting-procedure-as-a-linear-algebra-problem'),
              ('Rewriting the fitting procedure as a linear algebra problem, '
               'more details',
               2,
               None,
               'rewriting-the-fitting-procedure-as-a-linear-algebra-problem-more-details'),
              ('Generalizing the fitting procedure as a linear algebra problem',
               2,
               None,
               'generalizing-the-fitting-procedure-as-a-linear-algebra-problem'),
              ('Generalizing the fitting procedure as a linear algebra problem',
               2,
               None,
               'generalizing-the-fitting-procedure-as-a-linear-algebra-problem'),
              ('Optimizing our parameters',
               2,
               None,
               'optimizing-our-parameters'),
              ('Our model for the nuclear binding energies',
               2,
               None,
               'our-model-for-the-nuclear-binding-energies'),
              ('Optimizing our parameters, more details',
               2,
               None,
               'optimizing-our-parameters-more-details'),
              ('Interpretations and optimizing our parameters',
               2,
               None,
               'interpretations-and-optimizing-our-parameters'),
              ('Interpretations and optimizing our parameters',
               2,
               None,
               'interpretations-and-optimizing-our-parameters'),
              ('Some useful matrix and vector expressions',
               2,
               None,
               'some-useful-matrix-and-vector-expressions'),
              ('Interpretations and optimizing our parameters',
               2,
               None,
               'interpretations-and-optimizing-our-parameters'),
              ('Own code for Ordinary Least Squares',
               2,
               None,
               'own-code-for-ordinary-least-squares'),
              ('Adding error analysis and training set up',
               2,
               None,
               'adding-error-analysis-and-training-set-up'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('The $\\chi^2$ function', 2, None, 'the-chi-2-function'),
              ('Fitting an Equation of State for Dense Nuclear Matter',
               2,
               None,
               'fitting-an-equation-of-state-for-dense-nuclear-matter'),
              ('The code', 2, None, 'the-code'),
              ('Splitting our Data in Training and Test data',
               2,
               None,
               'splitting-our-data-in-training-and-test-data'),
              ('Exercises', 2, None, 'exercises'),
              ('Exercise 1: Setting up various Python environments',
               2,
               None,
               'exercise-1-setting-up-various-python-environments'),
              ('Exercise 2: making your own data and exploring scikit-learn',
               2,
               None,
               'exercise-2-making-your-own-data-and-exploring-scikit-learn'),
              ('Exercise 3: Split data in test and training data',
               2,
               None,
               'exercise-3-split-data-in-test-and-training-data')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="week34-bs.html">Week 34: Introduction to the course, Logistics and Practicalities</a>
  </div>
  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._week34-bs001.html#overview-of-first-week" style="font-size: 80%;"><b>Overview of first week</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs002.html#ai-ml-and-some-statements-you-may-have-heard-and-what-do-they-mean" style="font-size: 80%;"><b>AI/ML and some statements you may have heard (and what do they mean?)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs003.html#schedule-first-week" style="font-size: 80%;"><b>Schedule first week</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs004.html#reading-recommendations" style="font-size: 80%;"><b>Reading Recommendations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs005.html#lectures-and-computerlab" style="font-size: 80%;"><b>Lectures and ComputerLab</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs006.html#communication-channels" style="font-size: 80%;"><b>Communication channels</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs007.html#course-format" style="font-size: 80%;"><b>Course Format</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs008.html#teachers" style="font-size: 80%;"><b>Teachers</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs009.html#deadlines-for-projects-tentative" style="font-size: 80%;"><b>Deadlines for projects (tentative)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs010.html#recommended-textbooks" style="font-size: 80%;"><b>Recommended textbooks</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs011.html#prerequisites" style="font-size: 80%;"><b>Prerequisites</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs012.html#learning-outcomes" style="font-size: 80%;"><b>Learning outcomes</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs013.html#topics-covered-in-this-course-statistical-analysis-and-optimization-of-data" style="font-size: 80%;"><b>Topics covered in this course: Statistical analysis and optimization of data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs014.html#topics-covered-in-this-course" style="font-size: 80%;"><b>Topics covered in this course</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs015.html#extremely-useful-tools-strongly-recommended" style="font-size: 80%;"><b>Extremely useful tools, strongly recommended</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs016.html#other-courses-on-data-science-and-machine-learning-at-uio" style="font-size: 80%;"><b>Other courses on Data science and Machine Learning  at UiO</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs017.html#introduction" style="font-size: 80%;"><b>Introduction</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs018.html#what-is-machine-learning" style="font-size: 80%;"><b>What is Machine Learning?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs019.html#types-of-machine-learning" style="font-size: 80%;"><b>Types of Machine Learning</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs020.html#essential-elements-of-ml" style="font-size: 80%;"><b>Essential elements of ML</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs021.html#an-optimization-minimization-problem" style="font-size: 80%;"><b>An optimization/minimization problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs022.html#a-frequentist-approach-to-data-analysis" style="font-size: 80%;"><b>A Frequentist approach to data analysis</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs023.html#what-is-a-good-model" style="font-size: 80%;"><b>What is a good model?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs024.html#what-is-a-good-model-can-we-define-it" style="font-size: 80%;"><b>What is a good model? Can we define it?</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs025.html#software-and-needed-installations" style="font-size: 80%;"><b>Software and needed installations</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs026.html#python-installers" style="font-size: 80%;"><b>Python installers</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs027.html#useful-python-libraries" style="font-size: 80%;"><b>Useful Python libraries</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs028.html#installing-r-c-cython-or-julia" style="font-size: 80%;"><b>Installing R, C++, cython or Julia</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs029.html#installing-r-c-cython-numba-etc" style="font-size: 80%;"><b>Installing R, C++, cython, Numba etc</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs030.html#numpy-examples-and-important-matrix-and-vector-handling-packages" style="font-size: 80%;"><b>Numpy examples and Important Matrix and vector handling packages</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs031.html#numpy-and-arrays" style="font-size: 80%;"><b>Numpy and arrays</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs032.html#matrices-in-python" style="font-size: 80%;"><b>Matrices in Python</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs033.html#meet-the-pandas" style="font-size: 80%;"><b>Meet the Pandas</b></a></li>
     <!-- navigation toc: --> <li><a href="#simple-linear-regression-model-using-scikit-learn" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Simple linear regression model using <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="#to-our-real-data-nuclear-binding-energies-brief-reminder-on-masses-and-binding-energies" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;To our real data: nuclear binding energies. Brief reminder on masses and binding energies</a></li>
     <!-- navigation toc: --> <li><a href="#organizing-our-data" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Organizing our data</a></li>
     <!-- navigation toc: --> <li><a href="#and-what-about-using-neural-networks" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;And what about using neural networks?</a></li>
     <!-- navigation toc: --> <li><a href="#a-first-summary" style="font-size: 80%;"><b>A first summary</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs035.html#why-linear-regression-aka-ordinary-least-squares-and-family" style="font-size: 80%;"><b>Why Linear Regression (aka Ordinary Least Squares and family)</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs036.html#regression-analysis-overarching-aims" style="font-size: 80%;"><b>Regression analysis, overarching aims</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs037.html#regression-analysis-overarching-aims-ii" style="font-size: 80%;"><b>Regression analysis, overarching aims II</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs038.html#examples" style="font-size: 80%;"><b>Examples</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs039.html#general-linear-models" style="font-size: 80%;"><b>General linear models</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs040.html#rewriting-the-fitting-procedure-as-a-linear-algebra-problem" style="font-size: 80%;"><b>Rewriting the fitting procedure as a linear algebra problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs041.html#rewriting-the-fitting-procedure-as-a-linear-algebra-problem-more-details" style="font-size: 80%;"><b>Rewriting the fitting procedure as a linear algebra problem, more details</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs043.html#generalizing-the-fitting-procedure-as-a-linear-algebra-problem" style="font-size: 80%;"><b>Generalizing the fitting procedure as a linear algebra problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs043.html#generalizing-the-fitting-procedure-as-a-linear-algebra-problem" style="font-size: 80%;"><b>Generalizing the fitting procedure as a linear algebra problem</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs044.html#optimizing-our-parameters" style="font-size: 80%;"><b>Optimizing our parameters</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs045.html#our-model-for-the-nuclear-binding-energies" style="font-size: 80%;"><b>Our model for the nuclear binding energies</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs046.html#optimizing-our-parameters-more-details" style="font-size: 80%;"><b>Optimizing our parameters, more details</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs050.html#interpretations-and-optimizing-our-parameters" style="font-size: 80%;"><b>Interpretations and optimizing our parameters</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs050.html#interpretations-and-optimizing-our-parameters" style="font-size: 80%;"><b>Interpretations and optimizing our parameters</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs049.html#some-useful-matrix-and-vector-expressions" style="font-size: 80%;"><b>Some useful matrix and vector expressions</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs050.html#interpretations-and-optimizing-our-parameters" style="font-size: 80%;"><b>Interpretations and optimizing our parameters</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs051.html#own-code-for-ordinary-least-squares" style="font-size: 80%;"><b>Own code for Ordinary Least Squares</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs052.html#adding-error-analysis-and-training-set-up" style="font-size: 80%;"><b>Adding error analysis and training set up</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs058.html#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs058.html#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs058.html#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs058.html#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs058.html#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs058.html#the-chi-2-function" style="font-size: 80%;"><b>The \( \chi^2 \) function</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs059.html#fitting-an-equation-of-state-for-dense-nuclear-matter" style="font-size: 80%;"><b>Fitting an Equation of State for Dense Nuclear Matter</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs060.html#the-code" style="font-size: 80%;"><b>The code</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs061.html#splitting-our-data-in-training-and-test-data" style="font-size: 80%;"><b>Splitting our Data in Training and Test data</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs062.html#exercises" style="font-size: 80%;"><b>Exercises</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs062.html#exercise-1-setting-up-various-python-environments" style="font-size: 80%;"><b>Exercise 1: Setting up various Python environments</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs062.html#exercise-2-making-your-own-data-and-exploring-scikit-learn" style="font-size: 80%;"><b>Exercise 2: making your own data and exploring scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._week34-bs062.html#exercise-3-split-data-in-test-and-training-data" style="font-size: 80%;"><b>Exercise 3: Split data in test and training data</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->
<div class="container">
<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->
<a name="part0034"></a>
<!-- !split -->
<h3 id="simple-linear-regression-model-using-scikit-learn" class="anchor">Simple linear regression model using <b>scikit-learn</b> </h3>

<p>We start with perhaps our simplest possible example, using <b>Scikit-Learn</b> to perform linear regression analysis on a data set produced by us. </p>

<p>What follows is a simple Python code where we have defined a function
\( y \) in terms of the variable \( x \). Both are defined as vectors with  \( 100 \) entries. 
The numbers in the vector \( \boldsymbol{x} \) are given
by random numbers generated with a uniform distribution with entries
\( x_i \in [0,1] \) (more about probability distribution functions
later). These values are then used to define a function \( y(x) \)
(tabulated again as a vector) with a linear dependence on \( x \) plus a
random noise added via the normal distribution.
</p>

<p>The Numpy functions are imported used the <b>import numpy as np</b>
statement and the random number generator for the uniform distribution
is called using the function <b>np.random.rand()</b>, where we specificy
that we want \( 100 \) random variables.  Using Numpy we define
automatically an array with the specified number of elements, \( 100 \) in
our case.  With the Numpy function <b>randn()</b> we can compute random
numbers with the normal distribution (mean value \( \mu \) equal to zero and
variance \( \sigma^2 \) set to one) and produce the values of \( y \) assuming a linear
dependence as function of \( x \)
</p>

$$
y = 2x+N(0,1),
$$

<p>where \( N(0,1) \) represents random numbers generated by the normal
distribution.  From <b>Scikit-Learn</b> we import then the
<b>LinearRegression</b> functionality and make a prediction \( \tilde{y} =
\alpha + \beta x \) using the function <b>fit(x,y)</b>. We call the set of
data \( (\boldsymbol{x},\boldsymbol{y}) \) for our training data. The Python package
<b>scikit-learn</b> has also a functionality which extracts the above
fitting parameters \( \alpha \) and \( \beta \) (see below). Later we will
distinguish between training data and test data.
</p>

<p>For plotting we use the Python package
<a href="https://matplotlib.org/" target="_self">matplotlib</a> which produces publication
quality figures. Feel free to explore the extensive
<a href="https://matplotlib.org/gallery/index.html" target="_self">gallery</a> of examples. In
this example we plot our original values of \( x \) and \( y \) as well as the
prediction <b>ypredict</b> (\( \tilde{y} \)), which attempts at fitting our
data with a straight line.
</p>

<p>The Python code follows here.</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># Importing various packages</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2*</span>x<span style="color: #666666">+</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
linreg <span style="color: #666666">=</span> LinearRegression()
linreg<span style="color: #666666">.</span>fit(x,y)
xnew <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[<span style="color: #666666">0</span>],[<span style="color: #666666">1</span>]])
ypredict <span style="color: #666666">=</span> linreg<span style="color: #666666">.</span>predict(xnew)

plt<span style="color: #666666">.</span>plot(xnew, ypredict, <span style="color: #BA2121">&quot;r-&quot;</span>)
plt<span style="color: #666666">.</span>plot(x, y ,<span style="color: #BA2121">&#39;ro&#39;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0</span>,<span style="color: #666666">1.0</span>,<span style="color: #666666">0</span>, <span style="color: #666666">5.0</span>])
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">r&#39;$x$&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">r&#39;$y$&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">r&#39;Simple Linear Regression&#39;</span>)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>This example serves several aims. It allows us to demonstrate several
aspects of data analysis and later machine learning algorithms. The
immediate visualization shows that our linear fit is not
impressive. It goes through the data points, but there are many
outliers which are not reproduced by our linear regression.  We could
now play around with this small program and change for example the
factor in front of \( x \) and the normal distribution.  Try to change the
function \( y \) to
</p>

$$
y = 10x+0.01 \times N(0,1),
$$

<p>where \( x \) is defined as before.  Does the fit look better? Indeed, by
reducing the role of the noise given by the normal distribution we see immediately that
our linear prediction seemingly reproduces better the training
set. However, this testing 'by the eye' is obviouly not satisfactory in the
long run. Here we have only defined the training data and our model, and 
have not discussed a more rigorous approach to the <b>cost</b> function.
</p>

<p>We need more rigorous criteria in defining whether we have succeeded or
not in modeling our training data.  You will be surprised to see that
many scientists seldomly venture beyond this 'by the eye' approach. A
standard approach for the <em>cost</em> function is the so-called \( \chi^2 \)
function (a variant of the mean-squared error (MSE))
</p>

$$ \chi^2 = \frac{1}{n}
\sum_{i=0}^{n-1}\frac{(y_i-\tilde{y}_i)^2}{\sigma_i^2}, 
$$

<p>where \( \sigma_i^2 \) is the variance (to be defined later) of the entry
\( y_i \).  We may not know the explicit value of \( \sigma_i^2 \), it serves
however the aim of scaling the equations and make the cost function
dimensionless.  
</p>

<p>Minimizing the cost function is a central aspect of
our discussions to come. Finding its minima as function of the model
parameters (\( \alpha \) and \( \beta \) in our case) will be a recurring
theme in these series of lectures. Essentially all machine learning
algorithms we will discuss center around the minimization of the
chosen cost function. This depends in turn on our specific
model for describing the data, a typical situation in supervised
learning. Automatizing the search for the minima of the cost function is a
central ingredient in all algorithms. Typical methods which are
employed are various variants of <b>gradient</b> methods. These will be
discussed in more detail later. Again, you'll be surprised to hear that
many practitioners minimize the above function ''by the eye', popularly dubbed as 
'chi by the eye'. That is, change a parameter and see (visually and numerically) that 
the  \( \chi^2 \) function becomes smaller. 
</p>

<p>There are many ways to define the cost function. A simpler approach is to look at the relative difference between the training data and the predicted data, that is we define 
the relative error (why would we prefer the MSE instead of the relative error?) as
</p>

$$
\epsilon_{\mathrm{relative}}= \frac{\vert \boldsymbol{y} -\boldsymbol{\tilde{y}}\vert}{\vert \boldsymbol{y}\vert}.
$$

<p>The squared cost function results in an arithmetic mean-unbiased
estimator, and the absolute-value cost function results in a
median-unbiased estimator (in the one-dimensional case, and a
geometric median-unbiased estimator for the multi-dimensional
case). The squared cost function has the disadvantage that it has the tendency
to be dominated by outliers.
</p>

<p>We can modify easily the above Python code and plot the relative error instead</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">5*</span>x<span style="color: #666666">+0.01*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
linreg <span style="color: #666666">=</span> LinearRegression()
linreg<span style="color: #666666">.</span>fit(x,y)
ypredict <span style="color: #666666">=</span> linreg<span style="color: #666666">.</span>predict(x)

plt<span style="color: #666666">.</span>plot(x, np<span style="color: #666666">.</span>abs(ypredict<span style="color: #666666">-</span>y)<span style="color: #666666">/</span><span style="color: #008000">abs</span>(y), <span style="color: #BA2121">&quot;ro&quot;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0</span>,<span style="color: #666666">1.0</span>,<span style="color: #666666">0.0</span>, <span style="color: #666666">0.5</span>])
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">r&#39;$x$&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">r&#39;$\epsilon_{\mathrm</span><span style="color: #BB6688; font-weight: bold">{relative}</span><span style="color: #BA2121">}$&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">r&#39;Relative error&#39;</span>)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Depending on the parameter in front of the normal distribution, we may
have a small or larger relative error. Try to play around with
different training data sets and study (graphically) the value of the
relative error.
</p>

<p>As mentioned above, <b>Scikit-Learn</b> has an impressive functionality.
We can for example extract the values of \( \alpha \) and \( \beta \) and
their error estimates, or the variance and standard deviation and many
other properties from the statistical data analysis. 
</p>

<p>Here we show an
example of the functionality of <b>Scikit-Learn</b>.
</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span> 
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span> 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> mean_squared_error, r2_score, mean_squared_log_error, mean_absolute_error

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2.0+</span> <span style="color: #666666">5*</span>x<span style="color: #666666">+0.5*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
linreg <span style="color: #666666">=</span> LinearRegression()
linreg<span style="color: #666666">.</span>fit(x,y)
ypredict <span style="color: #666666">=</span> linreg<span style="color: #666666">.</span>predict(x)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;The intercept alpha: </span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&#39;</span>, linreg<span style="color: #666666">.</span>intercept_)
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Coefficient beta : </span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&#39;</span>, linreg<span style="color: #666666">.</span>coef_)
<span style="color: #408080; font-style: italic"># The mean squared error                               </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Mean squared error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> mean_squared_error(y, ypredict))
<span style="color: #408080; font-style: italic"># Explained variance score: 1 is perfect prediction                                 </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Variance score: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> r2_score(y, ypredict))
<span style="color: #408080; font-style: italic"># Mean squared log error                                                        </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Mean squared log error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> mean_squared_log_error(y, ypredict) )
<span style="color: #408080; font-style: italic"># Mean absolute error                                                           </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Mean absolute error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> mean_absolute_error(y, ypredict))
plt<span style="color: #666666">.</span>plot(x, ypredict, <span style="color: #BA2121">&quot;r-&quot;</span>)
plt<span style="color: #666666">.</span>plot(x, y ,<span style="color: #BA2121">&#39;ro&#39;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0.0</span>,<span style="color: #666666">1.0</span>,<span style="color: #666666">1.5</span>, <span style="color: #666666">7.0</span>])
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">r&#39;$x$&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">r&#39;$y$&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">r&#39;Linear Regression fit &#39;</span>)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>The function <b>coef</b> gives us the parameter \( \beta \) of our fit while <b>intercept</b> yields 
\( \alpha \). Depending on the constant in front of the normal distribution, we get values near or far from \( \alpha =2 \) and \( \beta =5 \). Try to play around with different parameters in front of the normal distribution. The function <b>meansquarederror</b> gives us the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss defined as
</p>
$$ MSE(\boldsymbol{y},\boldsymbol{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
$$

<p>The smaller the value, the better the fit. Ideally we would like to
have an MSE equal zero.  The attentive reader has probably recognized
this function as being similar to the \( \chi^2 \) function defined above.
</p>

<p>The <b>r2score</b> function computes \( R^2 \), the coefficient of
determination. It provides a measure of how well future samples are
likely to be predicted by the model. Best possible score is 1.0 and it
can be negative (because the model can be arbitrarily worse). A
constant model that always predicts the expected value of \( \boldsymbol{y} \),
disregarding the input features, would get a \( R^2 \) score of \( 0.0 \).
</p>

<p>If \( \tilde{\boldsymbol{y}}_i \) is the predicted value of the \( i-th \) sample and \( y_i \) is the corresponding true value, then the score \( R^2 \) is defined as</p>
$$
R^2(\boldsymbol{y}, \tilde{\boldsymbol{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
$$

<p>where we have defined the mean value  of \( \boldsymbol{y} \) as</p>
$$
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
$$

<p>Another quantity taht we will meet again in our discussions of regression analysis is 
 the mean absolute error (MAE), a risk metric corresponding to the expected value of the absolute error loss or what we call the \( l1 \)-norm loss. In our discussion above we presented the relative error.
The MAE is defined as follows
</p>
$$
\text{MAE}(\boldsymbol{y}, \boldsymbol{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n-1} \left| y_i - \tilde{y}_i \right|.
$$

<p>We present the 
squared logarithmic (quadratic) error
</p>
$$
\text{MSLE}(\boldsymbol{y}, \boldsymbol{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n - 1} (\log_e (1 + y_i) - \log_e (1 + \tilde{y}_i) )^2,
$$

<p>where \( \log_e (x) \) stands for the natural logarithm of \( x \). This error
estimate is best to use when targets having exponential growth, such
as population counts, average sales of a commodity over a span of
years etc. 
</p>

<p>Finally, another cost function is the Huber cost function used in robust regression.</p>

<p>The rationale behind this possible cost function is its reduced
sensitivity to outliers in the data set. In our discussions on
dimensionality reduction and normalization of data we will meet other
ways of dealing with outliers.
</p>

<p>The Huber cost function is defined as</p>
$$
H_{\delta}(\boldsymbol{a})=\left\{\begin{array}{cc}\frac{1}{2} \boldsymbol{a}^{2}& \text{for }|\boldsymbol{a}|\leq \delta\\ \delta (|\boldsymbol{a}|-\frac{1}{2}\delta ),&\text{otherwise}.\end{array}\right.
$$

<p>Here \( \boldsymbol{a}=\boldsymbol{y} - \boldsymbol{\tilde{y}} \).</p>

<p>We will discuss in more detail these and other functions in the
various lectures and lab sessions.
</p>
<h3 id="to-our-real-data-nuclear-binding-energies-brief-reminder-on-masses-and-binding-energies" class="anchor">To our real data: nuclear binding energies. Brief reminder on masses and binding energies </h3>

<p>Let us now dive into  nuclear physics and remind ourselves briefly about some basic features about binding
energies.  A basic quantity which can be measured for the ground
states of nuclei is the atomic mass \( M(N, Z) \) of the neutral atom with
atomic mass number \( A \) and charge \( Z \). The number of neutrons is \( N \). There are indeed several sophisticated experiments worldwide which allow us to measure this quantity to high precision (parts per million even). 
</p>

<p>Atomic masses are usually tabulated in terms of the mass excess defined by</p>
$$
\Delta M(N, Z) =  M(N, Z) - uA,
$$

<p>where \( u \) is the Atomic Mass Unit </p>
$$
u = M(^{12}\mathrm{C})/12 = 931.4940954(57) \hspace{0.1cm} \mathrm{MeV}/c^2.
$$

<p>The nucleon masses are</p>
$$
m_p =  1.00727646693(9)u,
$$

<p>and</p>
$$
m_n = 939.56536(8)\hspace{0.1cm} \mathrm{MeV}/c^2 = 1.0086649156(6)u.
$$

<p>In the <a href="http://nuclearmasses.org/resources_folder/Wang_2017_Chinese_Phys_C_41_030003.pdf" target="_self">2016 mass evaluation of by W.J.Huang, G.Audi, M.Wang, F.G.Kondev, S.Naimi and X.Xu</a>
there are data on masses and decays of 3437 nuclei.
</p>

<p>The nuclear binding energy is defined as the energy required to break
up a given nucleus into its constituent parts of \( N \) neutrons and \( Z \)
protons. In terms of the atomic masses \( M(N, Z) \) the binding energy is
defined by
</p>

$$
BE(N, Z) = ZM_H c^2 + Nm_n c^2 - M(N, Z)c^2 ,
$$

<p>where \( M_H \) is the mass of the hydrogen atom and \( m_n \) is the mass of the neutron.
In terms of the mass excess the binding energy is given by
</p>
$$
BE(N, Z) = Z\Delta_H c^2 + N\Delta_n c^2 -\Delta(N, Z)c^2 ,
$$

<p>where \( \Delta_H c^2 = 7.2890 \) MeV and \( \Delta_n c^2 = 8.0713 \) MeV.</p>

<p>A popular and physically intuitive model which can be used to parametrize 
the experimental binding energies as function of \( A \), is the so-called 
<b>liquid drop model</b>. The ansatz is based on the following expression
</p>

$$ 
BE(N,Z) = a_1A-a_2A^{2/3}-a_3\frac{Z^2}{A^{1/3}}-a_4\frac{(N-Z)^2}{A},
$$

<p>where \( A \) stands for the number of nucleons and the $a_i$s are parameters which are determined by a fit 
to the experimental data.  
</p>

<p>To arrive at the above expression we have assumed that we can make the following assumptions:</p>

<ul>
 <li> There is a volume term \( a_1A \) proportional with the number of nucleons (the energy is also an extensive quantity). When an assembly of nucleons of the same size is packed together into the smallest volume, each interior nucleon has a certain number of other nucleons in contact with it. This contribution is proportional to the volume.</li>
 <li> There is a surface energy term \( a_2A^{2/3} \). The assumption here is that a nucleon at the surface of a nucleus interacts with fewer other nucleons than one in the interior of the nucleus and hence its binding energy is less. This surface energy term takes that into account and is therefore negative and is proportional to the surface area.</li>
 <li> There is a Coulomb energy term \( a_3\frac{Z^2}{A^{1/3}} \). The electric repulsion between each pair of protons in a nucleus yields less binding.</li> 
 <li> There is an asymmetry term \( a_4\frac{(N-Z)^2}{A} \). This term is associated with the Pauli exclusion principle and reflects the fact that the proton-neutron interaction is more attractive on the average than the neutron-neutron and proton-proton interactions.</li>
</ul>
<p>We could also add a so-called pairing term, which is a correction term that
arises from the tendency of proton pairs and neutron pairs to
occur. An even number of particles is more stable than an odd number. 
</p>
<h3 id="organizing-our-data" class="anchor">Organizing our data </h3>

<p>Let us start with reading and organizing our data. 
We start with the compilation of masses and binding energies from 2016.
After having downloaded this file to our own computer, we are now ready to read the file and start structuring our data.
</p>

<p>We start with preparing folders for storing our calculations and the data file over masses and binding energies. We import also various modules that we will find useful in order to present various Machine Learning methods. Here we focus mainly on the functionality of <b>scikit-learn</b>.</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">skl</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> mean_squared_error, r2_score, mean_absolute_error
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>

<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, <span style="color: #008000">format</span><span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)

infile <span style="color: #666666">=</span> <span style="color: #008000">open</span>(data_path(<span style="color: #BA2121">&quot;MassEval2016.dat&quot;</span>),<span style="color: #BA2121">&#39;r&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Before we proceed, we define also a function for making our plots. You can obviously avoid this and simply set up various <b>matplotlib</b> commands every time you need them. You may however find it convenient to collect all such commands in one function and simply call this function. </p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">pylab</span> <span style="color: #008000; font-weight: bold">import</span> plt, mpl
plt<span style="color: #666666">.</span>style<span style="color: #666666">.</span>use(<span style="color: #BA2121">&#39;seaborn&#39;</span>)
mpl<span style="color: #666666">.</span>rcParams[<span style="color: #BA2121">&#39;font.family&#39;</span>] <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;serif&#39;</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">MakePlot</span>(x,y, styles, labels, axlabels):
    plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">6</span>))
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #008000">len</span>(x)):
        plt<span style="color: #666666">.</span>plot(x[i], y[i], styles[i], label <span style="color: #666666">=</span> labels[i])
        plt<span style="color: #666666">.</span>xlabel(axlabels[<span style="color: #666666">0</span>])
        plt<span style="color: #666666">.</span>ylabel(axlabels[<span style="color: #666666">1</span>])
    plt<span style="color: #666666">.</span>legend(loc<span style="color: #666666">=0</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Our next step is to read the data on experimental binding energies and
reorganize them as functions of the mass number \( A \), the number of
protons \( Z \) and neutrons \( N \) using <b>pandas</b>.  Before we do this it is
always useful (unless you have a binary file or other types of compressed
data) to actually open the file and simply take a look at it!
</p>

<p>In particular, the program that outputs the final nuclear masses is written in Fortran with a specific format. It means that we need to figure out the format and which columns contain the data we are interested in. Pandas comes with a function that reads formatted output. After having admired the file, we are now ready to start massaging it with <b>pandas</b>. The file begins with some basic format information.</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;                                                                                                                         </span>
<span style="color: #BA2121; font-style: italic">This is taken from the data file of the mass 2016 evaluation.                                                               </span>
<span style="color: #BA2121; font-style: italic">All files are 3436 lines long with 124 character per line.                                                                  </span>
<span style="color: #BA2121; font-style: italic">       Headers are 39 lines long.                                                                                           </span>
<span style="color: #BA2121; font-style: italic">   col 1     :  Fortran character control: 1 = page feed  0 = line feed                                                     </span>
<span style="color: #BA2121; font-style: italic">   format    :  a1,i3,i5,i5,i5,1x,a3,a4,1x,f13.5,f11.5,f11.3,f9.3,1x,a2,f11.3,f9.3,1x,i3,1x,f12.5,f11.5                     </span>
<span style="color: #BA2121; font-style: italic">   These formats are reflected in the pandas widths variable below, see the statement                                       </span>
<span style="color: #BA2121; font-style: italic">   widths=(1,3,5,5,5,1,3,4,1,13,11,11,9,1,2,11,9,1,3,1,12,11,1),                                                            </span>
<span style="color: #BA2121; font-style: italic">   Pandas has also a variable header, with length 39 in this case.                                                          </span>
<span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>The data we are interested in are in columns 2, 3, 4 and 11, giving us
the number of neutrons, protons, mass numbers and binding energies,
respectively. We add also for the sake of completeness the element name. The data are in fixed-width formatted lines and we will
covert them into the <b>pandas</b> DataFrame structure.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># Read the experimental data with Pandas</span>
Masses <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>read_fwf(infile, usecols<span style="color: #666666">=</span>(<span style="color: #666666">2</span>,<span style="color: #666666">3</span>,<span style="color: #666666">4</span>,<span style="color: #666666">6</span>,<span style="color: #666666">11</span>),
              names<span style="color: #666666">=</span>(<span style="color: #BA2121">&#39;N&#39;</span>, <span style="color: #BA2121">&#39;Z&#39;</span>, <span style="color: #BA2121">&#39;A&#39;</span>, <span style="color: #BA2121">&#39;Element&#39;</span>, <span style="color: #BA2121">&#39;Ebinding&#39;</span>),
              widths<span style="color: #666666">=</span>(<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">5</span>,<span style="color: #666666">5</span>,<span style="color: #666666">5</span>,<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">4</span>,<span style="color: #666666">1</span>,<span style="color: #666666">13</span>,<span style="color: #666666">11</span>,<span style="color: #666666">11</span>,<span style="color: #666666">9</span>,<span style="color: #666666">1</span>,<span style="color: #666666">2</span>,<span style="color: #666666">11</span>,<span style="color: #666666">9</span>,<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">1</span>,<span style="color: #666666">12</span>,<span style="color: #666666">11</span>,<span style="color: #666666">1</span>),
              header<span style="color: #666666">=39</span>,
              index_col<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">False</span>)

<span style="color: #408080; font-style: italic"># Extrapolated values are indicated by &#39;#&#39; in place of the decimal place, so</span>
<span style="color: #408080; font-style: italic"># the Ebinding column won&#39;t be numeric. Coerce to float and drop these entries.</span>
Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>] <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>to_numeric(Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>], errors<span style="color: #666666">=</span><span style="color: #BA2121">&#39;coerce&#39;</span>)
Masses <span style="color: #666666">=</span> Masses<span style="color: #666666">.</span>dropna()
<span style="color: #408080; font-style: italic"># Convert from keV to MeV.</span>
Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>] <span style="color: #666666">/=</span> <span style="color: #666666">1000</span>

<span style="color: #408080; font-style: italic"># Group the DataFrame by nucleon number, A.</span>
Masses <span style="color: #666666">=</span> Masses<span style="color: #666666">.</span>groupby(<span style="color: #BA2121">&#39;A&#39;</span>)
<span style="color: #408080; font-style: italic"># Find the rows of the grouped DataFrame with the maximum binding energy.</span>
Masses <span style="color: #666666">=</span> Masses<span style="color: #666666">.</span>apply(<span style="color: #008000; font-weight: bold">lambda</span> t: t[t<span style="color: #666666">.</span>Ebinding<span style="color: #666666">==</span>t<span style="color: #666666">.</span>Ebinding<span style="color: #666666">.</span>max()])
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>We have now read in the data, grouped them according to the variables we are interested in. 
We see how easy it is to reorganize the data using <b>pandas</b>. If we
were to do these operations in C/C++ or Fortran, we would have had to
write various functions/subroutines which perform the above
reorganizations for us.  Having reorganized the data, we can now start
to make some simple fits using both the functionalities in <b>numpy</b> and
<b>Scikit-Learn</b> afterwards. 
</p>

<p>Now we define five variables which contain
the number of nucleons \( A \), the number of protons \( Z \) and the number of neutrons \( N \), the element name and finally the energies themselves.
</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">A <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;A&#39;</span>]
Z <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;Z&#39;</span>]
N <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;N&#39;</span>]
Element <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;Element&#39;</span>]
Energies <span style="color: #666666">=</span> Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>]
<span style="color: #008000">print</span>(Masses)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>The next step, and we will define this mathematically later, is to set up the so-called <b>design matrix</b>. We will throughout call this matrix \( \boldsymbol{X} \).
It has dimensionality \( p\times n \), where \( n \) is the number of data points and \( p \) are the so-called predictors. In our case here they are given by the number of polynomials in \( A \) we wish to include in the fit. 
</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># Now we set up the design matrix X</span>
X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(A),<span style="color: #666666">5</span>))
X[:,<span style="color: #666666">0</span>] <span style="color: #666666">=</span> <span style="color: #666666">1</span>
X[:,<span style="color: #666666">1</span>] <span style="color: #666666">=</span> A
X[:,<span style="color: #666666">2</span>] <span style="color: #666666">=</span> A<span style="color: #666666">**</span>(<span style="color: #666666">2.0/3.0</span>)
X[:,<span style="color: #666666">3</span>] <span style="color: #666666">=</span> A<span style="color: #666666">**</span>(<span style="color: #666666">-1.0/3.0</span>)
X[:,<span style="color: #666666">4</span>] <span style="color: #666666">=</span> A<span style="color: #666666">**</span>(<span style="color: #666666">-1.0</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>With <b>scikitlearn</b> we are now ready to use linear regression and fit our data.</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;">clf <span style="color: #666666">=</span> skl<span style="color: #666666">.</span>LinearRegression()<span style="color: #666666">.</span>fit(X, Energies)
fity <span style="color: #666666">=</span> clf<span style="color: #666666">.</span>predict(X)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Pretty simple!  
Now we can print measures of how our fit is doing, the coefficients from the fits and plot the final fit together with our data.
</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #408080; font-style: italic"># The mean squared error                               </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&quot;Mean squared error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> mean_squared_error(Energies, fity))
<span style="color: #408080; font-style: italic"># Explained variance score: 1 is perfect prediction                                 </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Variance score: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> r2_score(Energies, fity))
<span style="color: #408080; font-style: italic"># Mean absolute error                                                           </span>
<span style="color: #008000">print</span>(<span style="color: #BA2121">&#39;Mean absolute error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> mean_absolute_error(Energies, fity))
<span style="color: #008000">print</span>(clf<span style="color: #666666">.</span>coef_, clf<span style="color: #666666">.</span>intercept_)

Masses[<span style="color: #BA2121">&#39;Eapprox&#39;</span>]  <span style="color: #666666">=</span> fity
<span style="color: #408080; font-style: italic"># Generate a plot comparing the experimental with the fitted values values.</span>
fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots()
ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">r&#39;$A = N + Z$&#39;</span>)
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">r&#39;$E_\mathrm</span><span style="color: #BB6688; font-weight: bold">{bind}</span><span style="color: #BA2121">\,/\mathrm</span><span style="color: #BB6688; font-weight: bold">{MeV}</span><span style="color: #BA2121">$&#39;</span>)
ax<span style="color: #666666">.</span>plot(Masses[<span style="color: #BA2121">&#39;A&#39;</span>], Masses[<span style="color: #BA2121">&#39;Ebinding&#39;</span>], alpha<span style="color: #666666">=0.7</span>, lw<span style="color: #666666">=2</span>,
            label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Ame2016&#39;</span>)
ax<span style="color: #666666">.</span>plot(Masses[<span style="color: #BA2121">&#39;A&#39;</span>], Masses[<span style="color: #BA2121">&#39;Eapprox&#39;</span>], alpha<span style="color: #666666">=0.7</span>, lw<span style="color: #666666">=2</span>, c<span style="color: #666666">=</span><span style="color: #BA2121">&#39;m&#39;</span>,
            label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Fit&#39;</span>)
ax<span style="color: #666666">.</span>legend()
save_fig(<span style="color: #BA2121">&quot;Masses2016&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h3 id="and-what-about-using-neural-networks" class="anchor">And what about using neural networks? </h3>
<p>The <b>seaborn</b> package allows us to visualize data in an efficient way. Note that we use <b>scikit-learn</b>'s multi-layer perceptron (or feed forward neural network) 
functionality.
</p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #f8f8f8">
  <pre style="line-height: 125%;"><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.neural_network</span> <span style="color: #008000; font-weight: bold">import</span> MLPRegressor
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> accuracy_score
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">seaborn</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">sns</span>

X_train <span style="color: #666666">=</span> X
Y_train <span style="color: #666666">=</span> Energies
n_hidden_neurons <span style="color: #666666">=</span> <span style="color: #666666">100</span>
epochs <span style="color: #666666">=</span> <span style="color: #666666">100</span>
<span style="color: #408080; font-style: italic"># store models for later use</span>
eta_vals <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-5</span>, <span style="color: #666666">1</span>, <span style="color: #666666">7</span>)
lmbd_vals <span style="color: #666666">=</span> np<span style="color: #666666">.</span>logspace(<span style="color: #666666">-5</span>, <span style="color: #666666">1</span>, <span style="color: #666666">7</span>)
<span style="color: #408080; font-style: italic"># store the models for later use</span>
DNN_scikit <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(eta_vals), <span style="color: #008000">len</span>(lmbd_vals)), dtype<span style="color: #666666">=</span><span style="color: #008000">object</span>)
train_accuracy <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros((<span style="color: #008000">len</span>(eta_vals), <span style="color: #008000">len</span>(lmbd_vals)))
sns<span style="color: #666666">.</span>set()
<span style="color: #008000; font-weight: bold">for</span> i, eta <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(eta_vals):
    <span style="color: #008000; font-weight: bold">for</span> j, lmbd <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">enumerate</span>(lmbd_vals):
        dnn <span style="color: #666666">=</span> MLPRegressor(hidden_layer_sizes<span style="color: #666666">=</span>(n_hidden_neurons), activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;logistic&#39;</span>,
                            alpha<span style="color: #666666">=</span>lmbd, learning_rate_init<span style="color: #666666">=</span>eta, max_iter<span style="color: #666666">=</span>epochs)
        dnn<span style="color: #666666">.</span>fit(X_train, Y_train)
        DNN_scikit[i][j] <span style="color: #666666">=</span> dnn
        train_accuracy[i][j] <span style="color: #666666">=</span> dnn<span style="color: #666666">.</span>score(X_train, Y_train)

fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots(figsize <span style="color: #666666">=</span> (<span style="color: #666666">10</span>, <span style="color: #666666">10</span>))
sns<span style="color: #666666">.</span>heatmap(train_accuracy, annot<span style="color: #666666">=</span><span style="color: #008000; font-weight: bold">True</span>, ax<span style="color: #666666">=</span>ax, cmap<span style="color: #666666">=</span><span style="color: #BA2121">&quot;viridis&quot;</span>)
ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&quot;Training Accuracy&quot;</span>)
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&quot;$\eta$&quot;</span>)
ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&quot;$\lambda$&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="a-first-summary" class="anchor">A first summary </h2>

<p>The aim behind these introductory words was to present to you various
Python libraries and their functionalities, in particular libraries like
<b>numpy</b>, <b>pandas</b>, <b>xarray</b> and <b>matplotlib</b> and other that make our life much easier
in handling various data sets and visualizing data. 
</p>

<p>Furthermore,
<b>Scikit-Learn</b> allows us with few lines of code to implement popular
Machine Learning algorithms for supervised learning. Later we will meet <b>Tensorflow</b>, a powerful library for deep learning. 
Now it is time to dive more into the details of various methods. We will start with linear regression and try to take a deeper look at what it entails.
</p>

<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._week34-bs033.html">&laquo;</a></li>
  <li><a href="._week34-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week34-bs026.html">27</a></li>
  <li><a href="._week34-bs027.html">28</a></li>
  <li><a href="._week34-bs028.html">29</a></li>
  <li><a href="._week34-bs029.html">30</a></li>
  <li><a href="._week34-bs030.html">31</a></li>
  <li><a href="._week34-bs031.html">32</a></li>
  <li><a href="._week34-bs032.html">33</a></li>
  <li><a href="._week34-bs033.html">34</a></li>
  <li class="active"><a href="._week34-bs034.html">35</a></li>
  <li><a href="._week34-bs035.html">36</a></li>
  <li><a href="._week34-bs036.html">37</a></li>
  <li><a href="._week34-bs037.html">38</a></li>
  <li><a href="._week34-bs038.html">39</a></li>
  <li><a href="._week34-bs039.html">40</a></li>
  <li><a href="._week34-bs040.html">41</a></li>
  <li><a href="._week34-bs041.html">42</a></li>
  <li><a href="._week34-bs042.html">43</a></li>
  <li><a href="._week34-bs043.html">44</a></li>
  <li><a href="">...</a></li>
  <li><a href="._week34-bs062.html">63</a></li>
  <li><a href="._week34-bs035.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->
</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<!-- Bootstrap footer
<footer>
<a href="https://..."><img width="250" align=right src="https://..."></a>
</footer>
-->
<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>
</body>
</html>

