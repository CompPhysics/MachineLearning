<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Data Analysis and Machine Learning: Using Neural networks to solve ODEs and PDEs">

<title>Data Analysis and Machine Learning: Using Neural networks to solve ODEs and PDEs</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Differential equations', 2, None, '___sec0'),
              ('Description of the equation to solve for', 2, None, '___sec1'),
              ('Ordinary Differential Equations', 2, None, '___sec2'),
              ('The trial solution', 2, None, '___sec3'),
              ('Minimizing the cost function using gradient descent and '
               'automatic differentiation',
               2,
               None,
               '___sec4'),
              ('Example: Exponential decay and setting up the network using '
               'Autograd',
               2,
               None,
               '___sec5'),
              ('The function to solve for', 2, None, '___sec6'),
              ('The trial solution', 2, None, '___sec7'),
              ('Reformulating the problem', 2, None, '___sec8'),
              ('A possible implementation of a neural network using Autograd',
               2,
               None,
               '___sec9'),
              ('Backpropagation using Autograd', 2, None, '___sec10'),
              ('Gradient descent', 2, None, '___sec11'),
              ('The network with one input, hidden, and output layer',
               2,
               None,
               '___sec12'),
              ('The network with one input layer, specified number of hidden '
               'layers, and one output layer  output layer',
               2,
               None,
               '___sec13'),
              ('Example: Population growth, comparing Autograd, TensorFlow, '
               "and Euler's scheme",
               2,
               None,
               '___sec14'),
              ('Setting up the problem', 2, None, '___sec15'),
              ('The trial solution', 2, None, '___sec16'),
              ('The program using Autograd', 2, None, '___sec17'),
              ('Using forward Euler to solve the ODE', 2, None, '___sec18'),
              ('Using TensorFlow to model logistic population growth',
               2,
               None,
               '___sec19'),
              ('The general program flow in TensorFlow', 2, None, '___sec20'),
              ('Program flow in TensorFlow - Construction phase',
               2,
               None,
               '___sec21'),
              ('Program flow in TensorFlow - Execution phase',
               2,
               None,
               '___sec22'),
              ('The full program modeling logistic population growth using '
               'TensorFlow',
               2,
               None,
               '___sec23'),
              ('Example: Solving the one dimensional Poisson equation using '
               'Autograd and TensorFlow',
               2,
               None,
               '___sec24'),
              ('The specific equation to solve for', 2, None, '___sec25'),
              ('Solving the equation using Autograd', 2, None, '___sec26'),
              ('Comparing with a numerical scheme', 2, None, '___sec27'),
              ('Using gradient descent in TensorFlow to solve Poisson equation',
               2,
               None,
               '___sec28'),
              ('Using a different optimization algorithm implemented in '
               'TensorFlow to solve Poisson equation',
               2,
               None,
               '___sec29'),
              ('Partial Differential Equations', 2, None, '___sec30'),
              ('Example: The diffusion equation', 2, None, '___sec31'),
              ('Defining the problem', 2, None, '___sec32'),
              ('Setting up the network using Autograd', 2, None, '___sec33'),
              ('Setting up the network using Autograd; The trial solution',
               2,
               None,
               '___sec34'),
              ('Setting up the network using Autograd; The full program',
               2,
               None,
               '___sec35'),
              ('Example: Solving the wave equation using Autograd and '
               'TensorFlow',
               2,
               None,
               '___sec36'),
              ('The problem to solve for', 2, None, '___sec37'),
              ('The trial solution', 2, None, '___sec38'),
              ('The analytical solution', 2, None, '___sec39'),
              ('Solving the wave equation - the full program using Autograd',
               2,
               None,
               '___sec40'),
              ('Solving the wave equation - the full program using TensorFlow',
               2,
               None,
               '___sec41'),
              ('Resources', 2, None, '___sec42')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="odenn-bs.html">Data Analysis and Machine Learning: Using Neural networks to solve ODEs and PDEs</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._odenn-bs001.html#___sec0" style="font-size: 80%;">Differential equations</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs002.html#___sec1" style="font-size: 80%;">Description of the equation to solve for</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs003.html#___sec2" style="font-size: 80%;">Ordinary Differential Equations</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs004.html#___sec3" style="font-size: 80%;">The trial solution</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs005.html#___sec4" style="font-size: 80%;">Minimizing the cost function using gradient descent and automatic differentiation</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs006.html#___sec5" style="font-size: 80%;">Example: Exponential decay and setting up the network using Autograd</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs007.html#___sec6" style="font-size: 80%;">The function to solve for</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs008.html#___sec7" style="font-size: 80%;">The trial solution</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs009.html#___sec8" style="font-size: 80%;">Reformulating the problem</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs010.html#___sec9" style="font-size: 80%;">A possible implementation of a neural network using Autograd</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs011.html#___sec10" style="font-size: 80%;">Backpropagation using Autograd</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs012.html#___sec11" style="font-size: 80%;">Gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs013.html#___sec12" style="font-size: 80%;">The network with one input, hidden, and output layer</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs014.html#___sec13" style="font-size: 80%;">The network with one input layer, specified number of hidden layers, and one output layer  output layer</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs015.html#___sec14" style="font-size: 80%;">Example: Population growth, comparing Autograd, TensorFlow, and Euler's scheme</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs016.html#___sec15" style="font-size: 80%;">Setting up the problem</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs017.html#___sec16" style="font-size: 80%;">The trial solution</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs018.html#___sec17" style="font-size: 80%;">The program using Autograd</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs019.html#___sec18" style="font-size: 80%;">Using forward Euler to solve the ODE</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs020.html#___sec19" style="font-size: 80%;">Using TensorFlow to model logistic population growth</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs021.html#___sec20" style="font-size: 80%;">The general program flow in TensorFlow</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs021.html#___sec21" style="font-size: 80%;">Program flow in TensorFlow - Construction phase</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs021.html#___sec22" style="font-size: 80%;">Program flow in TensorFlow - Execution phase</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs021.html#___sec23" style="font-size: 80%;">The full program modeling logistic population growth using TensorFlow</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs022.html#___sec24" style="font-size: 80%;">Example: Solving the one dimensional Poisson equation using Autograd and TensorFlow</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs023.html#___sec25" style="font-size: 80%;">The specific equation to solve for</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs024.html#___sec26" style="font-size: 80%;">Solving the equation using Autograd</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs025.html#___sec27" style="font-size: 80%;">Comparing with a numerical scheme</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs026.html#___sec28" style="font-size: 80%;">Using gradient descent in TensorFlow to solve Poisson equation</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs027.html#___sec29" style="font-size: 80%;">Using a different optimization algorithm implemented in TensorFlow to solve Poisson equation</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs028.html#___sec30" style="font-size: 80%;">Partial Differential Equations</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs029.html#___sec31" style="font-size: 80%;">Example: The diffusion equation</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs030.html#___sec32" style="font-size: 80%;">Defining the problem</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs031.html#___sec33" style="font-size: 80%;">Setting up the network using Autograd</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs032.html#___sec34" style="font-size: 80%;">Setting up the network using Autograd; The trial solution</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs033.html#___sec35" style="font-size: 80%;">Setting up the network using Autograd; The full program</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs034.html#___sec36" style="font-size: 80%;">Example: Solving the wave equation using Autograd and TensorFlow</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs035.html#___sec37" style="font-size: 80%;">The problem to solve for</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs036.html#___sec38" style="font-size: 80%;">The trial solution</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs037.html#___sec39" style="font-size: 80%;">The analytical solution</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs038.html#___sec40" style="font-size: 80%;">Solving the wave equation - the full program using Autograd</a></li>
     <!-- navigation toc: --> <li><a href="#___sec41" style="font-size: 80%;">Solving the wave equation - the full program using TensorFlow</a></li>
     <!-- navigation toc: --> <li><a href="._odenn-bs040.html#___sec42" style="font-size: 80%;">Resources</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0039"></a>
<!-- !split -->

<h2 id="___sec41" class="anchor">Solving the wave equation - the full program using TensorFlow </h2>
As the program using Autograd is fairly slow, one could hope that using TensorFlow
could make a naive implementation faster, and more numerically robust.

<p>
In addition, having TensorFlow at hand, it could be easier to experiment with different
optimization algorithms, and other constructions of the network.

<p>
The following program solves the given wave equation much faster,

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">tensorflow</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">tf</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib</span> <span style="color: #008000; font-weight: bold">import</span> cm
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">matplotlib</span> <span style="color: #008000; font-weight: bold">import</span> pyplot <span style="color: #008000; font-weight: bold">as</span> plt
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">mpl_toolkits.mplot3d</span> <span style="color: #008000; font-weight: bold">import</span> axes3d

Nx <span style="color: #666666">=</span> <span style="color: #666666">10</span>
x_np <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>,<span style="color: #666666">1</span>,Nx)

Nt <span style="color: #666666">=</span> <span style="color: #666666">10</span>
t_np <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>,<span style="color: #666666">1</span>,Nt)

X,T <span style="color: #666666">=</span> np<span style="color: #666666">.</span>meshgrid(x_np, t_np)

x <span style="color: #666666">=</span> X<span style="color: #666666">.</span>ravel()
t <span style="color: #666666">=</span> T<span style="color: #666666">.</span>ravel()

<span style="color: #408080; font-style: italic">## The construction phase</span>

zeros <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>reshape(tf<span style="color: #666666">.</span>convert_to_tensor(np<span style="color: #666666">.</span>zeros(x<span style="color: #666666">.</span>shape)),shape<span style="color: #666666">=</span>(<span style="color: #666666">-1</span>,<span style="color: #666666">1</span>))
x <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>reshape(tf<span style="color: #666666">.</span>convert_to_tensor(x),shape<span style="color: #666666">=</span>(<span style="color: #666666">-1</span>,<span style="color: #666666">1</span>))
t <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>reshape(tf<span style="color: #666666">.</span>convert_to_tensor(t),shape<span style="color: #666666">=</span>(<span style="color: #666666">-1</span>,<span style="color: #666666">1</span>))

points <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>concat([x,t],<span style="color: #666666">1</span>)

num_iter <span style="color: #666666">=</span> <span style="color: #666666">100000</span>
num_hidden_neurons <span style="color: #666666">=</span> [<span style="color: #666666">90</span>]

X <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>convert_to_tensor(X)
T <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>convert_to_tensor(T)


<span style="color: #008000; font-weight: bold">with</span> tf<span style="color: #666666">.</span>variable_scope(<span style="color: #BA2121">&#39;dnn&#39;</span>):
    num_hidden_layers <span style="color: #666666">=</span> np<span style="color: #666666">.</span>size(num_hidden_neurons)

    previous_layer <span style="color: #666666">=</span> points

    <span style="color: #008000; font-weight: bold">for</span> l <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(num_hidden_layers):
        current_layer <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>layers<span style="color: #666666">.</span>dense(previous_layer, num_hidden_neurons[l],activation<span style="color: #666666">=</span>tf<span style="color: #666666">.</span>nn<span style="color: #666666">.</span>sigmoid)
        previous_layer <span style="color: #666666">=</span> current_layer

    dnn_output <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>layers<span style="color: #666666">.</span>dense(previous_layer, <span style="color: #666666">1</span>)


<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">u</span>(x):
    <span style="color: #008000; font-weight: bold">return</span> tf<span style="color: #666666">.</span>sin(np<span style="color: #666666">.</span>pi<span style="color: #666666">*</span>x)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">v</span>(x):
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">-</span>np<span style="color: #666666">.</span>pi<span style="color: #666666">*</span>tf<span style="color: #666666">.</span>sin(np<span style="color: #666666">.</span>pi<span style="color: #666666">*</span>x)

<span style="color: #008000; font-weight: bold">with</span> tf<span style="color: #666666">.</span>name_scope(<span style="color: #BA2121">&#39;loss&#39;</span>):
    g_trial <span style="color: #666666">=</span> (<span style="color: #666666">1</span> <span style="color: #666666">-</span> t<span style="color: #666666">**2</span>)<span style="color: #666666">*</span>u(x) <span style="color: #666666">+</span> t<span style="color: #666666">*</span>v(x) <span style="color: #666666">+</span> x<span style="color: #666666">*</span>(<span style="color: #666666">1-</span>x)<span style="color: #666666">*</span>t<span style="color: #666666">**2*</span>dnn_output

    g_trial_d2t <span style="color: #666666">=</span>  tf<span style="color: #666666">.</span>gradients(tf<span style="color: #666666">.</span>gradients(g_trial,t),t)
    g_trial_d2x <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>gradients(tf<span style="color: #666666">.</span>gradients(g_trial,x),x)

    loss <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>losses<span style="color: #666666">.</span>mean_squared_error(zeros, g_trial_d2t[<span style="color: #666666">0</span>] <span style="color: #666666">-</span> g_trial_d2x[<span style="color: #666666">0</span>])

learning_rate <span style="color: #666666">=</span> <span style="color: #666666">0.01</span>
<span style="color: #008000; font-weight: bold">with</span> tf<span style="color: #666666">.</span>name_scope(<span style="color: #BA2121">&#39;train&#39;</span>):
    optimizer <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>train<span style="color: #666666">.</span>GradientDescentOptimizer(learning_rate)
    traning_op <span style="color: #666666">=</span> optimizer<span style="color: #666666">.</span>minimize(loss)

init <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>global_variables_initializer()

g_analytic <span style="color: #666666">=</span> tf<span style="color: #666666">.</span>sin(np<span style="color: #666666">.</span>pi<span style="color: #666666">*</span>x)<span style="color: #666666">*</span>tf<span style="color: #666666">.</span>cos(np<span style="color: #666666">.</span>pi<span style="color: #666666">*</span>t) <span style="color: #666666">-</span> tf<span style="color: #666666">.</span>sin(np<span style="color: #666666">.</span>pi<span style="color: #666666">*</span>x)<span style="color: #666666">*</span>tf<span style="color: #666666">.</span>sin(np<span style="color: #666666">.</span>pi<span style="color: #666666">*</span>t)
g_dnn <span style="color: #666666">=</span> <span style="color: #008000">None</span>

<span style="color: #408080; font-style: italic">## The execution phase</span>
<span style="color: #008000; font-weight: bold">with</span> tf<span style="color: #666666">.</span>Session() <span style="color: #008000; font-weight: bold">as</span> sess:
    init<span style="color: #666666">.</span>run()
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(num_iter):
        sess<span style="color: #666666">.</span>run(traning_op)

        <span style="color: #408080; font-style: italic"># If one desires to see how the cost function behaves during training</span>
        <span style="color: #408080; font-style: italic">#if i % 100 == 0:</span>
        <span style="color: #408080; font-style: italic">#    print(loss.eval())</span>

    g_analytic <span style="color: #666666">=</span> g_analytic<span style="color: #666666">.</span>eval()
    g_dnn <span style="color: #666666">=</span> g_trial<span style="color: #666666">.</span>eval()


<span style="color: #408080; font-style: italic">## Compare with the analutical solution</span>
diff <span style="color: #666666">=</span> np<span style="color: #666666">.</span>abs(g_analytic <span style="color: #666666">-</span> g_dnn)
<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&#39;Max absolute difference between analytical solution and TensorFlow DNN = &#39;</span>,np<span style="color: #666666">.</span>max(diff))

G_analytic <span style="color: #666666">=</span> g_analytic<span style="color: #666666">.</span>reshape((Nt,Nx))
G_dnn <span style="color: #666666">=</span> g_dnn<span style="color: #666666">.</span>reshape((Nt,Nx))

diff <span style="color: #666666">=</span> np<span style="color: #666666">.</span>abs(G_analytic <span style="color: #666666">-</span> G_dnn)

<span style="color: #408080; font-style: italic"># Plot the results</span>

X,T <span style="color: #666666">=</span> np<span style="color: #666666">.</span>meshgrid(x_np, t_np)

fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>gca(projection<span style="color: #666666">=</span><span style="color: #BA2121">&#39;3d&#39;</span>)
ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&#39;Solution from the deep neural network w/ </span><span style="color: #BB6688; font-weight: bold">%d</span><span style="color: #BA2121"> layer&#39;</span><span style="color: #666666">%</span><span style="color: #008000">len</span>(num_hidden_neurons))
s <span style="color: #666666">=</span> ax<span style="color: #666666">.</span>plot_surface(X,T,G_dnn,linewidth<span style="color: #666666">=0</span>,antialiased<span style="color: #666666">=</span><span style="color: #008000">False</span>,cmap<span style="color: #666666">=</span>cm<span style="color: #666666">.</span>viridis)
ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&#39;Time $t$&#39;</span>)
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&#39;Position $x$&#39;</span>);

fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>gca(projection<span style="color: #666666">=</span><span style="color: #BA2121">&#39;3d&#39;</span>)
ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&#39;Analytical solution&#39;</span>)
s <span style="color: #666666">=</span> ax<span style="color: #666666">.</span>plot_surface(X,T,G_analytic,linewidth<span style="color: #666666">=0</span>,antialiased<span style="color: #666666">=</span><span style="color: #008000">False</span>,cmap<span style="color: #666666">=</span>cm<span style="color: #666666">.</span>viridis)
ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&#39;Time $t$&#39;</span>)
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&#39;Position $x$&#39;</span>);

fig <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
ax <span style="color: #666666">=</span> fig<span style="color: #666666">.</span>gca(projection<span style="color: #666666">=</span><span style="color: #BA2121">&#39;3d&#39;</span>)
ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&#39;Difference&#39;</span>)
s <span style="color: #666666">=</span> ax<span style="color: #666666">.</span>plot_surface(X,T,diff,linewidth<span style="color: #666666">=0</span>,antialiased<span style="color: #666666">=</span><span style="color: #008000">False</span>,cmap<span style="color: #666666">=</span>cm<span style="color: #666666">.</span>viridis)
ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">&#39;Time $t$&#39;</span>)
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">&#39;Position $x$&#39;</span>);

<span style="color: #408080; font-style: italic">## Take some 3D slices</span>

indx1 <span style="color: #666666">=</span> <span style="color: #666666">0</span>
indx2 <span style="color: #666666">=</span> <span style="color: #008000">int</span>(Nt<span style="color: #666666">/2</span>)
indx3 <span style="color: #666666">=</span> Nt<span style="color: #666666">-1</span>

t1 <span style="color: #666666">=</span> t_np[indx1]
t2 <span style="color: #666666">=</span> t_np[indx2]
t3 <span style="color: #666666">=</span> t_np[indx3]

<span style="color: #408080; font-style: italic"># Slice the results from the DNN</span>
res1 <span style="color: #666666">=</span> G_dnn[indx1,:]
res2 <span style="color: #666666">=</span> G_dnn[indx2,:]
res3 <span style="color: #666666">=</span> G_dnn[indx3,:]

<span style="color: #408080; font-style: italic"># Slice the analytical results</span>
res_analytical1 <span style="color: #666666">=</span> G_analytic[indx1,:]
res_analytical2 <span style="color: #666666">=</span> G_analytic[indx2,:]
res_analytical3 <span style="color: #666666">=</span> G_analytic[indx3,:]

<span style="color: #408080; font-style: italic"># Plot the slices</span>
plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Computed solutions at time = </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">%</span>t1)
plt<span style="color: #666666">.</span>plot(x_np, res1)
plt<span style="color: #666666">.</span>plot(x_np,res_analytical1)
plt<span style="color: #666666">.</span>legend([<span style="color: #BA2121">&#39;dnn&#39;</span>,<span style="color: #BA2121">&#39;analytical&#39;</span>])

plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Computed solutions at time = </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">%</span>t2)
plt<span style="color: #666666">.</span>plot(x_np, res2)
plt<span style="color: #666666">.</span>plot(x_np,res_analytical2)
plt<span style="color: #666666">.</span>legend([<span style="color: #BA2121">&#39;dnn&#39;</span>,<span style="color: #BA2121">&#39;analytical&#39;</span>])

plt<span style="color: #666666">.</span>figure(figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">10</span>))
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Computed solutions at time = </span><span style="color: #BB6688; font-weight: bold">%g</span><span style="color: #BA2121">&quot;</span><span style="color: #666666">%</span>t3)
plt<span style="color: #666666">.</span>plot(x_np, res3)
plt<span style="color: #666666">.</span>plot(x_np,res_analytical3)
plt<span style="color: #666666">.</span>legend([<span style="color: #BA2121">&#39;dnn&#39;</span>,<span style="color: #BA2121">&#39;analytical&#39;</span>])

plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
The program manages to find a solution having max absolute difference to the analytical
at approximately 0.0059, by just using some minutes!
It was found, by some testing, that one hidden layer with 90 neurons actually performed well.

<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._odenn-bs038.html">&laquo;</a></li>
  <li><a href="._odenn-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._odenn-bs031.html">32</a></li>
  <li><a href="._odenn-bs032.html">33</a></li>
  <li><a href="._odenn-bs033.html">34</a></li>
  <li><a href="._odenn-bs034.html">35</a></li>
  <li><a href="._odenn-bs035.html">36</a></li>
  <li><a href="._odenn-bs036.html">37</a></li>
  <li><a href="._odenn-bs037.html">38</a></li>
  <li><a href="._odenn-bs038.html">39</a></li>
  <li class="active"><a href="._odenn-bs039.html">40</a></li>
  <li><a href="._odenn-bs040.html">41</a></li>
  <li><a href="._odenn-bs040.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


</body>
</html>
    

