<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods">

<title>Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Optimization, the central part of any Machine Learning '
               'algortithm',
               2,
               None,
               '___sec0'),
              ('Revisiting our Logistic Regression case', 2, None, '___sec1'),
              ('The equations to solve', 2, None, '___sec2'),
              ("Solving using Newton-Raphson's method", 2, None, '___sec3'),
              ("Brief reminder on Newton-Raphson's method", 2, None, '___sec4'),
              ('The equations', 2, None, '___sec5'),
              ('Simple geometric interpretation', 2, None, '___sec6'),
              ('Extending to more than one variable', 2, None, '___sec7'),
              ('Steepest descent', 2, None, '___sec8'),
              ('More on Steepest descent', 2, None, '___sec9'),
              ('The ideal', 2, None, '___sec10'),
              ('The sensitiveness of the gradient descent',
               2,
               None,
               '___sec11'),
              ('Convex functions', 2, None, '___sec12'),
              ('Convex function', 2, None, '___sec13'),
              ('Conditions on convex functions', 2, None, '___sec14'),
              ('More on convex functions', 2, None, '___sec15'),
              ('Some simple problems', 2, None, '___sec16'),
              ('Standard steepest descent', 2, None, '___sec17'),
              ('Gradient method', 2, None, '___sec18'),
              ('Steepest descent  method', 2, None, '___sec19'),
              ('Steepest descent  method', 2, None, '___sec20'),
              ('Final expressions', 2, None, '___sec21'),
              ('Simple codes for  steepest descent and conjugate gradient '
               'using a $2\\times 2$ matrix, in c++, Python code to come',
               2,
               None,
               '___sec22'),
              ('The routine for the steepest descent method',
               2,
               None,
               '___sec23'),
              ('Steepest descent example', 2, None, '___sec24'),
              ('Revisiting our first homework', 2, None, '___sec25'),
              ('Gradient descent example', 2, None, '___sec26'),
              ('The derivative of the cost/loss function', 2, None, '___sec27'),
              ('The Hessian matrix', 2, None, '___sec28'),
              ('Simple program', 2, None, '___sec29'),
              ('Gradient Descent Example', 2, None, '___sec30'),
              ('And a corresponding example using _scikit-learn_',
               2,
               None,
               '___sec31'),
              ('Gradient descent and Ridge', 2, None, '___sec32'),
              ('Automatic differentiation', 2, None, '___sec33'),
              ('Using autograd', 2, None, '___sec34'),
              ('Autograd with more complicated functions', 2, None, '___sec35'),
              ('More complicated functions using the elements of their '
               'arguments directly',
               2,
               None,
               '___sec36'),
              ('Functions using mathematical functions from Numpy',
               2,
               None,
               '___sec37'),
              ('More autograd', 2, None, '___sec38'),
              ('And  with loops', 2, None, '___sec39'),
              ('Using recursion', 2, None, '___sec40'),
              ('Unsupported functions', 2, None, '___sec41'),
              ('The syntax a.dot(b) when finding the dot product',
               2,
               None,
               '___sec42'),
              ('Recommended to avoid', 2, None, '___sec43'),
              ('Stochastic Gradient Descent', 2, None, '___sec44'),
              ('Computation of gradients', 2, None, '___sec45'),
              ('SGD example', 2, None, '___sec46'),
              ('The gradient step', 2, None, '___sec47'),
              ('Simple example code', 2, None, '___sec48'),
              ('When do we stop?', 2, None, '___sec49'),
              ('Slightly different approach', 2, None, '___sec50')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="Splines-bs.html">Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._Splines-bs001.html#___sec0" style="font-size: 80%;">Optimization, the central part of any Machine Learning algortithm</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs002.html#___sec1" style="font-size: 80%;">Revisiting our Logistic Regression case</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs003.html#___sec2" style="font-size: 80%;">The equations to solve</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs004.html#___sec3" style="font-size: 80%;">Solving using Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs005.html#___sec4" style="font-size: 80%;">Brief reminder on Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs006.html#___sec5" style="font-size: 80%;">The equations</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs007.html#___sec6" style="font-size: 80%;">Simple geometric interpretation</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs008.html#___sec7" style="font-size: 80%;">Extending to more than one variable</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs009.html#___sec8" style="font-size: 80%;">Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs010.html#___sec9" style="font-size: 80%;">More on Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs011.html#___sec10" style="font-size: 80%;">The ideal</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs012.html#___sec11" style="font-size: 80%;">The sensitiveness of the gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs013.html#___sec12" style="font-size: 80%;">Convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs014.html#___sec13" style="font-size: 80%;">Convex function</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs015.html#___sec14" style="font-size: 80%;">Conditions on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs016.html#___sec15" style="font-size: 80%;">More on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs017.html#___sec16" style="font-size: 80%;">Some simple problems</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs018.html#___sec17" style="font-size: 80%;">Standard steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs019.html#___sec18" style="font-size: 80%;">Gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs020.html#___sec19" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs021.html#___sec20" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs022.html#___sec21" style="font-size: 80%;">Final expressions</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs023.html#___sec22" style="font-size: 80%;">Simple codes for  steepest descent and conjugate gradient using a \( 2\times 2 \) matrix, in c++, Python code to come</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs024.html#___sec23" style="font-size: 80%;">The routine for the steepest descent method</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs025.html#___sec24" style="font-size: 80%;">Steepest descent example</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs026.html#___sec25" style="font-size: 80%;">Revisiting our first homework</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs027.html#___sec26" style="font-size: 80%;">Gradient descent example</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs028.html#___sec27" style="font-size: 80%;">The derivative of the cost/loss function</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs029.html#___sec28" style="font-size: 80%;">The Hessian matrix</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs030.html#___sec29" style="font-size: 80%;">Simple program</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs031.html#___sec30" style="font-size: 80%;">Gradient Descent Example</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs032.html#___sec31" style="font-size: 80%;">And a corresponding example using <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs033.html#___sec32" style="font-size: 80%;">Gradient descent and Ridge</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs034.html#___sec33" style="font-size: 80%;">Automatic differentiation</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs035.html#___sec34" style="font-size: 80%;">Using autograd</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs036.html#___sec35" style="font-size: 80%;">Autograd with more complicated functions</a></li>
     <!-- navigation toc: --> <li><a href="#___sec36" style="font-size: 80%;">More complicated functions using the elements of their arguments directly</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs038.html#___sec37" style="font-size: 80%;">Functions using mathematical functions from Numpy</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs039.html#___sec38" style="font-size: 80%;">More autograd</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs040.html#___sec39" style="font-size: 80%;">And  with loops</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs041.html#___sec40" style="font-size: 80%;">Using recursion</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs042.html#___sec41" style="font-size: 80%;">Unsupported functions</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs043.html#___sec42" style="font-size: 80%;">The syntax a.dot(b) when finding the dot product</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs044.html#___sec43" style="font-size: 80%;">Recommended to avoid</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs045.html#___sec44" style="font-size: 80%;">Stochastic Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs046.html#___sec45" style="font-size: 80%;">Computation of gradients</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs047.html#___sec46" style="font-size: 80%;">SGD example</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs048.html#___sec47" style="font-size: 80%;">The gradient step</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs049.html#___sec48" style="font-size: 80%;">Simple example code</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs050.html#___sec49" style="font-size: 80%;">When do we stop?</a></li>
     <!-- navigation toc: --> <li><a href="._Splines-bs051.html#___sec50" style="font-size: 80%;">Slightly different approach</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0037"></a>
<!-- !split -->

<h2 id="___sec36" class="anchor">More complicated functions using the elements of their arguments directly </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">autograd.numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">autograd</span> <span style="color: #008000; font-weight: bold">import</span> grad
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">f3</span>(x): <span style="color: #408080; font-style: italic"># Assumes x is an array of length 5 or higher</span>
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #666666">2*</span>x[<span style="color: #666666">0</span>] <span style="color: #666666">+</span> <span style="color: #666666">3*</span>x[<span style="color: #666666">1</span>] <span style="color: #666666">+</span> <span style="color: #666666">5*</span>x[<span style="color: #666666">2</span>] <span style="color: #666666">+</span> <span style="color: #666666">7*</span>x[<span style="color: #666666">3</span>] <span style="color: #666666">+</span> <span style="color: #666666">11*</span>x[<span style="color: #666666">4</span>]<span style="color: #666666">**2</span>

f3_grad <span style="color: #666666">=</span> grad(f3)

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0</span>,<span style="color: #666666">4</span>,<span style="color: #666666">5</span>)

<span style="color: #408080; font-style: italic"># Print the computed gradient:</span>
<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&quot;The computed gradient of f3 is: &quot;</span>, f3_grad(x))

<span style="color: #408080; font-style: italic"># The analytical gradient is: (2, 3, 5, 7, 22*x[4])</span>
f3_grad_analytical <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([<span style="color: #666666">2</span>, <span style="color: #666666">3</span>, <span style="color: #666666">5</span>, <span style="color: #666666">7</span>, <span style="color: #666666">22*</span>x[<span style="color: #666666">4</span>]])

<span style="color: #408080; font-style: italic"># Print the analytical gradient:</span>
<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&quot;The analytical gradient of f3 is: &quot;</span>, f3_grad_analytical)
</pre></div>
<p>
Note that in this case, when sending an array as input argument, the
output from Autograd is another array. This is the true gradient of
the function, as opposed to the function in the previous example. By
using arrays to represent the variables, the output from Autograd
might be easier to work with, as the output is closer to what one
could expect form a gradient-evaluting function.

<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._Splines-bs036.html">&laquo;</a></li>
  <li><a href="._Splines-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._Splines-bs029.html">30</a></li>
  <li><a href="._Splines-bs030.html">31</a></li>
  <li><a href="._Splines-bs031.html">32</a></li>
  <li><a href="._Splines-bs032.html">33</a></li>
  <li><a href="._Splines-bs033.html">34</a></li>
  <li><a href="._Splines-bs034.html">35</a></li>
  <li><a href="._Splines-bs035.html">36</a></li>
  <li><a href="._Splines-bs036.html">37</a></li>
  <li class="active"><a href="._Splines-bs037.html">38</a></li>
  <li><a href="._Splines-bs038.html">39</a></li>
  <li><a href="._Splines-bs039.html">40</a></li>
  <li><a href="._Splines-bs040.html">41</a></li>
  <li><a href="._Splines-bs041.html">42</a></li>
  <li><a href="._Splines-bs042.html">43</a></li>
  <li><a href="._Splines-bs043.html">44</a></li>
  <li><a href="._Splines-bs044.html">45</a></li>
  <li><a href="._Splines-bs045.html">46</a></li>
  <li><a href="._Splines-bs046.html">47</a></li>
  <li><a href="">...</a></li>
  <li><a href="._Splines-bs051.html">52</a></li>
  <li><a href="._Splines-bs038.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    

