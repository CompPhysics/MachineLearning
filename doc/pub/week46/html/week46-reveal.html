<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html week46-reveal.html week46-reveal reveal --html_slide_theme=beige
-->
<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Week 46: Decision Trees, Ensemble methods  and Random Forests">
<title>Week 46: Decision Trees, Ensemble methods  and Random Forests</title>

<!-- reveal.js: https://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<style type="text/css">
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.reveal .alert-text-small   { font-size: 80%;  }
.reveal .alert-text-large   { font-size: 130%; }
.reveal .alert-text-normal  { font-size: 90%;  }
.reveal .alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:5px solid #bababa;
  -webkit-border-radius: 14px; -moz-border-radius: 14px;
  border-radius:14px;
  background-position: 10px 10px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 30px; /* 55px; if icon */
}
.reveal .alert-block {padding-top:14px; padding-bottom:14px}
.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
/*.reveal .alert li {margin-top: 1em}*/
.reveal .alert-block p+p {margin-top:5px}
/*.reveal .alert-notice { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
.reveal .alert-summary  { background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
.reveal .alert-warning { background-image: url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
.reveal .alert-question {background-image:url(https://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */
/* Override reveal.js table border */
.reveal table td {
  border: 0;
}

<style type="text/css">
/* Override h1, h2, ... styles */
h1 { font-size: 2.8em; }
h2 { font-size: 1.5em; }
h3 { font-size: 1.4em; }
h4 { font-size: 1.3em; }
h1, h2, h3, h4 { font-weight: bold; line-height: 1.2; }
body { overflow: auto; } /* vertical scrolling */
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.slide .alert-text-small   { font-size: 80%;  }
.slide .alert-text-large   { font-size: 130%; }
.slide .alert-text-normal  { font-size: 90%;  }
.slide .alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:5px solid #bababa;
    -webkit-border-radius:14px; -moz-border-radius:14px;
  border-radius:14px
  background-position: 10px 10px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 30px; /* 55px; if icon */
}
.slide .alert-block {padding-top:14px; padding-bottom:14px}
.slide .alert-block > p, .alert-block > ul {margin-bottom:0}
/*.slide .alert li {margin-top: 1em}*/
.deck .alert-block p+p {margin-top:5px}
/*.slide .alert-notice { background-image: url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_notice.png); }
.slide .alert-summary  { background-image:url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_summary.png); }
.slide .alert-warning { background-image: url(https://hplgit.github.io/doconce/
bundled/html_images//small_gray_warning.png); }
.slide .alert-question {background-image:url(https://hplgit.github.io/doconce/
bundled/html_images/small_gray_question.png); } */
.dotable table, .dotable th, .dotable tr, .dotable tr td {
  border: 2px solid black;
  border-collapse: collapse;
  padding: 2px;
}
</style>


<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>


<body>
<div class="reveal">
<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




<section>
<!-- ------------------- main content ---------------------- -->
<center>
<h1 style="text-align: center;">Week 46: Decision Trees, Ensemble methods  and Random Forests</h1>
</center>  <!-- document title -->

<!-- author(s): Morten Hjorth-Jensen -->
<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>
<!-- institution(s) -->
<center>
[1] <b>Department of Physics, University of Oslo</b>
</center>
<center>
[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b>
</center>
<br>
<center>
<h4>Week 46, November 10-14</h4>
</center> <!-- date -->
<br>


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2025, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>
</section>

<section>
<h2 id="plan-for-week-46">Plan for week 46 </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Material for the lecture on Monday November 10, 2025</b>
<p>
<ol>
<p><li> Intro to and mathematics of  Recurrent Neural Networks (RNNs)</li>
<p><li> Lecture notes at <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week46/ipynb/week46.ipynb" target="_blank"><tt>https://github.com/CompPhysics/MachineLearning/blob/master/doc/pub/week46/ipynb/week46.ipynb</tt></a>
<!-- o Video of lecture at <a href="https://youtu.be/TfRKUfdJwB4" target="_blank"><tt>https://youtu.be/TfRKUfdJwB4</tt></a> -->
<!-- o Whiteboard notes at <a href="https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2024/NotesNovember11.pdf" target="_blank"><tt>https://github.com/CompPhysics/MachineLearning/blob/master/doc/HandWrittenNotes/2024/NotesNovember11.pdf</tt></a> --></li>
</ol>
</div>


<div class="alert alert-block alert-block alert-text-normal">
<b>Lab sessions on Tuesday and Wednesday</b>
<p>
<ol>
<p><li> Work on and discussions of project 3, deadline December 15 at midnight. Project 3 can be found at <a href="https://github.com/CompPhysics/MachineLearning/tree/master/doc/Projects/2025/Project3" target="_blank"><tt>https://github.com/CompPhysics/MachineLearning/tree/master/doc/Projects/2025/Project3</tt></a>. <b>Note that possible minor revisions may be done before the lab sessions start on November 11</b>.</li>
</ol>
</div>
</section>

<section>
<h2 id="reading-recommendations">Reading recommendations </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<ol>
<p><li> For RNNs, see Goodfellow et al chapter 10, see <a href="https://www.deeplearningbook.org/contents/rnn.html" target="_blank"><tt>https://www.deeplearningbook.org/contents/rnn.html</tt></a>.</li>
<p><li> Reading suggestions for implementation of RNNs in PyTorch: see Rashcka et al.'s chapter 15 and GitHub site at <a href="https://github.com/rasbt/machine-learning-book/tree/main/ch15" target="_blank"><tt>https://github.com/rasbt/machine-learning-book/tree/main/ch15</tt></a>.</li>
</ol>
</section>

<section>
<h2 id="tensorflow-examples">TensorFlow examples </h2>
<p>For TensorFlow (using Keras) implementations, we recommend</p>
<ol>
<p><li> David Foster, Generative Deep Learning with TensorFlow, see chapter 5 at <a href="https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html" target="_blank"><tt>https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch05.html</tt></a></li>
<p><li> Joseph Babcock and Raghav Bali Generative AI with Python and their GitHub link, chapters 2 and  3 at <a href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2" target="_blank"><tt>https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2</tt></a></li>  
</ol>
</div>
</section>

<section>
<h2 id="from-nns-and-cnns-to-recurrent-neural-networks-rnns">From NNs and CNNs to recurrent neural networks (RNNs) </h2>

<p>There are limitation of NNs, one of which being that FFNNs are not
designed to handle sequential data (data for which the order matters)
effectively because they lack the capabilities of storing information
about previous inputs; each input is being treated indepen-
dently. This is a limitation when dealing with sequential data where
past information can be vital to correctly process current and future
inputs. 
</p>
</section>

<section>
<h2 id="what-is-a-recurrent-nn">What is a recurrent NN? </h2>

<p>A recurrent neural network (RNN), as opposed to a regular fully
connected neural network (FCNN) or just neural network (NN), has
layers that are connected to themselves.
</p>

<p>In an FCNN there are no connections between nodes in a single
layer. For instance, \( (h_1^1 \) is not connected to \( (h_2^1 \). In
addition, the input and output are always of a fixed length.
</p>

<p>In an RNN, however, this is no longer the case. Nodes in the hidden
layers are connected to themselves.
</p>
</section>

<section>
<h2 id="why-rnns">Why RNNs? </h2>

<p>Recurrent neural networks work very well when working with
sequential data, that is data where the order matters. In a regular
fully connected network, the order of input doesn't really matter.
</p>

<p>Another property of  RNNs is that they can handle variable input
and output. Consider again the simplified breast cancer dataset. If you
have trained a regular FCNN on the dataset with the two features, it
makes no sense to suddenly add a third feature. The network would not
know what to do with it, and would reject such inputs with three
features (or any other number of features that isn't two, for that
matter).
</p>
</section>

<section>
<h2 id="feedback-connections">Feedback connections </h2>

<p>In contrast to NNs, recurrent networks introduce feedback
connections, meaning the information is allowed to be carried to
subsequent nodes across different time steps. These cyclic or feedback
connections have the objective of providing the network with some kind
of memory, making RNNs particularly suited for time- series data,
natural language processing, speech recognition, and several other
problems for which the order of the data is crucial.  The RNN
architectures vary greatly in how they manage information flow and
memory in the network.
</p>
</section>

<section>
<h2 id="vanishing-gradients">Vanishing gradients </h2>

<p>Different architectures often aim at improving
some sub-optimal characteristics of the network. The simplest form of
recurrent network, commonly called simple or vanilla RNN, for example,
is known to suffer from the problem of vanishing gradients. This
problem arises due to the nature of backpropagation in time. Gradients
of the cost/loss function may get exponentially small (or large) if
there are many layers in the network, which is the case of RNN when
the sequence gets long.
</p>
</section>

<section>
<h2 id="recurrent-neural-networks-rnns-overarching-view">Recurrent neural networks (RNNs): Overarching view </h2>

<p>Till now our focus has been, including convolutional neural networks
as well, on feedforward neural networks. The output or the activations
flow only in one direction, from the input layer to the output layer.
</p>

<p>A recurrent neural network (RNN) looks very much like a feedforward
neural network, except that it also has connections pointing
backward. 
</p>

<p>RNNs are used to analyze time series data such as stock prices, and
tell you when to buy or sell. In autonomous driving systems, they can
anticipate car trajectories and help avoid accidents. More generally,
they can work on sequences of arbitrary lengths, rather than on
fixed-sized inputs like all the nets we have discussed so far. For
example, they can take sentences, documents, or audio samples as
input, making them extremely useful for natural language processing
systems such as automatic translation and speech-to-text.
</p>
</section>

<section>
<h2 id="sequential-data-only">Sequential data only? </h2>

<p>An important issue is that in many deep learning methods we assume
that the input and output data can be treated as independent and
identically distributed, normally abbreviated to <b>iid</b>.
This means that the data we use can be seen as mutually independent.
</p>

<p>This is however not the case for most data sets used in RNNs since we
are dealing with sequences of data with strong inter-dependencies.
This applies in particular to time series, which are sequential by
contruction.
</p>
</section>

<section>
<h2 id="differential-equations">Differential equations </h2>

<p>As an example, the solutions of ordinary differential equations can be
represented as a time series, similarly, how stock prices evolve as
function of time is another example of a typical time series, or voice
records and many other examples.
</p>

<p>Not all sequential data may however have a time stamp, texts being a
typical example thereof, or DNA sequences.
</p>

<p>The main focus here is on data that can be structured either as time
series or as ordered series of data.  We will not focus on for example
natural language processing or similar data sets.
</p>
</section>

<section>
<h2 id="a-simple-regression-example-using-tensorflow-with-keras">A simple regression example using TensorFlow with Keras </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #228B22"># Start importing packages</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">tensorflow</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">tf</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">tensorflow.keras</span> <span style="color: #8B008B; font-weight: bold">import</span> datasets, layers, models
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">tensorflow.keras.layers</span> <span style="color: #8B008B; font-weight: bold">import</span> Input
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">tensorflow.keras.models</span> <span style="color: #8B008B; font-weight: bold">import</span> Model, Sequential 
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">tensorflow.keras.layers</span> <span style="color: #8B008B; font-weight: bold">import</span> Dense, SimpleRNN, LSTM, GRU
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">tensorflow.keras</span> <span style="color: #8B008B; font-weight: bold">import</span> optimizers     
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">tensorflow.keras</span> <span style="color: #8B008B; font-weight: bold">import</span> regularizers           
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">tensorflow.keras.utils</span> <span style="color: #8B008B; font-weight: bold">import</span> to_categorical 

<span style="color: #228B22"># convert into dataset matrix</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">convertToMatrix</span>(data, step):
 X, Y =[], []
 <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #658b00">len</span>(data)-step):
  d=i+step  
  X.append(data[i:d,])
  Y.append(data[d,])
 <span style="color: #8B008B; font-weight: bold">return</span> np.array(X), np.array(Y)

step = <span style="color: #B452CD">4</span>
N = <span style="color: #B452CD">1000</span>    
Tp = <span style="color: #B452CD">800</span>    

t=np.arange(<span style="color: #B452CD">0</span>,N)
x=np.sin(<span style="color: #B452CD">0.02</span>*t)+<span style="color: #B452CD">2</span>*np.random.rand(N)
df = pd.DataFrame(x)
df.head()
<span style="color: #228B22"># Setting up training data</span>
values=df.values
train,test = values[<span style="color: #B452CD">0</span>:Tp,:], values[Tp:N,:]
<span style="color: #228B22"># add step elements into train and test</span>
test = np.append(test,np.repeat(test[-<span style="color: #B452CD">1</span>,],step))
train = np.append(train,np.repeat(train[-<span style="color: #B452CD">1</span>,],step))
trainX,trainY =convertToMatrix(train,step)
testX,testY =convertToMatrix(test,step)
trainX = np.reshape(trainX, (trainX.shape[<span style="color: #B452CD">0</span>], <span style="color: #B452CD">1</span>, trainX.shape[<span style="color: #B452CD">1</span>]))
testX = np.reshape(testX, (testX.shape[<span style="color: #B452CD">0</span>], <span style="color: #B452CD">1</span>, testX.shape[<span style="color: #B452CD">1</span>]))
<span style="color: #228B22"># Defining the model with a simple RNN</span>
model = Sequential()
model.add(SimpleRNN(units=<span style="color: #B452CD">32</span>, input_shape=(<span style="color: #B452CD">1</span>,step), activation=<span style="color: #CD5555">&quot;relu&quot;</span>))
model.add(Dense(<span style="color: #B452CD">8</span>, activation=<span style="color: #CD5555">&quot;relu&quot;</span>)) 
model.add(Dense(<span style="color: #B452CD">1</span>))
model.compile(loss=<span style="color: #CD5555">&#39;mean_squared_error&#39;</span>, optimizer=<span style="color: #CD5555">&#39;rmsprop&#39;</span>)
model.summary()
<span style="color: #228B22"># Training</span>
model.fit(trainX,trainY, epochs=<span style="color: #B452CD">100</span>, batch_size=<span style="color: #B452CD">16</span>, verbose=<span style="color: #B452CD">2</span>)
trainPredict = model.predict(trainX)
testPredict= model.predict(testX)
predicted=np.concatenate((trainPredict,testPredict),axis=<span style="color: #B452CD">0</span>)
trainScore = model.evaluate(trainX, trainY, verbose=<span style="color: #B452CD">0</span>)
<span style="color: #658b00">print</span>(trainScore)
plt.plot(df)
plt.plot(predicted)
plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="corresponding-example-using-pytorch">Corresponding example using PyTorch </h2>

<p>The structure of the code here is as follows</p>
<ol>
<p><li> Generate a sine function  and splits it into training and validation sets</li>
<p><li> Create a custom data set for sequence generation</li>
<p><li> Define an RNN model with one RNN layer and a final plain linear layer</li>
<p><li> Train the model using the mean-squared error as cost function and the Adam optimizer</li>
<p><li> Generate predictions using recursive forecasting</li>
<p><li> Plot the results and training/validation loss curves</li>
</ol>
<p>
<p>The model takes sequences of 20 previous values to predict the next
value of the sine function. The recursive prediction uses the model's own
predictions to generate future values, showing how well it maintains
the sine wave pattern over time.
</p>

<p>The final plots show the the predicted values vs. the actual sine wave for the validation period
and the training and validation cost function curves to monitor for overfitting.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch.nn</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">nn</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>

<span style="color: #228B22"># Generate synthetic sine wave data</span>
t = torch.linspace(<span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>*np.pi, <span style="color: #B452CD">1000</span>)
data = torch.sin(t)

<span style="color: #228B22"># Split data into training and validation</span>
train_data = data[:<span style="color: #B452CD">800</span>]
val_data = data[<span style="color: #B452CD">800</span>:]

<span style="color: #228B22"># Hyperparameters</span>
seq_len = <span style="color: #B452CD">20</span>
batch_size = <span style="color: #B452CD">32</span>
hidden_size = <span style="color: #B452CD">64</span>
num_epochs = <span style="color: #B452CD">100</span>
learning_rate = <span style="color: #B452CD">0.001</span>

<span style="color: #228B22"># Create dataset and dataloaders</span>
<span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">SineDataset</span>(torch.utils.data.Dataset):
   <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__init__</span>(<span style="color: #658b00">self</span>, data, seq_len):
       <span style="color: #658b00">self</span>.data = data
       <span style="color: #658b00">self</span>.seq_len = seq_len

   <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__len__</span>(<span style="color: #658b00">self</span>):
       <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">len</span>(<span style="color: #658b00">self</span>.data) - <span style="color: #658b00">self</span>.seq_len

   <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__getitem__</span>(<span style="color: #658b00">self</span>, idx):
       x = <span style="color: #658b00">self</span>.data[idx:idx+<span style="color: #658b00">self</span>.seq_len]
       y = <span style="color: #658b00">self</span>.data[idx+<span style="color: #658b00">self</span>.seq_len]
       <span style="color: #8B008B; font-weight: bold">return</span> x.unsqueeze(-<span style="color: #B452CD">1</span>), y  <span style="color: #228B22"># Add feature dimension</span>

train_dataset = SineDataset(train_data, seq_len)
val_dataset = SineDataset(val_data, seq_len)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span style="color: #8B008B; font-weight: bold">True</span>)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=<span style="color: #8B008B; font-weight: bold">False</span>)

<span style="color: #228B22"># Define RNN model</span>
<span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">RNNModel</span>(nn.Module):
   <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__init__</span>(<span style="color: #658b00">self</span>, input_size, hidden_size, output_size):
       <span style="color: #658b00">super</span>(RNNModel, <span style="color: #658b00">self</span>).<span style="color: #008b45">__init__</span>()
       <span style="color: #658b00">self</span>.rnn = nn.RNN(input_size, hidden_size, batch_first=<span style="color: #8B008B; font-weight: bold">True</span>)
       <span style="color: #658b00">self</span>.fc = nn.Linear(hidden_size, output_size)

   <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">forward</span>(<span style="color: #658b00">self</span>, x):
       out, _ = <span style="color: #658b00">self</span>.rnn(x)  <span style="color: #228B22"># out: (batch_size, seq_len, hidden_size)</span>
       out = out[:, -<span style="color: #B452CD">1</span>, :]   <span style="color: #228B22"># Take last time step output</span>
       out = <span style="color: #658b00">self</span>.fc(out)
       <span style="color: #8B008B; font-weight: bold">return</span> out

model = RNNModel(input_size=<span style="color: #B452CD">1</span>, hidden_size=hidden_size, output_size=<span style="color: #B452CD">1</span>)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

<span style="color: #228B22"># Training loop</span>
train_losses = []
val_losses = []

<span style="color: #8B008B; font-weight: bold">for</span> epoch <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(num_epochs):
   model.train()
   epoch_train_loss = <span style="color: #B452CD">0</span>
   <span style="color: #8B008B; font-weight: bold">for</span> x_batch, y_batch <span style="color: #8B008B">in</span> train_loader:
       optimizer.zero_grad()
       y_pred = model(x_batch)
       loss = criterion(y_pred, y_batch.unsqueeze(-<span style="color: #B452CD">1</span>))
       loss.backward()
       optimizer.step()
       epoch_train_loss += loss.item()

   <span style="color: #228B22"># Validation</span>
   model.eval()
   epoch_val_loss = <span style="color: #B452CD">0</span>
   <span style="color: #8B008B; font-weight: bold">with</span> torch.no_grad():
       <span style="color: #8B008B; font-weight: bold">for</span> x_val, y_val <span style="color: #8B008B">in</span> val_loader:
           y_pred_val = model(x_val)
           val_loss = criterion(y_pred_val, y_val.unsqueeze(-<span style="color: #B452CD">1</span>))
           epoch_val_loss += val_loss.item()

   <span style="color: #228B22"># Calculate average losses</span>
   train_loss = epoch_train_loss / <span style="color: #658b00">len</span>(train_loader)
   val_loss = epoch_val_loss / <span style="color: #658b00">len</span>(val_loader)
   train_losses.append(train_loss)
   val_losses.append(val_loss)

   <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Epoch {</span>epoch+<span style="color: #B452CD">1</span><span style="color: #CD5555">}/{</span>num_epochs<span style="color: #CD5555">}, Train Loss: {</span>train_loss<span style="color: #CD5555">:.4f}, Val Loss: {</span>val_loss<span style="color: #CD5555">:.4f}&#39;</span>)

<span style="color: #228B22"># Generate predictions</span>
model.eval()
initial_sequence = train_data[-seq_len:].reshape(<span style="color: #B452CD">1</span>, seq_len, <span style="color: #B452CD">1</span>)
predictions = []
current_sequence = initial_sequence.clone()

<span style="color: #8B008B; font-weight: bold">with</span> torch.no_grad():
   <span style="color: #8B008B; font-weight: bold">for</span> _ <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #658b00">len</span>(val_data)):
       pred = model(current_sequence)
       predictions.append(pred.item())
       <span style="color: #228B22"># Update sequence by removing first element and adding new prediction</span>
       current_sequence = torch.cat([current_sequence[:, <span style="color: #B452CD">1</span>:, :], pred.unsqueeze(<span style="color: #B452CD">1</span>)], dim=<span style="color: #B452CD">1</span>)


<span style="color: #228B22"># Plot training and validation loss</span>
plt.figure(figsize=(<span style="color: #B452CD">10</span>, <span style="color: #B452CD">5</span>))
plt.plot(train_losses, label=<span style="color: #CD5555">&#39;Training Loss&#39;</span>)
plt.plot(val_losses, label=<span style="color: #CD5555">&#39;Validation Loss&#39;</span>)
plt.title(<span style="color: #CD5555">&#39;Training and Validation Loss&#39;</span>)
plt.xlabel(<span style="color: #CD5555">&#39;Epoch&#39;</span>)
plt.ylabel(<span style="color: #CD5555">&#39;Loss&#39;</span>)
plt.legend()
plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="rnns">RNNs </h2>

<p>RNNs are very powerful, because they
combine two properties:
</p>
<ol>
<p><li> Distributed hidden state that allows them to store a lot of information about the past efficiently.</li>
<p><li> Non-linear dynamics that allows them to update their hidden state in complicated ways.</li>
</ol>
<p>
<p>With enough neurons and time, RNNs
can compute anything that can be
computed by your computer.
</p>
</section>

<section>
<h2 id="what-kinds-of-behaviour-can-rnns-exhibit">What kinds of behaviour can RNNs exhibit? </h2>

<ol>
<p><li> They can oscillate.</li> 
<p><li> They can settle to point attractors.</li>
<p><li> They can behave chaotically.</li>
<p><li> RNNs could potentially learn to implement lots of small programs that each capture a nugget of knowledge and run in parallel, interacting to produce very complicated effects.</li>
</ol>
<p>
<p>But the extensive computational needs  of RNNs makes them very hard to train.</p>
</section>

<section>
<h2 id="basic-layout-figures-from-sebastian-rashcka-et-al-machine-learning-with-sickit-learn-and-pytorch-https-sebastianraschka-com-blog-2022-ml-pytorch-book-html">Basic layout,  <a href="https://sebastianraschka.com/blog/2022/ml-pytorch-book.html" target="_blank">Figures from Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch</a> </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN1.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="solving-differential-equations-with-rnns">Solving differential equations with RNNs </h2>

<p>To gain some intuition on how we can use RNNs for time series, let us
tailor the representation of the solution of a differential equation
as a time series.
</p>

<p>Consider the famous differential equation (Newton's equation of motion for damped harmonic oscillations, scaled in terms of dimensionless time)</p>

<p>&nbsp;<br>
$$
\frac{d^2x}{dt^2}+\eta\frac{dx}{dt}+x(t)=F(t),
$$
<p>&nbsp;<br>

<p>where \( \eta \) is a constant used in scaling time into a dimensionless variable and \( F(t) \) is an external force acting on the system.
The constant \( \eta \) is a so-called damping.
</p>
</section>

<section>
<h2 id="two-first-order-differential-equations">Two first-order differential equations </h2>

<p>In solving the above second-order equation, it is common to rewrite it in terms of two coupled first-order equations
with the velocity
</p>
<p>&nbsp;<br>
$$
v(t)=\frac{dx}{dt},
$$
<p>&nbsp;<br>

<p>and the acceleration</p>
<p>&nbsp;<br>
$$
\frac{dv}{dt}=F(t)-\eta v(t)-x(t).
$$
<p>&nbsp;<br>

<p>With the initial conditions \( v_0=v(t_0) \) and \( x_0=x(t_0) \) defined, we can integrate these equations and find their respective solutions.</p>
</section>

<section>
<h2 id="velocity-only">Velocity only </h2>

<p>Let us focus on the velocity only. Discretizing and using the simplest
possible approximation for the derivative, we have Euler's forward
method for the updated velocity at a time step \( i+1 \) given by
</p>
<p>&nbsp;<br>
$$
v_{i+1}=v_i+\Delta t \frac{dv}{dt}_{\vert_{v=v_i}}=v_i+\Delta t\left(F_i-\eta v_i-x_i\right).
$$
<p>&nbsp;<br>

<p>Defining a function</p>
<p>&nbsp;<br>
$$
h_i(x_i,v_i,F_i)=v_i+\Delta t\left(F_i-\eta v_i-x_i\right),
$$
<p>&nbsp;<br>

<p>we have</p>
<p>&nbsp;<br>
$$
v_{i+1}=h_i(x_i,v_i,F_i).
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="linking-with-rnns">Linking with RNNs </h2>

<p>The equation</p>
<p>&nbsp;<br>
$$
v_{i+1}=h_i(x_i,v_i,F_i).
$$
<p>&nbsp;<br>

<p>can be used to train a feed-forward neural network with inputs \( v_i \) and outputs \( v_{i+1} \) at a time \( t_i \). But we can think of this also as a recurrent neural network
with inputs \( v_i \), \( x_i \) and \( F_i \) at each time step \( t_i \), and producing an output \( v_{i+1} \).
</p>

<p>Noting that </p>
<p>&nbsp;<br>
$$
v_{i}=v_{i-1}+\Delta t\left(F_{i-1}-\eta v_{i-1}-x_{i-1}\right)=h_{i-1}.
$$
<p>&nbsp;<br>

<p>we have</p>
<p>&nbsp;<br>
$$
v_{i}=h_{i-1}(x_{i-1},v_{i-1},F_{i-1}),
$$
<p>&nbsp;<br>

<p>and we can rewrite</p>
<p>&nbsp;<br>
$$
v_{i+1}=h_i(x_i,h_{i-1},F_i).
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="minor-rewrite">Minor rewrite </h2>

<p>We can thus set up a recurring series which depends on the inputs \( x_i \) and \( F_i \) and the previous values \( h_{i-1} \).
We assume now that the inputs at each step (or time \( t_i \)) is given by \( x_i \) only and we denote the outputs for \( \tilde{y}_i \) instead of \( v_{i_1} \), we have then the compact equation for our outputs at each step \( t_i \)
</p>
<p>&nbsp;<br>
$$
y_{i}=h_i(x_i,h_{i-1}).
$$
<p>&nbsp;<br>

<p>We can think of this as an element in a recurrent network where our
network (our model) produces an output \( y_i \) which is then compared
with a target value through a given cost/loss function that we
optimize. The target values at a given step \( t_i \) could be the results
of a measurement or simply the analytical results of a differential
equation.
</p>
</section>

<section>
<h2 id="rnns-in-more-detail">RNNs in more detail  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN2.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="rnns-in-more-detail-part-2">RNNs in more detail, part 2  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN3.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="rnns-in-more-detail-part-3">RNNs in more detail, part 3  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN4.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="rnns-in-more-detail-part-4">RNNs in more detail, part 4  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN5.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="rnns-in-more-detail-part-5">RNNs in more detail, part 5  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN6.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="rnns-in-more-detail-part-6">RNNs in more detail, part 6  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN7.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="rnns-in-more-detail-part-7">RNNs in more detail, part 7  </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN8.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="backpropagation-through-time">Backpropagation through time </h2>

<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>We can think of the recurrent net as a layered, feed-forward
net with shared weights and then train the feed-forward net
with weight constraints.
</p>
</div>

<p>We can also think of this training algorithm in the time domain:</p>
<ol>
<p><li> The forward pass builds up a stack of the activities of all the units at each time step.</li>
<p><li> The backward pass peels activities off the stack to compute the error derivatives at each time step.</li>
<p><li> After the backward pass we add together the derivatives at all the different times for each weight.</li> 
</ol>
</section>

<section>
<h2 id="the-backward-pass-is-linear">The backward pass is linear </h2>

<ol>
<p><li> There is a big difference between the forward and backward passes.</li>
<p><li> In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding.</li>
<p><li> The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double.</li>
</ol>
<p>
<p>The forward pass determines the slope of the linear function used for
backpropagating through each neuron
</p>
</section>

<section>
<h2 id="the-problem-of-exploding-or-vanishing-gradients">The problem of exploding or vanishing gradients </h2>
<ul>
<p><li> What happens to the magnitude of the gradients as we backpropagate through many layers?
<ol type="a"></li>
 <p><li> If the weights are small, the gradients shrink exponentially.</li>
 <p><li> If the weights are big the gradients grow exponentially.</li>
</ol>
<p>
<p><li> Typical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.</li>
<p><li> In an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.
<ol type="a"></li>
 <p><li> We can avoid this by initializing the weights very carefully.</li>
</ol>
<p>
<p><li> Even with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.</li>
</ul>
<p>
<p>RNNs have difficulty dealing with long-range dependencies. </p>
</section>

<section>
<h2 id="mathematical-setup">Mathematical setup </h2>

<p>The expression for the simplest Recurrent network resembles that of a
regular feed-forward neural network, but now with
the concept of temporal dependencies
</p>

<p>&nbsp;<br>
$$
\begin{align*}
    \mathbf{a}^{(t)} & = U * \mathbf{x}^{(t)} + W * \mathbf{h}^{(t-1)} + \mathbf{b}, \notag \\
    \mathbf{h}^{(t)} &= \sigma_h(\mathbf{a}^{(t)}), \notag\\
    \mathbf{y}^{(t)} &= V * \mathbf{h}^{(t)} + \mathbf{c}, \notag\\
    \mathbf{\hat{y}}^{(t)} &= \sigma_y(\mathbf{y}^{(t)}).
\end{align*}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="back-propagation-in-time-through-figures-part-1">Back propagation in time through figures, part 1   </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN9.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="back-propagation-in-time-part-2">Back propagation in time, part 2   </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN10.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="back-propagation-in-time-part-3">Back propagation in time, part 3   </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN11.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="back-propagation-in-time-part-4">Back propagation in time, part 4   </h2>

<br/><br/>
<center>
<p><img src="figslides/RNN12.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="back-propagation-in-time-in-equations">Back propagation in time in equations </h2>

<p>To derive the expression of the gradients of \( \mathcal{L} \) for
the RNN, we need to start recursively from the nodes closer to the
output layer in the temporal unrolling scheme - such as \( \mathbf{y} \)
and \( \mathbf{h} \) at final time \( t = \tau \),
</p>

<p>&nbsp;<br>
$$
\begin{align*}
    (\nabla_{ \mathbf{y}^{(t)}} \mathcal{L})_{i} &= \frac{\partial \mathcal{L}}{\partial L^{(t)}}\frac{\partial L^{(t)}}{\partial y_{i}^{(t)}}, \notag\\
    \nabla_{\mathbf{h}^{(\tau)}} \mathcal{L} &= \mathbf{V}^\mathsf{T}\nabla_{ \mathbf{y}^{(\tau)}} \mathcal{L}.
\end{align*}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="chain-rule-again">Chain rule again </h2>
<p>For the following hidden nodes, we have to iterate through time, so by the chain rule, </p>

<p>&nbsp;<br>
$$
\begin{align*}
    \nabla_{\mathbf{h}^{(t)}} \mathcal{L} &= \left(\frac{\partial\mathbf{h}^{(t+1)}}{\partial\mathbf{h}^{(t)}}\right)^\mathsf{T}\nabla_{\mathbf{h}^{(t+1)}}\mathcal{L} + \left(\frac{\partial\mathbf{y}^{(t)}}{\partial\mathbf{h}^{(t)}}\right)^\mathsf{T}\nabla_{ \mathbf{y}^{(t)}} \mathcal{L}.
\end{align*}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="gradients-of-loss-functions">Gradients of loss functions </h2>
<p>Similarly, the gradients of \( \mathcal{L} \) with respect to the weights and biases follow,</p>

<p>&nbsp;<br>
$$
\begin{align*}
    \nabla_{\mathbf{c}} \mathcal{L} &=\sum_{t}\left(\frac{\partial \mathbf{y}^{(t)}}{\partial \mathbf{c}}\right)^\mathsf{T} \nabla_{\mathbf{y}^{(t)}} \mathcal{L} \notag\\
    \nabla_{\mathbf{b}} \mathcal{L} &=\sum_{t}\left(\frac{\partial \mathbf{h}^{(t)}}{\partial \mathbf{b}}\right)^\mathsf{T}        \nabla_{\mathbf{h}^{(t)}} \mathcal{L} \notag\\
    \nabla_{\mathbf{V}} \mathcal{L} &=\sum_{t}\sum_{i}\left(\frac{\partial \mathcal{L}}{\partial y_i^{(t)} }\right)\nabla_{\mathbf{V}^{(t)}}y_i^{(t)} \notag\\
    \nabla_{\mathbf{W}} \mathcal{L} &=\sum_{t}\sum_{i}\left(\frac{\partial \mathcal{L}}{\partial h_i^{(t)}}\right)\nabla_{\mathbf{w}^{(t)}} h_i^{(t)} \notag\\
    \nabla_{\mathbf{U}} \mathcal{L} &=\sum_{t}\sum_{i}\left(\frac{\partial \mathcal{L}}{\partial h_i^{(t)}}\right)\nabla_{\mathbf{U}^{(t)}}h_i^{(t)}.
    \tag{1}
\end{align*}
$$
<p>&nbsp;<br>
</section>

<section>
<h2 id="summary-of-rnns">Summary of RNNs </h2>

<p>Recurrent neural networks (RNNs) have in general no probabilistic component
in a model. With a given fixed input and target from data, the RNNs learn the intermediate
association between various layers.
The inputs, outputs, and internal representation (hidden states) are all
real-valued vectors.
</p>

<p>In a  traditional NN, it is assumed that every input is
independent of each other.  But with sequential data, the input at a given stage \( t \) depends on the input from the previous stage \( t-1 \)
</p>
</section>

<section>
<h2 id="summary-of-a-typical-rnn">Summary of a  typical RNN </h2>

<ol>
<p><li> Weight matrices \( U \), \( W \) and \( V \) that connect the input layer at a stage \( t \) with the hidden layer \( h_t \), the previous hidden layer \( h_{t-1} \) with \( h_t \) and the hidden layer \( h_t \) connecting with the output layer at the same stage and producing an output \( \tilde{y}_t \), respectively.</li>
<p><li> The output from the hidden layer \( h_t \) is oftem modulated by a \( \tanh{} \) function \( h_t=\sigma_h(x_t,h_{t-1})=\tanh{(Ux_t+Wh_{t-1}+b)} \) with \( b \) a bias value</li>
<p><li> The output from the hidden layer produces \( \tilde{y}_t=\sigma_y(Vh_t+c) \) where \( c \) is a new bias parameter.</li>
<p><li> The output from the training at a given stage is in turn compared with the observation \( y_t \) thorugh a chosen cost function.</li>
</ol>
<p>
<p>The function \( g \) can any of the standard activation functions, that is a Sigmoid, a Softmax, a ReLU and other.
The parameters are trained through the so-called back-propagation through time (BPTT) algorithm.
</p>
</section>

<section>
<h2 id="four-effective-ways-to-learn-an-rnn">Four effective ways to learn an RNN  </h2>
<ol>
<p><li> Long Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.</li>
<p><li> Hessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.</li>
<p><li> Echo State Networks (ESN): Initialize the input a hidden and hidden-hidden and output-hidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input. ESNs only need to learn the hidden-output connections.</li>
<p><li> Good initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum</li>
</ol>
</section>

<section>
<h2 id="the-mathematics-of-rnns-the-basic-architecture">The mathematics of RNNs, the basic architecture  </h2>

<p>See notebook at <a href="https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week7/ipynb/rnnmath.ipynb" target="_blank"><tt>https://github.com/CompPhysics/AdvancedMachineLearning/blob/main/doc/pub/week7/ipynb/rnnmath.ipynb</tt></a></p>
</section>

<section>
<h2 id="gating-mechanism-long-short-term-memory-lstm">Gating mechanism: Long Short Term Memory (LSTM) </h2>

<p>Besides a simple recurrent neural network layer, as discussed above, there are two other
commonly used types of recurrent neural network layers: Long Short
Term Memory (LSTM) and Gated Recurrent Unit (GRU).  For a short
introduction to these layers see <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_blank"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>
and <a href="https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b" target="_blank"><tt>https://medium.com/mindboard/lstm-vs-gru-experimental-comparison-955820c21e8b</tt></a>.
</p>

<p>LSTM uses a memory cell for 
modeling long-range dependencies and avoid vanishing gradient
 problems.
Capable of modeling longer term dependencies by having
memory cells and gates that controls the information flow along
with the memory cells.
</p>

<ol>
<p><li> Introduced by Hochreiter and Schmidhuber (1997) who solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).</li>
<p><li> They designed a memory cell using logistic and linear units with multiplicative interactions.</li>
<p><li> Information gets into the cell whenever its &#8220;write&#8221; gate is on.</li>
<p><li> The information stays in the cell so long as its <b>keep</b> gate is on.</li>
<p><li> Information can be read from the cell by turning on its <b>read</b> gate.</li> 
</ol>
<p>
<p>The LSTM were first introduced to overcome the vanishing gradient problem.</p>
</section>

<section>
<h2 id="implementing-a-memory-cell-in-a-neural-network">Implementing a memory cell in a neural network </h2>

<p>To preserve information for a long time in
the activities of an RNN, we use a circuit
that implements an analog memory cell.
</p>

<ol>
<p><li> A linear unit that has a self-link with a weight of 1 will maintain its state.</li>
<p><li> Information is stored in the cell by activating its write gate.</li>
<p><li> Information is retrieved by activating the read gate.</li>
<p><li> We can backpropagate through this circuit because logistics are have nice derivatives.</li> 
</ol>
</section>

<section>
<h2 id="lstm-details">LSTM details </h2>

<p>The LSTM is a unit cell that is made of three gates:</p>
<ol>
<p><li> the input gate,</li>
<p><li> the forget gate,</li>
<p><li> and the output gate.</li>
</ol>
<p>
<p>It also introduces a cell state \( c \), which can be thought of as the
long-term memory, and a hidden state \( h \) which can be thought of as
the short-term memory.
</p>
</section>

<section>
<h2 id="basic-layout-all-figures-from-raschka-et-al">Basic layout (All figures from Raschka <em>et al.,</em>) </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM1.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="lstm-details">LSTM details </h2>

<p>The first stage is called the forget gate, where we combine the input
at (say, time \( t \)), and the hidden cell state input at \( t-1 \), passing
it through the Sigmoid activation function and then performing an
element-wise multiplication, denoted by \( \odot \).
</p>

<p>Mathematically we have (see also figure below)</p>
<p>&nbsp;<br>
$$
\mathbf{f}^{(t)} = \sigma(W_{fx}\mathbf{x}^{(t)} + W_{fh}\mathbf{h}^{(t-1)} + \mathbf{b}_f)
$$
<p>&nbsp;<br>

<p>where the $W$s are the weights to be trained.</p>
</section>

<section>
<h2 id="comparing-with-a-standard-rnn">Comparing with a standard  RNN  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM2.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="lstm-details-i">LSTM details I </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM3.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="lstm-details-ii">LSTM details II  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM4.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="lstm-details-iii">LSTM details III  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM5.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="forget-gate">Forget gate  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM6.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="the-forget-gate">The forget gate </h2>

<p>The naming forget gate stems from the fact that  the Sigmoid activation function's
outputs are very close to \( 0 \) if the argument for the function is very
negative, and \( 1 \) if the argument is very positive. Hence we can
control the amount of information we want to take from the long-term
memory.
</p>
<p>&nbsp;<br>
$$
\mathbf{f}^{(t)} = \sigma(W_{fx}\mathbf{x}^{(t)} + W_{fh}\mathbf{h}^{(t-1)} + \mathbf{b}_f)
$$
<p>&nbsp;<br>

<p>where the $W$s are the weights to be trained.</p>
</section>

<section>
<h2 id="basic-layout">Basic layout </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM7.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="input-gate">Input gate </h2>

<p>The next stage is the input gate, which consists of both a Sigmoid
function (\( \sigma_i \)), which decide what percentage of the input will
be stored in the long-term memory, and the \( \tanh_i \) function, which
decide what is the full memory that can be stored in the long term
memory. When these results are calculated and multiplied together, it
is added to the cell state or stored in the long-term memory, denoted
as \( \oplus \). 
</p>

<p>We have</p>
<p>&nbsp;<br>
$$
\mathbf{i}^{(t)} = \sigma_g(W_{ix}\mathbf{x}^{(t)} + W_{ih}\mathbf{h}^{(t-1)} + \mathbf{b}_i),
$$
<p>&nbsp;<br>

<p>and</p>
<p>&nbsp;<br>
$$
\mathbf{g}^{(t)} = \tanh(W_{gx}\mathbf{x}^{(t)} + W_{gh}\mathbf{h}^{(t-1)} + \mathbf{b}_g),
$$
<p>&nbsp;<br>

<p>again the $W$s are the weights to train.</p>
</section>

<section>
<h2 id="short-summary">Short summary  </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM8.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="forget-and-input">Forget and input </h2>

<p>The forget gate and the input gate together also update the cell state with the following equation, </p>
<p>&nbsp;<br>
$$
\mathbf{c}^{(t)} = \mathbf{f}^{(t)} \otimes \mathbf{c}^{(t-1)} + \mathbf{i}^{(t)} \otimes \mathbf{g}^{(t)},
$$
<p>&nbsp;<br>

<p>where \( f^{(t)} \) and \( i^{(t)} \) are the outputs of the forget gate and the input gate, respectively.</p>
</section>

<section>
<h2 id="basic-layout">Basic layout </h2>

<br/><br/>
<center>
<p><img src="figslides/LSTM9.png" width="700" align="bottom"></p>
</center>
<br/><br/>
</section>

<section>
<h2 id="output-gate">Output gate </h2>

<p>The final stage of the LSTM is the output gate, and its purpose is to
update the short-term memory.  To achieve this, we take the newly
generated long-term memory and process it through a hyperbolic tangent
(\( \tanh \)) function creating a potential new short-term memory. We then
multiply this potential memory by the output of the Sigmoid function
(\( \sigma_o \)). This multiplication generates the final output as well
as the input for the next hidden cell (\( h^{\langle t \rangle} \)) within
the LSTM cell.
</p>

<p>We have </p>
<p>&nbsp;<br>
$$
\begin{aligned}
\mathbf{o}^{(t)} &= \sigma_g(W_o\mathbf{x}^{(t)} + U_o\mathbf{h}^{(t-1)} + \mathbf{b}_o), \\
\mathbf{h}^{(t)} &= \mathbf{o}^{(t)} \otimes \sigma_h(\mathbf{c}^{(t)}). \\
\end{aligned}
$$
<p>&nbsp;<br>

<p>where \( \mathbf{W_o,U_o} \) are the weights of the output gate and \( \mathbf{b_o} \) is the bias of the output gate.</p>
</section>

<section>
<h2 id="summary-of-lstm">Summary of LSTM </h2>

<p>LSTMs provide a basic approach for modeling long-range dependencies in sequences.
If you wish to read more, see <b>An Empirical Exploration of Recurrent Network Architectures</b>, authored
by Rafal Jozefowicz <em>et al.,</em>  Proceedings of ICML, 2342-2350, 2015).
</p>

<p>An important recent development are so-called noting is a more recent
method named <b>gated recurrent unit (GRU)</b>, se for example the article
by Junyoung Chung <em>et al.,</em>, at URL:"https://arxiv.org/abs/1412.3555.
This article is an excellent read if you are interested in learning
more about these modern RNN architectures
</p>

<p>The GRUs have a simpler
architecture than LSTMs. This leads to computationally more efficient methods, while their
performance in some tasks, such as polyphonic music modeling, is comparable to LSTMs.
</p>

<p>And recently Beck <em>et al.,</em> see <a href="https://arxiv.org/abs/2405.04517" target="_blank"><tt>https://arxiv.org/abs/2405.04517</tt></a>, have demonstrated that exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.</p>
</section>

<section>
<h2 id="lstm-implementation-using-tensorflow">LSTM implementation using TensorFlow </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #CD5555">&quot;&quot;&quot;</span>
<span style="color: #CD5555">Key points:</span>
<span style="color: #CD5555">1. The input images (28x28 pixels) are treated as sequences of 28 timesteps with 28 features each</span>
<span style="color: #CD5555">2. The LSTM layer processes this sequential data</span>
<span style="color: #CD5555">3. A final dense layer with softmax activation handles the classification</span>
<span style="color: #CD5555">4. Typical accuracy ranges between 95-98% (lower than CNNs but reasonable for demonstration)</span>

<span style="color: #CD5555">Note: LSTMs are not typically used for image classification (CNNs are more efficient), but this demonstrates how to adapt them for such tasks. Training might take longer compared to CNN architectures.</span>

<span style="color: #CD5555">To improve performance, you could:</span>
<span style="color: #CD5555">1. Add more LSTM layers</span>
<span style="color: #CD5555">2. Use Bidirectional LSTMs</span>
<span style="color: #CD5555">3. Increase the number of units</span>
<span style="color: #CD5555">4. Add dropout for regularization</span>
<span style="color: #CD5555">5. Use learning rate scheduling</span>
<span style="color: #CD5555">&quot;&quot;&quot;</span>

<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">tensorflow</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">tf</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">tensorflow.keras.models</span> <span style="color: #8B008B; font-weight: bold">import</span> Sequential
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">tensorflow.keras.layers</span> <span style="color: #8B008B; font-weight: bold">import</span> LSTM, Dense
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">tensorflow.keras.utils</span> <span style="color: #8B008B; font-weight: bold">import</span> to_categorical

<span style="color: #228B22"># Load and preprocess data</span>
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

<span style="color: #228B22"># Normalize pixel values to [0, 1]</span>
x_train = x_train.astype(<span style="color: #CD5555">&#39;float32&#39;</span>) / <span style="color: #B452CD">255.0</span>
x_test = x_test.astype(<span style="color: #CD5555">&#39;float32&#39;</span>) / <span style="color: #B452CD">255.0</span>

<span style="color: #228B22"># Reshape data for LSTM (samples, timesteps, features)</span>
<span style="color: #228B22"># MNIST images are 28x28, so we treat each image as 28 timesteps of 28 features</span>
x_train = x_train.reshape((-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">28</span>, <span style="color: #B452CD">28</span>))
x_test = x_test.reshape((-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">28</span>, <span style="color: #B452CD">28</span>))

<span style="color: #228B22"># Convert labels to one-hot encoding</span>
y_train = to_categorical(y_train, <span style="color: #B452CD">10</span>)
y_test = to_categorical(y_test, <span style="color: #B452CD">10</span>)

<span style="color: #228B22"># Build LSTM model</span>
model = Sequential()
model.add(LSTM(<span style="color: #B452CD">128</span>, input_shape=(<span style="color: #B452CD">28</span>, <span style="color: #B452CD">28</span>)))  <span style="color: #228B22"># 128 LSTM units</span>
model.add(Dense(<span style="color: #B452CD">10</span>, activation=<span style="color: #CD5555">&#39;softmax&#39;</span>))

<span style="color: #228B22"># Compile the model</span>
model.compile(loss=<span style="color: #CD5555">&#39;categorical_crossentropy&#39;</span>,
             optimizer=<span style="color: #CD5555">&#39;adam&#39;</span>,
             metrics=[<span style="color: #CD5555">&#39;accuracy&#39;</span>])

<span style="color: #228B22"># Display model summary</span>
model.summary()

<span style="color: #228B22"># Train the model</span>
history = model.fit(x_train, y_train,
                   batch_size=<span style="color: #B452CD">64</span>,
                   epochs=<span style="color: #B452CD">10</span>,
                   validation_split=<span style="color: #B452CD">0.2</span>)

<span style="color: #228B22"># Evaluate on test data</span>
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=<span style="color: #B452CD">2</span>)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;\nTest accuracy: {</span>test_acc<span style="color: #CD5555">:.4f}&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="and-the-corresponding-one-with-pytorch">And the corresponding one with PyTorch </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #CD5555">&quot;&quot;&quot;</span>
<span style="color: #CD5555">Key components:</span>
<span style="color: #CD5555">1. **Data Handling**: Uses PyTorch DataLoader with MNIST dataset</span>
<span style="color: #CD5555">2. **LSTM Architecture**:</span>
<span style="color: #CD5555">  - Input sequence of 28 timesteps (image rows)</span>
<span style="color: #CD5555">  - 128 hidden units in LSTM layer</span>
<span style="color: #CD5555">  - Fully connected layer for classification</span>
<span style="color: #CD5555">3. **Training**:</span>
<span style="color: #CD5555">  - Cross-entropy loss</span>
<span style="color: #CD5555">  - Adam optimizer</span>
<span style="color: #CD5555">  - Automatic GPU utilization if available</span>

<span style="color: #CD5555">This implementation typically achieves **97-98% accuracy** after 10 epochs. The main differences from the TensorFlow/Keras version:</span>
<span style="color: #CD5555">- Explicit device management (CPU/GPU)</span>
<span style="color: #CD5555">- Manual training loop</span>
<span style="color: #CD5555">- Different data loading pipeline</span>
<span style="color: #CD5555">- More explicit tensor reshaping</span>

<span style="color: #CD5555">To improve performance, you could:</span>
<span style="color: #CD5555">1. Add dropout regularization</span>
<span style="color: #CD5555">2. Use bidirectional LSTM</span>
<span style="color: #CD5555">3. Implement learning rate scheduling</span>
<span style="color: #CD5555">4. Add batch normalization</span>
<span style="color: #CD5555">5. Increase model capacity (more layers/units)</span>
<span style="color: #CD5555">&quot;&quot;&quot;</span>

<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch.nn</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">nn</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch.optim</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">optim</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">torchvision</span> <span style="color: #8B008B; font-weight: bold">import</span> datasets, transforms
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">torch.utils.data</span> <span style="color: #8B008B; font-weight: bold">import</span> DataLoader

<span style="color: #228B22"># Hyperparameters</span>
input_size = <span style="color: #B452CD">28</span>     <span style="color: #228B22"># Number of features (pixels per row)</span>
hidden_size = <span style="color: #B452CD">128</span>   <span style="color: #228B22"># LSTM hidden state size</span>
num_classes = <span style="color: #B452CD">10</span>    <span style="color: #228B22"># Digits 0-9</span>
num_epochs = <span style="color: #B452CD">10</span>     <span style="color: #228B22"># Training iterations</span>
batch_size = <span style="color: #B452CD">64</span>     <span style="color: #228B22"># Batch size</span>
learning_rate = <span style="color: #B452CD">0.001</span>

<span style="color: #228B22"># Device configuration</span>
device = torch.device(<span style="color: #CD5555">&#39;cuda&#39;</span> <span style="color: #8B008B; font-weight: bold">if</span> torch.cuda.is_available() <span style="color: #8B008B; font-weight: bold">else</span> <span style="color: #CD5555">&#39;cpu&#39;</span>)

<span style="color: #228B22"># MNIST dataset</span>
transform = transforms.Compose([
   transforms.ToTensor(),
   transforms.Normalize((<span style="color: #B452CD">0.1307</span>,), (<span style="color: #B452CD">0.3081</span>,))  <span style="color: #228B22"># MNIST mean and std</span>
])

train_dataset = datasets.MNIST(root=<span style="color: #CD5555">&#39;./data&#39;</span>,
                              train=<span style="color: #8B008B; font-weight: bold">True</span>,
                              transform=transform,
                              download=<span style="color: #8B008B; font-weight: bold">True</span>)

test_dataset = datasets.MNIST(root=<span style="color: #CD5555">&#39;./data&#39;</span>,
                             train=<span style="color: #8B008B; font-weight: bold">False</span>,
                             transform=transform)

train_loader = DataLoader(dataset=train_dataset,
                         batch_size=batch_size,
                         shuffle=<span style="color: #8B008B; font-weight: bold">True</span>)

test_loader = DataLoader(dataset=test_dataset,
                        batch_size=batch_size,
                        shuffle=<span style="color: #8B008B; font-weight: bold">False</span>)

<span style="color: #228B22"># LSTM model</span>
<span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">LSTMModel</span>(nn.Module):
   <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__init__</span>(<span style="color: #658b00">self</span>, input_size, hidden_size, num_classes):
       <span style="color: #658b00">super</span>(LSTMModel, <span style="color: #658b00">self</span>).<span style="color: #008b45">__init__</span>()
       <span style="color: #658b00">self</span>.hidden_size = hidden_size
       <span style="color: #658b00">self</span>.lstm = nn.LSTM(input_size, hidden_size, batch_first=<span style="color: #8B008B; font-weight: bold">True</span>)
       <span style="color: #658b00">self</span>.fc = nn.Linear(hidden_size, num_classes)

   <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">forward</span>(<span style="color: #658b00">self</span>, x):
       <span style="color: #228B22"># Reshape input to (batch_size, sequence_length, input_size)</span>
       x = x.reshape(-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">28</span>, <span style="color: #B452CD">28</span>)

       <span style="color: #228B22"># Forward propagate LSTM</span>
       out, _ = <span style="color: #658b00">self</span>.lstm(x)  <span style="color: #228B22"># out: (batch_size, seq_length, hidden_size)</span>

       <span style="color: #228B22"># Decode the hidden state of the last time step</span>
       out = out[:, -<span style="color: #B452CD">1</span>, :]
       out = <span style="color: #658b00">self</span>.fc(out)
       <span style="color: #8B008B; font-weight: bold">return</span> out

<span style="color: #228B22"># Initialize model</span>
model = LSTMModel(input_size, hidden_size, num_classes).to(device)

<span style="color: #228B22"># Loss and optimizer</span>
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

<span style="color: #228B22"># Training loop</span>
total_step = <span style="color: #658b00">len</span>(train_loader)
<span style="color: #8B008B; font-weight: bold">for</span> epoch <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(num_epochs):
   model.train()
   <span style="color: #8B008B; font-weight: bold">for</span> i, (images, labels) <span style="color: #8B008B">in</span> <span style="color: #658b00">enumerate</span>(train_loader):
       images = images.to(device)
       labels = labels.to(device)

       <span style="color: #228B22"># Forward pass</span>
       outputs = model(images)
       loss = criterion(outputs, labels)

       <span style="color: #228B22"># Backward and optimize</span>
       optimizer.zero_grad()
       loss.backward()
       optimizer.step()

       <span style="color: #8B008B; font-weight: bold">if</span> (i+<span style="color: #B452CD">1</span>) % <span style="color: #B452CD">100</span> == <span style="color: #B452CD">0</span>:
           <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Epoch [{</span>epoch+<span style="color: #B452CD">1</span><span style="color: #CD5555">}/{</span>num_epochs<span style="color: #CD5555">}], Step [{</span>i+<span style="color: #B452CD">1</span><span style="color: #CD5555">}/{</span>total_step<span style="color: #CD5555">}], Loss: {</span>loss.item()<span style="color: #CD5555">:.4f}&#39;</span>)

   <span style="color: #228B22"># Test the model</span>
   model.eval()
   <span style="color: #8B008B; font-weight: bold">with</span> torch.no_grad():
       correct = <span style="color: #B452CD">0</span>
       total = <span style="color: #B452CD">0</span>
       <span style="color: #8B008B; font-weight: bold">for</span> images, labels <span style="color: #8B008B">in</span> test_loader:
           images = images.to(device)
           labels = labels.to(device)
           outputs = model(images)
           _, predicted = torch.max(outputs.data, <span style="color: #B452CD">1</span>)
           total += labels.size(<span style="color: #B452CD">0</span>)
           correct += (predicted == labels).sum().item()

       <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&#39;Test Accuracy: {</span><span style="color: #B452CD">100</span> * correct / total<span style="color: #CD5555">:.2f}%&#39;</span>)

<span style="color: #658b00">print</span>(<span style="color: #CD5555">&#39;Training finished.&#39;</span>)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="dynamical-ordinary-differential-equation">Dynamical ordinary differential equation </h2>

<p>Let us illustrate how we could train an RNN using data from the
solution of a well-known differential equation, namely Newton's
equation for oscillatory motion for an object being forced into
harmonic oscillations by an applied external force.
</p>

<p>We will start with the basic algorithm for solving this type of
equations using the Runge-Kutta-4 approach. The first code example is
a standalone differential equation solver. It yields positions and
velocities as function of time, starting with an initial time \( t_0 \)
and ending with a final time.
</p>

<p>The data the program produces will in turn be used to train an RNN for
a selected number of training data. With a trained RNN, we will then
use the network to make predictions for data not included in the
training. That is, we will train a model which should be able to
reproduce velocities and positions not included in training data.
</p>
</section>

<section>
<h2 id="the-runge-kutta-4-code">The Runge-Kutta-4 code </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> *
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">os</span>

<span style="color: #228B22"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR = <span style="color: #CD5555">&quot;Results&quot;</span>
FIGURE_ID = <span style="color: #CD5555">&quot;Results/FigureFiles&quot;</span>
DATA_ID = <span style="color: #CD5555">&quot;DataFiles/&quot;</span>

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(PROJECT_ROOT_DIR):
    os.mkdir(PROJECT_ROOT_DIR)

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(FIGURE_ID):
    os.makedirs(FIGURE_ID)

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #8B008B">not</span> os.path.exists(DATA_ID):
    os.makedirs(DATA_ID)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">image_path</span>(fig_id):
    <span style="color: #8B008B; font-weight: bold">return</span> os.path.join(FIGURE_ID, fig_id)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">data_path</span>(dat_id):
    <span style="color: #8B008B; font-weight: bold">return</span> os.path.join(DATA_ID, dat_id)

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">save_fig</span>(fig_id):
    plt.savefig(image_path(fig_id) + <span style="color: #CD5555">&quot;.png&quot;</span>, <span style="color: #658b00">format</span>=<span style="color: #CD5555">&#39;png&#39;</span>)


<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">SpringForce</span>(v,x,t):
<span style="color: #228B22">#   note here that we have divided by mass and we return the acceleration</span>
    <span style="color: #8B008B; font-weight: bold">return</span>  -<span style="color: #B452CD">2</span>*gamma*v-x+Ftilde*cos(t*Omegatilde)


<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">RK4</span>(v,x,t,n,Force):
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n-<span style="color: #B452CD">1</span>):
<span style="color: #228B22"># Setting up k1</span>
        k1x = DeltaT*v[i]
        k1v = DeltaT*Force(v[i],x[i],t[i])
<span style="color: #228B22"># Setting up k2</span>
        vv = v[i]+k1v*<span style="color: #B452CD">0.5</span>
        xx = x[i]+k1x*<span style="color: #B452CD">0.5</span>
        k2x = DeltaT*vv
        k2v = DeltaT*Force(vv,xx,t[i]+DeltaT*<span style="color: #B452CD">0.5</span>)
<span style="color: #228B22"># Setting up k3</span>
        vv = v[i]+k2v*<span style="color: #B452CD">0.5</span>
        xx = x[i]+k2x*<span style="color: #B452CD">0.5</span>
        k3x = DeltaT*vv
        k3v = DeltaT*Force(vv,xx,t[i]+DeltaT*<span style="color: #B452CD">0.5</span>)
<span style="color: #228B22"># Setting up k4</span>
        vv = v[i]+k3v
        xx = x[i]+k3x
        k4x = DeltaT*vv
        k4v = DeltaT*Force(vv,xx,t[i]+DeltaT)
<span style="color: #228B22"># Final result</span>
        x[i+<span style="color: #B452CD">1</span>] = x[i]+(k1x+<span style="color: #B452CD">2</span>*k2x+<span style="color: #B452CD">2</span>*k3x+k4x)/<span style="color: #B452CD">6.</span>
        v[i+<span style="color: #B452CD">1</span>] = v[i]+(k1v+<span style="color: #B452CD">2</span>*k2v+<span style="color: #B452CD">2</span>*k3v+k4v)/<span style="color: #B452CD">6.</span>
        t[i+<span style="color: #B452CD">1</span>] = t[i] + DeltaT


<span style="color: #228B22"># Main part begins here</span>

DeltaT = <span style="color: #B452CD">0.001</span>
<span style="color: #228B22">#set up arrays </span>
tfinal = <span style="color: #B452CD">20</span> <span style="color: #228B22"># in dimensionless time</span>
n = ceil(tfinal/DeltaT)
<span style="color: #228B22"># set up arrays for t, v, and x</span>
t = np.zeros(n)
v = np.zeros(n)
x = np.zeros(n)
<span style="color: #228B22"># Initial conditions (can change to more than one dim)</span>
x0 =  <span style="color: #B452CD">1.0</span> 
v0 = <span style="color: #B452CD">0.0</span>
x[<span style="color: #B452CD">0</span>] = x0
v[<span style="color: #B452CD">0</span>] = v0
gamma = <span style="color: #B452CD">0.2</span>
Omegatilde = <span style="color: #B452CD">0.5</span>
Ftilde = <span style="color: #B452CD">1.0</span>
<span style="color: #228B22"># Start integrating using Euler&#39;s method</span>
<span style="color: #228B22"># Note that we define the force function as a SpringForce</span>
RK4(v,x,t,n,SpringForce)

<span style="color: #228B22"># Plot position as function of time    </span>
fig, ax = plt.subplots()
ax.set_ylabel(<span style="color: #CD5555">&#39;x[m]&#39;</span>)
ax.set_xlabel(<span style="color: #CD5555">&#39;t[s]&#39;</span>)
ax.plot(t, x)
fig.tight_layout()
save_fig(<span style="color: #CD5555">&quot;ForcedBlockRK4&quot;</span>)
plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section>
<h2 id="using-the-above-data-to-train-an-rnn">Using the above data to train an RNN </h2>

<p>In the code here we have reworked the previous example in order to generate data that can be handled by recurrent neural networks in order to train our model.</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="font-size: 80%; line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">torch.nn</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">nn</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">torch.utils.data</span> <span style="color: #8B008B; font-weight: bold">import</span> Dataset, DataLoader
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>


<span style="color: #228B22"># Newton&#39;s equation for harmonic oscillations with external force</span>

<span style="color: #228B22"># Global parameters</span>
gamma = <span style="color: #B452CD">0.2</span>        <span style="color: #228B22"># damping</span>
Omegatilde = <span style="color: #B452CD">0.5</span>   <span style="color: #228B22"># driving frequency</span>
Ftilde = <span style="color: #B452CD">1.0</span>       <span style="color: #228B22"># driving amplitude</span>

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">spring_force</span>(v, x, t):
    <span style="color: #CD5555">&quot;&quot;&quot;</span>
<span style="color: #CD5555">    SpringForce:</span>
<span style="color: #CD5555">    note: divided by mass =&gt; returns acceleration</span>
<span style="color: #CD5555">    a = -2*gamma*v - x + Ftilde*cos(Omegatilde * t)</span>
<span style="color: #CD5555">    &quot;&quot;&quot;</span>
    <span style="color: #8B008B; font-weight: bold">return</span> -<span style="color: #B452CD">2.0</span> * gamma * v - x + Ftilde * np.cos(Omegatilde * t)


<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">rk4_trajectory</span>(DeltaT=<span style="color: #B452CD">0.001</span>, tfinal=<span style="color: #B452CD">20.0</span>, x0=<span style="color: #B452CD">1.0</span>, v0=<span style="color: #B452CD">0.0</span>):
    <span style="color: #CD5555">&quot;&quot;&quot;</span>
<span style="color: #CD5555">    Returns t, x, v arrays.</span>
<span style="color: #CD5555">    &quot;&quot;&quot;</span>
    n = <span style="color: #658b00">int</span>(np.ceil(tfinal / DeltaT))

    t = np.zeros(n, dtype=np.float32)
    x = np.zeros(n, dtype=np.float32)
    v = np.zeros(n, dtype=np.float32)

    x[<span style="color: #B452CD">0</span>] = x0
    v[<span style="color: #B452CD">0</span>] = v0

    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n - <span style="color: #B452CD">1</span>):
        <span style="color: #228B22"># k1</span>
        k1x = DeltaT * v[i]
        k1v = DeltaT * spring_force(v[i], x[i], t[i])

        <span style="color: #228B22"># k2</span>
        vv = v[i] + <span style="color: #B452CD">0.5</span> * k1v
        xx = x[i] + <span style="color: #B452CD">0.5</span> * k1x
        k2x = DeltaT * vv
        k2v = DeltaT * spring_force(vv, xx, t[i] + <span style="color: #B452CD">0.5</span> * DeltaT)

        <span style="color: #228B22"># k3</span>
        vv = v[i] + <span style="color: #B452CD">0.5</span> * k2v
        xx = x[i] + <span style="color: #B452CD">0.5</span> * k2x
        k3x = DeltaT * vv
        k3v = DeltaT * spring_force(vv, xx, t[i] + <span style="color: #B452CD">0.5</span> * DeltaT)

        <span style="color: #228B22"># k4</span>
        vv = v[i] + k3v
        xx = x[i] + k3x
        k4x = DeltaT * vv
        k4v = DeltaT * spring_force(vv, xx, t[i] + DeltaT)

        <span style="color: #228B22"># Update</span>
        x[i + <span style="color: #B452CD">1</span>] = x[i] + (k1x + <span style="color: #B452CD">2.0</span> * k2x + <span style="color: #B452CD">2.0</span> * k3x + k4x) / <span style="color: #B452CD">6.0</span>
        v[i + <span style="color: #B452CD">1</span>] = v[i] + (k1v + <span style="color: #B452CD">2.0</span> * k2v + <span style="color: #B452CD">2.0</span> * k3v + k4v) / <span style="color: #B452CD">6.0</span>
        t[i + <span style="color: #B452CD">1</span>] = t[i] + DeltaT

    <span style="color: #8B008B; font-weight: bold">return</span> t, x, v


<span style="color: #228B22"># Sequence generation for RNN training</span>

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">create_sequences</span>(x, seq_len):
    <span style="color: #CD5555">&quot;&quot;&quot;</span>
<span style="color: #CD5555">    Given a 1D array x (e.g., position as a function of time),</span>
<span style="color: #CD5555">    create input/target sequences for next-step prediction.</span>

<span style="color: #CD5555">    Inputs:  [x_i, x_{i+1}, ..., x_{i+seq_len-1}]</span>
<span style="color: #CD5555">    Targets: [x_{i+1}, ..., x_{i+seq_len}]</span>
<span style="color: #CD5555">    &quot;&quot;&quot;</span>
    xs = []
    ys = []
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #658b00">len</span>(x) - seq_len):
        seq_x = x[i : i + seq_len]
        seq_y = x[i + <span style="color: #B452CD">1</span> : i + seq_len + <span style="color: #B452CD">1</span>]  <span style="color: #228B22"># shifted by one step</span>
        xs.append(seq_x)
        ys.append(seq_y)

    xs = np.array(xs, dtype=np.float32)      <span style="color: #228B22"># shape: (num_samples, seq_len)</span>
    ys = np.array(ys, dtype=np.float32)      <span style="color: #228B22"># shape: (num_samples, seq_len)</span>
    <span style="color: #228B22"># Add feature dimension (1 feature: the position x)</span>
    xs = np.expand_dims(xs, axis=-<span style="color: #B452CD">1</span>)         <span style="color: #228B22"># (num_samples, seq_len, 1)</span>
    ys = np.expand_dims(ys, axis=-<span style="color: #B452CD">1</span>)         <span style="color: #228B22"># (num_samples, seq_len, 1)</span>
    <span style="color: #8B008B; font-weight: bold">return</span> xs, ys


<span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">OscillatorDataset</span>(Dataset):
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__init__</span>(<span style="color: #658b00">self</span>, seq_len=<span style="color: #B452CD">50</span>, DeltaT=<span style="color: #B452CD">0.001</span>, tfinal=<span style="color: #B452CD">20.0</span>, x0=<span style="color: #B452CD">1.0</span>, v0=<span style="color: #B452CD">0.0</span>):
        t, x, v = rk4_trajectory(DeltaT=DeltaT, tfinal=tfinal, x0=x0, v0=v0)
        <span style="color: #658b00">self</span>.t = t
        <span style="color: #658b00">self</span>.x = x
        <span style="color: #658b00">self</span>.v = v
        xs, ys = create_sequences(x, seq_len=seq_len)
        <span style="color: #658b00">self</span>.inputs = torch.from_numpy(xs)  <span style="color: #228B22"># (N, seq_len, 1)</span>
        <span style="color: #658b00">self</span>.targets = torch.from_numpy(ys) <span style="color: #228B22"># (N, seq_len, 1)</span>

    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__len__</span>(<span style="color: #658b00">self</span>):
        <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">self</span>.inputs.shape[<span style="color: #B452CD">0</span>]

    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__getitem__</span>(<span style="color: #658b00">self</span>, idx):
        <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">self</span>.inputs[idx], <span style="color: #658b00">self</span>.targets[idx]


<span style="color: #228B22"># RNN model (LSTM-based in this example)</span>

<span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">RNNPredictor</span>(nn.Module):
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__init__</span>(<span style="color: #658b00">self</span>, input_size=<span style="color: #B452CD">1</span>, hidden_size=<span style="color: #B452CD">32</span>, num_layers=<span style="color: #B452CD">1</span>, output_size=<span style="color: #B452CD">1</span>):
        <span style="color: #658b00">super</span>().<span style="color: #008b45">__init__</span>()
        <span style="color: #658b00">self</span>.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=<span style="color: #8B008B; font-weight: bold">True</span>)
        <span style="color: #658b00">self</span>.fc = nn.Linear(hidden_size, output_size)

    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">forward</span>(<span style="color: #658b00">self</span>, x):
        <span style="color: #228B22"># x: (batch, seq_len, input_size)</span>
        out, _ = <span style="color: #658b00">self</span>.lstm(x)   <span style="color: #228B22"># out: (batch, seq_len, hidden_size)</span>
        out = <span style="color: #658b00">self</span>.fc(out)      <span style="color: #228B22"># (batch, seq_len, output_size)</span>
        <span style="color: #8B008B; font-weight: bold">return</span> out


<span style="color: #228B22"># Training part</span>

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">train_model</span>(
    seq_len=<span style="color: #B452CD">50</span>,
    DeltaT=<span style="color: #B452CD">0.001</span>,
    tfinal=<span style="color: #B452CD">20.0</span>,
    batch_size=<span style="color: #B452CD">64</span>,
    num_epochs=<span style="color: #B452CD">10</span>,
    hidden_size=<span style="color: #B452CD">64</span>,
    lr=<span style="color: #B452CD">1e-3</span>,
    device=<span style="color: #8B008B; font-weight: bold">None</span>,
):
    <span style="color: #8B008B; font-weight: bold">if</span> device <span style="color: #8B008B">is</span> <span style="color: #8B008B; font-weight: bold">None</span>:
        device = torch.device(<span style="color: #CD5555">&quot;cuda&quot;</span> <span style="color: #8B008B; font-weight: bold">if</span> torch.cuda.is_available() <span style="color: #8B008B; font-weight: bold">else</span> <span style="color: #CD5555">&quot;cpu&quot;</span>)
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&quot;Using device: {</span>device<span style="color: #CD5555">}&quot;</span>)

    <span style="color: #228B22"># Dataset &amp; DataLoader</span>
    dataset = OscillatorDataset(seq_len=seq_len, DeltaT=DeltaT, tfinal=tfinal)
    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=<span style="color: #8B008B; font-weight: bold">True</span>)

    <span style="color: #228B22"># Model, loss, optimizer</span>
    model = RNNPredictor(input_size=<span style="color: #B452CD">1</span>, hidden_size=hidden_size, output_size=<span style="color: #B452CD">1</span>)
    model.to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    <span style="color: #228B22"># Training loop</span>
    model.train()
    <span style="color: #8B008B; font-weight: bold">for</span> epoch <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(num_epochs):
        epoch_loss = <span style="color: #B452CD">0.0</span>
        <span style="color: #8B008B; font-weight: bold">for</span> batch_x, batch_y <span style="color: #8B008B">in</span> train_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)

            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item() * batch_x.size(<span style="color: #B452CD">0</span>)

        epoch_loss /= <span style="color: #658b00">len</span>(train_loader.dataset)
        <span style="color: #658b00">print</span>(<span style="color: #CD5555">f&quot;Epoch {</span>epoch+<span style="color: #B452CD">1</span><span style="color: #CD5555">}/{</span>num_epochs<span style="color: #CD5555">}, Loss: {</span>epoch_loss<span style="color: #CD5555">:.6f}&quot;</span>)

    <span style="color: #8B008B; font-weight: bold">return</span> model, dataset


<span style="color: #228B22"># Evaluation / visualization</span>

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">evaluate_and_plot</span>(model, dataset, seq_len=<span style="color: #B452CD">50</span>, device=<span style="color: #8B008B; font-weight: bold">None</span>):
    <span style="color: #8B008B; font-weight: bold">if</span> device <span style="color: #8B008B">is</span> <span style="color: #8B008B; font-weight: bold">None</span>:
        device = <span style="color: #658b00">next</span>(model.parameters()).device

    model.eval()
    <span style="color: #8B008B; font-weight: bold">with</span> torch.no_grad():
        <span style="color: #228B22"># Take a single sequence from the dataset</span>
        x_seq, y_seq = dataset[<span style="color: #B452CD">0</span>]  <span style="color: #228B22"># shapes: (seq_len, 1), (seq_len, 1)</span>
        x_input = x_seq.unsqueeze(<span style="color: #B452CD">0</span>).to(device)  <span style="color: #228B22"># (1, seq_len, 1)</span>
        <span style="color: #228B22"># Model prediction (next-step for whole sequence)</span>
        y_pred = model(x_input).cpu().numpy().squeeze(-<span style="color: #B452CD">1</span>).squeeze(<span style="color: #B452CD">0</span>)  <span style="color: #228B22"># (seq_len,)</span>
        <span style="color: #228B22"># True target</span>
        y_true = y_seq.numpy().squeeze(-<span style="color: #B452CD">1</span>)  <span style="color: #228B22"># (seq_len,)</span>
        <span style="color: #228B22"># Plot comparison</span>
        plt.figure()
        plt.plot(y_true, label=<span style="color: #CD5555">&quot;True x(t+t)&quot;</span>, linestyle=<span style="color: #CD5555">&quot;-&quot;</span>)
        plt.plot(y_pred, label=<span style="color: #CD5555">&quot;Predicted x(t+t)&quot;</span>, linestyle=<span style="color: #CD5555">&quot;--&quot;</span>)
        plt.xlabel(<span style="color: #CD5555">&quot;Time step in sequence&quot;</span>)
        plt.ylabel(<span style="color: #CD5555">&quot;Position&quot;</span>)
        plt.legend()
        plt.title(<span style="color: #CD5555">&quot;RNN next-step prediction on oscillator trajectory&quot;</span>)
        plt.tight_layout()
        plt.show()

<span style="color: #228B22"># This is the main part of the code where we define the network</span>

<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #00688B">__name__</span> == <span style="color: #CD5555">&quot;__main__&quot;</span>:
    <span style="color: #228B22"># Hyperparameters can be tweaked as you like</span>
    seq_len = <span style="color: #B452CD">50</span>
    DeltaT = <span style="color: #B452CD">0.001</span>
    tfinal = <span style="color: #B452CD">20.0</span>
    num_epochs = <span style="color: #B452CD">10</span>
    batch_size = <span style="color: #B452CD">64</span>
    hidden_size = <span style="color: #B452CD">64</span>
    lr = <span style="color: #B452CD">1e-3</span>

    model, dataset = train_model(
        seq_len=seq_len,
        DeltaT=DeltaT,
        tfinal=tfinal,
        batch_size=batch_size,
        num_epochs=num_epochs,
        hidden_size=hidden_size,
        lr=lr,
    )

    evaluate_and_plot(model, dataset, seq_len=seq_len)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

  // Display navigation controls in the bottom right corner
  controls: true,

  // Display progress bar (below the horiz. slider)
  progress: true,

  // Display the page number of the current slide
  slideNumber: true,

  // Push each slide change to the browser history
  history: false,

  // Enable keyboard shortcuts for navigation
  keyboard: true,

  // Enable the slide overview mode
  overview: true,

  // Vertical centering of slides
  //center: true,
  center: false,

  // Enables touch navigation on devices with touch input
  touch: true,

  // Loop the presentation
  loop: false,

  // Change the presentation direction to be RTL
  rtl: false,

  // Turns fragments on and off globally
  fragments: true,

  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,

  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,

  // Stop auto-sliding after user input
  autoSlideStoppable: true,

  // Enable slide navigation via mouse wheel
  mouseWheel: false,

  // Hides the address bar on mobile devices
  hideAddressBar: true,

  // Opens links in an iframe preview overlay
  previewLinks: false,

  // Transition style
  transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

  // Transition speed
  transitionSpeed: 'default', // default/fast/slow

  // Transition style for full page slide backgrounds
  backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

  // Number of slides away from the current that are visible
  viewDistance: 3,

  // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

  // Parallax background size
  //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

  theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'none', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
  dependencies: [
      // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
      { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

      // Interpret Markdown in <section> elements
      { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

      // Syntax highlight for <code> elements
      { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

      // Zoom in and out with Alt+click
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

      // Speaker notes
      { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

      // Remote control your reveal.js presentation using a touch device
      //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

      // MathJax
      //{ src: 'reveal.js/plugin/math/math.js', async: true }
  ]
});

Reveal.initialize({

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 1170,  // original: 960,
  height: 700,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
   end footer logo -->




</body>
</html>
