<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Data Analysis and Machine Learning: Linear Regression and more Advanced Regression Analysis">

<title>Data Analysis and Machine Learning: Linear Regression and more Advanced Regression Analysis</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Regression analysis, overarching aims', 2, None, '___sec0'),
              ('Regression analysis, overarching aims II', 2, None, '___sec1'),
              ('General linear models', 2, None, '___sec2'),
              ('Rewriting the fitting procedure as a linear algebra problem',
               2,
               None,
               '___sec3'),
              ('Rewriting the fitting procedure as a linear algebra problem, '
               'follows',
               2,
               None,
               '___sec4'),
              ('Generalizing the fitting procedure as a linear algebra problem',
               2,
               None,
               '___sec5'),
              ('Generalizing the fitting procedure as a linear algebra problem',
               2,
               None,
               '___sec6'),
              ('Optimizing our parameters', 2, None, '___sec7'),
              ('Optimizing our parameters, more details', 2, None, '___sec8'),
              ('Interpretations and optimizing our parameters',
               2,
               None,
               '___sec9'),
              ('Interpretations and optimizing our parameters',
               2,
               None,
               '___sec10'),
              ('Interpretations and optimizing our parameters',
               2,
               None,
               '___sec11'),
              ('The $\\chi^2$ function', 2, None, '___sec12'),
              ('The $\\chi^2$ function', 2, None, '___sec13'),
              ('The $\\chi^2$ function', 2, None, '___sec14'),
              ('The $\\chi^2$ function', 2, None, '___sec15'),
              ('The $\\chi^2$ function', 2, None, '___sec16'),
              ('The $\\chi^2$ function', 2, None, '___sec17'),
              ('Simple regression model', 2, None, '___sec18'),
              ('Simple regression model, now using _scikit-learn_',
               2,
               None,
               '___sec19'),
              ('Simple linear regression model using _scikit-learn_',
               2,
               None,
               '___sec20'),
              ('Simple linear regression model', 2, None, '___sec21'),
              ('Less noise', 2, None, '___sec22'),
              ('How to study our fits', 2, None, '___sec23'),
              ('Minimizing the cost function', 2, None, '___sec24'),
              ('Relative error', 2, None, '___sec25'),
              ('The richness of _scikit-learn_', 2, None, '___sec26'),
              ('Functions in _scikit-learn_', 2, None, '___sec27'),
              ('Other functions in  _scikit-learn_', 2, None, '___sec28'),
              ('The mean absolute error and other functions in  _scikit-learn_',
               2,
               None,
               '___sec29'),
              ('Cubic polynomial in  _scikit-learn_', 2, None, '___sec30'),
              ('Polynomial Regression', 2, None, '___sec31'),
              ('Linking the regression analysis with a statistical '
               'interpretation',
               2,
               None,
               '___sec32'),
              ('Expectation value and variance', 2, None, '___sec33'),
              ('The singular value decompostion', 2, None, '___sec34'),
              ('Code examples for Ridge and Lasso Regression',
               2,
               None,
               '___sec35'),
              ('From standard regression to Ridge regressions',
               2,
               None,
               '___sec36'),
              ('Fixing the singularity', 2, None, '___sec37'),
              ('A second-order polynomial with Ridge and Lasso',
               2,
               None,
               '___sec38'),
              ('Fitting vs. predicting when data is in the model class',
               2,
               None,
               '___sec39'),
              ('Fitting versus predicting when data is not in the model class',
               2,
               None,
               '___sec40'),
              ('The code', 2, None, '___sec41'),
              ('Generating test data', 2, None, '___sec42'),
              ('Lasso regression', 2, None, '___sec43'),
              ('Logistic regression', 2, None, '___sec44')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="Regression-bs.html">Data Analysis and Machine Learning: Linear Regression and more Advanced Regression Analysis</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._Regression-bs001.html#___sec0" style="font-size: 80%;">Regression analysis, overarching aims</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs002.html#___sec1" style="font-size: 80%;">Regression analysis, overarching aims II</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs003.html#___sec2" style="font-size: 80%;">General linear models</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs004.html#___sec3" style="font-size: 80%;">Rewriting the fitting procedure as a linear algebra problem</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs005.html#___sec4" style="font-size: 80%;">Rewriting the fitting procedure as a linear algebra problem, follows</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs006.html#___sec5" style="font-size: 80%;">Generalizing the fitting procedure as a linear algebra problem</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs007.html#___sec6" style="font-size: 80%;">Generalizing the fitting procedure as a linear algebra problem</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs008.html#___sec7" style="font-size: 80%;">Optimizing our parameters</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs009.html#___sec8" style="font-size: 80%;">Optimizing our parameters, more details</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs010.html#___sec9" style="font-size: 80%;">Interpretations and optimizing our parameters</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs011.html#___sec10" style="font-size: 80%;">Interpretations and optimizing our parameters</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs012.html#___sec11" style="font-size: 80%;">Interpretations and optimizing our parameters</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs013.html#___sec12" style="font-size: 80%;">The \( \chi^2 \) function</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs014.html#___sec13" style="font-size: 80%;">The \( \chi^2 \) function</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs015.html#___sec14" style="font-size: 80%;">The \( \chi^2 \) function</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs016.html#___sec15" style="font-size: 80%;">The \( \chi^2 \) function</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs017.html#___sec16" style="font-size: 80%;">The \( \chi^2 \) function</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs018.html#___sec17" style="font-size: 80%;">The \( \chi^2 \) function</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs019.html#___sec18" style="font-size: 80%;">Simple regression model</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs020.html#___sec19" style="font-size: 80%;">Simple regression model, now using <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec20" style="font-size: 80%;">Simple linear regression model using <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs022.html#___sec21" style="font-size: 80%;">Simple linear regression model</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs023.html#___sec22" style="font-size: 80%;">Less noise</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs024.html#___sec23" style="font-size: 80%;">How to study our fits</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs025.html#___sec24" style="font-size: 80%;">Minimizing the cost function</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs026.html#___sec25" style="font-size: 80%;">Relative error</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs027.html#___sec26" style="font-size: 80%;">The richness of <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs028.html#___sec27" style="font-size: 80%;">Functions in <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs029.html#___sec28" style="font-size: 80%;">Other functions in  <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs030.html#___sec29" style="font-size: 80%;">The mean absolute error and other functions in  <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs031.html#___sec30" style="font-size: 80%;">Cubic polynomial in  <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs032.html#___sec31" style="font-size: 80%;">Polynomial Regression</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs033.html#___sec32" style="font-size: 80%;">Linking the regression analysis with a statistical interpretation</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs034.html#___sec33" style="font-size: 80%;">Expectation value and variance</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs035.html#___sec34" style="font-size: 80%;">The singular value decompostion</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs036.html#___sec35" style="font-size: 80%;">Code examples for Ridge and Lasso Regression</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs037.html#___sec36" style="font-size: 80%;">From standard regression to Ridge regressions</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs038.html#___sec37" style="font-size: 80%;">Fixing the singularity</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs039.html#___sec38" style="font-size: 80%;">A second-order polynomial with Ridge and Lasso</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs040.html#___sec39" style="font-size: 80%;">Fitting vs. predicting when data is in the model class</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs041.html#___sec40" style="font-size: 80%;">Fitting versus predicting when data is not in the model class</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs042.html#___sec41" style="font-size: 80%;">The code</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs043.html#___sec42" style="font-size: 80%;">Generating test data</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs044.html#___sec43" style="font-size: 80%;">Lasso regression</a></li>
     <!-- navigation toc: --> <li><a href="._Regression-bs045.html#___sec44" style="font-size: 80%;">Logistic regression</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0021"></a>
<!-- !split -->

<h2 id="___sec20" class="anchor">Simple linear regression model using <b>scikit-learn</b> </h2>

<p>
We start with perhaps our simplest possible example, using <b>scikit-learn</b> to perform linear regression analysis on a data set produced by us. 
What follows is a simple Python code where we have defined  function \( y \) in terms of the variable \( x \). Both are defined as vectors of dimension \( 1\times 100 \). The entries to the vector \( \hat{x} \)  are given by random numbers generated with a uniform distribution with entries \( x_i \in [0,1] \) (more about probability distribution functions later). These values are then used to define a function \( y(x) \) (tabulated again as a vector) with a linear dependence on \( x \) plus a random noise added via the normal distribution.

<p>
The Numpy functions are imported used the <b>import numpy as np</b>
statement and the random number generator for the uniform distribution
is called using the function <b>np.random.rand()</b>, where we specificy
that we want \( 100 \) random variables.  Using Numpy we define
automatically an array with the specified number of elements, \( 100 \) in
our case.  With the Numpy function <b>randn()</b> we can compute random
numbers with the normal distribution (mean value \( \mu \) equal to zero and
variance \( \sigma^2 \) set to one) and produce the values of \( y \) assuming a linear
dependence as function of \( x \)

$$
y = 2x+N(0,1),
$$

<p>
where \( N(0,1) \) represents random numbers generated by the normal
distribution.  From <b>scikit-learn</b> we import then the
<b>LinearRegression</b> functionality and make a prediction \( \tilde{y} =
\alpha + \beta x \) using the function <b>fit(x,y)</b>. We call the set of
data \( (\hat{x},\hat{y}) \) for our training data. The Python package
<b>scikit-learn</b> has also a functionality which extracts the above
fitting parameters \( \alpha \) and \( \beta \) (see below). Later we will
distinguish between training data and test data.

<p>
For plotting we use the Python package
<a href="https://matplotlib.org/" target="_self">matplotlib</a> which produces publication
quality figures. Feel free to explore the extensive
<a href="https://matplotlib.org/gallery/index.html" target="_self">gallery</a> of examples. In
this example we plot our original values of \( x \) and \( y \) as well as the
prediction <b>ypredict</b> (\( \tilde{y} \)), which attempts at fitting our
data with a straight line.

<p>
The Python code follows here.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #408080; font-style: italic"># Importing various packages</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2*</span>x<span style="color: #666666">+</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
linreg <span style="color: #666666">=</span> LinearRegression()
linreg<span style="color: #666666">.</span>fit(x,y)
xnew <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[<span style="color: #666666">0</span>],[<span style="color: #666666">1</span>]])
ypredict <span style="color: #666666">=</span> linreg<span style="color: #666666">.</span>predict(xnew)

plt<span style="color: #666666">.</span>plot(xnew, ypredict, <span style="color: #BA2121">&quot;r-&quot;</span>)
plt<span style="color: #666666">.</span>plot(x, y ,<span style="color: #BA2121">&#39;ro&#39;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0</span>,<span style="color: #666666">1.0</span>,<span style="color: #666666">0</span>, <span style="color: #666666">5.0</span>])
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">r&#39;$x$&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">r&#39;$y$&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">r&#39;Simple Linear Regression&#39;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
<li><a href="._Regression-bs020.html">&laquo;</a></li>
  <li><a href="._Regression-bs000.html">1</a></li>
  <li><a href="">...</a></li>
  <li><a href="._Regression-bs013.html">14</a></li>
  <li><a href="._Regression-bs014.html">15</a></li>
  <li><a href="._Regression-bs015.html">16</a></li>
  <li><a href="._Regression-bs016.html">17</a></li>
  <li><a href="._Regression-bs017.html">18</a></li>
  <li><a href="._Regression-bs018.html">19</a></li>
  <li><a href="._Regression-bs019.html">20</a></li>
  <li><a href="._Regression-bs020.html">21</a></li>
  <li class="active"><a href="._Regression-bs021.html">22</a></li>
  <li><a href="._Regression-bs022.html">23</a></li>
  <li><a href="._Regression-bs023.html">24</a></li>
  <li><a href="._Regression-bs024.html">25</a></li>
  <li><a href="._Regression-bs025.html">26</a></li>
  <li><a href="._Regression-bs026.html">27</a></li>
  <li><a href="._Regression-bs027.html">28</a></li>
  <li><a href="._Regression-bs028.html">29</a></li>
  <li><a href="._Regression-bs029.html">30</a></li>
  <li><a href="._Regression-bs030.html">31</a></li>
  <li><a href="">...</a></li>
  <li><a href="._Regression-bs045.html">46</a></li>
  <li><a href="._Regression-bs022.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright only on the titlepage -->
</center>


</body>
</html>
    

