TITLE: Exercise week 47
AUTHOR: November 18-22, 2024
DATE: Deadline is Friday November 22 at midnight


=======  Overarching aims of the exercises this week =======

The exercise set this week is meant as a summary of many of the
central elements in various machine learning algorithms, with a slight
bias towards deep learning methods and their training. You don't need to answer all questions.

The last weekly exercise (week 48) is a general course survey.


===== Exercise 1: Linear and logistic regression methods  =====

o What is the main difference between ordinary least squares and Ridge regression?
o Which kind of data set would you use logistic regression for?
o In linear regression you assume that your output is described by a continuous non-stochastic function $f(x)$. Which is the equivalent function in logistic regression?
o Can you find an analytic solution to a logistic regression type of problem?
o What kind of cost function would you use in logistic regression?


===== Exercise 2: Deep learning  =====

o What is an activation function and discuss the use of an activation function? Explain three different types of activation functions?
o Describe the architecture of a typical feed forward  Neural Network (NN). 
o You are using a deep neural network for a prediction task. After training your model, you notice that it is strongly overfitting the training set and that the performance on the test isnâ€™t good. What can you do to reduce overfitting?
o How would you know if your model is suffering from the problem of exploding gradients?
o Can you name and explain a few hyperparameters used for training a neural network?
o Describe the architecture of a typical Convolutional Neural Network (CNN)
o What is the vanishing gradient problem in Neural Networks and how to fix it?
o When it comes to training an artificial neural network, what could the reason be for why the cost/loss doesn't decrease in a few epochs?
o How does L1/L2 regularization affect a neural network?
o What is(are) the advantage(s) of deep learning over traditional methods like linear regression or logistic regression?


===== Exercise 3: Optimization part =====

o Which is the basic mathematical root-finding method behind essentially all gradient descent approaches(stochastic and non-stochastic)? 
o And why don't we use it? Or stated differently, why do we introduce the learning rate as a parameter?
o What might happen if you set the momentum hyperparameter too close to 1 (e.g., 0.9999) when using an optimizer for the learning rate?
o Why should we use stochastic gradient descent instead of plain gradient descent?
o Which parameters would you need to tune when use a stochastic gradient descent approach?



===== Exercise 4: Analysis of results =====
o How do you assess overfitting and underfitting?
o Why do we divide the data in test and train and/or eventually validation sets?
o Why would you use resampling methods in the data analysis? Mention some widely popular resampling methods.
