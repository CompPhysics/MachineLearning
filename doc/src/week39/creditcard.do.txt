TITLE: Week 40: Credit card example
AUTHOR: Morten Hjorth-Jensen {copyright, 1999-present|CC BY-NC} at Department of Physics, University of Oslo
DATE: Week 40


!split
===== Example case: Logistic Regression on Taiwan Credit Card Default Dataset =====



=== Dataset Overview and Preparation ===


The Default of Credit Card Clients dataset (Taiwan credit card default
data) contains 30,000 instances and 23 features (plus an ID and a
binary default indicator). The target variable is _default payment
next month_ (1 = default, 0 = no default) . Key characteristics of the
data include:

o Instances: 30,000 card clients (each a unique ID) 
o Features: 23 predictive features (mostly numeric, including demographics, credit limit, payment history, bill amounts, etc.) 
o Target: Binary default indicator for the next month (Yes=1, No=0) 
o Missing Values: None – the dataset has no missing values recorded 
o Class Distribution: 22.1% of clients (6,636) defaulted vs 77.9% non-default  (i.e. the data is imbalanced about 1:3).
o Basic Stats: Average credit limit is around NT$167,500, and mean age is ~35.5 years . Bill statement amounts are on the order of tens of thousands NT (mean ≈ 40k), while monthly payment amounts average a few thousand NT (mean ≈ 5k) .


=== Feature details ===

All features are numeric. Some are categorical in nature (encoded as
integers): e.g. SEX (1=male, 2=female), EDUCATION (1=graduate,
2=university, 3=high school, 4=others, with 5-6 as unknown) and
MARRIAGE (1=married, 2=single, 3=others) . Others are continuous
(credit amount, bill amounts, payments) or ordinal (e.g. $PAY_0 – PAY_6$
repayment status, where -1 = pay duly, 1 = 1 month delay, 2 = 2 months
delay, etc.) .

=== Preprocessing steps ===

For binary classification, we prepare the data as follows:

o Remove the ID column (identifier not useful for prediction).
o Use the 23 remaining features as $X$ and the _default payment next month_ as the binary target $y$.
o The categorical features (SEX, EDUCATION, MARRIAGE) are already integer-encoded; for simplicity, we will use them as-is (one could one-hot encode these, but it’s not strictly necessary for logistic regression).
o Standardize the feature values – i.e. scale each feature to zero mean and unit variance. This helps the gradient descent in logistic regression converge faster.
o Split the dataset into a training set and testing set (we’ll use an 80/20 split). The model will be trained on the training set and evaluated on the test set to gauge performance on unseen data.


!split
===== Implementing Logistic Regression =====


We will implement a logistic regression classifier using only NumPy,
without any machine learning libraries. Logistic regression computes a
weighted linear combination of features (plus a bias term) and applies
a sigmoid function $\sigma(z) = 1/(1+e^{-z})$ to produce a probability
between 0 and 1. We train the model by minimizing the logistic loss
(binary cross-entropy) using gradient descent.

Below is a single Python script that loads and preprocesses the data,
then defines and trains the logistic regression model using gradient
descent. It also outputs the model’s performance metrics (accuracy and
loss). The file can be downloaded from the UCI database at URL:"https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients"

!bc pycod
import numpy as np
import pandas as pd

# Load the dataset (ensure the CSV file is in the working directory)
df = pd.read_csv('credit_card_clients.csv')

# Preprocess the data
df = df.drop('ID', axis=1)  # drop ID column
# Rename the target column for convenience (optional)
df = df.rename(columns={'default payment next month': 'default'})
# Separate features and target
X = df.drop('default', axis=1).values  # features (shape: [30000, 23])
y = df['default'].values              # target (shape: [30000,])

# Standardize features (zero mean, unit std dev)
X_mean = X.mean(axis=0)
X_std = X.std(axis=0)
X = (X - X_mean) / X_std

# Split into training and test sets (80/20 split)
m = X.shape[0]
train_size = int(0.8 * m)
indices = np.random.permutation(m)  # shuffle indices for randomness
train_idx, test_idx = indices[:train_size], indices[train_size:]
X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]

# Add intercept term (bias) to features by adding a column of 1s
X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])
X_test  = np.hstack([np.ones((X_test.shape[0], 1)), X_test])

# Initialize logistic regression parameters
n_features = X_train.shape[1]  # number of features including bias
theta = np.zeros(n_features)   # model weights (initialized to 0)

# Set training hyperparameters
learning_rate = 0.1
epochs = 500

# Training loop (gradient descent)
m_train = X_train.shape[0]
for epoch in range(epochs):
    # Compute predictions (sigmoid of linear combination)
    z = X_train.dot(theta)                   # linear combination
    predictions = 1 / (1 + np.exp(-z))       # sigmoid function
    
    # Compute the gradient of loss w.r.t. theta
    error = predictions - y_train            # vector of (pred - true) for each example
    grad = (X_train.T.dot(error)) / m_train  # gradient vector
    
    # Update weights
    theta -= learning_rate * grad
    
    # (Optional) compute and print loss every 50 epochs for monitoring
    if epoch % 50 == 0:
        # Binary cross-entropy loss
        loss = -np.mean(y_train * np.log(predictions + 1e-15) + 
                        (1 - y_train) * np.log(1 - predictions + 1e-15))
        print(f"Epoch {epoch}: Training loss = {loss:.4f}")

# 5. Evaluate the model on training and test data
# Predict probabilities for test set and classify as 1 if sigmoid >= 0.5
# (We can equivalently check linear term >= 0, since sigma(x)>=0.5 iff x>=0)
train_prob = 1 / (1 + np.exp(-X_train.dot(theta)))
test_prob  = 1 / (1 + np.exp(-X_test.dot(theta)))
train_pred = (train_prob >= 0.5).astype(int)
test_pred  = (test_prob  >= 0.5).astype(int)

# Calculate accuracy
train_accuracy = (train_pred == y_train).mean()
test_accuracy = (test_pred == y_test).mean()

# Calculate final loss on training set for reference
final_loss = -np.mean(y_train * np.log(train_prob + 1e-15) + 
                      (1 - y_train) * np.log(1 - train_prob + 1e-15))

# Print performance metrics
print(f"Final Training Loss: {final_loss:.4f}")
print(f"Training Accuracy: {train_accuracy * 100:.2f}%")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
!ec


Explanation: We initialize the weight vector theta (including a bias term) to zeros. In each training epoch, we compute the logistic predictions for all training examples and then update the weights using the gradient of the cross-entropy loss. The learning rate and number of epochs are set to fixed values (which can be tuned). We periodically print the training loss to ensure the model is learning (decreasing loss). After training, we obtain predicted probabilities on the train and test sets, convert them to class labels (threshold 0.5), and compute the accuracy.

!split
===== Model performance and results =====


After training, the code prints out the final loss and accuracies. You should observe that the training and test accuracy are around 78–80% for this logistic model. 

This performance is in line with expectations. The dataset’s class imbalance means that always predicting “no default” would already achieve ~77.9% accuracy . Our logistic regression does slightly better, around 79–81% accuracy, by identifying some of the default cases correctly. (In one reference implementation, logistic regression achieved ~81.3% accuracy on a test set .)

The model’s loss converges to a reasonable value, indicating the gradient descent optimization was successful.

Note: The relatively modest improvement over the baseline is due to the imbalance and the limited complexity of a linear model. In practice, one could improve default prediction by using more complex models or addressing the imbalance (e.g. with oversampling or adjusting the classification threshold). Nonetheless, the above example demonstrates the complete process: data loading, preprocessing, model training, and evaluation, fulfilling the binary classification task on the Taiwan credit card default dataset.


