{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374491a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html ae.do.txt  -->\n",
    "<!-- dom:TITLE: Week 48: Autoencoders -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793792e8",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Week 48: Autoencoders\n",
    "**Morten Hjorth-Jensen**, Department of Physics and Center for Computing in Science Education, University of Oslo, Norway\n",
    "\n",
    "Date: **Nov 24, 2025**\n",
    "\n",
    "Copyright 1999-2025, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236170ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Back to Autoencoders: Linear Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12263d",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Scikit-Learn >=0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow >= 2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"autoencoders\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "def generate_3d_data(m, w1=0.1, w2=0.3, noise=0.1):\n",
    "    angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "    data = np.empty((m, 3))\n",
    "    data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "    data[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "    data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * np.random.randn(m)\n",
    "    return data\n",
    "\n",
    "X_train = generate_3d_data(60)\n",
    "X_train = X_train - X_train.mean(axis=0, keepdims=0)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])\n",
    "decoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])\n",
    "autoencoder = keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "autoencoder.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1.5))\n",
    "\n",
    "codings = encoder.predict(X_train)\n",
    "fig = plt.figure(figsize=(4,3))\n",
    "plt.plot(codings[:,0], codings[:, 1], \"b.\")\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "save_fig(\"linear_autoencoder_pca_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129604ee",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More advanced features, stacked AEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525eb10",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# You can select the so-called fashion data as well, here we just use the MNIST standard set\n",
    "#(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5959c1e",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can now train all layers at once by building a stacked AE with 3 hidden layers and 1 output layer (i.e., 2 stacked Autoencoders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e026380",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def rounded_accuracy(y_true, y_pred):\n",
    "    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "stacked_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(30, activation=\"selu\"),\n",
    "])\n",
    "stacked_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\n",
    "stacked_ae.compile(loss=\"binary_crossentropy\",\n",
    "                   optimizer=keras.optimizers.SGD(learning_rate=1.5), metrics=[rounded_accuracy])\n",
    "history = stacked_ae.fit(X_train, X_train, epochs=20,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf6144",
   "metadata": {
    "editable": true
   },
   "source": [
    "This function processes a few test images through the autoencoder and\n",
    "displays the original images and their reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38531b5",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def show_reconstructions(model, images=X_valid, n_images=5):\n",
    "    reconstructions = model.predict(images[:n_images])\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plot_image(images[image_index])\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plot_image(reconstructions[image_index])\n",
    "show_reconstructions(stacked_ae)\n",
    "save_fig(\"reconstruction_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611e5fa",
   "metadata": {
    "editable": true
   },
   "source": [
    "Then visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e192ea",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "from sklearn.manifold import TSNE\n",
    "X_valid_compressed = stacked_encoder.predict(X_valid)\n",
    "tsne = TSNE()\n",
    "X_valid_2D = tsne.fit_transform(X_valid_compressed)\n",
    "X_valid_2D = (X_valid_2D - X_valid_2D.min()) / (X_valid_2D.max() - X_valid_2D.min())\n",
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451bc38",
   "metadata": {
    "editable": true
   },
   "source": [
    "And visualize in a nicer way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab12ac",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = plt.cm.tab10\n",
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=cmap)\n",
    "image_positions = np.array([[1., 1.]])\n",
    "for index, position in enumerate(X_valid_2D):\n",
    "    dist = np.sum((position - image_positions) ** 2, axis=1)\n",
    "    if np.min(dist) > 0.02: # if far enough from other images\n",
    "        image_positions = np.r_[image_positions, [position]]\n",
    "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
    "            mpl.offsetbox.OffsetImage(X_valid[index], cmap=\"binary\"),\n",
    "            position, bboxprops={\"edgecolor\": cmap(y_valid[index]), \"lw\": 2})\n",
    "        plt.gca().add_artist(imagebox)\n",
    "plt.axis(\"off\")\n",
    "save_fig(\"fashion_mnist_visualization_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed90fd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using Convolutional Layers Instead of Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae57873",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "conv_encoder = keras.models.Sequential([\n",
    "    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),\n",
    "    keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2)\n",
    "])\n",
    "conv_decoder = keras.models.Sequential([\n",
    "    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"VALID\", activation=\"selu\",\n",
    "                                 input_shape=[3, 3, 64]),\n",
    "    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "conv_ae = keras.models.Sequential([conv_encoder, conv_decoder])\n",
    "\n",
    "conv_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1.0),\n",
    "                metrics=[rounded_accuracy])\n",
    "history = conv_ae.fit(X_train, X_train, epochs=5,\n",
    "                      validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39f3cf",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "conv_encoder.summary()\n",
    "conv_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1a7bc",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_reconstructions(conv_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d46e9fb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Recurrent Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c397d4",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "recurrent_encoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(100, return_sequences=True, input_shape=[28, 28]),\n",
    "    keras.layers.LSTM(30)\n",
    "])\n",
    "recurrent_decoder = keras.models.Sequential([\n",
    "    keras.layers.RepeatVector(28, input_shape=[30]),\n",
    "    keras.layers.LSTM(100, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(28, activation=\"sigmoid\"))\n",
    "])\n",
    "recurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder])\n",
    "recurrent_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(0.1),\n",
    "                     metrics=[rounded_accuracy])\n",
    "history = recurrent_ae.fit(X_train, X_train, epochs=10, validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959f06d",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "show_reconstructions(recurrent_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a4f79",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Stacked denoising Autoencoder with Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8aa02e",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "denoising_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.GaussianNoise(0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(30, activation=\"selu\")\n",
    "])\n",
    "denoising_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "denoising_ae = keras.models.Sequential([denoising_encoder, denoising_decoder])\n",
    "denoising_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1.0),\n",
    "                     metrics=[rounded_accuracy])\n",
    "history = denoising_ae.fit(X_train, X_train, epochs=10,\n",
    "                           validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153827a",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "noise = keras.layers.GaussianNoise(0.2)\n",
    "show_reconstructions(denoising_ae, noise(X_valid, training=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55e727",
   "metadata": {
    "editable": true
   },
   "source": [
    "And using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4c8ee",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "dropout_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(30, activation=\"selu\")\n",
    "])\n",
    "dropout_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])\n",
    "dropout_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1.0),\n",
    "                   metrics=[rounded_accuracy])\n",
    "history = dropout_ae.fit(X_train, X_train, epochs=10,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc1d51",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "dropout = keras.layers.Dropout(0.5)\n",
    "show_reconstructions(dropout_ae, dropout(X_valid, training=True))\n",
    "save_fig(\"dropout_denoising_plot\", tight_layout=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6627e0",
   "metadata": {
    "editable": true
   },
   "source": [
    "## PyTorch example\n",
    "We will continue with the MNIST database, which has $60000$ training examples and a test set of 10000 handwritten numbers. The images have\n",
    "only one color channel and have a size of $28\\times 28$ pixels.\n",
    "We start by uploading the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4db14d6",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./mnist_data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "# import the Torch packages\n",
    "# transforms are used to preprocess the images, e.g. crop, rotate, normalize, etc\n",
    "import torch\n",
    "from torchvision import datasets,transforms\n",
    "\n",
    "# specify the data path in which you would like to store the downloaded files\n",
    "# ToTensor() here is used to convert data type to tensor\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "print(train_dataset)\n",
    "batchSize=128\n",
    "\n",
    "#only after packed in DataLoader, can we feed the data into the neural network iteratively\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batchSize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663d158",
   "metadata": {
    "editable": true
   },
   "source": [
    "We visualize the images here using the $imshow$ function  function and the $make\\_grid$ function from PyTorch to arrange and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe849670",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# package we used to manipulate matrix\n",
    "import numpy as np\n",
    "# package we used for image processing\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    #transpose: change array axis to correspond to the plt.imshow() function     \n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) \n",
    "    plt.show()\n",
    "\n",
    "# load the first 16 training samples from next iteration\n",
    "# [:16,:,:,:] for the 4 dimension of examples, first dimension take first 16, other dimension take all data\n",
    "# arrange the image in grid\n",
    "examples, _ = next(iter(train_loader))\n",
    "example_show=make_grid(examples[:16,:,:,:], 4)\n",
    "\n",
    "# then display them\n",
    "imshow(example_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dee131",
   "metadata": {
    "editable": true
   },
   "source": [
    "Our autoencoder consists of two parts, see also the TensorFlow example\n",
    "above. The encoder and decoder parts are represented by two fully\n",
    "connected feed forward neural networks where we use the standard\n",
    "Sigmoid function.  In the encoder part we reduce the dimensionality of\n",
    "the image from $28\\times 28=784$ pixels to first $16\\times 16=256$\n",
    "pixels and then to 128 pixels. The 128 pixel representation is then\n",
    "used to define the representation of the input and the input to the\n",
    "decoder part.  The latter attempts to reconstruct the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b45459",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 256  # 1st layer num features\n",
    "num_hidden_2 = 128  # 2nd layer num features (the latent dim)\n",
    "num_input = 784  # MNIST data input (img shape: 28*28)\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        # decoder part\n",
    "        self.fc3 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc4 = nn.Linear(h_dim1, x_dim)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def decoder(self, x):\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# When initializing, it will run __init__() function as above\n",
    "model = Autoencoder(num_input, num_hidden_1, num_hidden_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e3aba",
   "metadata": {
    "editable": true
   },
   "source": [
    "We define here the cost/loss function and the optimizer we employ (Adam here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db8daf6",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Training start====\n",
      "====> Epoch: 0 Average loss: 30.579419225\n",
      "====> Epoch: 10 Average loss: 5.677109637\n",
      "====> Epoch: 20 Average loss: 3.375376532\n",
      "====> Epoch: 30 Average loss: 2.541718153\n",
      "====> Epoch: 40 Average loss: 2.118307935\n",
      "====> Epoch: 50 Average loss: 1.866089026\n",
      "====> Epoch: 60 Average loss: 1.685287650\n",
      "====> Epoch: 70 Average loss: 1.547592661\n",
      "====> Epoch: 80 Average loss: 1.436785246\n",
      "====> Epoch: 90 Average loss: 1.342284530\n",
      "====Training finish====\n"
     ]
    }
   ],
   "source": [
    "# define loss function and parameters\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "epoch = 100\n",
    "# MSE loss will calculate Mean Squared Error between the inputs \n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "print('====Training start====')\n",
    "for i in range(epoch):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        # prepare input data\n",
    "        inputs = torch.reshape(data,(-1, 784)) # -1 can be any value.\n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        # feed inputs into model\n",
    "        recon_x = model(inputs)\n",
    "        # calculating loss \n",
    "        loss = loss_function(recon_x, inputs)\n",
    "        # calculate gradient of each parameter\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        # update the weight based on the gradient calculated\n",
    "        optimizer.step()\n",
    "    if i%10==0:    \n",
    "        print('====> Epoch: {} Average loss: {:.9f}'.format(i, train_loss ))\n",
    "print('====Training finish====')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ecd613",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we have trained the network, we will now reconstruct various test samples to see if the model can generalize to data which\n",
    "were not included in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93f8ef",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load 16 images from testset\n",
    "inputs, _ = next(iter(test_loader))\n",
    "inputs_example = make_grid(inputs[:16,:,:,:],4)\n",
    "imshow(inputs_example)\n",
    "\n",
    "#convert from image to tensor\n",
    "#inputs=inputs.cuda()\n",
    "inputs=torch.reshape(inputs,(-1,784))\n",
    "\n",
    "# get the outputs from the trained model\n",
    "outputs=model(inputs)\n",
    "\n",
    "#convert from tensor to image\n",
    "outputs=torch.reshape(outputs,(-1,1,28,28))\n",
    "outputs=outputs.detach().cpu()\n",
    "\n",
    "#show the output images\n",
    "outputs_example = make_grid(outputs[:16,:,:,:],4)\n",
    "imshow(outputs_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722bf6d7",
   "metadata": {
    "editable": true
   },
   "source": [
    "After training the auto-encoder, we can now use the model to reconstruct some images.\n",
    "In order to reconstruct different training images, the model\n",
    "has learned to recognize how the image looks like and describe it in\n",
    "the 128-dimensional  latent space. In other words, the visual information of\n",
    "images is compressed and encoded in the 128-dimensional representations. As we\n",
    "assume that samples from the same categories should be more visually\n",
    "similar than those from different classes, the representations can\n",
    "then be used for image recognition, i.e., handwritten digit images\n",
    "recognition in our case.\n",
    "\n",
    "One simple way to recognize images is to randomly select ten training\n",
    "samples from each class and annotate them with the corresponding label.\n",
    "Then given the\n",
    "test data, we can predict which classes they belong to by finding the\n",
    "most similar labelled training samples to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e732da",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get 100 image-label pairs from training set\n",
    "x_train, y_train = next(iter(train_loader))\n",
    "\n",
    "# 10 classes, 10 samples per class, 100 in total\n",
    "candidates = np.random.choice(batchSize, 10*10)\n",
    "\n",
    "# randomly select 100 samples\n",
    "x_train = x_train[candidates]\n",
    "y_train = y_train[candidates]\n",
    "\n",
    "# display the selected samples and print their labels\n",
    "\n",
    "imshow(make_grid(x_train[:100,:,:,:],10))\n",
    "print(y_train.reshape(10, 10))\n",
    "\n",
    "# get 100 image-label pairs from test set\n",
    "x_test, y_test = next(iter(train_loader))\n",
    "candidates_test = np.random.choice(batchSize, 10*10)\n",
    "\n",
    "x_test = x_test[candidates_test]\n",
    "y_test = y_test[candidates_test]\n",
    "\n",
    "# display the selected samples and print their labels\n",
    "imshow(make_grid(x_test[:100,:,:,:],10))\n",
    "\n",
    "print(y_test.reshape(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca335a",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# compute the representations of training and test samples\n",
    "#h_train=model.encoder(torch.reshape(x_train.cuda(),(-1,784)))\n",
    "#h_test=model.encoder(torch.reshape(x_test.cuda(),(-1,784)))\n",
    "h_train=model.encoder(torch.reshape(x_train,(-1,784)))\n",
    "h_test=model.encoder(torch.reshape(x_test,(-1,784)))\n",
    "\n",
    "# find the nearest training samples to each test instance, in terms of MSE\n",
    "MSEs = np.mean(np.power(np.expand_dims(h_test.detach().cpu(), axis=1) - np.expand_dims(h_train.detach().cpu(), axis=0), 2), axis=2)\n",
    "neighbours = MSEs.argmin(axis=1)\n",
    "predicts = y_train[neighbours]\n",
    "\n",
    "# print(np.stack([y_test, predicts], axis=1))\n",
    "print('Recognition accuracy according to the learned representation is %.1f%%' % (100 * (y_test == predicts).numpy().astype(np.float32).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc08359",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_ds = datasets.MNIST(., train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "def init(self, input_dim=28*28, hidden_dim=64, latent_dim=16):\n",
    "super().init()\n",
    "self.encoder = nn.Sequential(\n",
    "nn.Flatten(),\n",
    "nn.Linear(input_dim, hidden_dim),\n",
    "nn.ReLU(),\n",
    "nn.Linear(hidden_dim, latent_dim),\n",
    "nn.ReLU()\n",
    ")\n",
    "self.decoder = nn.Sequential(\n",
    "nn.Linear(latent_dim, hidden_dim),\n",
    "nn.ReLU(),\n",
    "nn.Linear(hidden_dim, input_dim),\n",
    "nn.Sigmoid(),\n",
    "nn.Unflatten(1, (1,28,28))\n",
    ")\n",
    "def forward(self, x):\n",
    "z = self.encoder(x)\n",
    "xhat = self.decoder(z)\n",
    "return xhat\n",
    "\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, 11):\n",
    "epoch_loss = 0.0\n",
    "for xb, _ in train_loader:\n",
    "optimizer.zero_grad()\n",
    "xb = xb\n",
    "xhat = model(xb)\n",
    "loss = criterion(xhat, xb)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "epoch_loss += loss.item() * xb.size(0)\n",
    "epoch_loss /= len(train_loader.dataset)\n",
    "print(fEpoch {epoch}, Loss: {epoch_loss:.6f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f4a08",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(float32) / 255.0\n",
    "x_train = x_train[, None]\n",
    "\n",
    "\n",
    "input_shape = (28,28,1)\n",
    "encoder_inputs = layers.Input(shape=input_shape)\n",
    "x = layers.Flatten()(encoder_inputs)\n",
    "x = layers.Dense(64, activation=relu)(x)\n",
    "latent = layers.Dense(16, activation=relu)(x)\n",
    "\n",
    "x = layers.Dense(64, activation=relu)(latent)\n",
    "x = layers.Dense(28*28, activation=sigmoid)(x)\n",
    "outputs = layers.Reshape((28,28,1))(x)\n",
    "\n",
    "autoencoder = models.Model(encoder_inputs, outputs)\n",
    "autoencoder.compile(optimizer=adam, loss=mse)\n",
    "\n",
    "\n",
    "autoencoder.fit(x_train, x_train, epochs=10, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
