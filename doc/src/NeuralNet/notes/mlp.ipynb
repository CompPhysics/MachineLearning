{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building neural networks in numpy and scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "Artificial neural networks are computational systems that can learn to perform tasks by considering examples,\n",
    "generally without being programmed with any task-specific rules. It is supposed to mimic a biological system, wherein neurons interact by sending signals in the form of mathematical functions between layers. All layers can contain an arbitrary number of neurons, and each connection is represented by a weight variable.  \n",
    "  \n",
    "In this tutorial we will build a feed-forward neural network, where information moves in only in direction:\n",
    "forward through the layers. Each neuron or *node* is represented by a circle, while arrows display the connections\n",
    "between the nodes and indicate the direction of information flow. Each node in a layer is connected to all nodes in the subsequent layer, which makes this a so-called *fully-connected* feed-forward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FFNN](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)  \n",
    "Via [Wikipedia](https://en.wikipedia.org/wiki/File:Colored_neural_network.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "To follow this tutorial we require an installation of Python with the numerical package **numpy**, either:  \n",
    "1) Python 2.7.x  \n",
    "2) Python 3.5.x or greater  \n",
    "  \n",
    "With a version of Numpy 1.0.x or greater.  \n",
    "We will also use the packages **matplotlib**, **scikit-learn**, **Tensorflow** and **Keras**, though these are not strictly necessary.  \n",
    "To open and run this notebook you also need an installation of **IPython** and **Jupyter Notebook**.  \n",
    "  \n",
    "# Anaconda\n",
    "Anaconda is a free and open source Python and R distribution, that aims to simplify package management and deployment. Anaconda comes with more than 1000 data packages, as well as the Conda package and package and virtual environment manager. Anaconda is available on Linux, OS X and Windows systems, and contains nearly all prerequisite software, it comes highly recommended.  \n",
    "If Anaconda is installed you can install Tensorflow and Keras using:  \n",
    "  \n",
    "```conda install tensorflow```  \n",
    "```conda install keras```  \n",
    "  \n",
    "(You may run into minor problems with conflicting package versions).  \n",
    "  \n",
    "# Pip package manager\n",
    "If you do not wish to install Anaconda you may download Python from [here](https://www.python.org/downloads/),\n",
    "or you can use package managers like **brew**, **apt**, **pacman**,...  \n",
    "Python distributions come with their own package manager, **pip**, and once you have Python installed\n",
    "you can run the following command:  \n",
    "  \n",
    "```pip install numpy matplotlib scikit-learn ipython jupyter```  \n",
    "  \n",
    "To install Tensorflow follow the instructions [here](https://www.tensorflow.org/install/).  \n",
    "After you have installed tensorflow you can install keras:  \n",
    "  \n",
    "```pip install keras```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Python](https://imgs.xkcd.com/comics/python_environment.png)  \n",
    "Via [xkcd](https://xkcd.com/1987/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "  \n",
    "One can identify a set of key steps when using neural networks to solve supervised learning problems:  \n",
    "  \n",
    "**\n",
    "1) Collect and pre-process data  \n",
    "2) Define model and architecture  \n",
    "3) Choose cost function and optimizer  \n",
    "4) Train the model  \n",
    "5) Evaluate model performance on test data  \n",
    "6) Adjust hyperparameters (if necessary, network architecture)\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Collect and pre-process data\n",
    "  \n",
    "In this tutorial we will be using the MNIST dataset, which is readily available through the **scikit-learn**\n",
    "package. You may also find it for example [here](http://yann.lecun.com/exdb/mnist/).  \n",
    "The **MNIST** (Modified National Institute of Standards and Technology) database is a large database\n",
    "of handwritten digits that is commonly used for training various image processing systems.  \n",
    "The MNIST dataset consists of 70 000 images of size 28x28 pixels, each labeled from 0 to 9.  \n",
    "  \n",
    "To feed data into a feed-forward neural network we need to represent the inputs as a feature matrix $X = [n_{inputs}, \n",
    "n_{features}]$.  \n",
    "Each row represents an **input**, in this case a handwritten digit, and each column represents a **feature**, in this case a pixel.  \n",
    "The correct answers, also known as **labels** or **targets** are represented as a 1D array of integers $Y = [n_{inputs}] = [5, 3, 1, 8,...]$.  \n",
    "  \n",
    "Say I wanted to build a neural network using supervised learning to predict Body-Mass Index (BMI) from\n",
    "measurements of height (in m)  \n",
    "and weight (in kg). If I had measurements of 5 people the feature matrix could be for example:  \n",
    "  \n",
    "$$ X = \\begin{bmatrix}\n",
    "1.85 & 81\\\\\n",
    "1.71 & 65\\\\\n",
    "1.95 & 103\\\\\n",
    "1.55 & 42\\\\\n",
    "1.63 & 56\n",
    "\\end{bmatrix} ,$$  \n",
    "  \n",
    "and the targets would be:  \n",
    "  \n",
    "$$ Y = (23.7, 22.2, 27.1, 17.5, 21.1) $$  \n",
    "  \n",
    "Since each input image is a 2D matrix, we need to flatten the image (i.e. \"unravel\" the 2D matrix into a 1D array)  \n",
    "to turn the data into a feature matrix. This means we lose all spatial information in the image, such as locality and translational invariance ([explanation](https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo)).  \n",
    "More complicated architectures such as Convolutional Neural Networks can take advantage\n",
    "of such information, and are most commonly applied when analyzing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs = (n_inputs, pixel_width, pixel_height) = (1797, 8, 8)\n",
      "labels = (n_inputs) = (1797,)\n",
      "X = (n_inputs, n_features) = (1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "#ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# display images in notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "\n",
    "# download MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# define inputs and labels\n",
    "inputs = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "print(\"inputs = (n_inputs, pixel_width, pixel_height) = \" + str(inputs.shape))\n",
    "print(\"labels = (n_inputs) = \" + str(labels.shape))\n",
    "\n",
    "\n",
    "# flatten the image\n",
    "# the value -1 means dimension is inferred from the remaining dimensions: 8x8 = 64\n",
    "n_inputs = len(inputs)\n",
    "inputs = inputs.reshape(n_inputs, -1)\n",
    "print(\"X = (n_inputs, n_features) = \" + str(inputs.shape))\n",
    "\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(n_inputs)\n",
    "random_indices = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[random_indices]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[random_indices[i]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test datasets\n",
    "\n",
    "Performing analysis before partitioning the dataset is a major error, that can lead to incorrect conclusions  \n",
    "(see \"Bias-Variance Tradeoff\", for example [here](https://ml.berkeley.edu/blog/2017/07/13/tutorial-4/)).  \n",
    "  \n",
    "We will reserve $80 \\%$ of our dataset for training and $20 \\%$ for testing.  \n",
    "  \n",
    "It is important that the train and test datasets are drawn randomly from our dataset, to ensure\n",
    "no bias in the sampling.  \n",
    "Say you are taking measurements of weather data to predict the weather in the coming 5 days.\n",
    "You don't want to train your model on measurements taken from the hours 00.00 to 12.00, and then test it on data\n",
    "collected from 12.00 to 24.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# one-liner from scikit-learn library\n",
    "train_size = 0.8\n",
    "test_size = 1 - train_size\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, train_size=train_size,\n",
    "                                                    test_size=test_size)\n",
    "\n",
    "# equivalently in numpy\n",
    "def train_test_split_numpy(inputs, labels, train_size, test_size):\n",
    "    n_inputs = len(inputs)\n",
    "    inputs_shuffled = inputs.copy()\n",
    "    labels_shuffled = labels.copy()\n",
    "    \n",
    "    np.random.shuffle(inputs_shuffled)\n",
    "    np.random.shuffle(labels_shuffled)\n",
    "    \n",
    "    train_end = int(n_inputs*train_size)\n",
    "    X_train, X_test = inputs_shuffled[:train_end], inputs_shuffled[train_end:]\n",
    "    Y_train, Y_test = labels_shuffled[:train_end], labels_shuffled[train_end:]\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split_numpy(inputs, labels, train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Define model and architecture\n",
    "  \n",
    "Our simple feed-forward neural network will consist of an **input** layer, a single **hidden** layer and an **output** layer. The activation $y$ of each neuron is a weighted sum of inputs, passed through an activation function:  \n",
    "  \n",
    "$$ z = \\sum_{i=1}^n w_i a_i ,$$\n",
    "  \n",
    "$$ y = f(z) ,$$\n",
    "  \n",
    "where $f$ is the activation function, $a_i$ represents input from neuron $i$ in the preceding layer\n",
    "and $w_i$ is the weight to neuron $i$.  \n",
    "The activation of the neurons in the input layer is just the features (e.g. a pixel value).  \n",
    "  \n",
    "The simplest activation function for a binary classifier (e.g. two classes, 0 or 1, cat or not cat)\n",
    "is the **Heaviside** function:\n",
    "  \n",
    "$$ f(z) = \n",
    "\\begin{cases}\n",
    "1,  &  z > 0\\\\\n",
    "0,  & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "  \n",
    "A feed-forward neural network with this activation is known as a **perceptron**.  \n",
    "This activation can be generalized to $k$ classes (using e.g. the *one-against-all* strategy), \n",
    "and we call these architectures **multiclass perceptrons**.  \n",
    "  \n",
    "However, it is now common to use the terms Single Layer Perceptron (SLP) (1 hidden layer) and  \n",
    "Multilayer Perceptron (MLP) (2 or more hidden layers) to refer to feed-forward neural networks with any activation function.  \n",
    "  \n",
    "Typical choices for activation functions include the sigmoid function, hyperbolic tangent, and Rectified Linear Unit (ReLU).  \n",
    "We will be using the sigmoid function $\\sigma(x)$:  \n",
    "  \n",
    "$$ f(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}} ,$$\n",
    "  \n",
    "which is inspired by probability theory (see logistic regression) and was most commonly used until about 2011.\n",
    "  \n",
    "# Layers\n",
    "  \n",
    "**Input**:  \n",
    "Since each input image has 8x8 = 64 pixels or features, we have an input layer of 64 neurons.  \n",
    "  \n",
    "**Hidden layer**:  \n",
    "We will use 50 neurons in the hidden layer receiving input from the neurons in the input layer.  \n",
    "Since each neuron in the hidden layer is connected to the 64 inputs we have 64x50 = 3200 weights to the hidden layer.  \n",
    "  \n",
    "**Output**:  \n",
    "If we were building a binary classifier, it would be sufficient with a single neuron in the output layer,\n",
    "which could output 0 or 1 according to the Heaviside function. This would be an example of a **hard** classifier, meaning it outputs the class of the input directly. However, if we are dealing with noisy data it is often beneficial to use a **soft** classifier, which outputs the probability of being in class 0 or 1.  \n",
    "  \n",
    "For a soft binary classifier, we could use a single neuron and interpret the output as either being the probability of being in class 0 or the probability of being in class 1. Alternatively we could use 2 neurons, and interpret each neuron as the probability of being in each class.  \n",
    "  \n",
    "Since we are doing multiclass classification, with 10 categories, it is natural to use 10 neurons in the output layer. We number the neurons $j = 0,1,...,9$. The activation of each output neuron $j$ will be according to the **softmax** function:  \n",
    "  \n",
    "$$ P(\\text{class $j$} \\mid \\text{input $\\boldsymbol{a}$}) = \\frac{e^{\\boldsymbol{a}^T \\boldsymbol{w}_j}}\n",
    "{\\sum_{k=0}^{9} e^{\\boldsymbol{a}^T \\boldsymbol{w}_k}} ,$$  \n",
    "  \n",
    "i.e. each neuron $j$ outputs the probability of being in class $j$ given an input from the hidden layer $\\boldsymbol{a}$, with $\\boldsymbol{w}_j$ the weights of neuron $j$ to the inputs.  \n",
    "The denominator is a normalization factor to ensure the outputs sum up to 1.  \n",
    "The exponent is just the weighted sum of inputs as before:  \n",
    "  \n",
    "$$ z_j = \\sum_{i=1}^n w_ {ij} a_i = \\boldsymbol{a}^T \\boldsymbol{w}_j .$$  \n",
    "  \n",
    "Since each neuron in the output layer is connected to the 50 inputs from the hidden layer we have 50x10 = 500\n",
    "weights to the output layer.\n",
    "  \n",
    "# Weights and biases\n",
    "  \n",
    "Typically weights are initialized with small values distributed around zero, drawn from a uniform\n",
    "or normal distribution. Setting all weights to zero means all neurons give the same output, making the network useless.  \n",
    "  \n",
    "Adding a bias value to the weighted sum of inputs allows the neural network to represent a greater range\n",
    "of values. Without it, any input with the value 0 will be mapped to zero (before being passed through the activation). The bias unit has an output of 1, and a weight to each neuron $j$, $b_j$:  \n",
    "  \n",
    "$$ z_j = \\sum_{i=1}^n w_ {ij} a_i + 1\\cdot b_j = \\boldsymbol{a}^T \\boldsymbol{w}_j + b_j .$$  \n",
    "  \n",
    "The bias weights $\\boldsymbol{b}$ are often initialized to zero, but a small value like $0.01$ ensures all neurons have some output which can be backpropagated in the first training cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias](http://ufldl.stanford.edu/tutorial/images/Network331.png)  \n",
    "Via [Stanford UFLDL](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building our neural network\n",
    "\n",
    "n_inputs, n_features = X_train.shape\n",
    "n_hidden_neurons = 50\n",
    "n_categories = 10\n",
    "\n",
    "# we make the weights normally distributed using numpy.random.randn\n",
    "\n",
    "# weights and bias in the hidden layer\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "# weights and bias in the output layer\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward pass\n",
    "\n",
    "For each input image we calculate a weighted sum of input features (pixel values) to each neuron $j$ in the hidden layer:  \n",
    "  \n",
    "$$ z_{j}^{hidden} = \\sum_{i=1}^{n_{features}} w_{ij}^{hidden} x_i + b_{j}^{hidden} = \\boldsymbol{x}^T \\boldsymbol{w}_{j}^{hidden} + b_{j}^{hidden} ,$$\n",
    "  \n",
    "this is then passed through our activation function  \n",
    "  \n",
    "$$ a_{j}^{hidden} = f(z_{j}^{hidden}) .$$  \n",
    "  \n",
    "We calculate a weighted sum of inputs (activations in the hidden layer) to each neuron $j$ in the output layer:  \n",
    "  \n",
    "$$ z_{j}^{output} = \\sum_{i=1}^{n_{hidden}} w_{ij}^{output} a_{i}^{hidden} + b_{j}^{output} = (\\boldsymbol{a}^{hidden})^T \\boldsymbol{w}_{j}^{output} + b_{j}^{output} .$$  \n",
    "  \n",
    "Finally we calculate the output of neuron $j$ in the output layer using the softmax function:  \n",
    "  \n",
    "$$ a_{j}^{output} = \\frac{\\exp{(z_j^{output})}}\n",
    "{\\sum_{k=1}^{n_{categories}} \\exp{(z_k^{output})}} .$$  \n",
    "   \n",
    "# Matrix multiplication\n",
    "  \n",
    "Since our data has the dimensions $X = (n_{inputs}, n_{features})$ and our weights to the hidden\n",
    "layer have the dimensions  \n",
    "$W_{hidden} = (n_{features}, n_{hidden})$,\n",
    "we can easily feed the network all our training data in one go by taking the matrix product  \n",
    "  \n",
    "$$ X W^{hidden} = (n_{inputs}, n_{hidden}),$$ \n",
    "  \n",
    "and obtain a matrix that holds the weighted sum of inputs to the hidden layer\n",
    "for each input image.  \n",
    "We also add the bias to obtain a matrix of weighted sums $Z^{hidden}$:  \n",
    "  \n",
    "$$ Z^{hidden} = X W^{hidden} + B^{hidden} ,$$\n",
    "  \n",
    "meaning the same bias (1D array) is added to each input image.  \n",
    "This is then passed through the activation  \n",
    "  \n",
    "$$ A^{hidden} = f(X W^{hidden} + B^{hidden}) .$$  \n",
    "  \n",
    "This is fed to the output layer:  \n",
    "  \n",
    "$$ Z^{output} = A^{hidden} W^{output} + B^{output} .$$\n",
    "  \n",
    "Finally we receive our output values for each image and each category by passing it through the softmax function:  \n",
    "  \n",
    "$$ output = softmax (Z^{output}) = (n_{inputs}, n_{categories}) .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities = (n_inputs, n_categories) = (1437, 10)\n",
      "probability that image 0 is in category 0,1,2,...,9 = \n",
      "[5.41511965e-04 2.17174962e-03 8.84355903e-03 1.44970586e-03\n",
      " 1.10378326e-04 5.08318298e-09 2.03256632e-04 1.92507116e-03\n",
      " 9.84443254e-01 3.11507992e-04]\n",
      "probabilities sum up to: 1.0\n",
      "\n",
      "predictions = (n_inputs) = (1437,)\n",
      "prediction for image 0: 8\n",
      "correct label for image 0: 6\n"
     ]
    }
   ],
   "source": [
    "# setup the feed-forward pass\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def feed_forward(X):\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z1 = np.matmul(X, hidden_weights) + hidden_bias\n",
    "    # activation in the hidden layer\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    # weighted sum of inputs to the output layer\n",
    "    z2 = np.matmul(a1, output_weights) + output_bias\n",
    "    # softmax output\n",
    "    # axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    exp_term = np.exp(z2)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "probabilities = feed_forward(X_train)\n",
    "print(\"probabilities = (n_inputs, n_categories) = \" + str(probabilities.shape))\n",
    "print(\"probability that image 0 is in category 0,1,2,...,9 = \\n\" + str(probabilities[0]))\n",
    "print(\"probabilities sum up to: \" + str(probabilities[0].sum()))\n",
    "print()\n",
    "\n",
    "# we obtain a prediction by taking the class with the highest likelihood\n",
    "def predict(X):\n",
    "    probabilities = feed_forward(X)\n",
    "    return np.argmax(probabilities, axis=1)\n",
    "\n",
    "predictions = predict(X_train)\n",
    "print(\"predictions = (n_inputs) = \" + str(predictions.shape))\n",
    "print(\"prediction for image 0: \" + str(predictions[0]))\n",
    "print(\"correct label for image 0: \" + str(Y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Choose cost function and optimizer (needs more work)\n",
    "  \n",
    "To measure how well our neural network is doing we need to introduce a cost function.  \n",
    "We will call the function that gives the error of a single sample output the **loss** function, and the function\n",
    "that gives the total error of our network across all samples the **cost** function.\n",
    "A typical choice for multiclass classification is the **cross-entropy** loss, also known as the negative log likelihood.  \n",
    "In multiclass classification it is common to treat each integer label as a so called **one-hot** vector:  \n",
    "  \n",
    "$$ y = 5 \\quad \\rightarrow \\quad \\boldsymbol{y} = (0, 0, 0, 0, 0, 1, 0, 0, 0, 0) ,$$  \n",
    "\n",
    "  \n",
    "$$ y = 1 \\quad \\rightarrow \\quad \\boldsymbol{y} = (0, 1, 0, 0, 0, 0, 0, 0, 0, 0) ,$$  \n",
    "  \n",
    "  \n",
    "i.e. a binary bit string of length $K$, where $K = 10$ is the number of classes.  \n",
    "If $\\boldsymbol{x}_i$ is the $i$-th input (image), $y_{ik}$ refers to the $k$-th component of the $i$-th\n",
    "output vector $\\boldsymbol{y}_i$. The probability of $\\boldsymbol{x}_i$ being in class $k$ is given by the softmax function:  \n",
    "  \n",
    "$$ P(y_{ik} = 1 \\mid \\boldsymbol{x}_i, \\boldsymbol{\\theta}) = \\frac{e^{(\\boldsymbol{a}_i^{hidden})^T \\boldsymbol{w}_k}}\n",
    "{\\sum_{k'=0}^{K-1} e^{(\\boldsymbol{a}_i^{hidden})^T \\boldsymbol{w}_{k'}}} ,$$\n",
    "  \n",
    "where $\\boldsymbol{a}_i^{hidden}$ is the activation in the hidden layer from input $\\boldsymbol{x}_i$.\n",
    "The vector $\\boldsymbol{\\theta}$ represents the weights and biases of our network.  \n",
    "The probability of not being in class $k$ is just $1 - P(y_{ik} = 1 \\mid \\boldsymbol{x}_i)$.  \n",
    "  \n",
    "For Maximum Likelihood Estimation (MLE) we choose the label with the largest probability.  \n",
    "Denote the output label $\\hat{y}$ and the correct label $y$, for example $\\hat{y} = 5$ and $y = 8$. The likelihood that input $\\boldsymbol{x}$\n",
    "gives an output $\\hat{y} = k'$ is then\n",
    "  \n",
    "$$ P(\\hat{y} = k' \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\prod_{k=0}^{K-1} [P(y_{k} = 1 \\mid \\boldsymbol{x}, \\boldsymbol{\\theta})]^{y_{k}} \n",
    "\\times [1 - P(y_{k} = 1 \\mid \\boldsymbol{x}, \\boldsymbol{\\theta})]^{1-y_{k}} ,$$  \n",
    "  \n",
    "where $y_k$ is the $k$-th component of the one-hot vector of (correct) labels.  \n",
    "A perfect classifier should give a $100 \\%$ probability of the correct label, so the product\n",
    "should just be 1 if $y = k$ and 0 otherwise. If the network is not a perfect classifier, the likelihood should be a number between 0 and 1.  \n",
    "  \n",
    "If we take the log of this we can turn the product into a sum, which is often simpler to compute:  \n",
    "  \n",
    "$$ \\log P(\\hat{y} = k' \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\sum_{k=0}^{K-1} y_{k} \\log P(y_{k} = 1 \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) \n",
    "+ (1-y_{k})\\log (1 - P(y_{k} = 1 \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}))$$  \n",
    "  \n",
    "For a perfect classifier this should just be $\\log 1 = 0$. Otherwise we get a negative number.  \n",
    "Since it is easier to think in terms of minimizing a positive number, we take our loss function\n",
    "to be the negative log-likelihood:  \n",
    "  \n",
    "$$ \\mathcal{L}(\\boldsymbol{\\theta}) = - \\log P(\\hat{y} = k' \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) $$  \n",
    "  \n",
    "We then take the average of the loss function over all input samples to define the cost function:  \n",
    "$$ \\begin{split} \\mathcal{C}(\\boldsymbol{\\theta}) &= \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(\\boldsymbol{w}) \\\\\n",
    " &= -\\frac{1}{N}\\sum_{i=1}^N \\sum_{k=0}^{K-1} y_{k} \\log P(y_{k} = 1 \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) \n",
    "+ (1-y_{k})\\log (1 - P(y_{k} = 1 \\mid \\boldsymbol{x}, \\boldsymbol{\\theta})) \\end{split} .$$\n",
    "  \n",
    "# Optimizing the cost function\n",
    "  \n",
    "The network is trained by finding the weights and biases that minimize the cost function. One of the most widely used classes of methods is **gradient descent** and its generalizations. The idea behind gradient descent\n",
    "is simply to adjust the weights in the direction where the gradient of the cost function is large and negative. This ensures we flow toward a **local** minimum of the cost function.  \n",
    "Each parameter $\\theta$ is iteratively adjusted according to the rule  \n",
    "  \n",
    "$$ \\theta_{i+1} = \\theta_i - \\eta \\nabla \\mathcal{C}(\\theta) ,$$\n",
    "\n",
    "where $\\eta$ is known as the **learning rate**, which controls how big a step we take towards the minimum.  \n",
    "This update can be repeated for any number of iterations, or until we are satisfied with the result.  \n",
    "  \n",
    "A simple and effective improvement is a variant called **Stochastic Gradient Descent** (SGD).  \n",
    "Instead of calculating the gradient on the whole dataset, we calculate an approximation of the gradient\n",
    "on a subset of the data called a **minibatch**.  \n",
    "If there are $N$ data points and we have a minibatch size of $M$, the total number of batches\n",
    "is $n/M$.  \n",
    "We denote each minibatch $B_k$, with $k = 1, 2,...,n/M$. The gradient then becomes:  \n",
    "  \n",
    "$$ \\nabla \\mathcal{C}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\nabla \\mathcal{L}(\\theta) \\quad \\rightarrow \\quad\n",
    "\\frac{1}{M} \\sum_{i \\in B_k} \\nabla \\mathcal{L}(\\theta) ,$$\n",
    "  \n",
    "i.e. instead of averaging the loss over the entire dataset, we average over a minibatch.  \n",
    "This has two important benefits:  \n",
    "1) Introducing stochasticity decreases the chance that the algorithm becomes stuck in a local minima.  \n",
    "2) It significantly speeds up the calculation, since we do not have to use the entire dataset to calculate the gradient.  \n",
    "  \n",
    "# Regularization\n",
    "  \n",
    "It is common to add an extra term to the cost function, proportional to the size of the weights.  \n",
    "This is equivalent to constraining the size of the weights, so that they do not grow out of control.  \n",
    "Constraining the size of the weights means that the weights cannot grow arbitrarily large to fit the training data, and in this way reduces overfitting.  \n",
    "  \n",
    "We will measure the size of the weights using the so called **L2-norm**, meaning our cost function becomes:  \n",
    "  \n",
    "$$ \\nabla \\mathcal{C}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\nabla \\mathcal{L}(\\theta) \\quad \\rightarrow \\quad\n",
    "\\frac{1}{N} \\sum_{i=1}^N \\nabla \\mathcal{L}(\\theta) + \\lambda \\lvert \\lvert \\boldsymbol{w}_2^2 \\rvert \\rvert \n",
    "= \\frac{1}{N} \\sum_{i=1}^N \\nabla \\mathcal{L}(\\theta) + \\lambda \\sum_{ij} w_{ij}^2,$$  \n",
    "  \n",
    "i.e. we sum up all the weights squared. The factor $\\lambda$ is known as a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Train the model\n",
    "  \n",
    "In order to train the model, we need to calculate the derivative of the cost function with respect\n",
    "to every bias and weight in the network. Using an approximation to the derivative (e.g. using the [finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method)) is much too costly.  \n",
    "In total our network has $(64 + 1) \\times 50 = 3250$ weights in the hidden layer and $(50 + 1) \\times 10 = 510$ weights to the output layer ($ + 1$ for the bias), and the gradient must be calculated for every parameter.  \n",
    "  \n",
    "The **backpropagation** algorithm is a clever use of the chain rule that allows us to calculate gradient efficently. Here we will simply state the backpropagation equations that we will use for our network, and then a derivation is given at the end of this tutorial.  \n",
    "  \n",
    "The error $\\delta_i^o$ at each output neuron $i$ is just the difference between the output probability $\\hat{y}_i$ and the correct label $y_i$ (0 or 1 using one-hot vectors):  \n",
    "  \n",
    "$$ \\delta_i^o = \\hat{y}_i - y_i .$$  \n",
    "  \n",
    "The gradient of the cost function with respect to each output weight $w_{i,j}^o$ is then  \n",
    "  \n",
    "$$ \\frac{\\partial \\mathcal{C}}{\\partial w_{i,j}^o} = \\delta_i^o a_j^h ,$$\n",
    "  \n",
    "where $a_j^h$ is the activation at the $j$-th neuron in the hidden layer.  \n",
    "The gradient with respect to each output bias $b_i^o$ is  \n",
    "  \n",
    "$$ \\frac{\\partial \\mathcal{C}}{\\partial b_i^o} = \\delta_i^o .$$  \n",
    "  \n",
    "The error at each hidden layer neuron $\\delta_i^h$ is given as  \n",
    "  \n",
    "$$ \\delta_i^h = \\sum_{k=0}^{K-1} \\delta_k^o w_{ki}^o f'(z_i^h) ,$$\n",
    "  \n",
    "where $K$ is the number of output neurons or categories and $f'(z_i^h)$ is the derivative of the activation function:  \n",
    "  \n",
    "$$ f'(z_i^h) = \\sigma '(z_i^h) = \\sigma(z_i^h)(1 - \\sigma(z_i^h)) = a_i^h (1 - a_i^h) ,$$\n",
    "  \n",
    "since our activation function is the sigmoid/logistic function.  \n",
    "The gradient with respect to each hidden layer weight is:  \n",
    "  \n",
    "$$ \\frac{\\partial \\mathcal{C}}{\\partial w_{i,j}^h} = \\delta_i^h x_j ,$$  \n",
    "  \n",
    "and the gradient with respect to the hidden bias  \n",
    "  \n",
    "$$ \\frac{\\partial \\mathcal{C}}{\\partial b_i^h} = \\delta_i^h .$$  \n",
    "  \n",
    "The regularization terms using the L2-norm are just  \n",
    "  \n",
    "$$ \\frac{\\partial }{\\partial w_{ij}} (\\lambda \\sum_{ij} w_{ij}^2) = 2 \\lambda w_{ij} = \\hat{\\lambda} w_{ij} ,$$\n",
    "  \n",
    "for the weights in both the output and hidden layers.  \n",
    "  \n",
    "# Matrix  multiplication\n",
    "  \n",
    "Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old accuracy on training data: 0.1440501043841336\n",
      "New accuracy on training data: 0.10368823938761308\n"
     ]
    }
   ],
   "source": [
    "# to categorical turns our integer vector into a onehot representation\n",
    "from keras.utils import to_categorical\n",
    "# calculate the accuracy score of our model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Y_train_onehot, Y_test_onehot = to_categorical(Y_train), to_categorical(Y_test)\n",
    "\n",
    "# equivalently in numpy\n",
    "def to_categorical_numpy(integer_vector):\n",
    "    n_inputs = len(integer_vector)\n",
    "    n_categories = np.max(integer_vector) + 1\n",
    "    onehot_vector = np.zeros((n_inputs, n_categories))\n",
    "    onehot_vector[range(n_inputs), integer_vector] = 1\n",
    "    \n",
    "    return onehot_vector\n",
    "\n",
    "#Y_train_onehot, Y_test_onehot = to_categorical_numpy(Y_train), to_categorical_numpy(Y_test)\n",
    "\n",
    "def feed_forward_train(X):\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z1 = np.matmul(X, hidden_weights) + hidden_bias\n",
    "    # activation in the hidden layer\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    # weighted sum of inputs to the output layer\n",
    "    z2 = np.matmul(a1, output_weights) + output_bias\n",
    "    # softmax output\n",
    "    # axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    exp_term = np.exp(z2)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "    \n",
    "    return a1, probabilities\n",
    "\n",
    "def backpropagation(X, Y):\n",
    "    a1, probabilities = feed_forward_train(X)\n",
    "    \n",
    "    # error in the output layer\n",
    "    error_output = probabilities - Y\n",
    "    # error in the hidden layer\n",
    "    error_hidden = np.matmul(error_output, output_weights.T) * a1 * (1 - a1)\n",
    "    \n",
    "    # gradients for the output layer\n",
    "    output_weights_gradient = np.matmul(a1.T, error_output)\n",
    "    output_bias_gradient = np.sum(error_output, axis=0)\n",
    "    \n",
    "    # gradient for the hidden layer\n",
    "    hidden_weights_gradient = np.matmul(X.T, error_hidden)\n",
    "    hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient\n",
    "\n",
    "print(\"Old accuracy on training data: \" + str(accuracy_score(predict(X_train), Y_train)))\n",
    "\n",
    "eta = 0.01\n",
    "lmbd = 0.01\n",
    "for i in range(1000):\n",
    "    dWo, dBo, dWh, dBh = backpropagation(X_train, Y_train_onehot)\n",
    "    \n",
    "    dWo += lmbd * output_weights\n",
    "    dWh += lmbd * hidden_weights\n",
    "    \n",
    "    output_weights -= eta * dWo\n",
    "    output_bias -= eta * dBo\n",
    "    hidden_weights -= eta * dWh\n",
    "    hidden_bias -= eta * dBh\n",
    "\n",
    "print(\"New accuracy on training data: \" + str(accuracy_score(predict(X_train), Y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full object-oriented implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_data,\n",
    "        Y_data,\n",
    "        n_hidden_neurons=50,\n",
    "        n_categories=10,\n",
    "        epochs=10,\n",
    "        batch_size=100,\n",
    "        eta=0.1,\n",
    "        lmbd=0.0,\n",
    "\n",
    "    ):\n",
    "        self.X_data_full = X_data\n",
    "        self.Y_data_full = Y_data\n",
    "\n",
    "        self.n_inputs = X_data.shape[0]\n",
    "        self.n_features = X_data.shape[1]\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.n_categories = n_categories\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = self.n_inputs // self.batch_size\n",
    "        self.eta = eta\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "        self.create_biases_and_weights()\n",
    "\n",
    "    def create_biases_and_weights(self):\n",
    "        self.hidden_weights = np.random.randn(self.n_features, self.n_hidden_neurons)\n",
    "        self.hidden_bias = np.zeros(self.n_hidden_neurons) + 0.01\n",
    "\n",
    "        self.output_weights = np.random.randn(self.n_hidden_neurons, self.n_categories)\n",
    "        self.output_bias = np.zeros(self.n_categories) + 0.01\n",
    "\n",
    "    def feed_forward(self):\n",
    "        self.z1 = np.matmul(self.X_data, self.hidden_weights) + self.hidden_bias\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "\n",
    "        self.z2 = np.matmul(self.a1, self.output_weights) + self.output_bias\n",
    "\n",
    "        exp_term = np.exp(self.z2)\n",
    "        self.probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "\n",
    "    def feed_forward_out(self, X):\n",
    "        z1 = np.matmul(X, self.hidden_weights) + self.hidden_bias\n",
    "        a1 = sigmoid(z1)\n",
    "\n",
    "        z2 = np.matmul(a1, self.output_weights) + self.output_bias\n",
    "        \n",
    "        exp_term = np.exp(z2)\n",
    "        probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "\n",
    "    def backpropagation(self):\n",
    "        error_output = self.probabilities - self.Y_data\n",
    "        error_hidden = np.matmul(error_output, self.output_weights.T) * self.a1 * (1 - self.a1)\n",
    "\n",
    "        self.output_weights_gradient = np.matmul(self.a1.T, error_output)\n",
    "        self.output_bias_gradient = np.sum(error_output, axis=0)\n",
    "\n",
    "        self.hidden_weights_gradient = np.matmul(self.X_data.T, error_hidden)\n",
    "        self.hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "        if self.lmbd > 0.0:\n",
    "            self.output_weights_gradient += self.lmbd * self.output_weights\n",
    "            self.hidden_weights_gradient += self.lmbd * self.hidden_weights\n",
    "\n",
    "        self.output_weights -= self.eta * self.output_weights_gradient\n",
    "        self.output_bias -= self.eta * self.output_bias_gradient\n",
    "        self.hidden_weights -= self.eta * self.hidden_weights_gradient\n",
    "        self.hidden_bias -= self.eta * self.hidden_bias_gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def predict_probabilities(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return probabilities\n",
    "\n",
    "    def train(self):\n",
    "        data_indices = np.arange(self.n_inputs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(self.iterations):\n",
    "                chosen_datapoints = np.random.choice(\n",
    "                    data_indices, size=self.batch_size, replace=False\n",
    "                )\n",
    "\n",
    "                self.X_data = self.X_data_full[chosen_datapoints]\n",
    "                self.Y_data = self.Y_data_full[chosen_datapoints]\n",
    "\n",
    "                self.feed_forward()\n",
    "                self.backpropagation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Evaluate model performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on test set:  0.08888888888888889\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "lmbd = 0.1\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "dnn = NeuralNetwork(X_train, Y_train_onehot, eta=eta, lmbd=lmbd, epochs=epochs, batch_size=batch_size,\n",
    "                    n_hidden_neurons=n_hidden_neurons, n_categories=n_categories)\n",
    "dnn.train()\n",
    "test_predict = dnn.predict(X_test)\n",
    "print(\"Accuracy score on test set: \", accuracy_score(Y_test, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Adjust hyperparameters (if necessary, network architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_vals = np.logspace(-5, 0, 6)\n",
    "lmbd_vals = np.logspace(-5, 0, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate  =  1e-05\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.17777777777777778\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.08888888888888889\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.125\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.05277777777777778\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.10277777777777777\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.06388888888888888\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.06944444444444445\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.16111111111111112\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.16944444444444445\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.16666666666666666\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.14722222222222223\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.13333333333333333\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.6083333333333333\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.6027777777777777\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.6222222222222222\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.6361111111111111\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.575\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.5972222222222222\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.8861111111111111\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.8916666666666667\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.9138888888888889\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.8861111111111111\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.9\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.6944444444444444\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.11388888888888889\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.08888888888888889\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.07777777777777778\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.125\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.125\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.10555555555555556\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.07777777777777778\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.07777777777777778\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.07777777777777778\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.07777777777777778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate  =  1.0\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.07777777777777778\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.11388888888888889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DNN_numpy = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = NeuralNetwork(X_train, Y_train_onehot, eta=eta, lmbd=lmbd, epochs=epochs, batch_size=batch_size,\n",
    "                            n_hidden_neurons=n_hidden_neurons, n_categories=n_categories)\n",
    "        dnn.train()\n",
    "        \n",
    "        DNN_numpy[i][j] = dnn\n",
    "        \n",
    "        test_predict = dnn.predict(X_test)\n",
    "        \n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", accuracy_score(Y_test, test_predict))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate  =  1e-05\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.15833333333333333\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.15555555555555556\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.2638888888888889\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.24722222222222223\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.26666666666666666\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.225\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.9\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.8888888888888888\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.9055555555555556\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.8888888888888888\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.8972222222222223\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.8805555555555555\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.9861111111111112\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.9861111111111112\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.9888888888888889\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.9861111111111112\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.9861111111111112\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.9805555555555555\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.9861111111111112\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.975\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.9916666666666667\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.9861111111111112\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.9805555555555555\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.975\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.8722222222222222\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.9083333333333333\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.8583333333333333\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.8555555555555555\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.7944444444444444\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.9055555555555556\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.10555555555555556\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.08611111111111111\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.10555555555555556\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.45555555555555555\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.23055555555555557\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.07777777777777778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = MLPClassifier(hidden_layer_sizes=(n_hidden_neurons), activation='logistic',\n",
    "                            alpha=lmbd, learning_rate_init=eta, max_iter=100)\n",
    "        dnn.fit(X_train, Y_train)\n",
    "        \n",
    "        DNN_scikit[i][j] = dnn\n",
    "        \n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", dnn.score(X_test, Y_test))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving the backpropagation equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
