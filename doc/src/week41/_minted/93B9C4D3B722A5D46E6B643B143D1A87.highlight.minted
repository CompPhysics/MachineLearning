\begin{MintedVerbatim}[commandchars=\\\{\},codes={\catcode`\$=3\catcode`\^=7\catcode`\_=8\relax}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{numpy}\PYG{+w}{ }\PYG{k}{as}\PYG{+w}{ }\PYG{n+nn}{np}
\PYG{c+c1}{\PYGZsh{} We use the Sigmoid function as activation function}
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{sigmoid}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{l+m+mf}{1.0}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mf}{1.0}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{z}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{forwardpropagation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} weighted sum of inputs to the hidden layer}
    \PYG{n}{z\PYGZus{}1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{w\PYGZus{}1}\PYG{p}{)} \PYG{o}{+} \PYG{n}{b\PYGZus{}1}
    \PYG{c+c1}{\PYGZsh{} activation in the hidden layer}
    \PYG{n}{a\PYGZus{}1} \PYG{o}{=} \PYG{n}{sigmoid}\PYG{p}{(}\PYG{n}{z\PYGZus{}1}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} weighted sum of inputs to the output layer}
    \PYG{n}{z\PYGZus{}2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{a\PYGZus{}1}\PYG{p}{,} \PYG{n}{w\PYGZus{}2}\PYG{p}{)} \PYG{o}{+} \PYG{n}{b\PYGZus{}2}
    \PYG{n}{a\PYGZus{}2} \PYG{o}{=} \PYG{n}{z\PYGZus{}2}
    \PYG{k}{return} \PYG{n}{a\PYGZus{}1}\PYG{p}{,} \PYG{n}{a\PYGZus{}2}

\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{backpropagation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{a\PYGZus{}1}\PYG{p}{,} \PYG{n}{a\PYGZus{}2} \PYG{o}{=} \PYG{n}{forwardpropagation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} parameter delta for the output layer, note that a\PYGZus{}2=z\PYGZus{}2 and its derivative wrt z\PYGZus{}2 is just 1}
    \PYG{n}{delta\PYGZus{}2} \PYG{o}{=} \PYG{n}{a\PYGZus{}2} \PYG{o}{\PYGZhy{}} \PYG{n}{y}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{o}{*}\PYG{p}{(}\PYG{p}{(}\PYG{n}{a\PYGZus{}2}\PYG{o}{\PYGZhy{}}\PYG{n}{y}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} delta for  the hidden layer}
    \PYG{n}{delta\PYGZus{}1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{delta\PYGZus{}2}\PYG{p}{,} \PYG{n}{w\PYGZus{}2}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)} \PYG{o}{*} \PYG{n}{a\PYGZus{}1} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{a\PYGZus{}1}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} gradients for the output layer}
    \PYG{n}{output\PYGZus{}weights\PYGZus{}gradient} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{a\PYGZus{}1}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{delta\PYGZus{}2}\PYG{p}{)}
    \PYG{n}{output\PYGZus{}bias\PYGZus{}gradient} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{delta\PYGZus{}2}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} gradient for the hidden layer}
    \PYG{n}{hidden\PYGZus{}weights\PYGZus{}gradient} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{delta\PYGZus{}1}\PYG{p}{)}
    \PYG{n}{hidden\PYGZus{}bias\PYGZus{}gradient} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{delta\PYGZus{}1}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{output\PYGZus{}weights\PYGZus{}gradient}\PYG{p}{,} \PYG{n}{output\PYGZus{}bias\PYGZus{}gradient}\PYG{p}{,} \PYG{n}{hidden\PYGZus{}weights\PYGZus{}gradient}\PYG{p}{,} \PYG{n}{hidden\PYGZus{}bias\PYGZus{}gradient}


\PYG{c+c1}{\PYGZsh{} ensure the same random numbers appear every time}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Input variable}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{4.0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Target values}
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{x}\PYG{o}{+}\PYG{l+m+mf}{1.0}

\PYG{c+c1}{\PYGZsh{} Defining the neural network, only scalars here}
\PYG{n}{n\PYGZus{}inputs} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{n\PYGZus{}features} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{n\PYGZus{}hidden\PYGZus{}neurons} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{n\PYGZus{}outputs} \PYG{o}{=} \PYG{l+m+mi}{1}

\PYG{c+c1}{\PYGZsh{} Initialize the network}
\PYG{c+c1}{\PYGZsh{} weights and bias in the hidden layer}
\PYG{n}{w\PYGZus{}1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}features}\PYG{p}{,} \PYG{n}{n\PYGZus{}hidden\PYGZus{}neurons}\PYG{p}{)}
\PYG{n}{b\PYGZus{}1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}hidden\PYGZus{}neurons}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{0.01}

\PYG{c+c1}{\PYGZsh{} weights and bias in the output layer}
\PYG{n}{w\PYGZus{}2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}hidden\PYGZus{}neurons}\PYG{p}{,} \PYG{n}{n\PYGZus{}outputs}\PYG{p}{)}
\PYG{n}{b\PYGZus{}2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}outputs}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{0.01}

\PYG{n}{eta} \PYG{o}{=} \PYG{l+m+mf}{0.1}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} calculate gradients}
    \PYG{n}{derivW2}\PYG{p}{,} \PYG{n}{derivB2}\PYG{p}{,} \PYG{n}{derivW1}\PYG{p}{,} \PYG{n}{derivB1} \PYG{o}{=} \PYG{n}{backpropagation}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} update weights and biases}
    \PYG{n}{w\PYGZus{}2} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n}{eta} \PYG{o}{*} \PYG{n}{derivW2}
    \PYG{n}{b\PYGZus{}2} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n}{eta} \PYG{o}{*} \PYG{n}{derivB2}
    \PYG{n}{w\PYGZus{}1} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n}{eta} \PYG{o}{*} \PYG{n}{derivW1}
    \PYG{n}{b\PYGZus{}1} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n}{eta} \PYG{o}{*} \PYG{n}{derivB1}



\end{MintedVerbatim}
