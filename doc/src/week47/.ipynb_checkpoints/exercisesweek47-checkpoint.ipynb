{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c036faa3",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek47.do.txt  -->\n",
    "<!-- dom:TITLE: Exercise week 47-48 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc42328c",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercise week 47-48\n",
    "**November 17-28, 2025**\n",
    "\n",
    "Date: **Deadline is Friday November 28 at midnight**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4d441",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Overarching aims of the exercises this week\n",
    "\n",
    "The exercise set this week is meant as a summary of many of the\n",
    "central elements in various machine learning algorithms we have discussed throught the semester. You don't need to answer all questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b726d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Linear and logistic regression methods\n",
    "\n",
    "1. Which of the following is not an assumption of ordinary least squares linear regression?\n",
    "\n",
    "a. Linearity between predictors/features and target/outout\n",
    "\n",
    "1. Are the inputs/features distributed according\n",
    "\n",
    "2. The mean squared error cost function for linear regression is convex in the parameters, guaranteeing a unique global minimum. True or False? Motivate your answer.\n",
    "\n",
    "3. Which statement about logistic regression is false?\n",
    "\n",
    "a. Logistic regression is used for binary classification.\n",
    "\n",
    "1. It uses the sigmoid function to map linear scores to probabilities.\n",
    "\n",
    "2. It has an analytical closed-form solution.\n",
    "\n",
    "3. Its log-loss (cross-entropy) is convex.\n",
    "\n",
    "4. Logistic regression produces a linear decision boundary in the input space. True or False? Explain.\n",
    "\n",
    "5. Give two reasons why logistic regression is preferred over linear regression for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8933b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Neural networks\n",
    "\n",
    "1. Which statement is not true for fully-connected neural networks?\n",
    "\n",
    "a. Without nonlinear activation functions they reduce to a single linear model.\n",
    "\n",
    "1. Training relies on backpropagation using the chain rule.\n",
    "\n",
    "2. A single hidden layer can approximate any continuous function on a compact set.\n",
    "\n",
    "a. The loss surface of a deep neural network is convex.\n",
    "\n",
    "2. Using sigmoid activations in many layers of a deep neural network can cause vanishing gradients. True or False? Explain.\n",
    "\n",
    "3. Describe the vanishing gradient problem: Why does it occur? Mention one technique to mitigate it and explain briefly.\n",
    "\n",
    "4. Consider a fully-connected network with layer sizes $n_0$ (the input layer) ,$n_1$ (first hidden layer), $\\dots, n_L$, where $n_L$ is the outut layer. Derive a general formula for the total number of trainable parameters (weights + biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52f1a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 3: Convolutional Neural Networks\n",
    "\n",
    "1. Which of the following is not a typical property or advantage of CNNs?\n",
    "\n",
    "a. Local receptive fields\n",
    "\n",
    "1. Weight sharing\n",
    "\n",
    "2. More parameters than fully-connected layers\n",
    "\n",
    "3. Pooling layers offering some translation invariance\n",
    "\n",
    "4. Using zero-padding in convolutional layers can preserve the input spatial dimensions when using a $3 \\times 3$ kernel/filter, stride 1, and padding $P = 1$. True or False? Explain using the convolution formula.\n",
    "\n",
    "5. Given input width $W$, kernel size $K$, stride S, and padding P, derive the formula for the output width $W_{\\text{out}} = \\frac{W - K + 2P}{S} + 1$.\n",
    "\n",
    "6. A convolutional layer has: $C_{\\text{in}}$ input channels, $C_{\\text{out}}$ output channels (filters) and kernel size $K_h \\times K_w$. Compute the number of trainable parameters including biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9522cdf",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 4: Recurrent Neural Networks\n",
    "\n",
    "1. Which statement about simple  RNNs is false?\n",
    "\n",
    "a. They maintain a hidden state updated each time step.\n",
    "\n",
    "1. They use the same weight matrices at every time step.\n",
    "\n",
    "2. They handle sequences of arbitrary length.\n",
    "\n",
    "3. They eliminate the vanishing gradient problem.\n",
    "\n",
    "4. LSTMs mitigate the vanishing gradient problem by using gating mechanisms (input, forget, output gates). True or False? Explain.\n",
    "\n",
    "5. What is Backpropagation Through Time (BPTT) and why is it required for training RNNs?\n",
    "\n",
    "6. What does a sliding window do? And why would we use it?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
